what three summari statist do all four of these scatterplot have in common mean, standard deviation, and pearson correl (http://en.wikipedia.org/wiki/anscombe's_quartet) the moral is that these summari statist can be mislead and it good to look at the actual distribut
one round of [...] involv partit a sampl of data into complementari subsets, per ing the analysi on one subset (call the train set), and valid the analysi on the other subset (call the test set). cross-valid (http://en.wikipedia.org/wiki/cross-validation_(statistics))
the purpos of cross-valid is to test the degre of model [...]. overfit (http://en.wikipedia.org/wiki/cross-validation_(statistics))
data [...] refer to the applic of a determinist mathemat function to each point in a data set -- that is, each data point zi is replac with the valu yi = f(zi), where f is a function. tran ation (http://en.wikipedia.org/wiki/data_tran ation_(statistics))
data tran ation refer to the applic of a [...] to each point in a data set -- that is, each data point zi is replac with the valu yi = f(zi), where f is a function. determinist mathemat function (http://en.wikipedia.org/wiki/data_tran ation_(statistics)) as oppos to a probabilist math function
the [...] of an estim of a paramet is equal to the number of independ data point that go into the estim minus the number of paramet use as intermedi step in the estimation. degre of freedom (http://en.wikipedia.org/wiki/degrees_of_freedom_(statistics))
what is lambda 
regularization: set a lambda valu too high can produc [...] regularization: set a lambda valu too high can produc underfit
regular linear regress - cost function ... note: you can also use lambda/2m * sum(theta.^2)
2 type of linear regress - gradient descent - normal equat
2 type of logist regress - gradient descent - advanc optim method
overfit can happen with not onli higher polynomi featur but with just [...] overfit can happen with not onli higher polynomi featur but with just a lot of featur
overfit can happen with not onli [...] but with just a lot of featur overfit can happen with not onli higher polynomi featur but with just a lot of featur
when use regular logist regression, give a good equat to plot and monitor whether the gradient descent is work correct 
regular logist regress - gradient descent ... 
regular logist regress - cost function ... note: theta0 should not be regular
ad a new featur to the model alway s in equal or better per anc [...] . ad a new featur to the model alway s in equal or better per anc on the train set (overfitting).
regular penal [...] (with larg valu of θ). regular penal complex model (with larg valu of θ).
regular use a veri larg valu of λ can lead to the [...] of the train set. regular use a veri larg valu of λ can lead to the underfit of the train set.
normal help to [...] . normal help to converg faster .
whi do you have to do theta*x instead of theta * x dim(x) = m*n and dim(theta) = n*1: x: m exampl * n featur ----------------------- n x0 x1 x2 ... xn ----------------------- 1 0.57 0.2 0.1 1 ... v m theta: n theta (n features) * 1 --- 1 t0 t1 t2 ... tn v n
matric multiplication: mxn * nxo = [...] matric multiplication: mxn * nxo = mxo
in mathemat as in octave, the first dimens of a matrix is alway the [...] and then the [...] . in mathemat as in octave, the first dimens of a matrix is alway the line and then the column . --------------- column line x column v line
simpli explained, regular is [... what to add in the equat ] simpli explained, regular is the addit of the cost of the paramet theta to penal the parameters: roughly: costfunction=cost(data)+cost(theta)
regular in the regular cost equation, what l1 and l2 correspond in the follow image: l1 = power 1 : cost(parameters)^1 l2 = power 2 : cost(parameters)^2
kernel trick = [... definit ...] kernel trick = ad artifici new featur from combin and higher polynom degrees, with the goal to complexifi the model.
use kernel trick (ad artifici quadratic/cubic/etc. features) can be too much [...] for comput time use kernel trick (ad artifici quadratic/cubic/etc. features) can be too much expens for comput time eg: o(n) = (n^2)/2 for quadrat features, o(n) = (n^3) for cubic features, etc...
in a neural network, there is one input layer, one output layer, and [...] in a neural network, there is one input layer, one output layer, and one or sever hidden layer (what is not an inpput layer or output layer is a hidden layer)
in neural networks, comput from the input to hidden layer to output is call [...] in neural networks, comput from the input to hidden layer to output is call forward propag
neural network forward propag vector equation: [...] neural network forward propag vector equation: a(j)0 = 1 z(j+1) = theta(j) * a(j) a(j+1) = g(z(j+1)) with g() be the sigmoid (logistic) function and with a(1) = x
if network has s(j) unit in layer j, s(j+1) unit in layer j+1, then theta(j) will be of dimens [...] if network has s(j) unit in layer j, s(j+1) unit in layer j+1, then theta(j) will be of dimens s(j+1) * s(j) + 1 note: after ad a(j)0 = 1 (the bias unit), dimens will be s(j+1)+1 * s(j) + 1
a neural network [...] defin how the differ neuron are connect to each other (how mani hidden layer and how mani hidden units). a neural network architectur defin how the differ neuron are connect to each other (and how mani hidden layers). altern answer: the connect pattern between neuron eg:
if z alia theta*x = 0, then [h(x)...y...] if z alia theta*x = 0, then g(z) alia h(x) = 0.5 and y = 1
if z (theta*x) 0, then [h(x)...y...] if z (theta*x) 0, then g(z) alia h(x) 0.5 and y = 0
g(-10) = [approximately...] g(-10) = 0
g(20) = [approximately...] g(20) = 1
g(4.6) = [approximately...] g(4.6) = 0.99 = 1
g(-4.6) = [approximately...] g(-4.6) = 0.01 = 0
in a neural network, the input and first hidden layer comput simpl functions, then subsequ layer comput more and more [...] function in a neural network, the input and first hidden layer comput simpl functions, then subsequ layer comput more and more complex function
if a neural network is overfit the data, we can [...] the regular paramet lambda to fix that if a neural network is overfit the data, we can increas the regular paramet lambda to fix that
if a and b are vector and a' is a transposed, then a' * b = [...] if a and b are vector and a' is a transposed, then a' * b = b' * a
when vector , one common strategi for debug is to [...] when vector , one common strategi for debug is to print out the size of the matric you are work with
[...] is the phenomenon that if a variabl is extrem on it first measurement, it will tend to be closer to the averag on a second measurement, and vice versa. regress toward the mean
if the probabl p is unknown, how do previous (independent) outcom in a bernoulli process provid ani in ation about futur outcom through infer about the paramet p (http://en.wikipedia.org/wiki/bernoulli_distribution)
what do you call a factor in an anova that is deliber arrang by the experiment a fix effect (http://www.statsoft.com/textbook/statistics-glossary/f/button/f/)
a one-way anova test the null hypothesi that there are [...] of three or more independ groups. no differ in the mean (note that it can also be use for two groups, but in that case a t-test is typic employ instead)
if you assum that your two group have equal varianc when calcul a t statistic, how will this chang the valu of the t statist it will increas it (bi decreas the denominator)
in a t-test, are the two compar popul assum to follow a normal distribut yes
in a t-test, are the two popul be compar assum to be sampl independ yes
in a t-test, are the two popul mean be compar assum to be equal no (that typic the point of the test)
what is the main differ between the shape of the student t-distribut and the normal distribut the t-distribut has heavier tail (i.e., it is more prone to produc valu far from it mean) (http://en.wikipedia.org/wiki/student's_t-distribution)
in the below chart of student t distributions, what is the order of the color in decreas degre of freedom (df) (yellow = 1 magenta = 2 blue = 5 black = ∞) (http://en.wikipedia.org/wiki/student's_t-distribution) higher df = more data point = more confid = narrow bound
the [...] is the cumul distribut function associ with the actual data, and is thus a step function that jump 1/n at each of the n data points. empir distribut function
the time complex of an algorithm quantifi [...] an algorithm to run as a function of the size of the input to the problem. the amount of time taken by (average, worst-case, etc, can be specifi a bit later)
two type of classification: [...] two type of classification: - binari classif - multiclass classif (k classes)
in a multi-class classif problem, we usual have (number of classes) k = [...] in a multi-class classif problem, we usual have (number of classes) k = 3 (becaus for 2 or 1 class we use the binari classification)
the cost function use for neural network is a general of [...] the cost function use for neural network is a general of the logist regress cost function (basically: ad multi-class y and multi layer of comput instead of just one)
with logist regress and neural network cost functions, we [...need or don't need...] to avoid regular theta0 with logist regress and neural network cost functions, we don't need to avoid regular theta0 as this is onli a common convent
neural network cost function: + [...regular term...] neural network cost function:
to use an advanc optim method (fminunc, conjug gradient, bfgs, l-bfgs, etc.), we need to suppli [...argument of the function...] to use an advanc optim method (fminunc, conjug gradient, bfgs, l-bfgs, etc.), we need to suppli cost function j(theta) and the (partial) deriv term delta/(delta*theta(l)ij) for everi i, j, l
for neural-network, the learn algorithm is basic split into these 4 steps: [...] for neural-network, the learn algorithm is basic split into these 4 steps: - set a(1) = x(i) - forward propag - backward propag - comput the error function (delta), which is the partial deriv (or gradient)
in neural networks, the back propag algorithm is, simpli put, the forward propag in reversal: we comput [...] in neural networks, the back propag algorithm is, simpli put, the forward propag in reversal: we comput the sum of the product of the theta (edges) and error (delta) eg:
formally, delta(l)j = partial deriv of [...] formally, delta(l)j = partial deriv of cost(i) (for j =0)
consid the follow neural network: what is associ to the red edg [...theta...] 
partial deriv of the cost function = [...] partial deriv of the cost function = gradient
the reshap command enabl one to [...] the reshap command enabl one to back a matrix from a vector
when you implement a back propag algorithm, you should alway implement a [...] to verifi that there no bug that deviat the when you implement a back propag algorithm, you should alway implement a numer gradient check to verifi that there no bug that deviat the
numer gradient check is simpli put [...] numer gradient check is simpli put an approxim of the gradient by take 2 point and calcul the line coefficient, and then compar to the real deriv found by back propagation. altern answer: a verif if one implement of back propag is bug-fre
@neural-network: if you set initial_theta to a 0 fill vector, after each update, the produc paramet for everi unit will be [...] @neural-network: if you set initial_theta to a 0 fill vector, after each update, the produc paramet for everi unit will be ident (henc whi one must random the initial_theta vector)
the two most common fix to get a good neural network back propag implement are: [...] the two most common fix to get a good neural network back propag implement are: - random initial_theta (so to avoid the symmetr weight problem, where each hidden unit get the exact same function) - numer gradient check (so that we can check that the gradient/parti deriv is ok and implement of backprop is bug-free)
in neural network back propagation, the symmetr weight problem is when [...] in neural network back propagation, the symmetr weight problem is when you get the exact same function for each hidden unit becaus of a non random initial_theta
neural network train algorithm [...7 steps...] neural network back propag algorithm - pick a network architectur (connect pattern between neurons: how mani hidden unit in how mani layers) - random initi initial_theta - implement forward propag to get h(x(i)) for ani x(i) to compar with y(i) - implement cost function j(theta) - comput back propag (error function) to comput partial deriv - gradient check then disabl gradient check - minim cost function j(theta) by use gradient descent or advanc optim function (use the partial derivative/gradi found by backprop)
gradient descent in neural network is non-convex, which mean that there are [...] gradient descent in neural network is non-convex, which mean that there are local optimum
the intuit behind the neural network back propag algorithm is as follows: given a train exampl (x(t);y(t)), we first run a [...] to comput all the ation throughout the network, includ the output valu of the hypothesi h(x). then, for each node j in layer l, we back propag the error by comput the &quot;error term&quot; delta(l)j that measur how much that node was &quot;responsible&quot; for the error in our output. the intuit behind the neural network back propag algorithm is as follows: given a train exampl (x(t);y(t)), we first run a forward propag to comput all the ation throughout the network, includ the output valu of the hypothesi h(x). then, for each node j in layer l, we back propag the error by comput the &quot;error term&quot; delta(l)j that measur how much that node was &quot;responsible&quot; for the error in our output.
gradient check work for ani function where you are comput the cost and the gradient, so it equal work for neural network, [...] cost functions. gradient check work for ani function where you are comput the cost and the gradient, so it equal work for neural network, logist regress and linear regress cost functions.
in neural network backprop, big delta is simpli put [...] in neural network backprop, big delta is simpli put the error cost of the theta (edges), alia theta_gradi (todo: add image)
small delta are, simpli put, [...] small delta are, simpli put, the error cost for the unit (neurons) (todo: add image)
what are the most common avenu to explor to fix big error in a learn algorithm prediction: [...4 common fix...] what are the most common avenu to explor to fix big error in a learn algorithm prediction: 1. add or reduc number of featur 2. add artifici featur (polynomi features, or kernel trick) 3. get more train exampl 4. increas or decreas the regular paramet lambda
a machin learn diagnost is a [...definition...] a machin learn diagnost is a test check what is/isn't work with a learn algorithm (which will indic what to do next to best improv per ances) note: this can be time-consum but is a veri import practice, as it will show what is more likely, or unlikely, to improv per anc significantly.
recommend approach when design a machin learn system [no answer]: - start with a simpl algorithm that you can implement as quick as possibl (in one or a few day at most ). implement it and test it on your cross-valid data. this initi implement of your algorithm is critic and a veri power tool to know where to go further. - plot learn curv to decid if more data, more features, etc. are like to help. - error analysis: manual examin the exampl (in cross-valid set) that your algorithm made error on. see if you spot ani systemat trend in what type of exampl it is make error on. (in summary: avoid prematur optim and prefer profil on a quick and dirti implement and then tri and test quick if a solut works)  
when design the enhanc to implement in a machin learn system, it is especi import to have a [...] of the success of the solut when design the enhanc to implement in a machin learn system, it is especi import to have a singl real number evalu of the success of the new solut (eg: cross-valid set error rate, error metric like accuracy, etc...)
it is alway recommend to do evalu on the [...] set , as this is more mathemat correct for error analysis. it is alway recommend to do evalu on the cross-valid set rather than on the test set , as this is more mathemat correct for error analysis. note: becaus the system will be over optimist on the cross-valid set sinc we'v done some tune of some paramet over this set, so it not object anymore.
the learn error rate is the opposit of the [...] the learn error rate is the opposit of the accuraci
accuraci or learn error rate is not alway a good error metric, particular on [...] accuraci or learn error rate is not alway a good error metric, particular on skew class
a skew class is when [...definition...] a skew class is when there are onli a few rare posit compar to the negat class (or the opposite) ie: alway predict y = 0 give a veri good accuraci (like over 90%) eg: anomali detect system alway have veri skew class so we can't use accuracy.
there are altern error metric for skew class like [...3 elements...] there are altern error metric for skew class like precis or recal or f1 score. eg: anomali detect system alway have veri skew class so we can't use accuracy. for ads, use these error metric make much more sense.
precis is [...definition...] precis is the number of true posit over the total number of predict posit (true posit + fals positives): precis = true posit / #predict posit precis = (true positives) / (true posit + fals positives)
accuraci is [...definition...] accuraci is the opposit of the error rate: accuraci = (true posit + true negatives) / (total examples) eg: if error rate = 0.5% then accuraci = 99.5%
recal is [...definition...] recal is of the actual posit (true posit + fals negatives), how mani did we detect recal = true posit / #actual posit recal = (true positives) / (true posit + fals negatives)
error metric are use to [...definition...] error metric are use to evalu the per anc of the system use a singl real number represent of the per anc (eg: accuraci or error rate, precision, etc...). simpli put, it tell you how well your classifi is doing. this is especi import to make decis (and the right ones).
generally, it is prefer for error metric to defin y = [...] for the rare class generally, it is prefer for error metric to defin y = 1 for the rare class
generally, there is a trade-off between precis and recall, so that if we set a high threshold for h_theta(x) we get [...] generally, there is a trade-off between precis and recall, so that if we set a high threshold for h_theta(x) we get a higher precis and a lower recal note: concret we have to retrain the system on the train set with the new threshold for h_theta(x) and then test the score (precision,recall,f1-score) on the cross-valid set.
generally, there is a trade-off between precis and recall, so that if we set a lower threshold for h_theta(x) we get [...] generally, there is a trade-off between precis and recall, so that if we set a lower threshold for h_theta(x) we get a higher recal and a lower precis note: concret we have to retrain the system on the train set with the new threshold for h_theta(x) and then test the score (precision,recall,f1-score) on the cross-valid set.
the extrem in error metric are veri bad, and we can spot them easili by [...] the extrem in error metric are veri bad, and we can spot them easili by see if one of the error metric is particular veri low (near 0) or by comput the f1 score
f1 score is just one of [...] to combin precis and recal and is histor the most use in machin learn f1 score is just one of mani possibl way to combin precis and recal and is histor the most use in machin learn
the f1 score is a way to [...] the f1 score is a way to automat choos the best threshold for h_theta(x) to get the best trade-off between precis and recal becaus it give a singl real number out of the combin of precis and recall: f1 score = (2 * precis * recall) / (precis + recall) note: concret we have to retrain the system on the train set with the new threshold for h_theta(x) and then test the score (precision,recall,f1-score) on the cross-valid set.
a key test to know if the featur x contain enough in ation to predict y is to ask: [...] a key test to know if the featur x contain enough in ation to predict y is to ask: &quot;given the input x, can a human expert confid predict y &quot;
larg data rational assum that the featur x contain enough in ation to predict y and we have a larg train set - jtrain(theta) will be small (with a low bias algorithm) - jtrain(theta) =~ jtest(theta) (=~ mean about equal) (hopefully) impli that: [...] larg data rational assum that the featur x contain enough in ation to predict y and we have a larg train set - jtrain(theta) will be small (with a low bias algorithm) - jtrain(theta) =~ jtest(theta) (=~ mean about equal) (hopefully) impli that: - jtest (theta) will be small
support vector machin (svm) is also call [...] support vector machin (svm) is also call larg margin classifi (becaus it will tri to maxim the project length p in order to suffic to the constraint theta * x(i) = 1 or = 1 while minim the cost of the paramet theta): (notic the length of p in the left example, and it length in the right example)
polynomi kernel general ulation: [...] polynomi kernel general ulation:
have two vector u and v, u'*v (matrix product of u transpos and v) is also call the [...] have two vector u and v, u'*v (matrix product of u transpos and v) is also call the vector inner product
u is call the norm or length of the vector u, which is the [...] u is call the norm or length of the vector u, which is the euclidian length of the vector u
in a graphic view, the vector inner product of two vector u and v (u * v) is [...mathemat definition...] in a graphic view, the vector inner product of two vector u and v (u * v) is the orthogon project p of the vector v onto the vector u multipli by the norm (or length) of the vector u: u' * v = p * u = orthogonal_projection(v,u) * sqrt(u(1)² + u(2)²) = v' * u
if the angl between the two vector v and u is greater than 90°, what doe p (the project of u onto v or v onto u) look like p will be negative:
svm is also call larg margin classifi becaus it will alway tri to [...graphic explan use orthogon projection...] svm is also call larg margin classifi becaus it will alway tri to maxim the project length p in order to suffic to the constraint theta * x(i) = 1 or = 1 while minim the cost of the paramet theta: (notic the length of p in the left example, and it length in the right example)
the project p of a vector u is a [...signed,unsign ...] integ the project p of a vector v onto a vector u is a sign integ (negat or posit depend if the angl between u and v 90)
k-mean is an iter algorithm that doe two things: [...] k-mean is an iter algorithm that doe two things: - cluster assign step - move centroid step
give the k-mean algorithm k-mean algorithm: (altern ulat from ai-class:) initially: select random k cluster center repeat until no chang --- correspond data point to their nearest cluster --- move each cluster center by mean of the correspond data point --- empti cluster center : move random or elimin
can k-mean be use to segment this popul into 3 groups: small, medium and larg (size of t-shirt) yes, k-mean with a fix number of cluster (here 3) to forc find a segment for the population.
everi machin learn algorithm has an [...object ...] everi machin learn algorithm has an optim objective, which is a minim of the cost function
know the optim object of a machin learn algorithm help to [...2 items...] know the optim object of a machin learn algorithm help to debug the algorithm and make it effici to avoid local optima
the k-mean cost function is also call [...] the k-mean cost function is also call the distort cost function or distort of the k-mean algorithm
in a graphic representation, what doe this repres this is the squar distanc between x(i) and mu(c(i)): note: the squar of the distanc is just a convention, we could use just the distanc instead.
k-mean cost function k-mean cost function (or optim objective): note: the squar distanc is a convention, one can just use the distanc instead. detail k-mean function with respect to the cost function:
suppos you have implement k-mean and you plot the cost function j(c(1),...,c(m),mu(1),...,mu(k)) as a function of the number of iter and your plot look like this: what do you conclud it is not possibl for the cost function to sometim increase. there must be a bug in the code. note: there no learn rate to tweak.
give the recommend method to initi k-means. random initi k cluster onto random k exampl from the train set: eventually, if k is small (2 = k = 10) then we can run multipl random initialization: becaus a lower valu for the distortion/cost function impli a better clustering, so you should choos the cluster with the smallest valu for the distort function.
use a good initi for k-mean (random initi on k train examples) help a lot to [...2 items...] use a good initi for k-mean (random initi on k train examples) help a lot to converg faster and to avoid local optimum
the number of cluster k for k-mean is most often chosen manual by human, but it can be semi-automat found with two methods: [...2 items...] the number of cluster k for k-mean is most often chosen manual by human, but it can be semi-automat found with two methods: - elbow method (give it a shot but most of the time it won't be useful): - evalu k-mean base on a metric for how well it per s in respect to the later purpos (eg: t-shirt business)
dimension reduct purpos is to [...] dimension reduct purpos is to project the dataset {x(1), ..., x(m)} with x(i) in r^n onto a lesser dimension dataset of {z(1), ..., z(m)} with z(i) is in r^k where k = n. altern answer: to compress data by merg redund or similar/ correl featur in order to save memori space and comput time (so that the machin learn algorithm is much more quick and converg faster). eg:
data compress by dimension reduct is onli an [...] of the data data compress by dimension reduct is onli an approxim (but a close one) of the data
dimension reduct can be use for two purposes: [...] dimension reduct can be use for two purposes: - data compress (merg similar features) * speedup machin learn algorithm (converg and predict faster) * reduc memory/disk storag space need - data visual of high-dimension data (to be abl to plot a high dimension chart into a lesser dimension graphic represent 2d or 3d) eg:
pca is not linear regression, becaus [...] pca is not linear regression, becaus there is no predict of a special variabl y, all variabl x are treat equally, and we comput orthogon project rather than the predict error: (linear regress on the left, pca on the right)
princip compon analysi (pca) goal is to [...] princip compon analysi (pca) goal is to find a lower dimension surfac (k vectors/directions) onto which to project the data while minim the squar project error. with xapprox = z altern answer: to find the eigenvector (sub dimens subspaces) which keep the maxim varianc of the origin dataset. eg: vector with maxim varianc bad vector: minim varianc
the most common algorithm for automat dimension reduct is call [...] the most common algorithm for automat dimension reduct is call princip compon analysi (pca)
befor use pca, it is veri import to proced to a data preprocess by do [...] . befor use pca, it is veri import to proced to a data preprocess by do mean normal (ensur everi featur has zero mean) and featur scale (ensur that everi featur is in the same rang of values) .
use pca, after calcul the svd of the covari matrix, to reduc the data from n dimens to k dimensions, we onli need to [...] use pca, after calcul the svd of the covari matrix, to reduc the data from n dimens to k dimensions, we onli need to take the k first column (or vectors) of the matrix u
princip compon analysi (pca) (dimension reduction) algorithm: [...] princip compon analysi (dimension reduction) algorithm: - data preprocess (mean normal + featur scaling) - comput the covari matrix sigma - comput the eigenvector matrix u - ureduc = extract k vector (columns) from u - z = project x exampl with ureduc note: rememb that there no x0 = 1 note2: in a vector implementation, z = x * ureduc
in princip compon analysi (pca), k is call the [...] in princip compon analysi (pca), k is call the number of princip compon (we have retained)
suppos you run k-mean use k=3 and k=5 and you find that the cost function j is much higher for k=5 than for k=3. what can you conclud for higher valu of k, k-mean should alway get a lower optimum valu (more cluster = smaller cost/averag distanc between cluster and examples). this mean that for k=5, k-mean got stuck in a bad local optimum, and you should tri re-run k-mean with multipl random initializations.
@pca give the 2 method to automat choos the valu for k (number of princip components) automat choos the number of princip compon k for pca - run pca, get uk vector and comput z^k, then comput the averag variat on the project over the averag variat of the origin dataset, and redo that for k = 1 to n until we have an accept varianc (eg: = 99%): note: we don't have to recomput the svd onli once, then we can comput sever time uk'*x - use the diagon valu of the s matrix given by svd function, which repres the cumul variat of the z^k dimension reduc dataset: note: this method is more effic than the other one. note: these method can also be use for plot to manual decid the best tradeoff
pca data compression: we can reconstruct a close approxim of the origin dataset in the origin dimens simpli by [...] pca data compression: we can reconstruct a close approxim of the origin dataset in the origin dimens simpli by project z onto the origin dimension: xapprox(i) = ureduc * z(i) = z * ureduc
we can use dimension reduct to speedup supervis learn by simpli [...] we can use dimension reduct to speedup supervis learn by simpli appli pca on the unlabel dataset (x without y) which give z and recoupl z with y. note: when we get a new exampl x(i) to add in the dataset, we can project it use our alreadi comput vector ureduce. note2: ureduc should onli be comput on the train set (just like paramet theta), not the cv or test set (but we can then comput zcv and ztest use ureduc and mean normalization+featur scale found before, see below) note: map (comput ureduc and mean normalization+featur scaling) x(i) - z(i) should be comput by run pca onli on the train set . this map can later be appli as well to the exampl xcv(i) and xtest(i) in the cross valid and test set or to new exampl in the train set.
3 bad use of pca are: [...] 3 bad use of pca are: - to prevent overfit by have fewer featur (use regular paramet lambda instead becaus pca doesn't use label y so it may throw away valuabl in ation while cost function with regular doesn't) - plan to use pca from the start (first tri to process the raw data and then onli if it doesn't work the intend way, tri with pca) - cluster (pca doe no cluster into coher groups, but it can help later to find cluster with anoth cluster algorithm like k-means)
svm (support vector machine) cost function: [...] svm (support vector machine) cost function: where the function cost0(z) and cost1(z) look like this:
in an svm, we have: - for everi exampl with y(i) = 0: theta * x(i) [...] - for everi exampl with y(i) = 1: theta * x(i) [...] in an svm, for everi exampl with y(i) = 1, we have: - for everi exampl with y(i) = 0: theta * x(i) = -1 - for everi exampl with y(i) = 1: theta * x(i) = 1
in anomali detection, we'r given a set of unlabel data, and the goal is to find [...] in anomali detection, we'r given a set of unlabel data, and the goal is to find a gaussian model p(x) for this data so that for a given exampl xtest: - if p(xtest) e - flag anomali - if p(xtest) = e - flag ok note: p(xtest) is the probabl for the exampl xtest of be in the model (be &quot;normal&quot;). note2: e is a constant threshold. eg:
the best approach in anomali detect is to model [...] . the best approach in anomali detect is to model the &quot;normality&quot;, so that if an exampl is outsid of the normal behavior we flag it as an anomali . eg:
cite some applic of anomali detection: [...3 items...] cite some applic of anomali detection: - fraud detect * x(i) = featur of user i iti * model p(x) from data. * identifi unusu user by check which have p(x) e * then report to admin to ask for review and more detail eg: x1 = how often user login, x2 = number of transact or webpag visited, x3 = number of post in the forum, x4 = type speed of the user in character/second, etc... - manufactur eg: aircraft engin qualiti with x1 = heat generated, x2 = vibrat intensity, etc... - monitor comput in a data center * x(i) = featur of machin i * if p(xtest) e then the machin is anomal and it mean it may soon shutdown eg: x1 = memori use, x2 = number of disk accesses/sec, x3 = cpu load, x4 = cpu load / network traffic, etc...
a gaussian distribut is also call a [...] . a gaussian distribut is also call a normal distribut .
a normal distribut is a [...paramet ...] a normal distribut is a distribut gaussian with mean mu, varianc sigma^2, and is written: x ~ n(mu, sigma^2) note: graphically, the mean mu repres the center of the gaussian, and the varianc sigma^2 repres the width of the gaussian: eg: note: the (integr of the) total densiti distribut must alway be = 1 (becaus it a sum of probabilities), that whi by chang sigma, the width chang but the height too.
one interpret of the varianc is that it is [...] one interpret of the varianc is that it is the averag of the squar differ (valu in the exampl minus the mean): note: these calcul are actual the maximum likelihood estim of the paramet mu and sigma.
the paramet estim problem is the problem of [...] the paramet estim problem is the problem of find the right paramet mu (mean) and sigma^2 (variance) for the gaussian distribution. altern answer: find the right paramet to model a normal distribut over the exampl data {x(1),x(2),...,x(m)} note: in some notat we can find 1/(m-1) instead of 1/m, but both work pretti much the same for machin learning.
to estim the paramet of a normal distribution, we can simpli [...] to estim the paramet of a normal distribution, we can simpli comput an estim of the mean mu and of the varianc sigma^2: mu = 1/m sum(x) sigma^2 = 1/m sum(x - mu)^2 note: x is a vector note: in some notat we can find 1/(m-1) instead of 1/m, but both work pretti much the same for machin learning. for each featur x^j, we should comput mu^j and sigma^j: vector implementation: same for sigma^2.
give the ula for the (univariate) gaussian density: [...] (univariate) gaussian densiti ula: note: if x has n features, then we do the product of each p(xj):
describ the problem of densiti estim densiti estimation: given a set of exampl (with each one contain a vector of n features), we must find the paramet mu^j and sigma^j of each one of these features: eg: note: this exampl here doe an independ assumpt (the probabl of each featur happen is independent, non correl to ani other in the set), but the algorithm we will use work just fine whether or not these are independ or not. we would then estim the paramet with these equations: vector implementation: same for sigma^2.
give the anomali detect algorithm: [...] anomali detect algorithm 1- choos featur x^i that you think might be indic of anomal examples. 2- fit paramet mu1, ..., mun, sigma1^2, ..., sigman^2 3- given new exampl x, comput p(x) (probabl that x is normal). eg: note: the purpl region contain onli anomalies, which is defin by the constant e (visual this defin a slice over the graph under which everyth is consid anomalous).
an anomali detect system should be train onli on [...] an anomali detect system should be train onli on normal exampl (a few anomal exampl can slip in, but not too much) . then we can use a few anomal exampl in the cross-valid and test sets. eg:
the test set exampl and the cross-valid set should [...] the test set exampl and the cross-valid set should never contain the same exampl (even randomized). the set should be complet different. altern answer: we should never reus the same exampl of the cross-valid set for the test set, as the system will be over optimist over the exampl use in the cv set (the cv set is not object anymor sinc we'v tune some paramet on it) . note: this is veri important!
we can evalu an anomali detect system by use the [...] but tune the constant e instead of h_theta(x). we can evalu an anomali detect system by use the same error metric as for supervis learn (precision, recall, f1 score) but tune the constant e instead of h_theta(x).
we can evalu an anomali detect system by use the same error metric as for supervis learn (precision, recall, f1 score) but [...] we can evalu an anomali detect system by use the same error metric as for supervis learn (precision, recall, f1 score) but tune the constant e instead of h_theta(x).
anomali detect is prefer than use supervis learn when [...] anomali detect is prefer than use supervis learn when we onli have negat exampl (or a veri few posit examples). eg:
supervis learn is prefer than use anomali detect when [...] supervis learn is prefer than use anomali detect when we have enough posit exampl (as mani as negat examples). eg:
concretely, anomali detect and supervis learn can be both use for the same problems, the onli differ are: - the constraint: * veri few posit exampl (veri skew classes) use anomali detection. * as mani posit as negat exampl (normal classes) use supervis learning. - the intent: * detect a specif posit class use supervis learning. * detect all anomali (even futur anomali not expect at the time of training): use anomali detect [...no answer...]  
the opposit of anomali detect (huge negat examples, veri few positive) is call misus detect (huge posit examples, veri few negative) . altern answer: anomali detect detect the normality, while misus detect detect the anomalies.  
choos the right featur to use is not onli import for ani machin learn algorithm, but especi for [...] choos the right featur to use is not onli import for ani machin learn algorithm, but especi for anomali detect systems.
to check that a featur is good to use for an anomali detect system, we can [...] . to check that a featur is good to use for an anomali detect system, we can plot the histogram and check that it gaussian-lik (bell-shap curve) . note: if it not a gaussian, the algorithm should still work well, but we can also tran the x data so that it get more gaussian-lik (eg: log(x + c), x.^1/c, etc...).
if after check the histogram, an x featur is not gaussian-like, we can [...] if after check the histogram, an x featur is not gaussian-like, we can tran it (eg: log(x + c), x.^1/c, etc...).
the most common problem in choos the featur for anomali detect is when [...] . the most common problem in choos the featur for anomali detect is when the featur aren't enough to distinguish between normal exampl and anomalies. we can then add a new feature. altern answer: p(x) is compar (say, both large) for normal and anomal examples.
error analysi for anomali detect help to [...] error analysi for anomali detect help to creat new featur base on check how anomali are distinguish with current features, use a plot. altern answer: error analysi for ad consist in check on the cross-valid set, and plot the exampl in function of the features, then highlight the anomal exampl to see if they are well separ from the normal examples. if not, we should add more features. note: this is pretti similar to error analysi for supervis learn where the algorithm is check on the cross-valid set to see if we should add more features. note: alternatively, we can also use multi-vari gaussian distribut instead for correl features:
for anomali detection, we can creat new featur base on [...] . for anomali detection, we can creat new featur base on ratio of current featur . note: this is particular appropri when there are anomali that correspond to an unusu combin of valu of multipl features. eg: detect when a webserv get in an infinit job loop (which produc high cpu load but veri low network traffic) note: alternatively, we can also use multi-vari gaussian distribut instead for correl features:
suppos your anomali detect algorithm is per ing poor and output a larg valu of p(x) for mani normal exampl and for mani anomal exampl in your cross valid dataset. which of the follow chang to your algorithm is most like to help - tri use fewer featur - tri come up with more featur to distinguish between the normal and the anomal exampl - get a larger train set (of normal examples) with which to fit p(x) - tri chang threshold epsilon - tri come up with more featur to distinguish between the normal and the anomal exampl
in a multivari gaussian distribution, we don't model p(x1),p(x2),...,p(xn) separ but [...] . in a multivari gaussian distribution, we don't model p(x1),p(x2),...,p(xn) separ but we model p(x) all in one go .
give the ula for the densiti estim p(x) in a multivari gaussian distribution: [...] densiti estim in multivari gaussian (normal) distribut addit in ations: eg:
integr is simpli put a [...] integr is simpli put a sum over a continu rang of valu for a variabl x (instead of just discret values) .
multivari gaussian distribut is particular use to model [...] multivari gaussian distribut is particular use to model high correl features. note: altern we can creat new featur that are ratio of current features. eg: note: the last exampl is an exampl of a negat correl of the featur x1 and x2.
give the ula for the paramet estim in multivari gaussian distribution: [...] paramet estim for multivari gaussian (normal) distribut note: the comput of sigma for mvgd is veri similar to the comput of the sigma for pca. then, we can comput p(x):
give the algorithm for anomali detect use multivari gaussian distribution: [...] anomali detect use multivari gaussian distribut algorithm
multivari gaussian model is a [...] of the origin (univariate) gaussian model where [...] multivari gaussian model is a general of the origin (univariate) gaussian model where p(x) is constrain to the axe (where the matrix sigma is diagonal): altern answer: where we can't model the correl features. eg: &quot;skewed&quot; graph of probabl (correl features) can't be model with univari gaussian:
a good rule of thumb of when to use multivari gaussian model instead of univari is when [...] a good rule of thumb of when to use multivari gaussian model instead of univari is when m = 10n
when a matrix is invertible/singular (eg: sigma in multivari gaussian model), there are general two reason for that: [...2 items...] when a matrix is invert (eg: sigma in multivari gaussian model), there are general two reason for that: - doe not satisfi m n (note that it must be strict greater) - redund featur (not correl but redundant! eg: x1 = x2 or x3 = x4 + x5) note: the debug proceduc is to manual check sequenti that these two condit are met (first that m n and then check for redund features).
formal speaking, redund featur are featur that are [...] formal speaking, redund featur are featur that are linear dependent.
advantag and disadvantag of multivari gaussian model compar to origin (univariate) gaussian model: [...] 
the multivari gaussian model can [...] the multivari gaussian model can automat captur correl between differ featur in x.
the origin model correspond to a multivari gaussian where [...] the origin model correspond to a multivari gaussian where the contour of are axis-aligned.
the origin model can be more [...] than the multivari gaussian model the origin model can be more comput effici than the multivari gaussian model, and thus might scale better to veri larg valu of n (number of features) . note: this is similar to the pros and con of normal equat vs gradient descent.
suppos you have train an anomali detect system that flag anomali when p(x) = e. in the cv set, it detect too mani fals posit (flag too mani thing as anomalies). what should you do decreas e
for anomali detect it is veri important, as for dimension reduct and other machin learn algorithms, to [...] . for anomali detect it is veri important, as for dimension reduct and other machin learn algorithms, to preprocess the data (mean normal and featur scaling) . note: else, some featur may have an artifici huge import compar to the others, and the algorithm should decid this for himself ( object feed ).
one of the big idea in machin learning, heavili appli in recommend systems, is the idea of [...] . one of the big idea in machin learning, heavili appli in recommend systems, is the idea of automat learning/choos what featur to use (also call featur learn ) . note: the system doe realli choos the featur for us. we simpli give a number of features, then base on given theta (the prefer of the users), the system will learn what are the valu of the featur for each exampl (which effect assign them a meaning, which is chosen automat by the system). eg: use collabor filter for recommend systems.
recommend system problem: the goal is to [...] . recommend system problem: the goal is to automat fill in the miss rate of a user base on it previous ratings. alternative: we tri to predict what rate the user would give to new product he didn't yet use (if high we recommend it) . eg:
notat in a recommend system: [...] notat in a recommend system: note: if we would compar with our previous machin learn algorithms: nm = m, nu = yn (consid that y is multidimensional, as in a multiclass problem) and size(x) = m x n matrix, and size(y) = m x yn matrix
the bias unit (x0 = 1) is also call the [...] . the bias unit (x0 = 1) is also call the interceptor .
the linear regress we studi use the [...] approach. the linear regress we studi use the least squar approach.
vector (vectorizing) consist in [...] . vector (vectorizing) consist in use matrix-specif and optim function (matrix multiplication, sum over rows/colums, etc...) in order to avoid use loop (which s in more comput efficiency) . note: the term vector come from the fact that in order to do that, most of the time you will have to convert the data to be store in a vector/matrix.
there are 2 approach to the problem of recommend systems: [...2 items...] there are 2 approach to the problem of recommend systems: - content base recommend (we have a set of featur defin each movi and we learn the paramet theta (prefer of the user) that predict how a user will rate a now movi base on the featur of this new movie) - featur learn (given the prefer [theta] of the user and it rate of a movie, we tri to find the valu of the featur for this rating) - content base recommend + featur learn = collabor filter (use in chain until it converges): we guess theta random at first. note: this is onli possibl (to use both) becaus each user rate multipl movi and each movi is rate by multipl users.
cost function for recommend system use content base recommend (or optim objective): [...] regular cost function for content base recommend system addit in ations: note: sinc m(j) is a constant, we can remov it without affect the cost function. note: this is essenti a variat of linear regression. eg:
give the optim algorithm (gradient descent) for content base recommend system: [...] content base recommend system - gradient descent addit in ations: note: this is essenti a variat linear regression.
content base recommend system are solv by a variat of [...] . content base recommend system and featur learn are solv by a variat of linear regress (but we can also use advanc optim techniqu like conjug descent or l-bfgs) .
a convex function is a function that is [...] a convex function is a function that is local optima free. note: a non convex function has multipl optima and is not appropri for machin learn (although we can still use the algorithms, but they may get stuck in non-optim solutions).
cost function for featur learn for collabor filter for recommend systems: [...] regular cost function for featur learn for collabor filter for recommend system note: this is basic the same as content base recommend cost function but here we tri to find the valu for the featur of x (notic that the regular paramet here is on x instead of theta). addit in ations:
gradient descent for featur learn for collabor filter for recommend system: [...] regular gradient descent for featur learn for collabor filter for recommend system
simpli put, with content base recommend , we are given [...] and must find [...] . simpli put, with content base recommend , we are given a set of featur for movi (x) and we must then get the user prefer (theta) base on their rate (y).
simpli put, with featur learn , we are given [...] and must find [...] . simpli put, with featur learn , we are given a set of user prefer (theta) and must find the valu of the featur (x) for each movi base on the rate (y) .
collabor filter = [...] collabor filter = content base recommend + featur learn note: the term &quot;collabor filtering&quot; refer to the observ that the user are collabor contribut to make the system better and get better recommend for everyone.
basically, what collabor filter algorithm doe is to [...] basically, what collabor filter algorithm doe is to random initi prefer (theta) then comput product featur valu (x) use content base recommendations, then again prefer use featur learning, then again featur values, then again preferences, etc... until it converges. note: there is actual a more effici algorithm that can comput both at the same time, but the principl work about the same.
cost function for collabor filter (aka optim objective): [...] regular cost function for collabor filter note: we should not use the interceptor/bia featur x0 in this cost function, contrari to the previous cost functions. we can do away with the interceptor becaus we are learn all the featur and theta at onc now, so if the system want an interceptor it now has the flexibl to creat one by itself (eg: x1 = 1). this is more objective. note2: this is basic a combin of content base recommend cost function + featur learn cost function: note: the highlight part are exact equivalent. this show how we combin the previous cost function into a singl one that comput theta and x at the same time. optim objective:
give the collabor filter algorithm: [...] collabor filter algorithm note: there no bias unit (no theta0 nor x0) so that whi there no special gradient descent case for k=0. note2: we can see that the gradient descent for collabor filter is simpli the combin of gradient descent for content base recommend + featur learning. note3: we need to preprocess the data (mean normal + featur scaling) prior to do this algorithm (particular if the dataset is a merg of sever dataset from differ sourc with differ rate scale eg: 1-5 on one websit merg with 1-100 on another). note4: the random of theta serv as a symmetri break (similar to the random initi of a neural network parameters) and ensur the algorithm learn featur x that are differ from each other.
give the gradient descent for collabor filtering: [...] regular gradient descent for collabor filter altern ulation: note: there no bias unit (no theta0 nor x0) so that whi there no special gradient descent case for k=0. note2: we can see that the gradient descent for collabor filter is simpli the combin of gradient descent for content base recommend + featur learning.
collabor filter is also call [...] collabor filter is also call low rank matrix factor
a low rank matrix is a matrix that has [...] a low rank matrix is a matrix that has mani linear depend colum vector (eg: in a movi rate website, we assum that the rate of the user are not independ of the rate of the others, there are certain featur that make them correlated) .
vector implement of the predict in collabor filtering: eg:
to find relat products, we simpli have to [...] to find relat products, we simpli have to find the product which have the most similar set of valu for the featur (the smallest distanc between the featur of these differ products): x(i) - x(j) = sqrt((x1(i) - x1(j))^2) + sqrt((x2(i) - x2(j))^2) + ... eg:
featur scale = [...] featur scale = max(x) - min(x)
mean normal = [...] mean normal = x - mu where x is a vector (so in fact we comput x_j(i) - mu_j for each featur x_j) and mu_j = mean of all the valu of featur x_j
without mean normalization, a user which has rate no product at all will get [...] without mean normalization, a user which has rate no product at all will get a recommend of 0 for all products. note: this can be fix by use mean normalization.
in collabor filtering, we can use [...] to propos a recommend for user who rate no product at all base on averag rate of all users. in collabor filtering, we can use mean normal to propos a recommend for user who rate no product at all base on averag rate of all users. eg: user who have not rate ani movies: the onli thing that will be stay for the recommend propos to a user who didn't rate anyth will be the mean mu (averag rate for the movie).
even if each user has rate onli a small fraction of all of your product (so r(i,j)=0 for the vast major of (i,j) pairs), you can still build a recommend system by use collabor filtering. true or fals true
recal that the cost function for the content-bas recommend system is . suppos there is onli one user and he has rate everi movi in the train set. this impli that nu=1 and r(i,j)=1 for everi i,j. in this case, the cost function j(θ) is equival to the one use for regular linear regression. true or fals true in this case, the cost function is just a sum of squar differ between a predict θtx and true vaue y; this is exact linear regression.
suppos you are write a recommend system to predict a user book preferences. in order to build such a system, you need that user to rate all the other book in your train set. true or fals fals
collabor filter is use when [...] . collabor filter is use when we must predict real-valu output (as in linear regression) but where the featur and theta are miss as well as some valu of the output y, but with multipl user (which choices/output y are not independ of each other) .
the threshold epsilon (to detect an anomaly) can be automat chosen by [...] the threshold epsilon (to detect an anomaly) can be automat chosen by use an automat check of the f1 score over the cross-valid set.
a common requir for machin learn algorithm is that m = n (or even m n). when we have m n (or even when m n doe not hold), what doe that mean when m n, we don't have enough observ to adequ estim the effect of all the featur
what do you call a signific test in which the distribut of the test statist under the null hypothesi is obtain by calcul all possibl valu of the test statist under rearrang of the data point a permut test
in statistics, what do uppercas letter typic refer to random variabl
what is the most standard way to compar data point from the same subject across differ samples, which have differ mean and standard deviat normal each data point to it sampl mean and sampl standard deviat (e.g., calcul t-statistics)
what doe it mean for a summari statist in a statist model to be unstabl with respect to differ realize of the random variabl differ realize of random variabl will yield high differ summari statist
what summari statist describ the probabl of obtain a test statist at least as extrem as the one that was actual observ the p-valu
is the sum of two normal distribut random variabl with the same mean normal distribut yes (http://en.wikipedia.org/wiki/normal_distribution)
is the sum of two normal distribut random variabl with differ mean normal distribut yes (http://en.wikipedia.org/wiki/normal_distribution)
will a mixtur densiti of two normal distribut with differ mean have one or two peak two (provid that their mean are far enough apart) (http://en.wikipedia.org/wiki/normal_distribution)
when a statist popul contain two or more sub-popul each with it own distribution, then what type of distribut aris natur to describ them a mixtur distribut (http://en.wikipedia.org/wiki/mixture_distribution)
if you are use model in order to make predictions, should you choos the best model first not usual (instead, you should averag the probabl predict over models) (ripley, "select amongst larg class of models", http://www.stats.ox.ac.uk/~ripley/nelder80.pdf)
if the differ between a and c is statist significant, and the differ between b and c is not statist significant, must the differ between a and b be statist signific a resound no (http://www.stat.columbia.edu/~gelman/research/published/signif4.pdf; doi: 10.1198/000313006x152649)
in compar two treatments, should you look at the statist signiﬁc of their differ or the differ between their signiﬁc level the statist signific of their differ (http://www.stat.columbia.edu/~gelman/research/published/signif4.pdf; doi: 10.1198/000313006x152649)
in order to employ an ordinari least squar estim in a regress model, do we need to assum that the residu are independ whi or whi not no; trick q, becaus in fact residu cannot be independent, sinc they must sum to 0 (http://en.wikipedia.org/wiki/errors_and_residuals_in_statistics)
are matric typic refer to use uppercas or lowercas letter (statistics) uppercas
are vector typic refer to use uppercas or lowercas letter (statistics) lowercas
for a continu pdf, what properti of the second deriv tell us whether that point is (local or absolutely) a maximum that it is less than zero (s&amp; p 21)
if a conclus is statist robust, doe that mean that it is right no (e.g., the model could be wrong) (http://www.ft.com/intl/cms/s/2/9219969e-6a28-11e0-86e4-00144feab49a.html#axzz1kknuh35y)
what doe john cook consid the key idea of bayesian statist to repres all uncertainti by probabl distribut (http://www.johndcook.com/blog/2011/04/20/teaching-bayesian-stats-backward/)
when read a chart, do peopl compar distanc or area more accur distanc (http://stats.stackexchange.com/questions/4810/how-to-use-cdf-and-pdf-statistics-for-analysis/4885#4885)
what has a simpler non-parametr estimator, the cumul distribut function or the probabl densiti function the cumul distribut function (estim by the empir distribut function) (for the pdf you have to choos arbitrari bin size or kernal shape) (http://stats.stackexchange.com/questions/4810/how-to-use-cdf-and-pdf-statistics-for-analysis/4891#4891)
if there is synergi between two treatment for a condition, then how strong (on average) must their effect be in combin greater than addit (doi: 10.1158/0008-5472.can-09-1947)
what doe it mean to say x ~ n(0,1) that the variabl x follow a normal distribut with a mean of 0 and a varianc of 1 (http://en.wikipedia.org/wiki/normal_distribution)
let say you per mani hypothesi test simultan on correl test statistics. how will your s be biased, relat to your prescrib error rate α, if you divid the α for each test by the total number of hypothes you'll have too mani fals negat (unless your test are independent, the bonferroni correct will tend to be too conservative) (http://en.wikipedia.org/wiki/multiple_comparisons#methods)
what is a statist a quantiti calcul from a set of data (e.g., the sampl mean)
is &quot;th probabl that a woman with a posit mammographi has breast cancer&quot; usual the same thing as &quot;th probabl that a woman with breast cancer has a posit mammography&quot; no (http://yudkowsky.net/rational/bayes)
in process for which mani independentfactor add togeth to generat a , what type of distribut doe the follow a normal distribut (http://en.wikipedia.org/wiki/central_limit_theorem)
in process for which mani independ factor multipli togeth to generat a , what type of distribut doe the follow a log normal distribut (http://en.wikipedia.org/wiki/central_limit_theorem)
what do you call the distribut of a statist deriv from a set of random sampl of a particular size from a popul the sampl distribut of that statist
what doe the word &quot;standard&quot; in standard deviat refer to &quot;standardized&quot; (the squar root of the variance, to adjust for the fact that the varianc squar the distanc from the mean)
for a normal distribution, around what percent of the data point will lie within two standard deviat of the mean 95% (http://en.wikipedia.org/wiki/68-95-99.7_rule)
even if our goal is mere to understand the relat between variables, how can predict be use it test our knowledg of relat (http://www.stat.cmu.edu/~cshalizi/uada/12/lectures/ch01.pdf)
what is a loss function a map of event onto number repres the associ cost
if e(i x=x) is constant for all valu of x, doe this mean that x and y are independ no (though that doe tell you that they are uncorrelated; you cannot assum independ from uncorrelatedness; e.g., think of a case in which x chang y variance) ("they are independent, then the regress function is a constant, but turn this around is the logic fallaci of “afﬁrm the consequent”") (http://www.stat.cmu.edu/~cshalizi/uada/12/lectures/ch01.pdf)
if x and y are independent, doe this mean that e(i x=x) is constant for all valu of x yes (http://www.stat.cmu.edu/~cshalizi/uada/12/lectures/ch01.pdf)
if x take on a relat small and finit set of values, then what is a simpl strategi to estim the regress function e[i x=x] use the condit sampl mean (1.4 http://www.stat.cmu.edu/~cshalizi/uada/12/lectures/adafaepov.pdf)
when use least squar regression, do you implicit assum a linear relationship between your variabl yes (http://www.annualreviews.org/doi/full/10.1146/annurev.soc.34.040507.134631)
if the window span in local smooth regress is veri small, what effect will that have on the varianc of the ing best fit line it will be too high (insuffici data will fall within each window and the line will shift too rapidly) (http://www.annualreviews.org/doi/full/10.1146/annurev.soc.34.040507.134631)
if the window span in local smooth regress is veri large, what effect will that have on the varianc of the ing best fit line it will be too low (the data will be oversmoothed, ing in bias in the fit curve) (http://www.annualreviews.org/doi/full/10.1146/annurev.soc.34.040507.134631)
you have access to plane that have return from militari mission and the distribut of the bullet &quot;wounds&quot; on the planes. which area should you recommend to have extra armor the area with no damag (select effects; those that fell like suffer an attack in a place that was untouch on those that survived) (http://stats.stackexchange.com/questions/23779/most-interesting-statistical-paradoxes/23814#23814)
what do you call a statist test that doe not assum a particular probabl distribut non-parametr (http://stats.stackexchange.com/a/7944/2073)
what doe it mean to center the column of a matrix to subtract the column mean from each entri of a column (http://stat.ethz.ch/r-manual/r-devel/library/base/html/scale.html)
if two or more of your explanatori variabl are collinear, then how mani optim paramet combin will there be for a given loss function infinit mani (i.e., we can make the coeffici whatev we like, provid we make a correspond adjust to the other(s), and it will have no effect on our prediction) (http://www.stat.cmu.edu/~cshalizi/uada/12/lectures/adafaepov.pdf sxn 2.1)
what is statist (acc'd to shalizi; four words) infer from imperfect data (http://www.stat.cmu.edu/~cshalizi/uada/12/lectures/adafaepov.pdf sxn 5.1)
when individu point are too close to one anoth in a chart to be abl to distinguish them, what techniqu can you use to random move some of them apart jitter (wickham p 17)
what is the phenomenon call in which there are so mani point in a scatterplot that it is difficult to discern firm trend overplot (wickham p 11)
what problem doe use semi-transpar data point in a chart help solv overplot (wickham p 12)
is it possibl to get a high valu for [$]r^2[/$] in a regress even if the true model is not linear it is emin so (shalizi 2.2)
if we add nois to our measur of explanatori variables, what will that do to our abil to predict the respons variabl it will decreas it (in linear regression, it push the coeffici vector closer to zero than it would be if our measur of the explanatori vector were noiseless) (shalizi 2.2)
what sign of skew doe this distribut show negat (left) skew (http://upload.wikimedia.org/wikipedia/commons/b/b3/skewness_statistics.svg)
what sign of skew doe this distribut show posit (right) skew (http://upload.wikimedia.org/wikipedia/commons/b/b3/skewness_statistics.svg)
what do you call this type of plot (note x1, x2, ..., y on both axes) a scatterplot matrix (with univari histogram on the diagonals) (http://stats.stackexchange.com/a/24529/2073)
what do you call the abil of a machin learn algorithm to per accur on new, unseen exampl after train on a finit data set general (http://en.wikipedia.org/wiki/machine_learning)
in sampl with replacement, when a ball of a particular color is random drawn from an urn, what is put back in the same ball (http://en.wikipedia.org/wiki/polya_urn_model)
what doe a one-way anova compar the mean of three or more independ group (http://en.wikipedia.org/wiki/one-way_anova)
what doe a one-sampl t-test compar a sampl mean with a fix valu (e.g., a known popul mean) (http://en.wikiversity.org/wiki/t-test)
what doe an independ sampl t-test compar two mean from independ group (http://en.wikiversity.org/wiki/t-test)
what doe a one-way repeat measur anova compar the mean of three or more match group (e.g., in a longitudin studi where each group has it own level on some variable) (http://en.wikiversity.org/wiki/analysis_of_variance)
when assess the normal distribut assumpt for a sample, is it more import that your particular sampl is distribut normally, or that the popul your sampl was drawn from is distribut normal the popul your sampl was drawn from (is distribut normally) (so if you have data from previous experi that measur the same variable, you can use that too) (http://www.graphpad.com/articles/interpret/anova/choosing_test.htm)
whi do we want the error of our model predict to be patternless becaus if there was a pattern to them, we should adjust for it, to make smaller mistak (and thus our model could be easili improved) (shalizi 3.2)
if two random variabl are independent, what is the relationship between their margin and joint probabl distribut their joint probabl distribut is the product of their margin probabl distribut (http://www.stat.cmu.edu/~cshalizi/uada/12/reminders/uncorrelated-vs-independent.pdf)
if x is uni ly distribut on the interv [-1,1], and y is uni ly distribut on the interv [0,1], and x and y are independent, then how must their joint distribut be distribut uni ly on the rectangl [-1,1] x [0,1] (http://www.stat.cmu.edu/~cshalizi/uada/12/reminders/uncorrelated-vs-independent.pdf)
even if you believ in some kind of ultim physic determinism, whi will the true, ideal statist model of a phenomenon alway have some non-zero predict error model input are never complet descript of the state of the universe, and this coars yield random (shalizi 3.2)
what is the origin of the bias term in the bias-vari decomposit of a model mis-specif of the model (e.g., we mess up on the function of the regression) (shalizi 3.2)
as we collect more data, doe our choic of causal assumpt becom more or less import neither, these remain substanti regardless of sampl size (trick question) (pearl p 101)
if a stochast process is stationary, what doe that say about it joint probabl distribut that it is invari to shift (in time or space)
what case of letter is typic use to refer to probabl densiti function lower (e.g., f(x)) (http://en.wikipedia.org/wiki/notation_in_probability_and_statistics)
what case of letter is typic use to refer to cumul distribut function upper (e.g., f(x)) (http://en.wikipedia.org/wiki/notation_in_probability_and_statistics)
what doe i.i.d. stand for independ and ident distribut (http://en.wikipedia.org/wiki/notation_in_probability_and_statistics)
when is the expect valu of a probabl distribut not equal to the arithmet mean when the probabl of all possibl outcom are not equal (shalizi, chapter 1)
what theorem say that the mean of a suffici larg number of iid rvs, each with finit mean and variance, will be approxim normal distribut the central limit theorem (http://en.wikipedia.org/wiki/normal_distribution)
what theorem say that the averag of the s obtain from a larg number of trial should be close to the expect value, and will tend to becom closer as more trial are per ed the law of larg number (http://en.wikipedia.org/wiki/law_of_large_numbers)
fit a model to one random chosen half of the data set and then evalu it on the other give you an unbias estim of what the general error (of the model) (shalizi 3.4)
in k-fold cross validation, what doe a &quot;fold&quot; refer to a random chosen, equally-s subset of the data (shalizi 3.4)
what is it call if you do k-fold cross-valid with k = n, so that your test set consist of singl point leave-one-out cross-valid (loocv) (shalizi 3.4)
becaus the amount of smooth has opposit effect on the bias and varianc of your model, what type of point will exist an optim amount of smooth (where you can't reduc one sourc of error without increas the other) (shalizi 4.1)
as we collect more data, what tend to happen to the varianc of our estim model it decreas (shalizi 4.1)
as we collect more data, in what way should we chang the amount that we smooth our model we should smooth less (becaus the estim varianc decreas and so contribut less to total error) (shalizi 4.1)
what happen to the relationship between two independ caus when a consequ common to them both is observ they are render depend (pearl p 106; http://en.wikipedia.org/wiki/berkson's_paradox)
if two event are known to be independent, is it possibl for the observ of anoth event to render them depend classic exampl yes; if you learn that exact one of two flip of a coin is tails, their outcom becom depend pearl p 106
what is the main way that the assumpt embodi in a structur equat model can confront the scrutini of nonexperiment data (accord to pearl) condit independ (induc by d-separation) (pearl p 106)
asymptotically, doe random neutral either measur or unmeasur confound both (trick question) (http://bayes.cs.ucla.edu/book-09/ch11-3-1-final.pdf p 340)
whi is it not alway a good idea to condit on as mani pre-treat measur as possibl condit on some variabl can increas bias (pearl p 116) e.g., berkson bias
in pearl method of estim causality, what qualit judgement are requir cause-effect relationship between variabl (pearl p 118)
consid the causal model x → z ← u → y, where onli u is unobserved. what would the relationship between x and y be if z were physic set to a constant they would remain independ (this would be equival to delet all arrow enter z) (pearl p 133)
consid the causal model x → z ← u → y, where onli u is unobserved. what would the relationship between x and y be if you were to condit on z you'd creat a spurious associ between x and y (that might be constru as a direct effect) (this is &quot;condit on a collider&quot;) (pearl p 133)
what doe pearl think of the mantra &quot;no causat without manipulation&quot; that it must be reject (becaus measur variabl is more fundament to causal than changing) (pearl p 136)
if a probabl distribut of wait time t [...], then pr(t 40 s t 30 s) = pr(t 10s). is memoryless (and therefor exponenti distributed, if continuous) (http://en.wikipedia.org/wiki/exponential_distribution)
accord to gelman, which are easier to interpret: standard deviat or varianc standard deviat (http://andrewgelman.com/2012/05/comments-on-a-bayesian-approach-to-complex-clinical-diagnoses-a-case-study-in-child-abuse/)
you live in a place with a constant probabl of be struck by lightn throughout the year. suppos that the strike are random: everi day the chanc of a strike is the same, and the rate work out to one strike a month. your hous is hit by lightn on monday afternoon. what is the most like day for the next bolt to strike your hous tuesday (the event are independ and this is a poisson process) (pinker ba, p 202)
if a person trait is polygenic, then dure inherit from one generat to the next, what statist law will it obey regress to the mean (pinker ba, 233)
if you are interest in infer the mechan that underli the ation and evolut of a data set, then is it like to matter whether the distribut follow a power law as oppos to some other yes (http://arxiv.org/abs/0706.1062)
if you are interest in predict the futur valu of a data set over a relat short time frame, is it like to matter whether the distribut follow a power law as oppos to some other nope (in fact you would ideal be model averag anyway) (http://arxiv.org/abs/0706.1062)
as you collect more i.i.d. observations, what will happen to your estim of the standard deviat of a distribut (in theory) it should not chang (on average; this is a fix paramet of a distribution. though actually, it will probabl increase) (http://stats.stackexchange.com/questions/32318/difference-between-standard-error-and-standard-deviation#comment63459_32385) (http://en.wikipedia.org/wiki/standard_error)
what is wide consid the most difficult topic to teach in intro statist the sampl distribut (e.g., of the mean) (http://stats.stackexchange.com/questions/34926/strategies-for-teaching-the-sampling-distribution)
imagin that your parent had roll a six-sid die to decid how mani children to have. what did they most like roll whi a 6 (eg, this is more like than them roll a 1); becaus there a higher chanc of you exist in that case (there are more &quot;you's&quot; to observ it) (http://en.wikipedia.org/wiki/self-indication_assumption) (http://lesswrong.com/r/discussion/lw/ef3/the_real_sia_doomsday/)
in layman terms, what doe statist power mean the chanc of find a signific effect if it realli there
from a 3 dimension data set (dimens x, y, and z), how mani uniqu covari measur can you comput 3 (cov(x,y), cov(x,z), and cov(y,z); combo = ((p)(p-1))/2) (smith, pca tutorial, 2002) think of it in term of the covari matrix: non-diagonals, one half
in pca, what is the price we pay for the reduct in dimens in term of understand the new direct the new axe will not (in general) map neat to the origin variabl (instead, the new direct will correspond to linear combin of the origin variables) (janetta p 335ish)
sets: what do you call the shade region in the venn diagram of the two set repres by circl below the symmetr differ i.e., the union without the intersection; for example, the symmetr differ of the set {1,2,3} and {3,4} is {1,2,4} http://en.wikipedia.org/wiki/symmetric_differ
if a probabl distribut of wait time t [...], then pr(t 40 s t 30 s) = pr(t 40s). is known to be zero between t = 0 and 30 s (this is independence, not memorylessness, though) (http://en.wikipedia.org/wiki/exponential_distribution) this card was a leech but remak b/c it import
what is the idea behind efron empir bay experi with mani parallel situat allow estim of their own prior http://www.sciencemag.org/content/340/6137/1177.ful rss=1
give an algorithm for integ divis without use multipl or divis operators. what issu might aris implement long divis as a bit-shift algorithm. issues: buffer size (32 bit ), negat inputs.
what is insert sort scan the array to be sort from left to right for each item, find it place in the fragment of the array to it left (so-far-sorted)
give the sort properti of insert sort adapt stabl in-plac onlin
give best and worst case instanc for insert sort best: small array worst: revers array
give the averag and worst-cas runtim of insert sort both $o(n^2)$
describ select sort iter over the array from left to right at each element, swap it for the smallest element in the fragment of the array to it right.
give the benefit of select sort in-plac low auxiliari memori use simpl adapt
give the averag and worst-cas runtim of select sort both $o(n^2)$
describ a method of exponenti faster than repeat squar addit chain exponenti an addit chain is a sequenc of integers, start with 1, where each element is the sum of two previous elements. an optim addit chain for $n$ can be use to comput $a^n$ in an optim manner find addit chain is np-complete.
give an algorithm to select $k$ element from $n$ when $n$ is unknown select the first $k$ element for each remain element, replac one of the current $k$ element with probabl $k/n$.
what are the runtim properti of the rabin-karp algorithm given a pattern of length $m$ and text of length $n$: expect runtim is $o(n+m)$ worst case is $o(nm)$ if the hash collid often.
describ an algorithm to find the first unbalanc parenthes in a string of parentheses. loop over the string push to stack the index of locat with a `( pop from the stack when encount a `)'. if the stack is empty, the `) is unbalanced. if the loop terminates, check the stack. if ani indic remain in it, the correspond `( is unbalanced. alternatively, count in from both ends.
give the master theorem let $t(n) = at(n/b) + f(n)$ if $f(n) = o(n^{\log_b a})$ then $t(n) = \theta(n^{\log_b a})$ if $f(n) = \theta(n^{\log_b a})$ then $t(n) = \theta(n^{\log_b a} \lg n)$ if $f(n) = \omega(n^{\log_b a})$ then $t(n) = \theta(f(n))$
give an algorithm to find the major element of an array in linear time mark the first element in the array, and initialis a counter to one move right. increment the counter if the next element match the mark element, els decrement it. if the counter reach zero, mark the element under consider instead. continu right. when the end of the array is reached, assum the current mark element is the major element. check this.
describ the karp-papadimitriou-shenk algorithm for find all element that appear with at least frequenc $\theta$ in an array. let $k$ be an initi empti set (implement as a hash table). scan the array. for each element in turn: if the element is not in $k$, add it to $k$ with a counter set to 1. if the element is in $k$, increment it counter. if $ k 1/\theta$, decrement the count of all element in $k$, and delet ani whose count reach zero. make a second pass of the array to identifi those item in $k$ which appear at least $\theta n$ times.
describ a linear-tim algorithm for find the median of an array divid the array into s of five element (ignor leftovers). find the median of each , and an array from these median (possibl in place). make a recurs call on the list of medians.
describ the oper of the mergesort algorithm. if the array is a singleton, return it. else, split the array into two subarray of size $~n/2$ and call mergesort on each. merg the sort subarray by repeat pop the smallest element from the head of the two subarrays. return the sort array.
describ the properti of mergesort run in $o(n \lg n)$ time, both worst- and average-case. requir $o(n)$ auxiliari space. is stable. is easili paralliz can be implement as stabl amp; in-place, but is more complicated. easier on link list than arrays.
describ the implement of the quicksort algorithm if the array is a singleton, return it else, pick a pivot by some heuristic. partit the array into a subarray greater than the pivot, and a subarray less than it. call quicksort on the subarrays.
describ the properti of the quicksort algorithm. $o(n^2)$ worst-cas for arbitrari pivot, $o(n \lg n)$ for median pivot is \emph{not} stabl $o(n)$ auxiliari space naively; can be implement in $o(\lg n)$ space by sort the smaller subarray first then make a tail-cal to the larger subarray. also make it in-place.
what is an adapt sort a sort algorithm that take advantag of pre-exist order in the input
give an algorithm for initi and access an array both in constant time. the array {\tt vec} store data the array {\tt to} list the cell of {\tt vec} which are initialized. the array \ {from} contain pointer to the cell of \ {to} that point to them the pointer {\tt top} give how mani cell of {\tt to} have been initialised. to access {\tt vec[i]}, check that {\tt from[i] top} and {\tt to[from[i]] = i}.
which data structur doe bfs correspond to a queue
what are the three state a vertex can be in dure a travers undiscov discov process
how can the shortest path from vertex $x$ to vertex $y$ be found conduct a bfs from $x$, record for each vertex $j$ the {\tt parent} vertex $i$ that discov it. follow the chain of {\tt parent} from $y$ back to $x$
at what three set of point in a graph search are action usual carri out befor ani of a vertic edg are process after all of a vertic edg are process dure the process of a vertex edge.
what is the runtim of bfs on an adjac list of $n$ vertex and $m$ edg $o(n+m)$
how can the compon of an undirect graph be identifi pick ani vertex and run a bfs to complet on it. mark all discov vertices. repeat.
how can we test whether a graph is bipartit colour each vertex alternately, then check each edg for violations.
what data structur is associ with a dfs a stack, although it is more clean implement as a recursion.
what three properti doe a dfs w/ timestep record about the vertic of the graph state - undiscovered, discovered, or process entri time - the number of step into the algorithm at which a vertex is set to discov exit time - the number of step into the algorithm at which a vertex is set to process
how can a dfs w/ timestep be use to identifi ancestor of a vertex $x$ in a dfs tree the entry-exit time interv of the ancestor contain the entry-exit time interv of $x$.
how can the trace of a dfs with timestep be use to count the descend of a vertex $v$ differ between entri and exit time is twice the number of descendents.
what are tree edg and back edg wrt a dfs tree edg are travers by the dfs back edg are edg point to earlier in the search tree
how can cycl be found in an undirect graph conduct a dfs from ani vertex. if an edg is found from a discov vertex to a discov one that isn't it parent (a back edge), the graph has a cycle.
how can articul vertic be identifi in a graph conduct a dfs, record for each vertex it parent, it dfs-tree outdegree, and the earliest ancestor reachabl through it children. after process each vertex, check whether it is an articul vertex use the in ation recorded. back-propog earliest-ancestor in ation at the conclus of processing.
what kind of edg can be encount dure a dfs on a direct graph tree edges, involv in the algorithm. forward edges, link an earlier vertex to a later one which isn't it dfs-child. back edges, link a later vertex to a dfs-ancestor. cross edges, link edg which are neither dfs-ancestor nor dfs-child.
on what graph is a topolog sort guarante to exist direct acycl graph
how can a topolog sort of a dag be generat conduct a dfs and push vertic into a stack in the order that they are mark processed. pop the stack to retriev the topolog order.
how can you tell whether a graph is a dag a dfs won't find ani back edges.
in a dfs, how can back edg be identifi they will link two discovered-but-unprocess vertic together.
how can strong connect compon be identifi in a direct graph run a dfs. if a back-edg is found, shrink the correspond cycl down to a singl vertex. repeat.
euler characterist ula is $v - e + f = 2$ for spheric polyhedra and connect planar graph
give a linear-tim algorithm for find the diamet of a tree. pick an arbitrari vertex and run a bfs to identifi the most distant vertex, $u$. then, find the vertex $v$ most distant from $u$. this is the diamet of the tree.
what is an arbores of a digraph a root tree subgraph such that there is a path from the root to everi node.
what is a mother vertex in a digraph a vertex from which all other can be reached.
give an algorithm to identifi whether a graph has a mother vertex. comput the strong connect compon repres each compon as a vertex. the ing graph is a dag. if more than one vertex in the deriv graph has in-degre 0, the origin graph doesn't have a mother vertex.
in the dfs algorithm for find a graph strong connect components, what happen when a vertex is first discov it is push into the e stack
in the dfs algorithm for find strong connect components, what happen dure the process of an edg $(x, y)$ if it a back edge, updat $x$ earliest reachabl vertex to be the older one of $x$ and $y$ reachabl vertic if it a cross edg and $y$ doe not yet have a component, do the same.
in the dfs algorithm for find strong connect components, what happen after a vertex $v$ has been process if the earliest reachabl vertex of $v$ is $v$, pop vertic from the stack until $v$ is found, mark each as belong to $v$ component. updat the earliest reachabl vertex of the parent of $v$ to be the earliest of $v$ and it own.
give the three type of articul vertex relev to the dfs algorithm to find them. if $v$ has no parent (is the root) but more than one child, it is an articul vertex. if the oldest ancestor reachabl from $v$ is $v$, it is an articul vertex. if the oldest ancestor reachabl from $v$ is the parent of $v$, the parent is an articul vertex.
describ prim algorithm for find a minimum span tree pick ani vertex while the tree is incomplete, add the lowest-weight edg adjac to it.
what is the runtim of prim algorithm $o(n^2)$ naiv $o(m + n\lg n)$ with a fibonacci heap
describ kruskal algorithm to find a minimum span tree repeat pick the cheapest edg between ani two disconnect components.
what is the runtim of kruskal algorithm $o(mn)$ naiv $o(m \lg m)$ on a union-find data structur with the shorter-tre heurist $o(m \alpha (n))$ amort on a disjoint-set data structur with shorter-tre and path compress heuristics.
describ the union-find datastructur and it various heuristics. a forest of `revers trees, with each element point to it parent. {\tt find(x)} return the repres element of $x$ set by walk up the tree {\tt union(x,y)} merg the set contain $x$ and $y$ by point the root of one tree at the root of the other. the shorter tree heurist keep track of tree height and merg the smaller tree into the larger. the path compress heurist point each travers node to the root dure {\tt find} oper
give the runtim of the union-find data structur method $o(\lg n)$ for both find and union with the shorter-tre heurist $o(\alpha (n))$ amort for both find and union with shorter-tre and path compress heurist
describ dijkstra algorithm for find shortest path in weight graphs. initialis the origin vertex $s$ as `current and all other vertic as `unvisited'. for each $(s,v)$, set ${\tt dist[v]} = w(s,v)$. while there are still unvisit vertices: set the unvisit vertex $v$ minim {\tt dist[v]} to be current for each $(v,x)$, updat {\tt dist[v]} to be the distanc to $s$ through $x$ if it shorter remov $v$ from the unvisit set.
what is the runtim of dijkstra algorithm $o(n^2)$ naiv $o(m + n\lg n)$ on a prioriti queue over a fibonacci heap
give a class of graph on which dijkstra fails. graph with negat weight edges.
give a class of graph on which floyd algorithm fails. graph with negative-weight cycles.
describ floyd algorithm for the all-pair shortest path problem initialis the weight matrix $[w_{ij}]$, with infinit valu for non-adjac initialis the parent matrix $[p_{ij}]$ for $i, j, k [0, n]^3$, $k$ in increas order: if $w_{ik} + w_{kj}$ is less than $w_{ij}$, updat the latter, and store $k$ in $p_{ij}$
what is the runtim of floyd algorithm $o(n^3)$.
what is floyd algorithm use for calcul the shortest path between all pair of vertic in a graph.
describ the basic ford-fulkerson algorithm for max network flow. initialis all flow to zero. find a path from sourc to sink with a posit remain capacity. augment the flow with this path. rememb to subtract the flow go the other way.
describ the karp-edmond algorithm for maximum network flow ford-fulkerson where the augment path are identifi with a bfs from the source.
what is the runtim of the edmonds-karp algorithm $o(nm^2)$
list a standard set of seven question to ask when stuck on an algorithm problem doe brute forc work doe it have an order would sort help is approxim easier doe it have subproblem how doe trivializing/ignor paramet simplifi thing have you follow polya
what is a dao data access object
what is a orm object-rel mapping, which allow databas tabl to be treat as objects.
what is the mvc the model-view-control softwar architectur it separ the in ation in a program (the model) from the user. the view compon s the model to the user; the control allow the user to modifi the model
in larg multi-project maven builds, what can be done to clean up depend pull ani depend that declar more than onc in a subtre up to the dependencymanag section of the root pom. use {\tt \{project.version\}} and {\tt \{project.groupid\}} when refer to a sibl pom.
in larg maven projects, how can the plugin of a pom be clean up consolid version number in {\tt property} in a top-level pom consolid configur into {\tt dependencymanagement} in a top-level pom
what is the differ between an option type and a nullabl type nullabl type allow the variabl to be set to null. option type allow the variabl not to be set at all.
what is the semipred problem that a function might fail, but signal the failur requir a use return valu to be used.
what are the heap creat by .net the code heap, which store instruct after they'v been jited. the small object heap (soh), for object $ 85k$. the larg object heap (loh), for object $ 85k$ (with some exceptions) the process heap.
in .net, how doe the stack work when a method is called, a contain (a stack frame) is creat which hold everyth need to complet the call. the frame is put on top of the stack. when the method completes, the frame is removed. the frame at the top of the stack is alway the one in use by the current execut method.
in net, what in ation doe a stack frame hold paramet of the method local variabl address of the code to exit to.
in net, how do stack interact with thread each thread has it own stack.
in net, what is box and unbox box is a valu type be assign to a refer type variable, caus it to be copi onto the heap. \ {int stackvar = 10; object boxedobject = stackvar;} unbox is a refer type be assign to a valu type variable, caus it to be copi onto the stack. \ {int unboxedvar = (int) boxedobject;}
which of .net heap are manag the soh and loh
in net, what are the five sourc of root refer stack refer global or static refer cpu regist object final refer interop refer (net object pass to com/api calls)
in net, what doe com stand for compon object model it is microsoft interfac standard for interprocess communic and object creation
in net, where are array of doubl store if the array has 999 item or less, on the soh otherwise, on the loh. yes, this is an except to the 85k rule
describ a constant space, linear time algorithm for merg two sort list of total length $n$. gather the $o(\sqrt{n})$ largest element in the list on the far left. call these the buffer. break each sublist - other than the buffer (`` 1'') - into s of $\sqrt{n}$ elements. sort the s from smallest-to-largest by the size of their tailmost element. let $i$ be the first such that the tail element of $i 2$ is larger than the head element of $i+1$. merg the element in s 2 through $i$ with $i+1$ by repeat compar the leftmost element in each seri of elements, and swap the smaller of the two with the leftmost buffer element. identifi the next two seri of element to merg in the same way.
what is the yale at a way to store two-dimension spars matricies. use three arrays: $a$ contain all non-zero elements, $ia$ contain the indic in $a$ of the first nonzero element of each row, $ja$ contain the column indic of each nonzero element in $a$
what is bandwidth minim minim the number of nonzero diagon in a matrix by reorder the row and columns.
what is the upper bandwidth of a matrix the smallest $p$ such that everi element $a_{ij}$ is zero for $i j - p$.
what a chain datastructur a singly-link list with a null pointer at the end.
what is a general list a list in which each element is either atom or anoth general list.
how can a queue be effici implement on an array implement it on a circular array and use modular arithmet
what is a thread binari tree a binari tree where each null left child point to the inord predecessor of the node and each null right child point to the inord successor
what is a tournament tree a complet binari tree in which each node contain the smaller (larger) of it two children.
how can a tournament tree be use for sort when combin $k$ sort lists, popul a tournament tree with the head of each list. the root is then the first item in the merg list. pop it then repopul the tree with $\lg(k)$ comparisons.
how can set be implement by a bit vector set the $i$th bit to 1 if the $i$th object is in the set.
what are the drawback of a bit vector set implement space is linear in the size of the univers list set element is linear in the size of the univers
how do bloom filter work on insert an item $x$ into a set $s$, evalu hash function $h_1(x), \dotsc, h_k(x)$ and set the correspond memori locat to 1. to check whether $x$ is in $s$, hash it and check the correspond memori locations. fals posit possibl element can't be remov can estim number of element in s
how do count filter work bloom filter with a counter at each memori location. insert increment counters, remov decrement them. a `full counter should no longer be increment or decremented.
what is the fals posit rate in a bloom filter $p \left( 1 - e^{-kn/m} \right)^k$ on $n$ elements, $m$ bit and $k$ hash function
what is a general bit vector use for repres partit $i$th cell contain the name of the subset contain the $i$th element
how do general bit vector per set identif is constant time single-el modif is constant time union is proport to the size of the univers
what is a k-d tree a binari tree in which everi node is a k-dimension point. left children have a smaller valu than the parent along the axi associ with the parent, right children have a larger value. non-leaf node implicit generat a hyperplan perpendicular to the axi associ with the node.
what is a quadtre a variant of a k-d tree for partit the plane. each node has four children, correspond to the four quadrant around the node.
what is a bsp tree a binari space partit tree a variant of a k-d tree where each intern node has a general (not axis-parallel) plane associ with it.
what is an r-tree a rectangl tree a variant on k-d tree where each intern node repres a minimum bound rectangl of it children
what are k-d tree common use for point locat nearest neighbour search rang search partial key search
what is the end-to-end principl it far easier to obtain a necessari standard of reliabl by alter the end host in a network rather than the machin in between.
what are the standard defens against mitm attack third-parti authent session checksum share secret
what is the standard exampl of a race condit attack a program verifi a file then read it in seper oper an attack can substitut the file for a malici one after the verif has been done.
what is the standard defens against race condit attack avoid non-atom oper unless you'r confid there'r no secur implic if you don't know that an oper is atomic, assum it isn't.
what are the standard defens against replay attack session token timestamp nonc
how do session token work alic generat a token and send it to bob who mutat the share secret with it and send the back to alic
how do cryptograph nonc work alic request a nonc from bob and then use it to salt the rest of the exchange. this prevent replay attack
how are pars error attack best defend against use specialist librari for user input process check input against whitelist of safe token
what are the three factor at work against secur code technic - the complex of the task psycholog - mental model ill-adapt for spot flaw real-world - econom and social pressur
what are the four classic stage of in ation secur protect deter detect react
in a hard disk, what is a cylind the vertic align set of tracks, one on each platter, which correspond to the same head position.
in a hard disk, what is a track a circl trace out by a stationari head
in most hard disks, how mani track can be read simultan usual one.
in a hard disk, what is a sector a subdivis of a track hold a fix amount of data (4kb nowadays) plus a header and ecc
how long doe it typic take a read head to move from one cylind to anoth 3-10ms
what is the rotat latenc of a hard disk the time it take for the platter to make half a revolut which is 3-10ms
what are the main paramet of the parallel disk model n, the problem size m, the intern memori size b, the transfer size d, the number of disk p, the number of cpus
what are the paramet of the parallel disk model specif to queri q, the number of queri in a batch problem z, the answer size
what are the normal paramet of the parallel disk model $n = n/b$, the problem size in term of disk s $m = m/b$, the intern memori size in term of disk s $q = q/b$, the queri specif size in term of disk s $z = z/b$, the answer size in term of disk s
what is assum about the processor interconnect network of the parallel disk model that item in the collect intern memori of the processor can be sort in optim $o((m/p) \lg m)$ time
what restrict is there on the input and output of data in the parallel disk model it should be stripe across the $d$ disk
in the parallel data model, how long doe it take to read/writ stripe data $o(n/db)$ i/o op
when is a type constructor covari when it preserv the order of types.
when is a type constructor contravari when it invert the order of types.
what is an inner join \ {employe inner join department} will return all \ {(employee, department)} pairs.
what is a left outer join \ {employe left outer join department} will return a pair \ {(employee, department)} for everi employe in the database. in some pairs, \ {department} might be null. the same employe might appear multipl times.
what is a full outer join \ {employe full outer join department} will return a pair \ {(employee, department)} for everi employe and department. in some pairs, \ {employee} or \ {department} might be null.
what is preemption it the system of thread manag where a thread manag switch context without the thread approval.
what is a time slice the time between thread schedul intervent aka a quantum
what is a quantum the time between thread schedul intervent aka a time slice
how doe a semaphor work \ {wait()} decrement the semaphor by 1. if it becom negative, the thread execut \ {wait()} will and is ad to the queue. \ {signal()} increment the semaphor by 1. if the pre-incr valu was negative, a thread is releas from the queue.
how do you write to the consol in unity3d \ {debug.log(message)}
what is the standard unit test librari for vs2012 \ {microsoft.visualstudio.testtools.unittesting}
what is a test har the code use to test a piec of softwar (test scripts), along with the code that run those test (test execut engine)
what are the two qualiti of a good unit test it run fast ( $ $100ms) it local failures.
what is the legaci code chang algorithm identifi chang point find test point break depend write test make chang and refactor
what are the two reason to break depend in code sensing: when otherwis you couldn't access what our code computes. separation: when otherwis you couldn't get the code to run in a test har
what is a fake object an object that imperson a collabor of the class under test.
what is the domin method of break depend for sens purpos fake a collaborator, and queri the fake as to what data it recieved.
what is a mock object a fake object which per s assert internally.
as far as legaci code goes, what is a seam a place where you can alter the behaviour of your program without do ani edit at that location.
how can you suppress call to \ { include}d static function use an object seam, ie by creat a subclass and overrid the function and use the overrid to either pass the call on to the intend recepi or redirect it.
how can you intercept call to global function use an object seam, ie by creat a subclass and overrid the function and use the overrid to either pass the call on to the intend recepi or redirect it.
what is an object seam a place where you can modifi behaviour by replac one object with another. usual by creat a subclass that overrid various methods.
what featur specif to c++ and c (and a few other languages) can be use for sens the preprocessor
how can a languag preprocessor be use for sens by \ { include} a file which, condit on \ { ifdef testing} will \ { define} fake version of the function you want to intercept
what is an enabl point the place associ with each seam where you can make the decis for a seam to use one behaviour or anoth
give an exampl of an enabl point. in a preprocessor seam, the enabl point is the \ {def testing}
what is an effect way to break mani depend with a third parti librari by use a linker seam and (dynam linking) ad a fake librari of the same name earlier in the \ {classpath} or (static linked) by creat a fake librari and alter the test environ compil script to link against the fake librari instead of the real one
what is a linker seam a call to an extern librari which can be intercept by creat a fake librari of the same name and chang the classpath of the linker (dynam linking) or by creat a fake librari and chang the test environ build script to link against it.
what is a danger in use a link seam the enabl point is outsid the program text so it can easili be miss if it not made obvious.
how can call to a \ {privat static} method easili be intercept by relax it to \ {protected} and overrid it in a subclass.
what the enabl point of a object seam the place where you choos between creat a fake object or a real one
what are the problem with preprocess seam they'r not as explicit as object seam test depend on them can be hard to maintain
what are the problem with linker seam they'r not as explicit as object seam test depend on them can be hard to maintain
what is refactor chang the structur of softwar with the purpos of make it easier to understand and cheaper to modifi without chang it behaviour.
give a general outlin of how xunit and it ilk work internally. given a test class contain a setup method, some test methods, and a teardown method xunit use reflect to identifi the test method for each test method, it creat a standard test class contain ani variabl declar in the class, the setup method, the test method in question, and the teardown method. the test class run the setup method, then the test method, then the tear down method.
what is the fit test har the framework for integr test each test function is associ with a html tabl that describ input to and the expect output of the test function. run fit return a tabl say if expect case were satisfied.
what is fitness a wiki front-end for fit, a html-tabl base test harness.
when is the sprout method appropri when a chang can be implement as a singl sequenc of statement at one place in a method.
how is the sprout method carri out insert a call at the requir place to the new method, but comment it out. identifi the local variabl need for the new method it do it job, and make those the arguments. add them to the commented-out method. determin whether a return is needed, and add the assign to the commented-out method. implement the method use tdd. remov the comment.
how can you appli the sprout method to a class with heavi initi depend make it a public static method and pass in ani necessari instanc variables. can then write the test for it without instanti the class.
if you'r appli the sprout method mani time to a class with heavi initi dependencies, what can be done to simplifi matter if the public static sprout method share mani arguments, they can be farm off to a new class and the argument can be pass via the new class constructor.
what are the advantag of the sprout method there a clean interfac between new, under-test code and old code.
what are the disadvantag of the sprout method you'r effect give up on get the whole class under test. it might not be clear to the next dev whi that code farm off to it own function.
describ test-driven development. assum the class is alreadi under test: write a test case that describ desir behaviour. let it fail. get it to compile. write the code neccessari to make it pass. remov duplication. repeat.
what is the use case for a sprout class when it isn't possibl to initi a class in a test har so that a method can be sprouted.
describ the sprout class process. insert the class instanti into the requir place in the code, along with the method call. comment both out. determin the local variabl the method will need, and make them argument to the class constructor. determin whether a valu will need to be returned, and add a (comment out) assign to do that if necessary. write the sprout class use tdd uncom the class instanti and method calls.
what are the disadvantag of sprout class conceptu complexity. it make it harder for subsequ dev to work out how class fit together.
when is a wrap method appropri when new function need to be ad to a method simpli becaus it need to run at the same time as that method. no logic need be shared.
describ the process of implement a wrap method. identifi the method that need to change. renam the method, and creat a new wrap method with the origin name and signature. call the old method from within the wrap method. develop a method for the new feature, then call it from the wrap method too.
when is a wrap class appropri when the behaviour that need ad to a class is independ of the current content of the class
what is the decor pattern given a class \ {component}, creat a decor class \ {decorator} which extend \ {component} and which as a \ {component} field internally. forward ani call to \ {component} method to it internally-held \ {component}. then creat a subclass \ {concretedecorator} of \ {decorator} to add functionality, possibl still forward call to the intern \ {component}s.
what is a wrap class a subclass that hold an instanc of it parent class intern and which forward call to it own method to the parent instanc methods.
what is program by differ creat a fail test for a new featur of a class \ {parent}. deriv a new class \ {child} make the test instanti an instanc of \ {child} rather than \ {parent}. it should still fail. implement the new featur in \ {child}, overrid whatev method are needed. when the test passes, can refactor \ {child} back into \ {parent} until everyth in \ {child} can be comment out. if the test fails, you broke something. when \ {child} is fulli comment out, delet it and switch the test back to instanti \ {parent}.
what the differ between method hide and method overrid if you instanti a subclass and cast it to the parent class, call to the instanc will make use of overrid (\ {override}) in the subclass. will not make use of hidden (\ {new}) method in the subclass.
what is the singl respons principl a class should have a singl responsibility: it should have a singl purpos in the system, and there should onli be one reason to chang it.
what is the liskov substitut principl subclass should be abl to substitut for their parent throughout the code without caus problems.
what are the rule of thumb for avoid lsp violat don't overrid concret methods. and if you must, tri to call the base method from insid the deriv method.
what a nomal class hierarchi one where no class overrid a concret method of it superclass
what the advantag of have a normal class hierarchi you can divin which implement of a method a class is use by just look in the class. becaus either it defin an overrid implement itself, or it hasn't and it use the same implement all of it ancestor are using.
what are the four most common problem with get a class into a test har object of the class can't be creat easili the test har won't easili build with the class in it. the constructor has bad side effect the constructor doe a lot of work and you need to sens it.
what an easi way to work out which argument a constructor call in a test case requir pass \ {null} as each constructor argument and see what complains. don't do this in unmanag memori (c, c++)
what is the null object pattern a way to avoid use \ {null}. if a request usual return an \ {object}, creat a subclass \ {nullobject} with empti method and fields.
what'r the problem with the null object pattern null object still exist as object - count the number of \ {objects} in a collect without filter out \ {nullobjects} first could return an inaccur count.
describ three way of deal with irrit paramet when tri to get a class under test. extract an interfac and pass a fake object to the constructor. pass \ {null} to the paramet if it unneed by the code under test (on in pointer-saf code). subclass the irrit paramet type and overrid the part of it that make it irritating.
when put a class under test, what is an irrit paramet a paramet to the constructor or setup method that is difficult to instanti in a test environment.
what is parameter a constructor if an object is be creat in a constructor, creat a test version by pass it as an argument to the constructor instead.
when is it use to parameter a constructor when you want to sens through a item initi insid the constructor
describ the process of parameter a constructor. make a copi of the constructor add the requir paramet rewrit the origin constructor to forward it call to the new constructor, with the addit paramet `fill in
what are the disadvantag of parameter a constructor anyon els can use the constructor too. this isn't usual much of a problem though.
what is supersed an instanc variabl if an instanc variabl \ {xxx} that need to be replac is creat in a constructor add a method \ {supersede{xxx}(parameter)} that destroy (!!!) the old variabl content and replac it with \ {parameter}
when is it appropri to supersed an instanc variabl when it awkward to parameter the constructor when virtual method can't be call from the constructor, stop you from extract amp; overrid a factori method
what are the disadvantag of supersed instanc variabl it break encapsulation. someon els might use it. fortun \ {supercedexxx} is an unusu enough name that it easi to recognis destroy refer type can caus troubl if the destructor doe much work.
what the best way to deal with hard-cod initi work in a constructor you'r tri to sens extract it to a virtual factori method and then overrid it in a test subclass.
what'r the problem with extract amp; overrid factori method it requir the languag allow call to virtual method from constructor it can caus problem if someon els decid to overrid the virtual method in product
how can you sens a variabl initializ in the constructor in c++ by extract and overrid the getter for it.
what'r better altern to extract amp; overrid a getter extract amp; overrid a factori method if the languag allow it extract amp; overrid a call if it just one method use the object that problemat
what'r the problem with extract and overrid a getter you have to be care to destroy the test instanc in the same way product code destroy the instanc someon might tri to access the extract variabl befor it initialized.
how do you extract and overrid a getter creat a lazi getter for the constructor-initi variabl which creat the neccessari object on first call. then replac all use of the variabl with use of the getter. and initi the refer that hold the object to null in all constructors. then subclass and overrid the getter.
what is the singleton pattern a pattern use to ensur there onli ever one instanc of a class. it has privat constructors. and a static variabl that hold the onli instanc of the class, and static method with name like \ {instance} are use to provid access to the class.
how do you deal with singleton depend when tri to get a class under test, assum they have access constructor add a \ {settestinginstance(newinstance)} static method that allow the test to replac the singleton instanc or add a \ {resetfortesting()} method that set the singleton instanc to null (prefer if there is a easi way of set the instanc up in a suitabl state)
how do you deal with singleton depend when tri to get a class under test if they don't have access constructor either: add a static setter, make the constructor protected, then subclass the singleton and creat a fake or extract an interface, swap all refer to the singleton out with interfac references, then pass in a fake.
how do you extract the implement of a class make a copi of \ {object} sourc call \ {productionobject}. turn the sourc class into an abstract interface. remov ani now-unneccessari import and includes. make \ {productionobject} implement \ {object} replac ani instanti of \ {objects} in the rest of the system with instanti of \ {productionobjects}.
how can you sens on an object a method construct intern parameter the method and pass in a fake.
how do you parameter a method make a copi of the method. add a paramet to the method for the object you want to sense. replac the bodi of the origin method with the origin construct of the object, and then forward call to the parameter method.
what the problem with parameter a method it break encapsulation. this can be minim by pick a suitabl \ {test}-i name.
what an altern to parameter a method if it not too much work extract and overrid a factori method instead.
how can you deal with a larg depend graph when develop test for c++ creat a header \ {fakes.h} which contain a collect of overrid for method that'd otherwis requir mani \ { include}s. then creat a separ compil script to avoid ani issu with the same function be defin more than once.
what the problem with use \ {fakes.h} file in c++ the duplic definit have to be maintain along with the rest of the program it doesn't help break depend in the main program.
what is the total pressur gradient forc on an infinitesim parcel of air $$\mathbf{f} = -\frac{m}{\rho} \nabla p$$
what doe $\mu$ repres in meteorolog the dynam viscos coeffici
what is the dynam viscos coeffici if a fluid with dynam viscos $\mu$ is place between two plate with seper $d$, and one plate is push sideway with a shear stress of 1pa, the top plate move a distanc $\mu d$ in one second.
what is the shear stress in the direct of $x$ across an infinitesim layer $\delta z$ $$\tau_{zx} = \mu \frac{ u}{ z}$$
what is the kinemat viscos coeffici $$\nu = \frac{\mu}{\rho}$$
in meteorology, what is the molecular viscos of a fluid $$\mbf{f}_{r} = \nu (\delta u, \delta v, \delta w)$$
what doe $\nu$ repres in meterolog the kinemat viscos coefficient, $\nu = \mu / \rho$
what is the ula for centripet acceler $$\frac{d\mathbf{v}}{dt} = -\omega^2 \mbf{r}$$
what the length of a sider day 23hrs, 56m, 4s
in meterology, what is appar graviti the sum of graviti and centrifug force, defin as $\mathbf{g} = -g \mathbf{k} \mathbf{g}^* + \omega^2 \mathbf{r}$ where $\mbf{k}$ is parallel to vertical. this lead to $g = 9.81m/s$ everywher thank to the oblat shape of the earth
in meteorology, what is $\phi(z)$ the work need to rais a unit mass to height $z$ from sea level.
give the ula for the corioli effect. $$\frac{d\mbf{v}}{dt} = -2\omega \mbf{v}$$
what is the corioli paramet $$f = 2 \omega \phi$$
give the horizontally-restrict corioli effect in term of the corioli parameter. $$\frac{d\mbf{v}}{dt} = -f\mbf{k} \mbf{v}$$
what is the qualit of the corioli effect trajectori are deflect right in the northern hemispher and left in the southern
what doe $\alpha$ denot in meteorolog specif volume, the reciproc of densiti
what is the equat of state for dri air $$p = \rho r t$$
what is the gas constant for dri air $$r = 287\,j\cdot kg^{-1} k^{-1}$$
what is the equat of hydrostat balanc $$\frac{dp}{dz} = -\rho g$$
what is the hypsometr equat $$\phi(z_1) - \phi(z_2) = r t^{p_1}_{p_2} t d \ln p$$
what is the geopotenti height $z = \frac{1}{g_0} \phi(z)$, where $g_0$ is the global averag of graviti as mean sea level
what the transliter of ``hypsometric' ``height measure'
what is the scale height in meteorolog the distanc over which the atmospher pressur chang by a factor of $e$
what is the thick of an atmospher layer between two pressur surfac $p_1, p_2$ in term of the layer mean scale height $$z_t = h \ln(p_1/p_2)$$
what is the layer mean scale height $$h = \frac{1}{g_0} r \langl t \rangle$$
how is the layer mean temperatur between two pressur surfac $p_1, p_2$ calcul $$\langl t \rangl = t^{p_1}_{p_2} t d \ln p \left[ t^{p_1}_{p_2} d \ln p \right]^{-1}$$
how doe the deriv of pressur along the $x$ axi correspond to the deriv of pressur along the $z$ axi $$\left(\frac{ p}{ x}\right)_z = -\left(\frac{ p}{ z}\right)_x\left(\frac{ z}{ x}\right)_p$$
how is the horizont pressur gradient measur in an isobar coordin system by the gradient of the geopotenti at constant pressure: $-\frac{1}{\rho} \left(\frac{ p}{ x}\right)_z = -\left(\frac{ \phi}{ x}\right)_p$ $-\frac{1}{\rho} \left(\frac{ p}{ y}\right)_z = -\left(\frac{ \phi}{ y}\right)_p$
what is the $\sigma$ coordin system a vertic axi parameter as multipl of the pressur at the ground
given a vertic coordin $s$ that a monoton function of height, how can the chang in pressur along the $x$ axi at a surfac of constant $s$ be calcul $$\left(\frac{ p}{ x}\right)_ = \frac{ p}{ z} \left(\frac{ z}{ x}\right)_ + \left(\frac{ p}{ x}\right)_z$$
what the differ between the total and the local deriv the total deriv is the rate of chang of a field variabl when you follow the motion of the fluid the local deriv is the rate of chang of a field variabl at a fix point.
what the differ between a eularian and lagrangian frame of refer in a eularian frame, the control volum is a parallelpip whose posit is fix relat to the coord axe in a lagrangian frame, the control volum consist of an infinitesim mass of ``tagged' fluid particles.
for the field variabl $t$, what is the relat between the total and local deriv $$\frac{dt}{d t} = \frac{ t}{ t} + \mbf{u} \cdot \nabla t$$
what are other name for the total deriv the substanti deriv or the materi deriv
what is the advect term for a field variabl $t$ $$- \mbf{u} \cdot \nabla t$$
in fluid dynamics, what doe it mean to say that a field variabl is conserv that the total deriv of the variabl is zero; the local chang is entir due to advection.
what is the relationship between the total deriv of a vector in an inerti frame and one in a rotat frame $\frac{d_a \mbf{a}}{d t} = \frac{d \mbf{a}}{d t} + \mbf{\omega} \mbf{a}$, where $d_a$ is the deriv with respect to the inerti frame.
what is the relationship between a veloc vector in an inerti frame and one in a rotat refer frame $\frac{d_a \mbf{u_a}}{d t} = \frac{d \mbf{u}}{d t} + 2 \mbf{\omega} \mbf{u} - \omega^2 \mbf{r}$, where $\mbf{r}$ is the vector perpendicular to the axi and with magnitud equal to the distanc to the axi of rotat
what is the vector momentum equat when appli to the pressur gradient, gravit and friction act on a parcel of the atmospher use a rotat refer frame. $$\frac{d \mbf{u}}{d t} = - 2 \mbf{\omega} \mbf{u} - \frac{1}{\rho} \nabla p + \mbf{g} + \mbf{f}_r$$
what is the equat for the total deriv of $u$ when treat in a spheric rotat coordin system $\frac{d u}{d t} -\frac{uv \phi}{a} + \frac{uw}{a} = - \frac{1}{\rho} \frac{ p}{ x} + 2\omega (v \phi - w \phi) + f_{rx}$
what is the geostroph relationship $-fv -\frac{1}{\rho} \frac{ p}{ x}$ $fu -\frac{1}{\rho} \frac{ p}{ y}$
qualitatively, what is the geostroph wind the horizont veloc field that satisfi the geostroph relationship exact
give the ula for the geostroph wind. $$\mbf{v}_g \mbf{k} \frac{1}{\rho f} \nabla p$$
under what condit should the geostroph wind be use as an approxim onli for large-scal motion away from the equator.
how is the geostroph relationship obtain by ignor all but the pressur gradient and corioli term of the horizont equat of momentum.
what are the predict equat deriv under the geostroph approxim $\frac{du}{dt} = fv - \frac{1}{\rho} \frac{ p}{ x}$ $\frac{dv}{dt} = -fu - \frac{1}{\rho} \frac{ p}{ y}$
what the problem with use the predict equat deriv from the geostroph approxim for actual predict their rhss are take the differ between two acceler term with veri similar values, ing in a lhs that about an order of magnitud smaller than them both. slight error in initi can lead to dramat differ s.
what is the rossbi number the ratio of the characterist scale of the acceleration, $v \cdot \nabla v u^2/l$ to the characterist scale of the corioli force, $\omega v f u$.
what are the characterist scale for synopt motion $u 10m/s$, the horizont veloc scale $w 1cm/s$, the vertic veloc scale $l 1,000km$, the length scale $h 10km$, the depth scale $\delta p / \rho 1,000m^2s^{-2}$, the horizont pressur fluctuat scale $l/u 1 \mbox{day}$, the time scale
what are some featur of interest in synopt scale meteorlog extratrop cyclones, baroclin trough and ridges, frontal zones, jet stream
what is the tropospher the lowest part of the atmosphere, contain about 80 of it mass and is about $ $17km thick at the mid-latitud
what can be assum about the vertic compon of synoptic-scal motion that it can be veri well approxim by a pressur field in hydrostat equilibrium.
what is the mass diverg of the continu equat $$\frac{ \rho}{ t} + \mbf{\nabla \cdot} (\rho \mbf{u}) = 0$$
what is the veloc diverg of the continu equat $$\frac{1}{\rho} \frac{d\rho}{d t} + \nabla \cdot \mbf{u} = 0$$
qualitatively, what is the continu equat the express of conserv of mass for a fluid.
how can the vertic structur of the tropospher be character when under hydrostat equilibrium by $\frac{1}{\rho_0} \frac{d p_0}{dz} -g$, where $p_0(z)$ and $\rho_0(z)$ are the averag pressur and densiti at each height
at the synopt scale, how can the continu equat be approxim $$\nabla \cdot (\rho_0 \mbf{u}) = 0$$
what is an adiabat process a transfer of energi as work without a transfer of heat between the system and it surroundings.
what is the mechan energi equat $$\rho \frac{d}{dt} \left( \frac{1}{2} \mbf{u \cdot u} + \phi \right) = - \mbf{u \cdot \nabla} p$$
what is the thermodynam energi equat $c_\nu \frac{d t}{d t} + p \frac{d \alpha }{d t} = j$, where $c_\nu$ is the specif heat at constant volum of air, and $j$ is the rate of heat per unit mass
where in the equat of atmospher dynam doe solar heat contribut energi to the system via the $p \frac{d \alpha}{d t}$ term of the thermodynam energi equation, which repres the rate of work of the fluid system per unit mass.
what is poisson equat $\theta = t (p_s/p)^{r/c_p}$, where $p_s$ is a standard pressur and $c_p$ is the specif heat at constant pressur of dri air.
what doe $\theta$ repres in meterolog the potenti temperature, the temperatur a parcel of dri air would have if it were adiabat expanded/compress to a standard pressur $p_s$.
what is the restrict on the movement of a parcel of fluid that conserv entropi it must move along an isentrop (constant $\theta$) surface.
what is the entropi of the first law of thermodynam $$c_p \frac{d \ln t}{dt} - r \frac{d \ln p}{d t} = \frac{j}{t} = \frac{ds}{dt}$$ where $c_p$ is the specif heat of the fluid at constant pressure, $j$ is the rate of extern heating, and $s$ is the entropi of the parcel of fluid in question
how is the chang in potenti temperatur relat to the chang in entropi of a parcel of fluid $$c_p \frac{d \ln \theta}{d t} = \frac{ds}{dt}$$
what is the dri adiabat laps rate of temperatur the expect rate of decreas in temperatur with respect to height when potenti temperatur is constant with respect to height. $$\gamma_d \frac{g}{c_p} = -\frac{dt}{dz}$$
what is the stabil criterion for dri air if $\frac{d\theta_0}{dz} 0$, a parcel is static stabl and will oscil around it level. if $\frac{d\theta_0}{dz} =0$, a parcel is static neutral if $\frac{d\theta_0}{dz} 0$, a parcel is static unstabl and displac will increas exponenti with time.
how is the environment potenti temperatur defin $$\left( \frac{\rho_0 - \rho}{\rho} \right) = \frac{\theta}{\theta_0}$$
how is the first law of thermodynam estim if a parcel of fluid is not undergo strong diabet heat $$\left( \frac{d\theta}{dt} + u \frac{d\theta}{dx} + v \frac{d\theta}{dy} \right) - w \frac{d\theta_0}{dw}$$
what doe the gas constant repres it relat the energi scale to the temperatur scale when one mol of ideal gas is be considered.
what is the equat for the total deriv of $v$ when treat in a spheric rotat coordin system $$\frac{d v}{d t} + \frac{u^2 \phi}{a} + \frac{vw}{a} = - \frac{1}{\rho} \frac{ p}{ y} - 2\omega u \phi + f_{rz}$$
what is the equat for the total deriv of $w$ when treat in a spheric rotat coordin system $$\frac{d w}{d t} - \frac{u^2 + v^2}{a} \qquad \quad = - \frac{1}{\rho} \frac{ p}{ z} - g + 2\omega u \phi + f_{rz}$$
what is the courant number of a simul $$\frac{u\delta t}{\delta x}$$ where $u$ is the fastest wave on the grid and $\delta t, \delta x$ are the resolutions.
what is the courant-friedrich-lewi criterion a neccesari condit for the advect term in a eulerian framework to be stabl is that the courant number is less than 1.
how mani gridpoint are need to reason resolv a wave approxim 10 per wavelength.
what is the optim relationship between vertic grid increment and horizont grid increment in an atmospher model $$\delta z_{opt} = s\delta y$$ where $s$ is the frontal slope
what are the six visibl notat avail for a class member in uml $+$, public $-$, privat $ $, protect $/$, deriv $\_$, static $ $, packag
how is scope indic on a uml class diagram classifi member (static members) are underlin instanc member are not
how are the various instance-level link in uml indic bi-directional/uni-directional/reflex associ (plain line) aggreg (hollow diamond on the contain class) composit aggreg (solid diamond on the contain class)
what is the differ between a composit and an aggreg link in uml aggreg repres a `has a' associ composit repres a `own a' association. destroy the owner should destroy the own instances.
how are the various class-level relationship denot in uml general (hollow arrow on solid line) realize (hollow arrow on dot line)
what is the differ between general and realize in uml general repres a `is a' relationship realize repres a `implement relationship
describ three high-level perspect to consid object orient code from the conceptu level - what the respons of an object are the specif level - what the object look like from the outsid the implement level - how the object is implement
what are the three rule given by the gang of four design to interfac favour composit over inherit find what vari and encapsul it.
in short, what is the intent of the facad pattern to simplifi the use of an exist system.
what are the consequ of the facad design pattern it simplifi the use of a system, but may hide some function in do so.
what the relationship between the conceptu and specif perspect of oo design commonality: the specif identifi the interfac need to captur all the case of a concept
what the relationship between the specif and implement perspect on oo design variation: the implement identifi the various case of the specifi interfac
what is the problem solv by the bridg pattern the deriv of an abstract class must use multipl implement without caus an explos in the number of classes.
what are the particip and collabor of the bridg pattern abstraction: defin the interfac for the object be implement implementor: defin the interfac for specif implement class class deriv from abstract use class deriv from implementor without know which concreteimplementor is be use
how are case statement a code smell they might indic misplac respons that could be factor out in favour of polymorph
what is the problem that the abstract factori pattern solv famili of relat object need to be instanti
what are the particip and collabor of the abstractfactori abstractfactory: the collect of interfac for how to creat each member of the famili of object required. concretefactory: the implement factori for a famili of object
what are the rule for design pattern applic appli one at a time context first, ie top-down
what is the open-clos principl that softwar compon should be open for extension, but close for modif
what a better altern to the question "which implement is better " "under what circumst would each of the possibl altern be optim " "which of those circumst is most like my problem domain "
what is the problem solv by the strategi pattern the algorithm to be appli is depend on which client is make the request or what data is be act on
what are the particip and collabor of the strategi pattern strategy: the interfac to the various algorithm concretestrategy: the implement of the various algorithms, implement strategy. context: hold a refer of type strategi and interpret call from the client into call to the strategy. client: make call to the context
what the problem solv by the decor pattern when you have an object which fulfil some basic functionality, but it need to be extend in a varieti of ways.
what are the particip of the decor pattern component: the abstract class from which concretecompon and decor deriv from. has an operation() method. concretecomponent: the class be decor decorator: the abstract class from which concretedecor derive. hold a refer to a component. concretedecorator: a subclass of decor which conduct some work, make a call to the operation() of compon it holds, then doe some more work.
what problem doe the singleton pattern solv sever differ client refer to the same object, and you want to ensur it is onli instanti once.
how is the singleton pattern implement add a static properti to the class that hold an instanc of the class. add a public method that check whether the properti is null, and if it is instanti a new object and store it there. set the constructor to private.
what is the double-check lock pattern a variat on the singleton for use in multithread applications.
what problem doe the double-check lock pattern solv the race condit that can emerg when two thread instanti a singleton.
what are the gang of four three categori of pattern structur behaviour creation
what are the gang of four five creation pattern abstract factori builder factori method prototyp singleton
what are the gang of four seven structur pattern adapt bridg composit decor facad flyweight proxi
what are the gang of four eleven structur pattern chain of respons command interpret iter mediat memento observ state strategi templat method visitor
what doe solid stand for singl respons open/clos liskov substitut interfac segreg depend invers
what is the problem solv by the observ pattern you want to notifi a vari list of object of an event.
what are the particip in the observ pattern subject: abstract class with method to let an observ register, deregister, and a notify() method that call the update() method of each regist observ concretesubject: implement of subject observer: abstract class with an update() method. concreteobserver: implement of observ
how can preexist object be ad as observ in the observ pattern by use an adapter.
which parti should filter extran notif in the observ pattern the subject.
how can extran notif be filter in the observ pattern use the strategi pattern when added, each observ pass to the subject a strategi on how to filter notif to them.
how can the in ation pass to an observ in the observ pattern be specialis use the strategi pattern. each observ pass to the subject a strategi concern which in ation to pass to it.
what the differ between a templat method and a collect of strategi the process in the templat method are conceptu similar
what is the problem solv by the templat method pattern there is a procedur that is consist at one level of detail, but individu step vari between implement at a lower level of detail.
what are the particip in the templat method pattern abstractclass: might not actual be abstract, contain a skeleton of the algorithm with call to virtual method in the same class. concreteclass: deriv from abstractclass, overrid the virtual method to produc differ behaviour
what the problem solv by the factori method pattern a class need to instanti an interface, but the implement varies.
what'r the particip in the factori method pattern product: the interfac to be instanti concreteproduct: an implement of product creator: the interfac contain a virtual factorymethod() concretecreator: an implement of creator that overrid factorymethod() to return a concreteproduct
how is shalloway analysi matrix structur each row describ a rule each column describ a case
what doe dci stand for data, context and interact
in what seven way doe lean architectur differ from classic softwar architectur defer engin accomod chang defer implement lightweight document people-focus collect plan end-us mental model
what are the valu of the agil manifesto individu and interact over process and tool work softwar over comprehens document custom collabor over contract negoti respond to chang over follow a plan
what the valu chain the chain of iti a firm per s in order to tran it input into a valuabl product
what situat are agil approach maladapt for those where there littl valuabl user feedback ie when develop a protocol to a al specif or build a library, where there no short feedback loop
what the differ between a complic and a complex system complic systems: onli known unknowns, can reli on fact-bas manag complex systems: have unknown unknowns, no predict path from current state to better state
what situat are lean architectur maladapt for simpl system with no unknowns, for which lean architectur is overkil chaotic system without (m)ani pattern enough to make assumpt off of
what'r the fundament mismatch between lean and agil methodolog lean emphas throughput over latenc lean emphas processes; agil emphas people. lean emphas bring decis forward; agil emphas defer them
what the lean architectur rule of thumb everybody, all together, earli on
what are the two purpos of document as a tutori for peopl who weren't there when a decis was made as a remind for those who were
fundamentally, what is scrum an agil framework for the manag side of develop
give a good definit of `problem'. the differ between the current state and the desir state
what binari divis of a system doe lean architectur propos what the system is: the system architecture; the stabl structur of the busi over time; how the user think what the system does: the system functionality; what the user do
who are the five key stakehold in the develop of softwar end user the busi custom domain expert develop
what metric do end user evalu softwar against whether it doe what they expect it to
what is the end user cognit model the end user percept of the system .
what doe dft stand for in the context of softwar design for testabl
what an autopoeiet system a self-organ system.
what doe a bug discov in the field cost to fix vs one in requir review 70 time more
what are the five properti of a good problem definit it is written down and share it is a differ between a current state and a desir state of an organ it achiev is measur at some mutual understood point in time. it is one or two sentanc in simpl languag it is intern consistent.
what'r the properti of a wick problem the problem is not understood until after the ulat of a solut they don't have a stop rule solut are not right or wrong everi wick problem is novel and uniqu everi solut is a one-shot there are no altern solut
in agil methodology, what the differ between an object and a goal an object is a waypoint that must be met. failur is indic that the process need to be improved. a goal is the desir endpoint in the best of all possibl worlds.
with respect to softwar architecture, what is form is the essenc of structure, without the structur itself.
whi doe top-down design break down for complex system becaus complex system have mani top
what is the usual divid line between code that is stabl and code that chang frequent stabl code: what the system is unstabl code: what the system doe
how should expect fluctuat in code affect architectur the compon of an architectur should be seper by their differ rate of chang
when design a system architecture, what interpret should be focus on the essenc of the system , ie what the system \emph{is} don't worri about what the system \emph{does}
how should a system be partit when develop an architectur in whichev way lead to subsystem that are as autonom as possibl
what should drive the choic of architectur partit of a system human considerations. softwar engin consider are secondary.
with respect to softwar architecture, what is a domain a busi area of focus/interest/study/specialization. an area for which a bodi of knowledg (possibl tacit) exists.
how are domain of interest to the partit of a system each domain s a "top" of the system and shouldn't be split across architectu units, and certain not across geograph locations.
how do domain influenc softwar product variant they common design set of close relat product in a way that support code reus
what conway law that softwar architectur tend to reflect the organiz structur of the team that built it.
how are user mental model of the of a system reflect in architectur through modules, which are the partit of the subsystem
what are the most import consider when establish the modul in a partit they correspond to the end-us cognit model each modul is as independ as possibl each modul should be cohes each modul should correspond to a specif domain (expert)
which is the most import consider in the modul partit of an architectur domain knowledge. domain expert have acquir a knowledg of what stay constant and what vari
in the absenc of domain knowledge, what the most import consider when creat the modul partit of an architectur that they follow the end user cognit model.
what doe common mean in the context of softwar architectur the invari properti across concurr option and also over time
what the differ between general and abstract in describ a softwar architectur abstract ignor certain attribut in favour of other general ignor the attribut that can be infer from context
what are the five common categori of softwar compon in von neumann languag behaviour (like move or rotate) algorithm (like the alg for move or rotating) state data structur type (in the program sense)
what a von neumann program languag one which at a high level is isomorph to a von neumann architectur
what are common exampl of a von neumann languag most everi procedur or oo language.
how is softwar architectur best repres in the most compressed, general possibl given the context and domain dictionari at hand but noth should be abstracted, which might throw away import in ation
how should the program paradigm for implement an architectur be select choos the one whose common and variat best match that of the domain model/end user model
when is the object-ori paradigm best suit for implement an architectur when variat of the system over time will like requir chang to algorithms, while the data structur stay constant.
what are the famili within a domain of a system architectur a set of element in the domain which have some properti in common and some paramet that character how they vary.
how are object-shap domain of a system architectur usual repres as an abstract base class, with a small set of support type or constant
when construct the domain class associ with a system , how much logic should they contain veri little. they should be as dumb as possible.
how are procedur domain of a system architectur usual repres with procedur declar
how are generat domain of a system architectur usual repres with templat or generic declar
in the concret base class of the architectur of a system , how can the intent of the api be effect communic with assert or (even better) compile-tim assert
what architectur test imagin way in which the architectur might have to chang over time, and evalu how pain implement those chang would be
what a rule of thumb for evalu the cost of a chang count the number of architectur interfac that'll have to chang to accomod it
when doe architectur test indic the architectur need to chang when it identifi a larg set of expens chang that would be consid easi from a busi or end user perspect
how mani use case can singl system handl at most $ $240
what'r the respons of the model in mvc updat data at the request of command from the control notifi appropri view when it state chang can be ask to register/de-regist view
what are the respons of a view in mvc present a particular represent of the data can ask the model for the current valu of the data can handl it own input togeth with the controller, handl select
what are the respons of the control in mvc creat and manag view handl select across view pass command to the model handl command that appli to the entir system
what are the four compon of dci represent of a system domain class object method role methodless role (identifiers)
which part of the dci architectur repres a system behaviour method and methodless role
which part of the dci architectur repres a system class and object
what part of a use case do methodless role follow from habit
in dci, what a trait a holder of stateless method
how are dci context deriv from use case each use case has a correspond context
what is the job of the context in dci to it identifi to match objects, then to call the first method of the first object role
what are the respons of a dci context in the execut of a use case to identifi the object involv in a use case invoc to associ these object with their appropri object role to start enact when the trigger is call to publish interfac ing for the method object role
in mvc, where do dci domain object resid in the model
in mvc, where do dci context resid most in the control
in dci, what an environ the code that initi the enact of a use case
in dci, what the job of the context accessor to stack execut context
what are the five rule to write context object in dci creat a new context class for each use case context class should have a parameterless constructor context class should have a run method, or have a convent that call the constructor run the use case context class should publish identifi (pointers) for all object role involv in the use case identifi should be type as methodless object role type
what are the four way to pass around context in dci pass the individu object role ing to the method in the method object role pass the context to the method role interfac of each domain object pass the context object as an argument to each method in the method object role let method object role access a global context object which manag access to other role
what the best algorithm for the longest common subsequ between two permut the robinson-schensted-knuth algorithm
given a 0-1 array $a$, what the best way to identifi whether there a integ $k$ such that the $k$th row is all zero, and the $k$th column is all 1 (except for $a[k,k]$ start with a list $1, 2, \cdots, n$ and at each step pull the first two element out (say $i, j$) and test whether $a[i, j]$ is 1. if it is, delet $i$, if it isn't, delet $j$.
how do you generat uni ly random point on a sphere by rememb that the area of a slice of a sphere between two parallel plane is depend onli on the distanc between the planes.
what is the stout-warren algorithm an in-plac linear-tim algorithm for balanc a binari tree
what the best way to balanc a binari tree the stout-warren algorithm
how would the fact ``john like mary`` be written in prolog \ {likes(john, mary).}
in the prolog fact \ {likes(john, mary).}, what are the name of the compon \ {likes} is a predic with two objects, \ {john, mary} as it argument
what a databas in prolog a collect of fact and rule for a particular problem
how might the question ``doe mari own the book '' be phrase in prolog \ { - owns(mary, book).}
what doe it mean for a fact in prolog to unifi a question the fact has the same predic and argument as the question
when will a prolog question return \ {yes} when there is a unifi fact in databas for that question.
what the name rule for prolog variabl a variabl must begin with a capit letter.
what is an instanti prolog variabl a variabl is instanti when there an object the variabl stand for
how doe prolog instanti a variabl in a question it search the databas for the first unifi fact (in input order) and then pattern-match the variabl against the fact argument and mark it place in the database.
in inter e mode, how can you request prolog move to the next instanti of a variabl colon, then enter.
in inter e prolog, how can you indic you'r satisfi with the current instanti of variabl hit enter.
how is a conjunct describ in prolog with a comma: \ { - likes(john, mary), likes(mary, john)}
how doe prolog attempt to satisfi conjunct by backtracking: it find the first unifi fact for the first goal of the question, then continu it search from that point if the second goal isnt also satisfied, etc.
how might the rule ``john like anyon who like wine`` be written in prolog \ {likes(john, x) :- likes(x, wine).}
what are the compon of a prolog rule the head, which come befor the \ {:-}, describ what the rule is intend to define. the body, which come after the \ {:-}, describ the conjunct of goal that must be satisfi in order for the head to be true.
what the name for compund name in prolog use underscor and lower case.
what a claus in prolog either a fact or a rule.
how are comment written in prolog with \ {/*...*/}
what are the four `primat type support by json string number boolean null
what are the two compound type support by json array, an order sequenc of valu object, an unord collect of name/valu pairs.
what type are the name of an object in json strings.
what is glob the type of pattern match tradit use by the unix shell
what are the five featur common to most glob syntax \ { }, match a singl unknown charact \ {*}, match ani number of unknown charact \ {[abc]}, match ani contain charact \ {[!a]} match all but the contain charact \ {\textbackslash *}, match the escap charact
how is the program \ {egrep} usual invok \ {egrep pattern filename}
what is \ {egrep} the \ {grep} patternmatch invok with the \ {-e} switch to enabl extend regular express
how can the begin of a line be denot in egrep \ {\^{}cat}
how can the end of a line be denot in an egrep pattern \ {cat }
what doe egrep use to match exact one of a class of charact \ {[ea]}
how can a rang of possibl match charact be denot in an egrep pattern \ {[a-c]}
how can multipl rang of possibl charact match be denot in an egrep pattern \ {[1-5b-f]}
how can you denot a set of charact that shouldn't be match in an egrep pattern \ {[\^{}ea]}
what doe \ {[\^{}ea]} mean in an egrep pattern that a charact other than \ {e, a} should be matched. this is differ from ``don't match e or a'
what differ about metacharact insid and outsid a charact class in an egrep pattern almost everything. it best to think as regex insid and outsid a charact class as differ languages.
how can you match ani singl charact in an egrep regex \ {.}
when is a dash in a charact class in an egrep regex not consid a metacharact when it appear first in the class, immedi after \ {[} or \ {[\^{}}
how doe egrep pattern match against newlin charact it encount it doesn't - it strip them out and consid the line in isolation. this is a peculiar of grep though, it isn't standard across all such tools.
how can you match regex over multipl line in linux use \ {pcregrep -m}. in contrast, \ {grep} consid line in isolation.
how can you match one of multipl possibl subpattern in an egrep regex \ {(pattern1 pattern2)}
how can you make egrep do a case-insensit match use the \ {-i} switch.
how can you match against word boundari in some version of egrep \ {\textbackslash cat\textbackslash }
how can you mark a charact as option in an egrep regex \ {colou r} mark \ {u} as option
how can you match one or more of a charact in a egrep regex \ {a+}
how can you match zero or more of a charact in a egrep regex \ {a*}
what are the quantif metacharact in a egrep regex \ {+} \ {*} \ { }
how can you match against a rang of repetit of a charact in some version of egrep \ {a\{1, 5\}}
how doe backreferenc work in egrep version that support it text match the $i$th parenthesis pattern in a regex can be match against later with \ {\textbackslash i} parenthesis express are count by open parenthes from the left
how can you escap a metacharact in an egrep regex if it outsid a charact class, prefix it with \ {\textbackslash} if it inside, you can't escap it, though this is specif to egrep.
how are the subexpress altern by a \ { } in an egrep regex delimit by the end of the string or by enclos parentheses.
what a network coordin system an assign of coordin to server such that the distanc between pair of coordin reflect the latenc (or some other use metric) between each pair of server
what the standard way of identifi the same client over a sequenc of interact by give the client a cooki dure the first interact
what nat network address translation, usual use to "hide" mani client ip address behind a singl gateway address
how can a distribut system local a client request to the nearest server ask the client to execut a network coordin algorithm to find it `distanc from a set of `landmark server then use the report in ation to calcul the client coordin and `nearest server. and have the dns cach it ip for some period (shorter = finer control)
what dns domain name system associ domain name with various in ation, most import ip address
sketch how domain name resolut works. each network host has a hint file of address of root name servers. a name queri is first made to a root name server; it will return the address of anoth name server which deal with the appropri top-level domain a name queri is then made to that name server; it will return the address of anoth name server which deal with the appropri second-level domain etc until you get the ip address sought.
how is the load on high-level name server disapp through the use of cach name servers, which will cach fetch dns record for a period of time.
what are the four layer of the ip suit applic layer (dns, http, etc) transport layer (tcp, udp, etc) internet layer (ipv4, ipv6, etc) link layer (arp, ndp, etc)
what the purpos of tcp to provid reliable, ordered, error-check deliveri of a stream of octets.
what the purpos of udp to send simpl messag (datagrams) quickly.
what is the internet protocol respons for address host and rout datagram
what doe tcp stand for transmiss control protocol
what doe udp stand for user datagram protocol
give twelv interpret of the idea of reliabl in the context of distribut systems. fault toler high avail continu avail recover consist scalabl secur privari correct specif correct implement predict per anc timeli
give eight kind of common failur in distribut system halt failur (process time out) fail-stop failur (process report it failed) send-omiss failur receive-omiss failur network failur (between specif pair of processes) network partit failur time failur byzantin failur (arbitrari failures)
how is reliabl of distribut system best judg by the experi of the end user
what doe acid stand for in the context of distribut system a set of properti that guarante that databas transact will be process reliabl atom - each transact is all or noth consist - ani transact will take the system from one valid state to anoth isol - appli transact concurr will achiv the same as appli them serial durabl - onc a transact is committed, it will remain so.
what doe base stand for in the context of distribut comput basic avail soft-stat servic with eventu consist ie, a highly-avail first-tier with some background servic to clean up consist
what is the cap theorem that no system can simultan be consist - all node see the same data at the same time avail - everi request reciev a respons partition-toler - the system function despit arbitrari messag loss
what import to rememb when compar cap and acid in distribut system acid consist mean that a transact preserv all databas rule cap consist mean that all node see the same data (single-copi consistency), which is a strict subset of acid consist
how doe tcp open a connect alic send a syn, with a segment sequenc number $a$ bob respond with a syn-ack, with acknowledg number $a+1$ and sequenc number $b$ alic respond with an ack with sequenc number $a+1$ and acknowledg number $b+1$.
how doe tcp close a connect alic send a fin bob send an ack bob send a fin alic send an ack. usual compress to a three-way handshak with a fin/ack.
what tcp flow control protocol a slide window: the reciev specifi in each segment the reciev window field, the amount of addit data it will to buffer. a reciev window of zero mean the sender will stop transmit data until reciev a new window or the persist timer run out. if the persist timer run out, the sender ask for a new reciev window.
what are the phase of tcp standard congest avoid algorithm slow start (exponenti increas in window size until loss occur) congest avoid (addit increase, multipl decreas each time loss occur)
sketch how soap works. a remot procedur call is encod in html and sent to the server which decod it, execut the call, and return anoth soap- at html page
what is soap in the context of web servic simpl object access protocol, an altern to rpc
how can a connect be maintain with a devic with a chang ip address by make the connect through a tunnel which maintain the illus of a static ip
how do you calcul the area of a planar triangl $a = (v_2 - v_0) (v_1 - v_0) /2$
what doe the bodi tag repres in html that everyth contain in it element should be ed in the browser window
what doe the head element repres in html that it element is in ation about the page (like the title)
what doe the titl tag specifi in html the text that should be ed in the browser bar/tab
how is an attribut store in a html tag \ { p lang="fr" } has the attribut \ {lang} with valu \ {fr}
what the paragraph tag in html \ { p }
how do you declar text to be a head in html use \ { h1 , h2 } etc, from most import (1) to least import (6).
what the top-level tag of a html document \ { html }
in web design, what a cms a content manag system, which usual exist to allow the user to edit compon of a websit effici
what structur and semant markup in html structur markup denot how the page should be laid out semant markup add in ation like where emphasi should be place in a sentanc
how are bold and ital indic in html \ { b } \ { i }
how are superscript and subscript denot in html \ { sup } \ { sub }
what whitespac collaps in html ani whitespac charact in the code is treat as a space (includ linebreaks!) two or more sequenti whitespac charact will be collaps to a singl space.
how can you insert a linebreak in html \ { br / }
what an empti element in html an open tag which is it own close tag, like \ { br / }
how is a horizont linebreak insert into a html document \ { hr / }
when is a horizont linebreak suitabl in a html document when there a chang in the theme or topic treat in the text
what are the standard emphasi tag in html \ { strong }, usual interpret as bold \ { em }, usual interpret as ital
what the tag for quot a paragraph in html \ { quot }
what the tag for an inlin quot in html \ { q }
how can you cite a quot in html use the \ {cite} attribute, with a url to the sourc as it valu
how can you emb an abbriv in html \ { abbr title="professor" prof /abbr } will usual professor on mouseov
what should be use for denot acronym in html5 the abbr tag, as the acronym tag is deprec
how should citat be mark up in html with the \ { cite } tag
how should a defin term (the first use of a term in the text) be mark up in html \ { dfn }
where should the contact in ation for the author of a html page go in the \ { address } element
what hcard a micro at for publish contact detail
how can you highlight newli insert and delet text in html \ { in } \ { del }
whi shouldn't you use the \ { u } tag in html becaus underlin text like that is deprec and should be done with s instead (like span or em)
what are the three type of list support by html order list (\ { ol } to open, \ { li } for each item) unord list (\ { ul } to open, \ { li } for each item) definit list (\ { dl } to open, \ { dt } for the term, \ { dd } for the definition)
how are link denot in html with a \ { a } tag which has a \ {href} attribut that has the url as it valu
when a url to a folder is passed, what do browser usual do tri to return the content of the index.html file in that folder
how do you mark up an email link in html \ { a href="mailto:aaa@bbb.com" }
how can you indic a link should be open in a new window in html use the \ {target="\_blank"} attribut in the link tag.
what should come with ani link that open a page in a new window text notifi the user the page will be open in a new window
how can you link between point in the same html document mark the destin tag with the attribut \ {id="name"} tag the link with \ {href=" name"}
how can you link to a specif tag in anoth page in html mark the destin with the \ {id="name"} attribut append \ { name} to the link to the page.
where should imag in a webpag come from place that give away/sel copyright to them, idiot.
what five guidelin should you follow when pick imag for a websit they should be relev they should convey some in ation they should convey the right mood they should be instant recognis they should fit the colour palett
what tag do you use to mark up imag in html \ { img }
what are the three most common attribut of html \ {img} tag \ {src}, which give the imag file locat \ {alt}, which is the text to be ed if the imag can't be load \ {title}, which give the mouseov text
what should go in the \ {alt} attribut of an imag in html an accur descript of the imag unless the imag has no in ation content (such as a divid line), in which case it should be the empti string
how do you indic the size of an imag in html use the \ {height, width} attribut in the \ {img} tag, with measur given in pixel
whi shouldn't you use the \ {height, width} attribut in an imag tag becaus this role is be taken over by css
what'r the rule about newlin and element vs inlin element in html block element alway start a new line inlin element insid a element do not start a new line
whi shouldn't you use the \ {align} tag in html becaus it been deprec in html5 in favour of css
what are the three rule regard adapt imag for websit use a web at (jpeg, gif or png) use the height and width the img tag specifi use the resolut you want them to be ed at
when should you use gif vs png vs jpeg use jpeg when there are mani colour in the image, or when smooth gradient are import (ie photos) use gif or png when there are few colours, or larg area with the exact same colour (ie diagram and drawings)
what the target pixel densiti for most websit 72ppi
when the svg at appropri vector graphic and fair modern browsers.
what web at support transpar png, svg and onli on modern browser (post-ie6)
how can you attach a caption to an imag in html with html \ { figur } tag contain one or more \ { img }s and a \ { figcapt }
how do older browser treat html5 tag they ignor them and simpli the content.
what are the three fundament tag of a html tabl \ { tabl }, use to creat a tabl \ { tr }, use to start a row \ { td }, use to add a cell (a \emph{d}ata) to the row
how do you add a head to a row or column of a html tabl \ { th scope="col" } tag \ { th scope="row" } tag
how can you get a cell to span more than one column in a html tabl \ { td colspan="2" }
how can you get a cell to span more than one row in a html tabl \ { td rowspan="2" }
how can you distinguish the first and last row of a tabl in html use the \ { thead }, \ { tbodi } and \ { tfoot } tag around the relev row
whi shouldn't you specifi the width or space of a tabl in html becaus these attribut have been repla by css
whi shouldn't you specifi the border or background of a tabl in html becaus these role have been taken over by css
what are general the element of html4 that have becom deprec the present one - alignment, font appearance, strikethrough, etc
what xhtml html re ulat to follow the rule of xml (everi open tag must have a close tag, nest is correct, etc)
how can you declar which version of html your page use with a \ { !doctyp } tag at the top of the page.
how can you declar that your page use html5 with a \ { !doctyp html } tag at the top of the page.
how are comment denot in html \ { !-- comment -- }
where can the \ {id} attribut be use in html in ani tag. it a global attribute.
how can you refer a set of element in a html page use the \ {class} attribute, whose valu is a space-separ list of identifi of class the tag belong to
where can html \ {class} attribut be use in ani tag. it a global attribute.
what are element in html element that will alway appear to start on a new line
how can you group html element togeth in a singl -level box use the \ { div } tag.
how can you group a set of inlin html element or text into one inlin element use the \ { span } tag
how would you emb someth like a googl map into a html page use an \ { ifram }
what are the attribut usual associ with a html \ {iframe} \ {src}, which contain the url of the page to in the ifram \ {height} and \ {width}, which give the ifram size.
how can you or avoid ing scrollbar on a html \ {iframe} with the html5 \ {seamless} attribute, which suppress the scrollbars. it doesn't need a value, but often peopl will use \ {seamless="seamless"} for consist sake.
what attribut of an \ {iframe} have been deprec in html5 \ {scrolling} and \ {frameborder}
where do you usual keep in ation about a html page that'd onli be relev to machin a \ { meta } tag in the \ {head} which carri ani data in it attributes.
what six valu of \ {name} often appear in a \ {meta} tag \ {description}, a 155-charact descript of the page. often ed in search engin s. \ {keywords}, a comma-separ list of keyword that appear on the page. larg deprec nowadays. \ {robots} with \ {content="noindex"} or \ {content="nofollow"} \ {author} \ {pragma} with \ {content="no-cache"} \ {expires} with the expir date in \ {content}
how are escap charact repres in html with an \ { amp;} follow by the escap code
what'r the prefer altern to flash nowaday javascript html5 \ { video } and \ { audio } tags.
what at are support by differ browser for html5 \ {video} tag safari and ie: h264 opera, firefox, chrome: webm and h264
in what four way can you control the load and play of a html video the \ {src} attribut give the path to the video the \ {preload} attribut can be \ {none, auto, metadata} the \ {autoplay} attribut the \ {loop} attribut
in what four way can you specifi the appear of a html5 video the \ {poster} attribut give the imag to while the video is load the \ {height, width} attribut defin the size of the player the \ {controls} attribut indic whether the browser should suppli it own control
how can you indic a html5 video is avail in multipl at by omit the \ {src} attribut in the \ {video} tag in favour of multipl \ { sourc } tag insid the video environment.
what two attribut often appear in a html5 video \ {source} tag \ {src}, specifi the path to the file \ {type}, indic the at of the video. the codec is list in it valu after a semicolon use \ {codecs}. example: \ {type='video/webm;codecs="vp8, vorbis"'}
what the prefer method for embed audio in html \ { audio }
what a railroad diagram a syntax diagram, a way to illustr the grammar of a languag
how can you insert comment in js \ {//} don't use \ {/* */} becaus \ {*/} can occur in js code
what are js number type there is onli one: the double.
how can you check whether an arithmet oper has fail in js \ {isnan(number)} \ {nan} is not equal to anyth (even itself!) so dont tri to use equal
what'r js textual type there onli one: strings. charact are just single-el strings.
are string mutabl in js no, they'r immut
how doe a railroad diagram denot whether whitespac can be insert between it token a vertic singl bar on the lh and rh terminals. a vertic doubl bar forbid extra whitespac
what distinguish js s from s in most other languag js s do not creat a new scope
where should variabl be defin in js function at the top of the function, as s don't creat new scope.
what are the six `falsi valu in js fals null undefin empti string 0 nan
what are the "truthy" valu of js all but the six falsey values.
what condit statement are avail in js if switch
what are the three loop statement avail in js while for do
what disrupt statement are avail in js break return throw
what decid whether a js \ {if} statement will be ate whether it express evalu to a truthi valu or a falsi valu
what s of \ {for} are use in js \ {for (initi expression; condit expression; increment expression)} \ {for (variabl name in object expression)}
what doe the \ {for in} statement in js do iter over the properti name (keys) of an object.
what the equal oper in js \ {===}
how can you write an in-lin if express in js \ { condit : option : alternative}
what the not oper in javascript \ {!a}
how do you defin a function in js \ {function [name] paramet body}
what are the six type in js number string boolean null undefin object
what an object in js a contain of properti (name valu pairs)
what valu can object properti in js take on anyth but \ {undefined}
what an object liter in js \ {\{ "propertyname1" : value1, "propertyname2" : value2 \}} don't need the quotat mark if the string is a legal js name
how can you access the valu of the properti of a js object \ {objectname.propertyname} \ {objectname["propertyname"]}, if the name isn't a legal js name
what happen if you tri to retriev a non-exist properti of an object in js it return \ {undefined}
how can you guard against \ {undefined} be return when you access a properti in js \ {var variablenam = objectname.propertynam defaultvalu }
what happen if you tri to retriev a properti valu from an undefin object in js you get a \ {typeerror} exception.
how can you guard against error thrown from access the properti on undefin object in js \ {undefinedvari amp; amp; undefinedvariable.propertyname} will return \ {undefined} rather than throw a \ {typeerror}
what happen in js when you assign to a properti that doesn't exist on an object the object is augment with that property.
how are object pass around in js alway by reference.
how do prototyp interact with retriev in js through delegation. if a properti can't be found on an object, it'll look for it in the object prototype. undefin will be return if a properti can't be found anywher in the retriev chain.
how do you set the prototyp of an object in js \ {objectname.prototyp = prototypename}
how can you set the prototyp of an object at creation in js either use an object liter and set it there or creat a function that take an object return an empti object with the argument as it prototype.
how can you test whether a specif object has a properti in js \ {objectname.hasownproperty("propertyname")} ignor the prototyp chain.
how can you test whether some object in a prototyp chain has a properti in js check whether \ {typeof objectname.propertyname} is undefin
what order will the properti of an object be iter through by the \ {for in} js statement the order not defined.
what should you be care to do when iter through the data field of an object with js \ {for in} exclud the properti fetch from the object prototyp exclud properti of the type \ {function}
how can you iter over the properti of a js object in a particular order by creat an array of the properti name and iter over that
how can you remov a properti from an js object \ {delet object\_name.property\_name} this will not touch the prototype.
how can you suppress mani of the problem with global variabl in js creat a singl global \ {myapp} object in which all the top-level variabl are stored.
what the hidden prototyp of function in js \ {function.prototype}
what the hidden prototyp for object in js \ {object.prototype}
what'r the hidden properti of a function object in js a link to \ {function.prototype} a link to it closur a link to the code that implement the function
what a function liter in js \ {function [name] (args) body}
what are the four invoc pattern in js method invoc function invoc constructor invoc appli invoc
what the invoc oper in js what doe it do an \ {()} (possibl non-empty) follow a function name. it tri to assign each express in it to one of the paramet of the function
what happen if js invoc oper is suppli with fewer express than the function has paramet the trail paramet will be left \ {undefined}
what happen if js invoc oper is suppli with more express than the function has paramet the trail express will be discarded.
what the method invoc pattern in js if a function is store in an object then call that function through a refin of the object will set the \ {this} variabl in the function to be the object through which it was called. example: \ {object\_name.function\_name()}
what a refin in js a \ {.} dot express or \ {["subscript"]} express that retriev the valu of some properti of an object.
in the method invoc pattern in js, when is \ {this} bound to the contain object at invoc time.
what the function invoc pattern in js \ {example: function\_name()} set \ {this} to be the global object. which was a bad idea on the design part.
what the \ {that} idiom in js becaus \ {this} is set to the global object in the function invoc pattern, a method can't employ an inner function to do work on the method \ {this}. the work around is to defin \ {var that = this;} in the method. the inner function will then have access to \ {that} through it closure.
what the constructor invoc pattern in js example: \ {var variable\_nam = new function();} if a function is call with \ {new}, a new object will be creat with a hidden link to the valu of the function protoyp member. and \ {this} in the function will be bound to the new object. this is general a shitfest. avoid it.
what distinguish function intend for constructor invoc in js a capit name. a constructor function call without \ {new} can be a shitfest, so this is important.
what the appli invoc pattern in js example: \ {function\_name.apply(name\_of\_this, array\_of\_arguments)} appli invok the function with a specifi valu of \ {this} and the specifi arguments.
how can you retriev argument that overflow the number of paramet a function has in js use the \ {arugments} variable, which return an array of all the argument that were supplied.
what should you be care about when use js \ {arguments} variabl it not actual an array. it has a length property, but none of the other method you'd expect of an array.
what defin the return of a function in js if there a \ {return} statement, it'll return the valu of the statement expression. otherwise, it'll return undefin unless it was invok with \ {new} in which case it'll return \ {this}
how do you throw an except in js \ {throw \{ name: "errorname", message: "errormessag \} } more properti can be added.
how doe except catch work in js ani except thrown in a \ {try} will be dealt with in \ {catch (e)} . if there are a varieti of error that need handling, the catch code should inspect \ {e.name}
when ad method to basic js types, what should be done add it conditionally: make the assign onli if some method with that name doesn't alreadi exist on the object.
doe js have tail call optim nope.
what variabl won't be captur by a function closur in js the contain function \ {this} and \ {arguments}
what the modul pattern in js creat a function which hold privat variabl and privat function which then return the function it wish to be public (or store them in a public place).
what cascad in js have state-modifi method return \ {this} so other state-modifi method can be call on the same object.
what the \ {create} method crockford recommend to encapsul \ {new} object.cr = function (obj) \{ var f = function () \{\}; f.prototyp = obj; return new f(); \}
what the method \ {method} crockford recommend defin function.prototype.method = function (name, func) \{ this.prototype[name] = func; return this; \};
what the \ {inherits} method crockford recommend to help encapsul \ {new} function.method('inherits', function (parent) \{ this.prototyp = new parent(); return this; \})
what happen if you forget a \ {new} statement when call a constructor function in js \ {this} won't be bound to a new object, but instead to the global object. so all your shini new properti will be clobber the global namespace.
what the best way to avoid issu with constructor in js don't use constructors.
what the best way to deal with constructor with larg paramet list in js have them instead accept a singl object with the paramet as it properti and then loop over the properti in the argument and copi them into a similar default paramet object (which allow some argument to be omit from the initi object)
when doe a name need to be specifi in quotat mark in js when it not a legal js name.
how do you emul privat variabl in js use the modul pattern.
how do you construct a modul use js make a function that doe the following: it creat a new object (use an object liter or \ {new}) it defin what will be the privat variabl and function of the object as variabl of an object call \ {my} it augment the new object with it public variabl and function it return the new object.
how can you emul an inherit chain in js modul pattern have each constructor take a \ {spec} object, contain the specif for construct the object and an option \ {my} object, which will be instanti to a blank object if it isn't passed. then \ {my} can be use to pass in ation among constructors.
how can you decompos the function of an object in js use parts: a part is a function that take an object and attach method to it whose state is store in a \ {my} object
what an array liter in js \ {[item1, item2]}
what import to rememb about js array they'r not regular contiguous-memori array instead the subscript are convert to string and use as properti
what the hidden prototyp of js array \ {array.prototype}
what the length properti of js array the largest integ properti name in the array, plus one. it not an upper bound.
how can you index into an element in a js array \ {array\_name[3]} the \ {[]} oper convert it argument to a string use the argument \ {tostring} and retriev the valu at that propeti
what happen if you chang the valu of a js array \ {length} increas it doe nothing. decreas it delet all properti with a equal or larger index than the new length.
how doe the js array method \ {splice} work \ {array\_name.splice(index, delete\_count, new\_element1, new\_element2)} will delet the specifi number of element at the given index, decrement ani higher entri then insert the specifi new element at the same locat
what import to rememb about js \ {typeof} oper it can't recognis array or \ {null} - it'll report them as an object.
what five thing do you need to check when test whether an object is a js array verifi it truthi verifi \ {typeof obj === "object"} verifi \ {typeof obj.length === "number"} verifi \ {typeof obj.splic === "function"} verifi \ {!(obj.propertyisenumerable("length"))}
how is a regular express construct in js either with a regex literal, enclos in \ {/.. ../} or with the \ {regexp} constructor, which take a string.
what should you be care of with regexp liter in js the regex object they produc all share the same instance!
what are the five properti of a regexp object in js \ {global}, which is true if the \ {g} flag was use \ {ignorecase}, which is true if the \ {i} flag was use \ {lastindex}, which indic the index to start the next \ {exec} match at \ {multiline}, which is true if the \ {m} flag was use \ {source}, which is the sourc text of the regexp.
how are flag use on js regexp liter append them to a regexp liter or provid them as a second argument to a regexp constructor.
what flag are there for js regexp \ {g} is the global flag, which make the regexp match multipl times, store the last match in \ {lastindex} \ {i} is the case sensit flag \ {m} is the multilin flag
what ten method are avail on js array .concat(item...) (with individu item or anoth array) .join(separator) (convert to string, interleav with the separator) .pop() (remov amp; return last element) .push(item... ) (push at back) .reverse() (revers array, return it) .shift() (remov first element and return it) .slice(start, end) (copi portion of array) .sort(comparefn) (sort in place) .splice(start, deletecount, item...) (delet and/or insert at index) .unshift(item...) (push at front)
what import to rememb about the js array \ {sort} function by default, it convert to a string and then sort lexicographically. even on an array of numbers.
what four method are avail on js number \ {.toexponential(numberofsf)} convert it to a string \ {.tofixed(numberofdp)} convert it to a string \ {.toprecision(numberofsf)} convert it to a string \ {.tostring(radix)} convert it to a string in the given base
what method is call by \ {string(obj)} in js the \ {tostring} method, if \ {obj} has one.
what two method are avail on js regex \ {.exec(string)} search for the regex in the string and return an array of the s (without \ {g} flag), or set \ {lastindex} to the the index of the next match (with \ {g} flag. \ {.test(string)} is much faster than \ {.exec}, but onli return \ {true} or \ {false}. don't use \ {g} with this.
what method are avail on js string do this later, there'r a lot of them :(
what brace should be use with js k amp;r opening-brace-on-same-line-as-control-stat becaus allman (ie, c ) can break return statements.
whi is allman brace a problem in js becaus if the brace is on the next line from say a return statement, js will automat insert a semicolon at the end of the \ {return}. good lord that stupid.
what should you be care with when use switch statement in js break at the end of each case, becaus fall through the option produc some hard-to-spot bugs.
what the name of the global object in js in browsers, it usual \ {window}
what an impli global in js use a variabl without declar it: \ {foo = value} this'll creat a \ {foo} object in the global namespace.
how do you test for \ {null} in js \ {variable\_nam === null}
how can you test whether someth is an object in js verifi it truthi (exclud \ {null}) verifi \ {typeof variable\_nam === "object"}
what a common issu with js \ {+} oper it can both sum and concatenate. so make sure that the argument are both number if you want to sum them otherwis it'll convert them both to string and concaten them
what a common issu with js \ {parseint} function if you don't suppli a radix as the second argument, it'll tri and deduc it from the string return a valu in base 8 if the number start with a zero.
what'r the issu with js \ {eval} oper it incred tricki and hard to understand. don't use it.
how do you establish a vagrantfil vagrant init
how do you add a new box to vagrant vagrant box add boxnam boxurl
how do you configur a vagrantfil to use a specif box add the line \ {config.vm.box = "boxname"} to the vagrantfil
how do you boot a vagrant instanc \ {vagrant up}
in what three way can you tear down a vagrant instanc \ {vagrant destroy} remov all trace of the machin from your hard disk \ {vagrant suspend} will save the current state and stop it \ {vagrant halt} will grace shut it down
what folder is share by default by vagrant the folder contain the vagrantfil is share as \ {/vagrant}
what a shebang \ { !}, follow by the name of the interpret to load the script into. example: \ { !/bin/sh} load it into the bourn shell.
what unix \ {ln} command a command use to creat links.
what'r the most common switch for unix \ {ln} \ {-f} remov exist destin file \ {-s} creat symbol link rather than hard links.
how do you provis a vagrant instanc with auxilliari program add to the vagrantfil the line \ {config.vm.provis :shell, :path = "scriptname.sh"} where the script is in the same directori and detail what shell command to run.
how do you re-provis a vagrant machin \ {vagrant reload --provision}
what the smallest free number problem find the smallest natur number not in a finit set \ {x}
what the trick to solv the smallest free number problem at least one number in $\{0, 1, .., x \}$ must not be in $x$
what are the two main option when seek a $o(n)$ algorithm for process a list of $n$ element either $n$ oper each take (amortized) constant time. or a recurs process where each of the $k$ subproblem is of size at most $n/k$, and the process take $o(n)$ time.
what the surpass count of an array element the number of element to the right which are larger than the element.
what the optim runtim for comput the maximum surpass count of an array $o(n \lg n)$
what the optim runtim for solv the minimum free number problem $o(n)$
what a good heurist for identifi whether a divid amp; conquer algorithm will work for a problem look at the in ation present in the solut to the subproblems. is there enough there to comput a solut to the whole problem if not, is there a more general problem where it is
what doe an $o(n \lg n)$ runtim suggest about how an algorithm work divid and conquer, with $k$ subproblem of size $1/k$ and a linear amount of work at each step. probabl some sort too.
what doe sql notion of a tabl correspond to in the relat model a relat
what doe sql notion of a row correspond to in the relat model a tupl
what doe sql notion of a column correspond to in the relat model an attribut
what anoth name for a relat type a domain
what a relat type a named, finit pool from which valu can be drawn
what a candid key in relat theori a set of attribut which will uniqu identifi ani tupl in a *relvar*. and such that no subset has the uniqu properti
what a primari key in relat theori a candid key that been singl out for special treatment. the distinct is main syntactic; the theori doesn't care much for primari vs candid keys.
what a foreign key in relat theori a set of attribut in one relvar that'r requir to match the valu of some candid key in the same or anoth relvar.
what an integr constraint in relat theori a boolean express which must evalu to true on everi tupl in a relvar
what are the two generic integr constraint that appli to everi relat databas entiti integrity: primari key attribut can't be null. referenti integrity: there can't be unmatch foreign key values.
what a null in relat theori a marker repres an unknown value. it not a valu itself!
what are the eight origin oper of relat theori restrict project product intersect union differ join divid
what the restrict oper in relat theori return a relat contain all tupl from a specifi relat that satisfi a specifi condition.
what the project oper in relat theori return a relat contain all tupl that remain in a specifi relat after all but specifi attribut have been removed.
what the product oper in relat theori return a relat contain all possibl tupl that are a combin of two tuples, one from each of two specifi relations.
what the intersect oper in relat theori return a relat contain all tupl that appear in both of two specifi relations.
what the union oper in relat theori return a relat contain all tupl that appear in either or both of two specifi relations.
what the differ oper in relat theori return a relat contain all tupl that appear in the first but not the second of two specifi relations.
what the join oper in relat theori return a relat contain all possibl tupl that are a combin of two tuples, one from each of two specifi relations, such that the two tupl contribut to ani given tupl have a common valu for the common attribut of the two relations. (and the common valu appear just once, not twice, in the tuple)
what the divid oper in relat theori take two relations, one binari and one unary, and return a relat consist of all valu of one attribut of the binari relat that match (in the other attribute) ani valu in the unari relation.
what the differ between relat algebra and relat calculus relat algebra provid a procedur way to describ databas queri relat calculus provid a declar way to describ databas queries. they'r logic equival though: a queri describ in one can be translat to the other.
what a data model in relat theori first sense: the abstract, self-contained, logic definit of the data structures, data operators, etc that make up the abstract machin with which user interact. the interface. second sense: a model of the persist data of some system. a databas design.
what data independ in relat theori it mean that the way the data is store and access can be chang without have to make chang to the way the data is perciev by the user.
what a relat in relat theori a heading: a set of attribut a body: a set of tupl that con to the head
what an attribut in relat theori an attribute-name:type-nam pair.
what the degre of a relat in relat theori the number of attribut in the heading.
what the cardin of a relat in relat theori the number of tupl in the body.
what'r two import implic of the definit of a head and a bodi of a relat in relat theori the bodi is a *set* of tuples. so no duplicates. and they'r unordered.
what first normal in relat theori everi tupl in a relat contain a singl valu correspond to each attribute. each valu is atom with respect to the dbms.
what'r base and deriv relat in relat theori base relat are defin ex nihilo. deriv relat are deriv from base relat through a combin of relat operators.
what a relvar in relat theori a relat variable, which is to relat (relat values) as an \ {int i} is to an integ value.
when can valu be test for equal in relat theori when they have the same type.
what a dco in relat theori a domain check override. it say to ignor the type of valu when per ing an operation.
how are dcos implement in most relat databas system they aren't. they'll general be will to compar valu from across types.
what a selector oper in relat theori an oper associ with everi type, which allow us to retriev an arbitrari valu of the type in question. ex: \ {type\_name('value')}
what coercion in relat theori an implicit type conversion.
what a \ {the\_} oper in relat theori the oper defin on everi type which convert a given valu of the type to the use to repres it physically. ie \ {the\_char}
what atom with respect to relat theori a valu is atom if it can't be decompos into smaller piec by the dbms.
what an rva in relat theori a relation-valu attribute. ie, valu which is also a relation.
what a tupl in relat theori a set of attribute:valu pairs.
when are two tupl equal in relat theori when their head are equal and for each attribut in the heading, the valu in each tupl are equal.
how is the primari key of a relat databas usual depict pictori by double-underlin the appropri attribute.
what are the two variat of \ {select} in sql \ {select distinct} return onli distinct tuples. \ {select} aka \ {select all} return all tuples.
what'r the advantag and issu of prefer \ {select distinct} over \ {select} in sql \ {select distinct} is closer to the relat theori and make oper a lot more predictable. but it also not the default and can be much slower.
what the main advantag of prohibit duplic tupl in relat databas it clarifi the of queries: there are mani queri that, without this restriction, could plausibl produc differ multipl of a tupl and so allow much more effect queri optimization.
what 3vl in relat theori three-valu logic: there is true, fals and null/unknown
what the main advantag of disallow null in a databas becaus sql queri onli return tupl which caus the queri to evalu to true, there are mani queri whose s can chang under obvious optimizations. ex: \ {select .. where p.citi = p.city} when \ {p.city} is alway null.
what are the relat of degre zero the empti relat the relat contain the empti tuple.
what \ {table\_dum} in relat theori the zero-degre empti table.
what \ {table\_dee} in relat theori the zero-degre tabl contain onli the empti tuple.
what the logic valu of \ {table\_dum} in relat theori \ {false}
what the logic valu of \ {table\_dee} in relat theori \ {true}
whi is the distinct between relvar and relat import in relat theori relat are immutable, relvar aren't!
what the idea under foreign key in relat theori they'r not fundamental; they'r just shorthand for certain integr constraint that are common requir in practice.
what anoth name for a virtual relvar in relat theori a view.
what a virtual relvar in relat theori a relvar whose valu is the of evalu a relat expression.
what a materi view in databas theori a queri evalu at a fix time, saved, and possibl refresh at period intervals. not, in fact, a view. aka a `snapshot'.
what anoth name for the intens of a relvar the relvar predic
what the relvar predic the predic which instanti true proposit for exact the tupl in the relvar.
what the extens of a relvar predic or intens the bodi of the valu of that relvar at a given point in time.
what the intens of a relvar the predic that describ how the relat should be interpreted. ie, \ {supplier sno has name sname and is in citi city}
what the differ between a predic and a proposit a proposit is an uncondit predic
what the logic interpret of relat databas tupl are axiom (true propositions) queri are proofs, deriv new truth from given ones.
how do you implement the relat restrict oper in sql \ {select s.*} \ {from s} \ {where s.citi = paris'}
how do you implement the project relat oper in sql \ {select distinct s.sname, s.city} \ {from s}
how do you implement the relat join oper in sql \ {select *} \ {from p natur join s} not implement in all sql products, but is in the standard.
what the relat semijoin oper join \ {r} and \ {s}, then project the back onto the attribut of \ {r}.
how do you implement the relat semijoin oper in sql \ {select distinct s.*} \ {from s, p} \ {where s.sno = p.sno}
what the of the join of no relat in relat theori \ {join\{\}} is \ {table\_dee}
what the join on a singl relat in relat theori \ {join\{r\}} is just \ {r}
what the relationship between \ {table\_dee} and \ {join} in relat theori it the ident element.
how do you implement the relat intersect oper in sql \ {select distinct s.city} \ {from s} \ {intersect} \ {select distinct p.city} \ {from p}
how do you implement the relat union oper in sql \ {select distinct s.city} \ {from s} \ {union distinct} \ {select distinct p.city} \ {from p}
what are the variant of the union oper avail in sql \ {union, union distinct} suppress duplic \ {union all} allow duplicates.
how do you implement the relat differ oper in sql \ {select s.city} \ {from s} \ {except} \ {select p.city} \ {from p}
whi is the relat divid oper name as it is becaus if \ {r} and \ {s} have disjoint headings, then \ {(r time s) divid s} is \ {r}, assum \ {s} isn't empty.
what the relat semidiffer oper \ {r} except \ {r semijoin s}
how do you express a relat semidiffer in sql \ {select s.*} \ {from s} \ {except} \ {select s.*} \ {from s, p} \ {where s.citi = p.city}
what one possibl set of primat relat oper restrict, project, join, union and semidifference.
what relat oper doe a sql \ {select-from-where} queri rough correspond to a project (use the select clause) of a restrict (use the where clause) of a cartesian product (use the from clause)
how do you repres a cartesian product in sql \ {select *} \ {from p, s}
what the extend relat oper \ {extend r add (exp as x)} return a relat match \ {r} except the head is extend with a new attribut \ {x}, and each tupl has a new valu which is \ {exp} evalu on that tupl
what the relat summar oper \ {summar r per ( s ) add (summari as x)} is a relat with a head of \ {s} extend with \ {x} and a bodi of all tupl \ {t} such that \ {t} is a tupl of \ {r} extend with a valu for \ {x} and this valu is comput by evalu \ {summary} over all tupl \ {r} that match \ {t} on \ {s}
what are some common sql oper that can be interpret as relat summar oper count sum avg max and etc
what the differ between the summar relat oper and an sql aggreg oper summar return a relat the aggreg oper will return some kind of scalar
which relat oper deal with rvas group and ungroup sql has no facil for this unfortunately.
what anoth name for relat queri optim queri rewriting.
what doe a relat restrict distribut over intersect union differ join if the restrict is at it most complex an and of two separ conditions, one for each of the join operands.
what doe relat project usual distribut over intersect union join if all the join attribut are includ in the projection.
which relat oper are commut intersect union join (though it isn't in sql!)
which relat oper are associ intersect union join
what are the four way in which the build s of a distribut system vari what the communic entiti are the communic paradigm the role amp; respons the entiti have in the architectur the placement of the entiti
what `placement in the context of distribut system how softwar entiti are map onto the infrastructure.
from a system perspective, what'r the usual entiti communic in a distribut system process run on thread in most cases. in primat environ without a process abstraction, nodes.
from a program perspective, what are the three kind of common communic entiti in a distribut system objects, which defin interfac components, which defin interfac and defin the assumpt they make about other compon web services, which are intrins integr into the web (identifi by a uri, describ in xml or json, etc)
what are the three type of communic paradigm for a distribut system interprocess communic (messag passing, socket programming, etc) remot invoc indirect communic
what are three common remot invoc paradigm in distribut system request-repli remot procedur call remot method invoc
what the request-repli communic paradigm a pattern impos on top of a message-pass service. a messag is sent with an encod of the oper for the receiv to carri out and the bytearray to use as an argument. and the receiv return ani s as anoth bytearray. primative, efficient.
what the remot procedur call communic paradigm in distribut system procedur in process on remot comput can be call as if they were procedur in the local address space. built on top of (and hides) a request-repli paradigm.
what the remot method invoc communic paradigm in distribut system rpc adapt to distribut objects, possibl with ad support for pass around object identifi as parameters.
what are the four main challeng in handl failur in distribut system how to detect them. how to them. how to toler them. how to recov from them.
what transpar in the context of distribut system the conceal from the client of the separ of the compon in a distribut system so it appear as a whole rather than a collect of components.
what are the eight key s of transpar in distribut system access transpar locat transpar concurr transpar replic transpar failur transpar mobil transpar per anc transpar scale transpar
what access transpar in the context of distribut system local and remot resourc can be access use ident operatiosn
what locat transpar in the context of distribut system resourc can be access without knowledg of their physic or network locat
what concurr transpar in the context of distribut system process can oper concurr on share resourc without requir knowledg of the others.
what replic transpar in the context of distribut system multipl instanc of a resourc can be use to improv per anc without the knowledg of the client
what failur transpar in the context of distribut system client can complet their task ignor of failur in the system components.
what mobil transpar in the context of distribut system resourc and client can move about within a system without imped the client tasks.
what per anc transpar in the context of distribut system the load on a system doesn't affect clients.
what scale transpar in the context of distribut system the system can be vast expand without affect clients.
what network transpar in the context of distribut system the combin of access and locat transparancy.
what'r the general goal of indirect communic paradigm in distribut system spatial decoupl (sender don't need to know who they'r send to) tempor decoupl (sender and reciev don't need to exist at the same time)
what are five indirect communic techniqu in distribut comput group communcat publish-subscrib messag queue tupl space distribut share memori
what the group communic techniqu in distribut system recipi join a group and reciev all messag sent to it sender send messag to the group identifier.
what the key use case for publish-subscrib model in distribut system a larg number of produc want to distribut in ation to a larg number of consumers. publish-subscrib model provid an intermediari that allow for effici dissemination.
what a tupl space in the context of distribut system process can place arbitrari item of structur data (tuples) into a persist tupl space. other process can then read or remov the tupl by specifi pattern of interest.
what distribut share memori in the context of distribut system a system by which client can read from or write to share data structur as if they resid in their local address space.
what are the two general architectur for a distribut system client-serv peer to peer
what are the four main placement architectur for distribut system map servic to multipl server cach mobil code mobil agents.
what are some common architectur pattern in distribut comput to assist with segreg of servic layer tier
what layer with respect to distribut system a system is partit into layers, with each layer make use of the servic provid by the layer below
what the name of the lowest level layer in a distribut system the plat layers, which includ the os and hardwar
what middlewar in the context of distribut system a layer whose purpos is to heterogen in the lower layers, provid as conveni a program model as possible.
what tier in the context of distribut system a way to organ function within a layer and place those function onto appropri servers.
what a common three-tier decomposit of a softwar layer in distribut system present logic tier applic logic tier and data logic tier
what a common two-tier decomposit of a softwar layer in distribut system user view, control and manipul in the client tier applic and data manag in the server tier.
what ajax asynchron javascript and xml
what a mime type an internet media type, a standard identifi to indic the type of data a file contains.
what vnc in the context of distribut comput virtual network computing, a way to support remot desktop etc
what the brokerag pattern in distribut system a servic provid a servic request and a servic broker which match provid to requesters.
what jitter in the context of distribut system variat in the time taken to deliv a seri of messages.
how long doe a l1 cach refer take approx 500ps
how long doe a branch mispredict take approx 5ns
how long doe a l2 cach refer take approx 7ns
how long doe a mutex take to lock or unlock approx 25ns
how long doe read 1mb from memori take approx 250us
how long doe a roundtrip on a local network take approx 500us
how long doe read 1mb from a ssd take approx 1ms
how long doe a hard drive disk seek take approx 10ms
how long doe read 1mb sequenti from a hard disk take approx 20ms
how long doe a main memori refer take approx 100ns
how long doe a random read for a ssd take approx 150us
how larg is a haswel l1 cach 64kb per core
how larg is a l2 haswel cach 256kb per core
how larg is a l3 haswel cach 2mb-8mb share
what frame relay switch in the context of distribut system transmit small packet call frame and switch them base on their first few bits. frame as a whole aren't store by node but pass through them as a short stream of bits.
how fast is frame relay switch cross a larg network take $ 100$us
what are the four type of switch use in comput network broadcast circuit swith (ie pots) packet switch frame switch
what pot in the context of comput network plain old telephon system
what iso osi in the context of comput network the iso open system interconnect protocol model which provid a framework for the definit of protocol
what'r the seven layer of the iso osi protocol model applic present session transport network data link physic
what the applic layer of the iso osi protocol model applic layer protocol are design to the communic requir of specif applications. often defin the interfac to a service.
what the present layer of the iso osi protocol model protocol at this level transmit data in a network represent that independ of the represent use in individu computers. handl encryption.
what the session layer of the iso osi protocol model protocol at this level deal with reliabl and adaptation. doe failur detect and automat recovery.
what the transport layer of the iso osi protocol model protocol at this layer are concern with reliabl deliveri of messages. the lowest layer deal with messag (rather than packets).
what the network layer of the iso osi protocol model protocol at this layer rout packet between endpoint within a network.
what the data link layer of the iso osi protocol model protocol at this layer deal with transmiss of packet between physic connect nodes.
what the physic layer of the iso osi protocol model protocol at this layer deal with transmit binari data by analogu signalling.
what the disadvantag of use a tall protocol stack control has to be transfer from each layer to the next layer, which can reduc transfer rates.
what the mtu in the context of comput network maximum transfer unit
what the maximum transfer unit in the context of comput network the maximum size of a packet that can be transmit over the connect in question.
what the mtu for the ip suit 64kb, though 8kb is often used.
what a port in the context of comput network a software-defin destin point at a host computer. they'r attach to process to allow that process to reciev data.
what a contact port number in the context of comput network a port number assign to an internet servic (like http) to access a servic on a host, a request is sent to the servic contact port.
what the contact port for http port 80.
how is the rang of port avail on a comput partit 0-1023: well-known port 1024-49151: regist port 49152-65535: dynam port
what'r well known port in the context of comput network port restrict to priviledg process on most oses, use for network services.
what'r regist port in the context of comput network port number regist with the iana for specif applic (they can be use by ordinari user too for ani old thing, but it'll prevent the regist servic from connecting)
what are the two approach to packet deliveri that can be taken in comput network datagram deliveri virtual circuit deliveri
what datagram packet deliveri in the context of comput network each packet is a `one-shot whose deliveri requir no set up and for which the network retain no in ation. as such, everi packet contain the rout in ation need to get it to it destin or back to it origin.
what virtul circuit packet deliveri in the context of comput network a rout is establish from sourc to destination, with entri store in the intermedi rout tables. `dumb packet without rout in ation are then transmit over the route.
what atm in the context of comput network asynchron transfer mode
what asynchron transfer mode in the context of comput network a protocol develop for unifi telecommun and comput networks. nowaday be phase out in favour of all ip
what the biggest exampl of a virtual circuit packet deliveri protocol atm
what a rip in the context of comput network router in ation protocol, a way to updat rout tables.
what the vector distanc rout algorithm each node hold a tabl which gives, for each other node, the link to take to travel toward that node and the cost of the journey. when a local link is modified, a node updat it own rout tabl then send a copi to each of it neighbours. when a tabl is receiv from a node neighbour, if the neighbour offer a cheaper rout than is current known, updat that entri in the node own rout table.
what a high-level way of look at vector distanc rout algorithm they'r a distribut implement of the bellman-ford shortest-path-in-a-graph algorithm.
what are link state rout algorithm algorithm where the onli in ation propag is about connectivity. everi node build it own map of the network, and share with it neighbour who it own neighbour are.
what the rule of thumb as to when congest start to becom a problem for packet-switch network when load reach 80 of capacity, throughput tend to drop as a of packet loss.
what arp in the context of comput network address resolut protocol
what doe the address resolut protocol do convert internet address to network address for the specif under network. how it doe this is depend on the network technolog be used.
how doe arp work on ethernet each host has a ip address to ethernet address cache. when tri to send a packet to an ip address, arp check the cache. if the address is there, great! otherwise, a broadcast is sent out ask that the machin which control that ip address respond. each listen machin check to see if it has that ip, and the one that doe respond with it ethernet address. the respond machin address is ad to the cache.
what two method have been use to control the expans of router tabl size in the internet topolog group of ip addresses. default entri
how do default entri help control the size of rout tabl the accuraci of rout in ation can be relax for most routers, as long as a few key router (close to backbon links) have reason complet tables. so most router forward packet destin for unknown address onward to one of these key routers, and let them sort it out.
what cidr in the context of comput network classless interdomain routing.
what the purpos of cidr in the context of comput network to allevi address shortag by batch class c address togeth to a class b address.
how doe cidr work in the context of comput network it ad a field to router tables. the is a bit pattern that is use to select the portion of the address to be compar with the rout tabl entry. this allow the host/subnet address to be ani portion of the ip address.
what the differ between a class c and a class b ipv4 address class b has a 16 bit network id and so can support $2^{16}$ comput in the subnet class c has a 24 bit network id and so can support $2^8$ comput in the subnet
what dhcp in the context of comput network dynam host configur protocol, which assign ip address to comput in the network.
how doe nat work in the context of comput network when a comput send a packet to a comput outsid the network the router save the sourc ip and port number in a slot in it address translat table, then substitut the ip in the packet for it own and the port for a virtual port number that index the slot in the tabl hold the sourc address and port.
what the main disadvantag of network address translat comput behind a nat have to initi connect unless some port in the router is specif forwarded.
what datagram fragment in comput network when a packet is larger than the next router mtu, it broken up by the current router into smaller packets.
what are the seven main advantag of ipv6 larger address space faster rout speed (no header checksum, no fragmentation) traffic class header field (ie priority, transmit prompt vs drop) flow label header field (resourc reservation) `next header field (enabl option header field in packet) multicast and anycast ip-level authent amp; encrypt
what are three aim of a firewal servic control behaviour control access control
in what three way doe a firewal common filter traffic ip packet filter tcp gateway applic level gateway
what the purpos of an application-level gateway to proxi an applic process example: user telnet through an application-level gateway could be captur by a tcp gateway and prompt the creation of a telnet proxy. the user then s a connect with the proxi and the proxi with the destination.
what the purpos of a tcp gateway to valid tcp connect request and segment transmissions. can be use to suppress mal ed segment and to rout connect through an application-level gateway
what a bastion in the context of comput network a comput insid the firewal which conduct gateway processes.
what a mac address in the context of comput network a medium access control address, intend for use by the ethernet protocol. (but also use by other protocol becaus they'r ubiquit and unique)
how doe collis detect work in the ethernet protocol if a station hear a differ signal on the carrier than it transmit it transmit a jam signal and then back off for a random (but bounded) length of time. if, on retransmit, anoth collis occurs, the backoff time is doubled.
how doe collis detect work in wifi slot reservation: the sender transmit a request-to-send (rts) frame contain a durat and the receiv repli with a clear-to-send (cts) frame. as such all station within rang of the sender or receiv will hear at least one of the rts or cts frames, and refrain from transmit dure that period. if a collis occurs, or a rts doesn't get a cts back, there an exponentially-increas random backoff period.
defin p-hack tri multipl statist test until you get your desir low p-valu http://www.nature.com/news/scientific-method-statistical-errors-1.14700
what is it call when a scientist tri multipl hypothesi test use differ approach until she get the she want p-hack http://www.nature.com/news/scientific-method-statistical-errors-1.14700
if you want to show the world that you plan on do a hypothesi test prior to actual do it, how can you do this preregist it at the open scienc framework or at anoth pre-registr websit can distinguish btwn exploratori and confirmatory, too; https://osf.io/getting-started/
what abstract doe the api for udp present a messag pass abstract
what abstract doe the api for tcp present a two-way stream between processes.
what the differ between synchron and asynchron communic in synchron communication, send and receiv process sync at everi message: the \ {send} oper s it process until a \ {receive} oper is issu by the other process and vice versa. in asynchron operation, \ {send} is non- ing and \ {receive} will be either ing or non- ing. in the non- ing case, the receiv process will provid a buffer to be fill in the background.
do most modern system prefer synchron or asynchron communic asynchronous, with a ing \ {receive} operation.
how mani sender and receiv can a port have ani number of send process but exact one receiv process.
what a socket address in comput network an ip address and port number.
what a socket in the context of comput network a softwar abstract of an internet address, compos of an ip and a port.
what determin the size of udp messag the receiv process specifi the size and it limit by ip 64kb restriction, though it usual further limit to 8kb
what failur are udp datagram subject to omiss (bi fail checksum or buffer overflow) order
what are some popular servic that use udp dns voip
how doe tcp recogn when a messag has been lost on receiv a packet, the receiv respond with a packet contain the highest contigu sequenc number so far *accepted*. on a success accept, this shift the transmit window along.
when do process when use tcp they can on receiv if there no data in the receiv buffer they can on send if the transmit window is closed.
how is the fact that receiv oper are often ing usual dealt with in program if thread are available, monitor a socket is deleg to it own thread otherwis you simpli have to test as to whether there data avail befor tri to read.
what marshal in the context of comput network convert data into a at ameni to transmiss in a messag
what an extern data represent in comput network an agre standard for the represent of data structur and primat values.
what unmarshal in the context of comput network convert marshal data back into usabl ats.
what are five of the most common extern data represent in use corba cdr language-specif serial ats, like java object serial xml protocol buffer json
what corba common object request broker architectur
what corba cdr stand for corba common data represent
how doe the corba cdr at data fixed-length type (bools, longs, floats, arrays, etc) are encod as-i variabl length type are given as a length follow by it fixed-length individu element (usual in the at of possibly-pad unsign longs)
how doe corba cdr encod type in ation it doesn't. it assum the sender and receiv have common knowledg of the order and type of the item in a message.
how doe java serial refer to other object each serial object is given a handle. and refer to the object are serial as that handle.
what type in ation doe java serial the name of the serial class and a version number (either set manual or comput from a hash of the class contents)
what a remot object refer in the context of comput network an identifi for an object that is valid throughout a distribut system.
what a simpl way to creat a remot object refer concaten the ip of the host and port of the process with the current time and the current valu of a local objects-cr counter.
at a high-level, how doe ipv4 multicast work an udp datagram is sent to a class d ip address ani comput with a socket regist with that ip address will receiv a copi of the datagram.
what a class d ip address a multicast ip, whose first four bit are 1110
what are some common use of multicast protocol fault toler through replic servic servic discoveri data replic event notif propagation.
what reliabl multicast a protocol type where either all subscrib receiv messag or none of them do.
what'r ip multicast failur mode omiss (can be partial, where onli some recipi miss the message) order (again, can be partial)
what an overlay network a network of node and virtual link that sit on top of anoth network, and offer someth that is not otherwis provided.
what are some exampl of overlay network distribut hash tabl peer-to-p file share content distribut network
what mpi in the context of comput network the messag pass interfac standard, develop in high-per anc computing.
what the general structur of a request-repli messag \ {messagetype} int (either 0 for request or 1 for reply) \ {requestid} int (uniqu wrt sender) \ {remotereference} int (usual address of the sender) \ {operationid} int \ {arguments} byte array
what an idempot oper with respect to comput network an oper that can be per ed repeat with the same effect had it been execut once.
how are timeout dealt with in request-repli protocol typically, the request is resubmit a number of time befor a failure. which mean the oper either has to be idempot or the receiv need to recogn duplic use a history.
what the structur of a request-repli protocol a client will execut \ {dooperation(remoteref s, int operationid, byte[] args)}, which send a messag to the server \ {s} contain the procedur and argument to execute. upon receiv the request, the server \ {getrequest()} procedur will execut the request oper and call \ {sendreply(byte[] , remoteref c)} to repli to client \ {c} with the . upon receiv the reply, \ {dooperation} will return.
what are the r, rr and rra protocol request protocol (use when no repli is needed) request-repli protocol request-reply-acknowledg protocol
what an exampl of a request repli protocol http
what are the seven most common http method get head post put delet option trace
what doe the get http method do request a resourc at a specif url. if the resourc is data, the data is returned. if it a program, the program is run with the suppli paramet and the is returned. can also be configur to be condit on the date the data was last modified, and to return onli part of the data.
what the http head method same as get, but return statist about the resourc (size, type, last modified, etc)
what the http post method specifi the url of a resourc that can deal with the data suppli in the bodi of the request.
what the http put method request that the bodi of the request is store with the specifi url as it identifier.
what the http option method fetch the list of oper that can be appli to the specifi url.
what doe the http trace method do the server repli with the request message. diagnostic.
what the content of a http request messag name of the method (get, post, etc) url http version header bodi
what usual held in the header of a http request request modifi (like condit on the latest date of modification) client in ation (like it dns name and what datatyp it can handle) credenti
what the structur of a http repli messag http version status code (404, etc) reason (not found, etc) header bodi
what an idl in the context of comput network an interfac definit language.
what are the possibl choic of call semant for remot procedur call protocol mayb semant at-least-onc semant at-most-onc semant
what are `mayb call semant in distribut system the remot procedur call is execut either onc or not at all. aris when no fault toler measur are applied.
what are `at-least-onc call semant in distribut system the invok of the remot procedur call will receiv either a , or an except say no was received. aris when request are retransmitted. requir idempot oper to avoid errors.
what are `at-most-onc call semant in distribut system invok of the remot procedur call receiv either a or an error say that no was received, which mean either the oper was carri out onc or not at all. aris with messag retransmission, duplic filter and repli retransmission.
how are rpc protocol implement each procedur in the servic interfac has a correspond stub procedur in the client, which marshal the argument and send the request to the server. each procedur in the servic interfac has a correspond stub procedur in the server, which unmarshal the argument and call the servic procedure. the of the servic procedur is then marshal and return by the server stub procedure. the client stub procedur unmarshal the and return to the caller.
what are the core concept of the distribut object model remot object refer remot interfac a process with a remot object refer can invok the method list in it remot interface.
how doe java implement remot interfac remot interfac are simpli interfac deriv from \ {remote}
what are the server compon of an rmi protocol implement communic modul remot refer modul servant dispatch skeleton
what are the client compon of an rmi protocol implement communic modul remot refer modul proxi
what doe the communic modul of an rmi protocol implement do conduct all communic between client and server. at the client end, is pass a call to the proxy, replac proxi with the correspond remot reference, and send the request to the server. at the server end, get the local object refer from the remot refer via the remot refer module, then pass the local refer to the relev dispatcher.
what doe the remot refer modul of an rmi protocol implement do keep a remot refer table, contain all the remot object held by the process and all local proxies. creat remot refer for object that are about to be pass remot for the first time convert remot refer to local references, possibl creat a proxi if the remot refer refer to a non-loc object
what doe the servant in an rmi protocol implement do it the instanc of a class that provid the bodi of a remot object. handl the remot request pass on by the correspond skeleton
what do the proxi of an rmi protocol implement do make rmi transpar to client by behav like a local object to the invoker. one creat for each remot refer the object holds, and implement all the method in the remot object remot interface. call to these method are forward to the communic modul
what doe the dispatch of an rmi protocol implement do one for each class of object for which a remot refer has been given out. receiv request messag from the communic module, along with the local refer for the correspond obejct. select the relev method from the skeleton and pass on the request method.
what do the skeleton of an rmi protocol implement do one for each class of object for which a remot refer has been hand out. unmarshal the argument in a request method and invok the correspond method in the servant for which a local refer has been given. when the servant returns, it marshal the into a repli messag and return it to the dispatcher.
how do rmi protocol implement accomod object whose interfac weren't known at compil time dynam invocation: instead of proxies, a generic \ {dooperation} procedur is avail that take a remot object reference, a method name and arguments. requir dynam skeleton if the object are also unknown to the server at compil time.
what a er in a rmi protocol implement a servic that maintain a map of textual name to remot object refer
what an ator in an rmi protocol implement recreat passiv object from save state, possibl boot up new server process to run them keep a registri of passiv object avail for ation.
what ation in a rmi protocol implement object can be `passiv into passiv object to save resourc if they'r not access for a while. their state will be save and they can be spun back up at a later date by an ator if needed.
what a persist object store in a rmi protocol implement a way to preserv object between ation and de ation of processes. transpar return e object on request.
what a locat servic in a rmi protocol implement a databas that map remot object refer into their probabl current locations. also manag locat object if they'v move from their last record location.
how doe distribut garbag collect usual work in rmi protocol implement refer counting. when a client tri to instanti a proxy, it'll tri and add itself to the server refer list first. if the addit fails, the proxi won't be created. the server consid the proxi to be leased: the client must request a leas renew period in order to avoid be remov from the refer list. a temporari entri is ad to the refer list between a remot refer be hand out and the server be in ed of the new proxy. this stop the object be destroy befor the client can register.
what spatial coupl in the context of distribut system the sender has to know the ident of the receiv to send a messag to them.
what tempor coupl in the context of distribut system the sender has to be aliv at the same time as the receiver.
what are the two main type of group communic process group object group
what a process group in the context of distribut system communic entiti are process messag are deliv to process messag are typic unstructur byte arrays.
how object group invok in the context of distribut system client object invok oper on a proxi object which use a group communic system to send the invoc to each object in the group.
what the differ between a close and open group communic protocol in the context of distribut system in close groups, onli member of the group can multicast to it in open groups, anyon can multicast to the group.
what the differ between an overlap and non-overlap group protocol in the context of distribut system in overlap group protocols, an entiti can be a member of mani groups. in non-overlap group protocols, an entiti can be a member of at most one group.
what integr in the context of distribut system the guarante that a receiv messag will be the same as the one sent, and that it will onli be receiv once.
what valid in the context of distribut system the guarante that ani messag sent will eventu be delivered.
what agreement in the context of distribut system the guarante that in a multicast protocol, if one process receiv a message, all the process in the group will receiv the message.
what order properti can a group communic protocol enforc fifo ordering: the messag from a particular sender will be deliv in the same order they'r sent causal ordering: if one messag is sent befor another, it'll be deliv befor the other. total ordering: if one messag arriv befor anoth at one process, that messag will arriv befor the other at all processes.
what are the main respons of the membership manag servic in a group communic protocol provid an interfac for group membership chang failur detect notifi the rest of the group of membership chang per ing address expansion, from the group identifi to the current group membership list
what anoth name for publish-subscrib communic protocol distribut event-bas systems.
what are the two main characterist of publish-subscrib protocol heterogeneity: subscrib onli need to provid a notif interfac asynchronicity: publish push notif out asynchron
what are the common filter type for publish-subscrib protocol channel-bas filters: publish and subscript is done to name channel topic-bas filters: publish and subscript is done with respect to a (possibl hierarchical) topic field content-bas filters: subscript is done with express that are evalu over each notif type-bas filters: when notif are objects, subscript can be done by type of object that desired. context-bas filters: notif are propag on the basi of the phase of the moon
what cbr in the context of distribut system content-bas routing, a problem in distribut publish-subscrib protocols.
what are the various architectur option for content-bas rout in publish-subscrib protocol central distribut flood distribut filter distribut filter w/ advertis distribut rendezv distribut in ed gossip
what flood in the context of distribut publish-subscrib protocol send an event notif to everi subscrib in the network, and let them figur out if it match the select criteria.
how can a flood protocol be optim in the context of distribut publish-subscrib protocol use multicast protocol if they'r avail structur broker as an acycl graph
what filter base rout in the context of distribut publish-subscrib protocol subscript in ation is propag backward to all potenti publishers, and then event notif are onli propag along rout which contain interest subscribers. requir that all broker hold a rout tabl as well as a match implementation.
what are advertis in the context of distribut publish-subscrib protocol filtering-bas rout can generat a lot of traffic due to propag of subscript if publish have an idea as to the kind of notif they'll be pushing, they can `advertis this. advert are then propag toward subscribers, while subscript are propag backwards.
what rendezv rout in the context of distribut publish-subscrib protocol the space of possibl notif is partit among brokers. two function are defined: one which take a subscript and return the list of broker whose partit intersect it (who subscrib can then subscrib to), and one which take a notif and doe the same. then when an event is published, it evalu against the function and push to the relev brokers, which then distribut it to their subscrib list.
what in ed gossip in the context of distribut publish-subscrib protocol a gossip protocol that take into account local in ation when decid how to propag the gossip.
what the obvious implement for rendezv rout in the context of distribut publish-subscrib system a distribut hash table. requir intellig select of a hash function though.
what anoth name for messag queue messag orient middleware.
what are the s of receiv common support in a messag queue protocol block receive, which will wait until there a messag avail to return with non- ing receive, which will return a messag if there is one or a not avail notif otherwis notify, which issu an event notif when a messag becom avail
what the usual content of a messag in a messag queue a destin queue metadata about the messag which can be use for select an opaqu bodi which the messag must be dequeu to view.
what a messag broker in the context of a messag queue protocol the servic that manag the tran ation to be appli to arriv messages.
what the differ between messag pass protocol and messag queue in messag passing, the queue are implicit attach to their receiv process, make it a tempor coupl protocol. in messag queues, the queue are host by a third party.
what'r the differ between messag pass and distribut share memori dsm doesn't requir marshal messag pass sync via a lock server; dsm use locks, semaphores, etc dsm can be made persist and tempor decoupled. dsm per anc can fall off dramat with larg number of comput access a small number of items. dsm hide communication; messag pass is explicit about it
what are the standard oper on a tupl space write read take
how are tupl found in a tupl space via associ address a tupl specif is given, and all match tupl are returned.
what the structur of a tupl in a tupl space \ { "string", 2, c }
are tupl in tupl space immut yep
how can tupl space be broken up by creat name tupl spaces, possibl dynamically.
what are some up-and-com variant of tupl space use set instead of tuples, so that tupl and tupl space are self-similar. use object instead of tupl to object space
what the state machin approach to tupl space when use replic tupl spaces, the replica can be kept consist if everi replica must start in the same (empty) state everi replica must execut event in the same order (assur by a total order multicast) replica must react determinist to event
what the `hash approach to tupl space replic tupl are hash to a replica and place onli in that replica. the hash algorithm is pick so that read or take oper can calcul a small set of possibl server where the desir tupl could reside.
what variabl hoist in js if a variabl is initi somewher in a function, the *declaration* will be move to the top of the scope by the compiler, but the initi won't be. which will leav an \ {undefined} variabl in an unexpect place.
how larg is a 2014 harddriv write cach 8mb or larger
how would you defin a 2d point type in sql \ {creat type point as (x numeric, y numeric);}
how would you creat a type synonym for an integ in sql \ {creat type qti as integer;}
how do you creat subtyp in sql parent type must have been declar with \ {not final} subtyp is then \ {creat type subtypenam under parentname;}
how would you assert that all \ {val} valu in a sql tabl \ {s} were greater than 0 \ {creat assert assertionnam check} \ {(not exists} \ {(select s.* from s} \ { where s.val = 0 ))}
how would you assert that all \ {val} valu in a sql tabl \ {s} were uniqu \ {creat assert assertionnam check} \ {(unique} \ {(select s.val from s ))}
what problemat about sql \ {unique} oper from a relat perspect it meaningless in relat theori becaus everi tupl in a relat is alreadi unique!
what the equal oper in sql \ {(=)}
when are databas constraint appli in relat theory, immedi - they'r appli whenev the relat is updat sql deviat from relat theori by support defer updat too though.
how can you updat multipl tabl in sql simultan \ {start transaction;} \ {updat s set citi = pari where sno = sno('s1');} \ {updat p set citi = pari where pno = pno('s1');} \ {commit;}
convent in sql, where are integr constraint enforc at the boundari of transactions.
what it mean to say that a databas is correct a correct databas faith reflect the true state of affair in the real world. slight more ally, in a correct relvar, everi tupl satisfi the relvar predicate.
what it mean to say that a databas is consist everi tupl satisfi the databas constraint
how do correct and consist relat with respect to databas correct *implies* consist
what the total relvar constraint the conjunct of all the constraint enforc on the relvar.
what the golden rule of databas no updat oper must ever violat the total databas constraint
what the total databas constraint the conjunct of the relvar constraint of that databas
what should happen when a constraint is appli to a databas that doesn't satisfi it the constraint should be rejected.
what a transit constraint in databas theori a constraint that restrict the legal transit that relvar can make from one valu to another.
what the class name convent in java noun with pascal case.
what the java convent for interfac name adject in pascal case that end with `ibl or `able'.
what the java name convent for method amp; variabl camel cased.
what the java name convent for packag name lowercas and underscore.
how do you write a comment in java \ {//} (preferred) \ {/*..*/}
how do you write a javadoc comment in java \ {/** .. */}
what the type comparison oper in java \ {instanceof}
how do you concaten string in java \ {"a" + "b";}
how do you give a hex liter in java \ {int i = 0x3a;}
how do you enter binari liter in java \ {int i = 0b011101;}
doe java intern string yeah, it intern string literals.
how do you enter a doubl in scientif notat in java \ {doubl a = 10.1e4;}
what type is a java numer liter which contain a decim point float if it got a \ {f} or \ {f} doubl if it \ {d} or \ {d} or doesn't have one
what'r java unsign type \ {char} is the onli one
what'r java special float point entiti \ {double.positive\_infinity} \ {double.negative\_infinity} \ {double.nan} there also a \ {-0.0} entity, but it doesn't have a constant.
how do you test if a java doubl is infinit \ {double.isinfinite(x)}
how do you test if a java doubl is nan \ {double.isnan(x)}
what the differ between \ {boolean} and \ {boolean} in java \ {boolean} is a primat type \ {boolean} is a refer type same for all the primat types.
what autobox in java implicit convert a primat type to it refer type when necessary.
what the best way to unbox a primat refer type in java if it not done automatically, \ {valueofintegertype.intvalue()} and similar for other types.
what are java valu type the boolean and numer type (aka the primat types)
how doe java initi variabl by default instanc variabl are initi to null local variabl aren't initialized. access them is an error.
how do you declar an array in java \ {int[] array;} (preferred) \ {int array[];}
how do you declar a multidimension array in java \ {int[][] array;} \ {int array[][];}
how do you write a multidimension array liter in java \ {int[][] array = \{\{1, 2\}, \{3, 4\}\};}
how do you creat an anonym array in java \ {int[] array = new int[] \{7, 3, 4\};}
doe java pass by refer or valu valu (but it the refer to an object that passed, not the object itself)
what the default implement of equal for refer type in java refer equality.
how is string equal test implement in java \ {equals()} test for equal charact by charact \ {==} test for refer equality!
what'r java mutabl string type \ {stringbuffer} \ {stringbuilder}
what clone in java copi an object rather than just the refer to the object.
how do you creat shallow clone of object in java implement \ {cloneable} and call the \ {(type) clone()} method cast is need as it return an \ {object} by default.
what the prefer way to conduct deep clone in java copi constructors. peopl often use serial though.
how do you creat a subclass in java \ {class subclassnam extend superclassname}
how do you implement an interfac in java \ {class implementornam implement interface1, interface2}
which method can be overridden in java one that aren't \ {final, static, private}
what'r the rule about overrid and error in java the overrid can't throw check except other than those declar by the function it overriding.
how do you instanti a java class \ {typenam variablenam = new constructorname()}
what are getter and setter known as in java accessor and mutat functions.
how can a subclass refer to it parent in java \ {super}
how do you deleg to the parent class constructor in java the first statement in the constructor should be \ {super()}
how do you refer to the contain object in java \ {this}
how do you implement variable-length argument list in java \ {void functionname(string... arglist)} and \ {arglist} can be access as an array.
how do you denot an abstract class in java \ {abstract class classnam \{ \}}
how do you denot an abstract method in java \ {abstract void functionname()}
how do you give a class member static durat in java \ {static int i;}
what a static initi in java \ {static \{ classvariable1 = initialvalue1; classvariable2 = initialvalue2; \}} ani number of static initi are allow amp; they'll be execut in order.
how do you creat an enum in java {verbatim} enum enumnam { value_1 (arg1, arg2), value_2 (arg1, arg2); enumname(type1 param1, type2 param2) {..} } {verbatim}
how do you iter over an enum in java \ {for (enumnam v : enumname.values())}
what privaci modif are avail in java \ {public} (class, package, subclass, world) \ {protected} (class, package, subclass) no modifi (class, package) \ {private} (class)
what are java annot \ {@annotationname} provid a way to associ metadata with class or methods.
what'r java built-in annot \ {@override} \ {@deprecated} \ {@suppresswarnings}
what a marker in java an annot with no parameters.
how do you defin custom annot in java \ {public @interfac annotationnam \{ } \ {keytyp keyname1() default defaultvalu \} }
what restrict are there on custom annot in java method can't have ani paramet or a throw claus and can onli return specif type
how do you use a multivalu annot in java \ {@annotationname(keyname1 = value1, keyname2 = value2)}
how do you implement annot in java that'll be read at runtim annot the annot defint with \ {@retention(retentionpolicy.runtime)}
how do you use a singl valu annot in java \ {@annotationname(value)}
how do you restrict where a java annot can appear decor it with \ {@target(\{elementtype.method\})}
do java switch statement fall through yup.
how do you implement a foreach loop in java \ {for (typenam a : as)} where \ {as} is an \ {iterable} or an array.
how are assert implement in java \ {assert boolean\_express : error\_string\_expression;} in debug mode, if the boolean express evalu to false, the error string is ed and the program terminates.
what doe the \ {final} modifi do in java on classes, it prevent subclassing. on methods, it prevent overrid on variables, it prevent modification.
how do you denot the packag of a java class \ {packag com.oreilly.tutorial;}
how do you import packag into a java class \ {import java.util.gregoriancalendar}
what packag are import by default in java \ {java.lang}
how do you defin final in java overrid \ {finalize()} tri not to though, becaus it a poor way to clean up resources.
what six type can java annot key return primat string type enum annot array of the aforementioned.
what are the basic navig key in vim h (left) j (down) k (up) l (right)
how do you return to normal mode in vim esc ctrl-c
how do you exit vim and discard chang :q!
how do you delet singl charact in vim in normal mode, \ {x}
how do you insert text in vim from normal mode, enter insert mode with \ {i}
how do you append text to a line in vim \ {a} in normal mode
how do you quit and save chang in vim \ {:wq}
what the end-of-the-lin motion in vim \ { }
what the end-of-the-current-word motion in vim \ {e}
what the before-the-start-of-the-next-word motion in vim \ {w}
what the delet oper in vim \ {d}
how do you repeat a motion in vim oper then number then motion. ie \ {d3w} delet the next three word
how do you move to the start of the line in vim \ {0} in normal mode
how do you delet a whole line in vim \ {dd} in normal mode
how do you undo all edit to a particular line in vim \ {u} in normal mode
how do you undo a command in vim \ {u} in normal mode
how do you undo an undo in vim \ {ctrl-r}
how do you `put a line in vim \ {p} in normal mode
how do you cut and past a line in vim in normal mode \ {dd} to cut it \ {p} to past it
how do you replac a singl charact in vim \ {rx} in normal mode, where \ {x} is the substitut charact
what doe the chang oper do in vim delet a portion of text and put you in insert mode so you can replac it
what the chang oper in vim \ {c} in normal mode
how do you find your current locat in a file in vim \ {ctrl-g}
how do you move to the end of the file in vim \ {g} in normal mode
how do you move to the start of the file in vim \ {gg} in normal mode
how do you move to a specif line number in vim in normal mode, \ {45g} will move you to line 45.
how do you conduct a forward search in vim \ {/searchphrase} in normal mode then \ {n} to move forward, \ {n} to move back
how can you skip between a search and the origin of the search in vim \ {ctrl-o} to skip back to the start of the search and \ {ctrl-i} to skip to where you skip back from.
how do you find a parenthes partner in vim \ { } in normal mode.
how do you substitut for the first occur of a phrase in a line in vim \ {:s/oldphrase/newphrase} to replac the first occur of oldphras with newphras
how do you substitut for the first occur of a phrase in a file in vim \ {: s/oldphrase/newphrase} to replac the first occur of oldphras with newphras
how do you substitut for the first occur of a phrase in a specif rang of line in vim \ {:3,7s/oldphrase/newphrase} to replac the first occur of oldphras between line 3 and 7 with newphras
what are some common switch for vim substitut command \ {oldphrase/newphrase/g} caus it to replac everi instanc of oldphras in it rang \ {oldphrase/newphrase/gc} caus it to replac everi instanc of oldphras in it range, give a prompt each time
how do you execut a shell command in vim \ {:!command} in vim
when must a vim normal mode command be termin with \ {enter} when it start with \ {:}
what the linux command to get the current directori \ {pwd}
how do you save a file under a new name in vim \ {:w filename}
how do you select text in vim \ {v} in normal mode
how do you copi the content of a file into the current file in vim \ {:r filename} in normal mode
how do you copi the of a command into the current file in vim \ {:r !command}
how do you insert a line below the cursor in vim \ {o} in normal mode
how do you insert a line abov the cursor in vim \ {o} in normal mode
how do you insert text after the cursor in vim \ {a} in normal mode
how do you enter replac mode in vim \ {r} in normal mode
what the copi oper in vim \ {y} in normal mode
how do you ignor case in a vim search \ {/searchpattern\textbackslash c}
how do you enabl and disabl option in vim \ {:set optionname} \ {:set nooptionname}
how do you open a file for edit in vim \ {:e filename} in normal mode
what are the autocomplet command in vim \ {ctrl-d} to get a list of command match the charact enter so far \ {tab} to complet
how do you access help in vim \ {:help}
how do you instanti a maven archetyp \ {mvn archetype:generate}
what a maven archetyp a skeleton for a project.
what the structur of a maven command \ {mvn [plugin:]go -keyname=keyvalue}
what are maven coordin four element of the pom: \ {groupid} \ {artifactid} \ {packaging} \ {version} that uniqu identifi a project.
how do maven pom work the pom in a directori extend a parent pom (which extend it parent, etc) and togeth they describ what maven should do with a project
how can you view the total pom of a maven project \ {mvn help:effective-pom}
what are the basic phase of the default maven lifecycl valid compil test packag integration-test verifi instal deploy
what a maven lifecycl phase an order list of zero or more goals.
what happen when you execut a maven goal all phase and goal up to and includ that goal will be execut in order.
what pom in maven project object model, a declar descript of a project.
which part of a maven pom are meant for human consumpt \ {name} \ {url}
how do you add a depend to a maven project in the \ {dependencies} environ of the pom add a \ {dependency} environ which has the \ {groupid}, \ {artifactid} and \ {version} of the dependency.
how do you find the maven coordin of a depend use \ {repository.sonatype.org}
how do you move to the first non-blank charact in a line in vim \ {\^{}}
what are java standard stream system.in system.out system.err
how do you interact with the consol in java through the \ {console} class
what are the four basic compon of java io class hierarchi \ {reader} \ {writer} \ {inputstream} \ {outputstream}
how do you read a line of charact text from a file in java creat a \ {filereader("filename")} then use it to creat a \ {bufferedreader(filereader)} and call \ {breader.readline()} while it not null. then close it with \ {breader.close()}.
how do you read binari data from a file in java creat a \ {fileinputstream("filename")} and use it to creat a \ {datainputstream(filestream)} then call \ {instream.read()}
how do you write charact data to file in java creat a \ {filewriter("filename")} and use it to creat a \ {printwriter(filewriter)} then call \ {printwriter.println(str)}
how do you write binari data to a file in java creat a \ {file("filename")} object and use it to creat a \ {fileoutputstream(file)} and use that to creat a \ {dataoutputstream(fileoutputstream)} then call \ {outstream.writeint(i)} or similar
when read or write a larg amount of binari data in java, what should you do use a \ {bufferedinputstream} or \ {bufferedoutputstream} rather than a filestream
how do you get the input stream of a socket in java creat a \ {socket("ip", portnum)} call \ {socket.getinputstream()}
how do you exclud a data member in java from serial mark it \ {transient}
how do you serial an object in java creat a \ {fileoutputstream("filename")} and use it to creat an \ {objectoutputstream(filestream)} the use \ {objectstream.writeobject(objname)}
how do you deseri an object in java creat a \ {fileinputstream("filename")} and use that to creat an \ {objectinputstream(filestream)} then call \ {(type) objectstream.readobject()}
what the old way of interact with directori and file in java use the \ {file} class. \ {delete()}, \ {exists()}, \ {list()}, etc
how do you get random access to a file in java creat a \ {file("filename")} use it to creat a \ {randomaccessfile(file, "rw")}
what the new way of interact with the filesystem in java nio 2.0 (new input/output) \ {java.nio.file}
what are the main class of nio2.0 in java \ {path} interfac and \ {paths} class (an upgrad version of \ {java.io.file}) \ {files}, a static set of method on paths.
how do you creat thread in java extend \ {thread} and overrid \ {run()} or implement \ {runnable} and defin \ {run()}
how do you start a java thread \ {thread.start()}
how do you paus and resum a java thread use \ {wait()} and \ {notify()}
what the simplest implement of a mutex in java mark a with \ {synchron (syncobject) \{..\}} decor a method with \ {synchronized}, which use \ {this} as the sync object
how can you creat a thread pool in java use the factori method in the class \ {executors} to make an \ {executorservice}
how do you use thread pool in java given a threadpool \ {executorservice} and a \ {runnable} object \ {task} call \ {executorservice.execute(task)}
what are the common interfac of the java collect framework \ {collection} \ {list} \ {map} \ {queue} \ {set}
what are the common implement of java \ {list} interfac arraylist linkedlist
what are the common implement of java \ {map} interfac hashmap linkedhash treemap
what java \ {linkedhash} datatyp a hashtabl which is also a link list, allow order access
what are the common implement of java \ {set} interfac \ {hashset} \ {linkedhashset} \ {treeset}
what'r the common implement of java \ {queue} interfac \ {priorityqueue}
where are java method on the \ {collection} interfac store in the \ {collections} class
how do you order item in java collect implement \ {comparator} and overrid it \ {compare} function, then pass it to the collect constructor.
what the diamond oper in java when initi a generic type, the initi can use \ {generictyp } rather than the full type ie \ {list integ list = new arraylist ();}
how do you constrain a type paramet to implement multipl interfac in java \ { t extend p amp; s } where \ {s} and possibl \ {p} is an interfac
how do you constrain a type paramet in java to deriv a subclass \ { t super p }
what a generic wildcard in java \ { extend p } or similar use when you don't need to know the specif type
when should \ { extend p } and \ { super p } be use in java \ { extend p } when you onli *get* item out of a structur (covariant) \ { super p } when you onli *put* item into a structur (contravariant)
how do you declar a generic java method in a nongener class preced the return type with a generic parameter, and preced the call with one too ie \ {public t t func(t t)} \ { integ func(i)}
what the under implement of generic in java type erasur at compil time, the generic paramet is replac with \ {object} and ani cast necessari are introduced.
what'r the problem with java implement of generic can't access type in ation at runtime. can't overload method with onli generic parameters, as they'll all be eras to \ {object} cast to generic type are unchecked.
what reific in the context of java refin the java generic framework to retain type in ation at runtime.
how do you write lambda in java 8 \ {(int i) - f(x)}
how do you refer static method in java 8 \ {typename::methodname}
how do you refer method in java 8 \ {objectname::methodname}
how doe java doubl brace initi work \ {list integ () \{\{ add(1); \}\} } outer set of brace declar an anonym inner subclass inner set of brace defin an instanc initializer.
what are the restrict on java doubl brace initi can't be use on \ {final} class can't be use with the diamond generic operator.
what java boolean type \ {boolean}
what'r the step in johnson algorithm start with a singl node. then: add a node $q$ to the graph and connect it with weight 0 edg to everi other vertex. use bellman-ford to find the minimum weight $h_q(u)$ path from $q$ to each node $u$ in the graph. negative-weight cycl might be detect here, in which case terminate. reweight all edg $uv$ from $w_{uv}$ to $w_{uv} + h_q(u) - h_q(v)$ edg are now all posit weight, so run dijkstra on each vertex.
what the runtim of johnson algorithm $o(n(m + n \lg n))$
which algorithm solv the all-pair shortest path problem on graph with negat weight edg johnson algorithm.
what'r the step in the bellman-ford algorithm initi assum the distanc from the sourc to each vertex $d(v)$ is infinite. repeat $n$ times: for each edg $uv$, set $d(v) = \min(d(v), d(u) + w_{uv})$. have updat all edges, if $d(v) d(u) + w_{uv}$, then the graph contain a negat weight cycle. abort.
what algorithm would you use to solv the single-sourc shortest path problem when there are negat weight edg bellman-ford.
what the runtim of bellman-ford $o(nm)$
what algorithm can you use to detect negative-weight cycl in a graph bellman-ford
what a harvard comput architectur one where data and instruct have separ path
what a modifi harvard architectur one where data and instruct are back by the same storage, but have separ path to the processor by far the most common architectur nowaday
what a static import in java \ {import static packagename;} which allow you to refer static method without qualifi them
how do you build a regex in java call \ {pattern.compile("patternstring")} to get a pattern object then call \ {pattern.matcher("targetstring")} to get a matcher object which you can then call differ regex oper from
what are the core compon of typescript the compil the ts languag service, a vs plugin the declar file
what typescript a superset of js that compil to idiomat js
how can you get split-screen edit for ts in vs instal web essenti extens
how do you declar a variabl in ts \ {var variablename: variabletyp = value;}
what a ts declar file a file contain variabl and type of variables, to provid support to the tsls
how do you import anoth file in ts add a declar file for the librari to the project and refer it with \ {/// refer path="deffilepathname.d.ts"/ } at the top of the file
how do you declar a class in ts \ {class classnam \{ .. \}}
how do you declar static method in ts in a class, defin \ {static methodname(): returntyp \{ ... \}}
what the differ between regular method and static method in ts regular method have to be access through an instanti copi of a class static method can be access from the class itself.
how is a modul declar in ts \ {modul module.nam \{ export class classnam \{ .. \} \}}
how are class in other modul referenc in ts by the fulli qualifi name; there no \ {import} keyword
what type is an untyp variabl in ts \ {any}
how do you declar an array type in ts \ {var arrayname: any[] = new array();}
what the differ between index into an array with a number vs a string in ts index into an array with a number will give a type index into an array with a string will give a \ {any} type
how do you cast a variabl in ts \ {var variablename: int = int anothervariable;}
what an ambient declar in ts \ {declar var variablename;}, which is a standin for a variabl that'll be provid by the script environ
what are the three type of function in ts global functions, defin outsid of ani class or modul class method anonym function
how do you give a function declar in ts \ {functionname(argname: argtype) = returntyp \{ ... \}}
what the type of a function in ts \ {(argname: argtype) = returntype}
how do you mark a paramet as option in ts \ {function functionname(argnam : argtyp = defaultvalue): returntype}
how do you declar a vararg paramet in ts \ {function functionname(...paramname: paramtype[]): returntype}
how do you test whether a variabl is without a valu in ts \ {variablenam === null variablenam === undefined}
what the differ between loop over an array with \ {for in} and with \ {for} in ts \ {for in} will convert each valu to a string \ {for} will preserv the type
how do you overload function in ts \ {function functionname(paramname: childtype1): returntype;} \ {function functionname(paramname: childtype2): returntype; } \ {function functionname(paramname: parenttype): returntype;} \ { if (paramnam instanceof childtype1) \{ .. \}} \ { if (paramnam instanceof childtype2) \{ .. \}} \ {\}}
how do you defin getter and setter in ts \ {class classnam \{} \ { privat fieldname: typename; } \ { get fieldname: typenam \{ return this.fieldname; \}} \ { set fieldname(value: typename) \{ this.fieldnam = value; \}} \ {\}}
how do you access an instanc field in the same object in ts \ {this.fieldname} have to use \ {this}!
what an arrow function in ts \ {argnam = \{ .. \}} is like \ {function(argname) \{ ..\}} but \ {this} will inherit it definit from it lexic scope rather than be set to global
how do you declar a constructor in ts \ {constructor (paramname: paramtype) \{ .. \}}
how do you declar a class event in ts \ {class classnam \{ eventnam : (ev: event) = any; \}}
how do you refer static method from elsewher in a class in ts \ {classname.methodname}. no \ {this} required.
how do you creat an anonym type in ts \ {var anon = \{ fieldname1: value1, fieldname2: value2 \}} type will be infer by the tsls.
how do you creat a subtyp in ts \ {class classnam extend parentclassnam \{ .. \} }
how do you refer a parent type constructor in ts \ {super(args)}
how do you overrid a method in ts just give a method with the same type signatur in the child class.
how can you alia a function type in ts \ {interfac ialiasnam \{} \ { (paramname: paramtype): returntype;} \ {\}}
how do you declar an interfac in ts \ {interfac iinterfacenam \{ ... \}} with the bodi be a list of signatur
how can you alia an import in ts \ {/// refer path="importedfile.ts"/ } \ {import aliasnam = importedfileclassfullname;}
what are the three main type of machin learn predict learn descript learn reinforc learn
what supervis learn anoth name for predict learn
what unsupervis learn anoth name for descript learn
what the general outlin of a predict learn problem given a train set of input-output pair $d = \{(x_i, y_i)\}$ then given some other $x$, predict it associ $y$.
what are two other name for featur in predict learn attribut covari
what the name for the output variabl $y$ from a predict learn algorithm the respons variabl
what the differ between the classif problem and the regress problem in machin learn classif problem have a categor respons variabl regress problem have a respons variabl with an order
what anoth name for classif problem in machin learn pattern recognit problem
what a knowledg discoveri problem in machin learn anoth name for descript learn
what the outlin of a descript learn problem given a set of input $d = \{x_i\}$ find some interest pattern in the data
what reinforc learn teach a system how to behav through reward and punish signal
what the differ between multiclass and multilabel classif in multiclass, each input can be assign to one of mani class in multilabel, each input can be assign to ani number of label
what doe $p(i x, d)$ denot in machin learn the probabl distribut of respons $y$ given input vector $x$ and train set $d$
what the map estim in machin learn maximum a posteriori estim ie assign to $x$ the mode of $p(i x, d)$
what densiti estim in machin learn reconstruct a probabl distribut from observ data
what a latent variabl in machin learn a variabl that doesn't appear in the dataset.
what anoth name for a latent variabl a hidden variabl
what pca stand for in machin learn princip compon analysi
what the use of princip compon analysi a machin learn approach to dimension reduction: given a set of high-dimension $x$, deduc the latent low-dimension $z$ that inferr them
what anoth name for imput in machin learn matrix complet
what imput in machin learn infer the valu of miss data from the data you do have
what the main problem in k-nearest neighbour algorithm dimensionality. in high-dimension data, there a looong way between datapoints.
what the interpret of the posterior distribut the probabl of an event given evid
what the prior distribut the probabl of an event befor take some piec of evid into account
what the likelihood in bayesian statist the probabl of the evid given some event
how are the posterior, prior and likelihood distribut relat $posterior \propto likelihood prior$
what induct bias in machin learn the set of assumpt made by the learner in order to make predict
how is a linear respons often written in machin learn by defin the featur vector to be $x^\prime = (1, x)$, a linear equat with a constant term can be written as $w^t x^\prime$
what the sigmoid function $sigm(\eta) = \frac{1}{1+e^{-\eta}}$
what anoth name for the sigmoid function logist
what distribut is use in logist regress $p(i x, w) = \text{ber}(i \text{sigm}(w^t x))$ though it actual a of classification, not regress
what doe $\text{ber}(i \mu(x))$ repres in machin learn the bernouilli distribut with mean $\mu(x)$
what a valid set in machin learn a set of input against which a model can be test often generat by split off part of the data intend for train
what the general error in machin learn the expect valu of the error on futur data
what the relationship between the training, valid and test set in machin learn a set of model should be train on the train set and the one with the lowest error on the valid set should be evalu against the test set.
what the usual size of the valid set in machin learn 1/4 of the size of the train set
what cross valid in machin learn split the train data into $k$ `fold train on all but the $k$th and then test on the $k$th for each $k$. the test error is then approxim as the averag of the error on each of the fold
what loocv in machin learn leave-one-out cross validation, where there are as mani fold as datapoint
how mani fold are typic use in cross valid 5
what the no free lunch theorem in ml ani two optim algorithm are equival when their per anc is averag across all possibl problem
what knuth shuffl algorithm given a string $s$ of length $n$ for $0 i n$ swap $s_i$ with $s_k$, where $k _r \{i, .., n-1\}$
what the standard algorithm for random permut a string knuth shuffl algorithm
what a common problem with initi static variabl in c++ if the initi depend on other static variables, the c++ spec doesn't lay out in what order the variabl will be initi
what anoth name for a merkl tree a hash tree
what doe $x y$ repres in machin learn uncondit independence; $p(x, y) = p(x)p(y)$
what doe $x y z$ repres in machin learn condit independence; $p(x, y z) = p(x z)p(i z)$
what margin independ anoth name for uncondit independ
what the pdf of the categor distribut in machin learn $\text{cat}(x \theta) = \prod \theta^{\mathbb{i}(x_j = 1)}_j$
what anoth name for the categor distribut in machin learn the multinouilli distribut
what the empir distribut of $d = \{x_1, \dotsc x_n\}$ $p_{emp}(a) = \frac{1}{n} \sum \delta_{x_i}(a)$ where $\delta$ is the dirac measur
what the dirac measur $\delta_x(a) = 1$ iff $x a$ els 0
what the precis of a gaussian distribut $\lambda = 1/\sigma^2$
what the fundament reason that the gaussian distribut is so popular it make the least number of assumpt in the sens that it the distribut with maxim entropi when constrain to a specif mean and varianc
what the sift properti of a delta function $ t^ fty_{- fty} f(x) \delta(x - \mu)dx = f(\mu)$
what a cauchi distribut a student distribut with a singl degre of freedom
what a lorentz distribut anoth name for a cauchi distribut
what notabl about the cauchi distribut the tail are so heavi that the integr for the mean doesn't converg
what do various degre of freedom get you from a student distribut $\nu = 1$ is a cauchi distribut $\nu = 2$ is the minimum requir to get finit varianc $\nu = 4$ is the usual set that'll give good per anc in a rang of problem $\nu 5$ rapid approach a gaussian
in machin learning, what general the advantag in use a student distribut over a gaussian heavier tail mean it less sensit to outlier
what the advantag of the laplac distribut over the gaussian heavier tail and a higher densiti at the origin, which is use for encourag sparsiti in a model
what the pdf of the laplac distribut $lap(x \mu, b) = \frac{1}{2b}exp(-\frac{ x-\mu }{b})$
what are the common special case of the gamma distribut exponential, $exp(x \lambda) = ga(x 1, \lambda)$ erlang, $erl(x k, \lambda) = ga(x k,\lambda)$ chi-squared, $\chi^2(x \nu) = ga(x \frac{\nu}{2}, \frac{1}{2})$
what doe the invers gamma distribut describ if $x ga(a, b)$ then $\frac{1}{x} ig(a, b)$
how do differ paramet affect the beta distribut if $a 0$ or $b 0$ then it non-integr if $a, b = 1$ you get the uni distribut if $a, b 1$ you get a bimod distribut with spike at $0, 1$ if $a, b 1$ you get a unimod distribut
what the pareto distribut usual use for model quantiti with veri heavi tail
what zipf law that the frequenc of word follow a power law
what the pmf of the poisson distribut $\text{poi}(x \lambda) = e^{-\lambda}\frac{\lambda^x}{x!}$
what the support of the poisson distribut nonneg integ
what the usual use of the poisson distribut modeel the count of rare event (like nuclear decays)
what the pdf of the gaussian distribut $n(x \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{(x-\mu)^2}{2\sigma^2})$
what the error function in probabl $\text{erf}(x) = \frac{2}{\sqrt{\pi}} t^x_0 e^{-t^2}dt$
how do you defin the cdf of the gaussian in term of the error function $\phi(x; \mu, \sigma) = \frac{1}{2} + \frac{1}{2}\text{erf}(\frac{x-\mu}{\sqrt{2}\sigma})$
what the pdf of the student $t$ distribut $t(x \mu, \sigma^2, \nu) \propto \left[ 1 + \frac{1}{\nu} \left(\frac{x-\mu}{\sigma}\right)^2 \right]^{-\frac{\nu+1}{2}}$
what'r the mean and varianc of student t distribut mean is $\mu$ varianc is $\frac{\nu}{\nu - 2} \sigma^2$
what the mean and varianc of the laplac distribut mean is $\mu$ varianc is $2b^2$
what are the name of the gamma distribut paramet either shape ($a$) and rate ($b$) or shape ($a$) and scale ($1/b$)
what the pdf of the gamma distribut with the shape and rate parameterization: $\text{ga}(x a, b) = \frac{b^a}{\gamma(a)}x^{a-1}e^{-bx}$
what the definit of the gamma function $\gamma(x) = t^ fty_0 u^{x-1} e^{-u} du$
what'r the mean and varianc of the gamma distribut use the shape and rate parameter mean is $a/b$ varianc is $a/b^2$
what the mode of the gamma distribut with the shape and rate parameter the mode is $\frac{a-1}{b}$
what the pdf of the beta distribut $beta(x a,b) = \frac{1}{b(a,b)}x^{a-1}(1-x)^{b-1}$
what the definit of the beta function $b(a, b) = \frac{\gamma(a)\gamma(b)}{\gamma(a+b)}$
what the mean and varianc of the beta distribut mean is $\frac{a}{a+b}$ varianc is $\frac{ab}{(a+b)^2(a+b+1)}$
what the mode of the beta distribut $\frac{a-1}{a+b-2}$
what the pdf of the pareto distribut $\text{pareto}(x k, m) = \frac{km^k}{x^{k+1}}\mathbb{i}(x m)$
how do the paramet to the pareto distribut control it shape $m$ control the minimum nonzero valu $k$ control the falloff abov that value; as $k \rightarrow fty$, the distribut converg on $\delta(x-m)$
what doe the pareto distribut take when plot on a log-log scale a straight line.
what the mean of the pareto distribut $\frac{k}{k-1}m$ if $k 1$
what the definit of the pearson correl coeffici $\text{corr}(x, y) = \frac{\text{cov}(x, y)}{\sqrt{\text{var}(x)\text{var}(y)}}$
what the definit of the covari $cov(x, y) = e[(x-e[x])(y-e[y])^t]$
what'r the structur properti of a covari matrix symmetr posit definit
what the concentr matrix in statist invers of the covari matrix
what anoth name for the precis matrix in statist the concentr matrix
what it mean for a covari matrix to be isotrop it got the $\sigma^2 \mathbb{i}$, give it one free paramet
which distribut is most common use which has a support of the probabl simplex the dirichlet distribut
what the multivari general of the beta distribut the dirichlet distribution.
what the covari matrix of $ax+b$ $\text{cov}(ax+b) = a \text{cov}(x) a^t$
what the entropi of a distribut $h(x) = e_x[\ln p(x)]$
what the binari entropi function $h(\theta) = - \theta \ln \theta - (1-\theta)\ln(1-\theta)$
what relat entropi anoth name for the kullback-leibl diverg
how doe the kullback-leibl diverg relat to the entropi for discret distribut $kl(p q) = -h(p)+h(p,q)$
what the definit of the cross entropi for discret distribut $h(p, q) = -\sum p_k \ln q_k$
what the intuit interpret of the cross entropi it the averag number of bit need to encod data come from a sourc with distribut $p$ when model $q$ is use to defin the codebook
what the intuit interpret of the kl diverg it the averag number of extra bit need to encod a sourc with distribut $p$ use a codebook defin by a model $q$ compar to use a codebook defin by a model with distribut $p$
what laplac principl of insuffici reason if you'v got a set of indistinguishable, mutual exclus and collect exhaust events, then you should use the uni distribut to model them
what the definit of the condit entropi $h(i x) = \sum p(x) h(i x = x)$
what the fundament definit of mutual in ation $\mbb{i}(x;y) = \sum_{x, y} p(x,y) \ln \frac{p(x, y)}{p(x)p(y)}$
what the definit of mutual in ation in term of the kl diverg $\mbb{i}(x;y) = kl(p(x, y) p(x)p(y))$
what the definit of mutual in ation in term of condit entropi $\mbb{i}(x;y) = h(x) - h(x y) = h(y) - h(i x)$ ie it the reduct in uncertainti about one variabl you get from observ the other
what the intuit interpret of the mutual in ation it tell you how much knowledg one variabl give you about the other - like a more general correl coeffici
what the defint of the pointwis mutual entropi $\text{pmi}(x, y) = \lg \frac{p(x,y)}{p(x)p(y)}$
what the intuit interpret of the pointwis mutual in ation it the amount learnt by updat the prior $p(x)$ into the posterior $p(x y)$
how doe the mutual in ation and the pointwis mutual in ation relat pmi is mi on a singl event mi is the expect valu of the pmi
how is the mutual in ation of an empir distribut usual calcul by discret the data into bin and use that to calcul the mi
what the use of the maxim in ation coeffici when discret a distribut so you can calcul the mi, the choic of bin grid can affect the s. the mic is calcul over all possibl grid and a valu of 0 repres no relationship; 1 repres a noise-fre relationship *of ani *
what the defint of the maxim in ation coeffici let $m(i, j) = \frac{1}{\log \min(i, j)} \max_g i(x(g), y(g))$ where $g$ is a 2d grid of dimens $x y$ use to discret the variabl then $mic = \max_{i, j : ij b} m(i, j)$, where $b$ is some upper bound on the number of bins.
what the typic number of bin use in calcul the mic of a distribut $b = n^{0.6}$
what a version space in machin learn it the subset of the hypothesi space that consist with the data
what the extens of a concept in machin learn the set of datapoint that'd be consist with the concept
what the size principl in machin learn a alize of occam razor: you should favour the hypothesi in the version space with the smallest extension.
how do you general deal with "unnatural" hypothes in a generat classifi by suppress them in the prior.
what the relationship between the map and mle estim in statist under certain regular conditions, the mle converg to the map as the amount of data increas and overwhelm the prior.
what the mle in statist maximum likelihood estim
what doe it mean for a hypothesi space to be identifi in the limit that given infinit data, the true hypothesi could be recov
what the analogu of the observ belief state in bayesian statist the posterior distribut
what bay model averag construct a predict densiti by sum over all possibl hypotheses, weight by their posterior probabl ie $p(\tild x d) = \sum p(\tild x h)p(h d)$
what the plug-in approxim in machin learn the predict distribut attain by assum a posterior distribut entir concentr at the map estim ie $p(\tild x c d) = p(\tild x \hat h)$
what usual the problem with the plug-in approxim to the predict densiti it drastic overestim our confid
what the main contrast in how plug-in approximation-bas predict system learn compar to how bma-bas predict system system learn plug-in base system will start with a veri narrow predict distribut and gradual widen it as they receiv data bma base system will start with a veri broad predict distribut and gradual narrow it as they receiv data
what a suffici statist for a data $s(d)$ is suffici iff $p(\theta d) = p(\theta s(d))$
what'r hyperparamet in statist the paramet of the prior distribut
what are the assumpt of the naiv bay classifi that the featur are condit independ given the class label.
what stem in machin learn strip common suffix and prefix off word to get them in their "base "
what the log-sum-exp trick in machin learn in order to avoid numer underflow when work with veri small probabilities, it best to appli bay rule use logarithm howev this can requir take the log of a sum, which obvious can't be broken down. solut is to factor the largest term out of the sum and take that outsid the logarithm. ie $\log \sum e^{b_c} = \log\left(\sum e^{b_c - b}\right) + b$ where $b = \max b_c$
what featur select in machin learn remov `irrelev featur from a dataset to both acceler ani method appli and to reduc the effect of overfit
what'r the three simplest approach to document classif in machin learn bernouilli product model: consid whether each word appear in a document multinomi model: consid number of time each word appear dirichlet compound multinomial: consid number of time and account for `bursti
what the bursti problem in machin classif most word don't appear in most documents, but when they do appear they appear a lot!
what the simplest solut to the bursti problem in machin learn encount one occur of a word updat the posterior count on that word, make anoth occur more likely. this is the dirichlet compound model. also equival to a polya urn, where on draw a ball you replac it along with an addit copy.
what the problem with the dirichlet compound multinomi model for document classif fit it is hard.
what the pdf of the dirichlet compound multinomi model $p(\mathbf{x} y = c, \mathbf{\alpha}) = \frac{n!}{\prod_j x_j!} \frac{b(\mathbf{x} + \mathbf{\alpha_c})}{b(\mathbf{\alpha_c})}$
how doe student t distribut aris it the distribut of the sampl mean of a $\nu+1$ item sampl drawn from a gaussian distribut
how doe a chi squar distribut aris it the distribut of the sum of squar of $k$ independ standard normal variabl
how doe a laplac distribut aris it the distribut of the differ between two exponenti distribut random variables.
how doe an erlang distribut aris as the sum of $k$ exponenti distribut variabl
what the pdf of the dirichlet distribut $dir(x \alpha) = \frac{ \mathbb{i}(x s_k)}{b(\alpha)}\prod x_k^{\alpha_k - 1}$ where $s_k$ is the probabl simplex on $k$ variabl
how is the beta function defin on $k$ variabl $b(\alpha) = \frac{\prod_1^k \gamma(\alpha_k)}{\gamma(\alpha_0)}$ where $\alpha_0 = \sum_1^k \alpha_k$
what the usual mental model for a dirichlet distribut it the distribut of ball color in a polya urn - each time a ball is drawn, the ball is replac along with one of the same color.
what'r the mean and varianc of a dirichlet distribut mean of variabl $x_k$ is $\frac{\alpha_k}{\alpha_0}$ varianc of $x_k$ is $\frac{\alpha_k(\alpha_0 - \alpha_k)}{\alpha_0^2(\alpha_0 + 1)}$ where $\alpha_0 = \sum_1^k \alpha_k$
what the mode of the dirichlet distribut for variabl $x_k$ it $\frac{\alpha_k - 1}{\alpha_0 - k}$ where $\alpha_0 = \sum_1^k \alpha_k$
what are the categori of command in sql schema statements, use to creat tables/indexes/constraints/etc data statements, use to create/manipulate/retriev data transact statements, use to begin/end/rollback transact
what a surrog key in databas system an attribut generat by the dbms, not deriv from the data, whose onli signific is to act as a key
how do you write a comment in sql \ {/*..*/}
how do you log into mysql as root from the commandlin \ {mysql -u root -p}
how do you creat a new empti databas in sql \ {creat databas databasename}
how do you add a local user with full privileg to a mysql databas \ {grant all privileg on databasename.* to username'@'localhost identifi by password';}
how do you attach to a databas in mysql \ {use databasename;}
how do you load a \ {.sql} file into mysql \ {sourc filename}
how do you fetch the current date and time in mysql \ {select now();}
what the differ between char and varchar type in sql \ {char(n)} valu must be exact \ {n} charact long (if the provid valu is shorter, it'll be right-pad wtih space (these space will be strip when the string is retrieved)) \ {varchar(n)} valu can be less than \ {n} charact
how can you set the charact set of a attribut in mysql \ {varchar(20) charact set utf 8}
what sql type should be use for long tract of text \ {text} or one of it variant (\ {mediumtext, longtext})
how are \ {text} valu compar in mysql on the first \~{}1024 bytes, though this can be increas
what'r the mysql numer type \ {int} and it variant \ {int unsigned} and it variant \ {float, double} and their variant
what mysql boolean type doesn't have one - just alias for \ {tinyint}, with 0 consid fals and non-zero valu consid to be true.
what'r mysql main tempor type date datetim time
how do you creat a sql tabl \ {creat tabl table\_name(attribute\_nam type\_name, constraint constraint\_nam constraint);}
how do you constrain the valu that an attribut can take in sql \ {creat tabl table\_name(attribute\_nam type\_nam check (attribute\_nam in (value1, value2, value3)));}
how do you get a descript of a tabl in mysql \ {desc tablename;}
how do you defin a foreign key constraint in sql \ {constraint fk\_name foreign key (key\_in\_this\_table) refer other\_t (key\_in\_other\_table)}
how do you constrain an attribut to be non-nul in sql \ {creat tabl table\_name(attribute\_nam type\_nam not null);}
how do you design a primari key in sql \ {constraint pk\_constraint\_nam primari key (key\_attr\_1, key\_attr\_2)}
what are the four sql data statement \ {insert} \ {update} \ {delete} \ {select}
how do you automat generat a primari key in psql \ {attr\_nam int unsign auto\_increment;}
how do you modifi the definit of a sql tabl \ {alter tabl table\_nam modifi component\_nam /* new defn */}
how do you insert data into an sql tabl \ {insert into table\_nam (attr\_name\_1, attr\_name\_2) valu (value\_a1, value\_a2), (value\_b1, value\_b2);} valu for attribut not specifi are assum to be null
how do you order data return by an sql queri \ {select * from table\_nam order by attr\_name}
how do you updat a row in a tabl in sql \ {updat table\_nam set attr\_name\_1 = val\_1, attr\_name\_2 = val\_2 where expr;}
how do you delet row from a tabl in sql \ {delet from table\_nam where expr}
how do you set tempor data in mysql \ {attr\_nam = str\_to\_date('jan-1-2010', ' b- d- y')}
how do you list the tabl in a sql databas \ {show tables;}
how do you remov a tabl from an sql databas \ {drop tabl table\_name;}
at a high level, how is a queri process by a sql databas permiss are check syntax is check queri optim generat an execut plan result set is return to the call applic
what are the six standard claus possibl in an sql select statement select from where group by have order by
what doe the \ {having} claus do in sql filter group accord to some predic
how do you select all column in an sql queri \ {select *}
how can you tran the of a sql select queri with a literal: \ {select string\_literal'} with an expression: \ {select col\_nam * 2} with a function call: \ {select upper(col\_name)}
how do you alia the column head return by a sql select queri \ {select col\_nam alias\_name} \ {select col\_nam as alias\_name}
how do you remov duplic from the set of a sql queri \ {select distinct col\_name}
how do you includ duplic in the set of an sql queri \ {select all col\_name} this is the default though
how do you creat a subqueri in sql \ {... from (/* queri */) as alias\_name;}
how do you creat a view in sql \ {creat view a (x, y, z) as /* queri */;}
what a view in sql a tabl defin in term of other tabl by way of a queri statement
what'r the standard boolean oper in sql and or not
how do you order s in specif ascend or descend order in sql \ {order by expr desc;} \ {order by expr asc;} (default)
how do you sort s by multipl attribut in sql \ {order by expr\_1, expr\_2}
what the inequ oper in sql \ { } sometim \ {!=} too
what the shorthand for a range-filt queri in sql \ {where exrp between lower\_bound and upper\_bound} will expand to a pair of \ { =, =} restrict
how do you test set membership in sql \ {expr in ('member1', member2', member3')}
how do you match string against a simpl pattern in sql \ {expr like pattern'}
what'r the wildcard avail for use in sql \ {like} oper \ { } (zero or more characters) \ {\_} (ani singl character) \ {[charlist]} (singl charact from the class) \ {[\^{}charlist]} (singl charact not from the class)
how do you match against a rang of charact with sql like oper \ {expr like [a-c]'}
how do you match against regular express in mysql \ {expr regexp pattern'}
what'r the rule with null and equal in sql while valu can be null, they'r never equal to null, nor are they not equal to null! and null is not equal to itself, nor is it not equal to itself! tl;dr 3vl
how do you test whether a valu is null in sql \ {expr is null}
how do you test whether a valu is not null in sql \ {expr is not null}
what'r the two main kind of join avail in sql \ {inner join} (default for \ {join}) \ {outer join}
what a cross join in sql the queri \ {x cross join y} that construct a cartesian product also the default of \ {x join y}
what the differ between an inner join and an outer join in sql inner join will onli return row for which the \ {on} claus is true outer join will return row for which the \ {on} claus is true, and row for which there no match row in the other tabl (in which case the data is fill in with null)
what implicit join syntax in sql it the pre-sql92 syntax for specifi join \ {from a, b where a.x = b.y} is implicit equival to the sql92 \ {from a inner join b on a.x = b.y}
how do you join on three tabl in sql \ {from a inner join b on a.x = b.i inner join c on a.x = c.z}
doe the order in which tabl are given in an sql join matter not usually. the queri optim will pick the tabl to use as the `drive tabl unless you hint that it shouldn't.
how do you use the same tabl twice in an sql queri give it two differ aliases; \ {from abc as a inner join abc as b}
what an equi-join in sql a join whose condit use onli equal comparison
what the shorthand for equi-join in sql \ {a inner join b use (x, y)} where \ {x, y} are the column to join on
what a natur join in sql a join on all column with match names, return a tabl with one column for each match pair of columns.
what the problem with natur join in sql they'r sensit to schema changes; ad a new column to one tabl could chang the
what a left outer join in sql an join that return row satisfi it condition, but also row from the left tabl with no match row in the right tabl
how do you per a natur join in sql \ {a natur join b}
how do you per a left outer join in sql \ {a left outer join b}
what the set oper preced in sql intersect has highest preced and the rest is determin top-to-bottom
how do you escap apostraph in sql with anoth apostraphe: \ {'don''t'}
what should you alway do when pass string data in sql wrap the string in a function like mysql \ {quote()} to automat escap them
how do you count the number of row in each group in a sql queri \ {select count(*) from a group by x}
what'r the five most common sql aggreg function \ {max()} \ {min()} \ {avg()} \ {sum()} \ {count()}
what implicit group in sql in the queri \ {select x, max(x) from a;} the set is implicit gather into a singl group over which \ {max} is taken. this onli work though if the attribut thing are be group by (\ {x}) has it own column in the s set.
how do you count onli distinct member of a group in sql \ {count(distinct x)}
how doe \ {count()} interact with null in sql it ignor them unless \ {count(*)} is use
how would you get the year part of a date in sql \ {extract(year from date)}
when aggreg data over group defin by multipl express in sql, how can you automat generat summari for the subgroup \ {group by a, b with rollup;} which will generat group on \ {(), (a), (a, b)} \ {group by a, b, c with cube;} which will generat group on \ {(), (a), (b), (a, b)}
what'r correl and noncorrel subqueri in sql correl subqueri refer column from the contain queri noncorrel subqueri are standalon
what a scalar subqueri in sql a subqueri that return a tabl with a singl row and column
what special about scalar subqueri in sql they can be use in equal and comparison oper
when can a subqueri be use as a set in sql when it return a singl column.
how are the \ {any} and \ {all} oper use in sql \ {expr = all set} test whether expr is equal to all element of set \ {expr ani set} test whether expr is not equal to ani one of the element of set etc for the other comparison oper
what a common mistak when use \ {in} and \ {all} in sql if the set be use contain \ {null}, you need to consid the queri in the context of 3vl
how do you use subqueri that return multipl column in sql use an alia use a parenthes express contain the same column name in the same order as the set of the subquery. example: \ {where (x, y) in (select x y from a)}
whi is the distinct between correl and uncorrel subqueri import uncorrel subqueri execut onc correl subqueri execut onc for each candid row
what sql \ {exists} oper it return true if it argument (usual a subquery) is nonempty.
when should you use a plain \ {join} in sql you shouldn't. when you intend to use a cross join, specifi \ {cross join} explicit
how are switch statement implement in sql \ {case when expr1 then 1 when expr2 then 2 els 3 end} \ {case expr when val1 then 1 when val2 then 2 els 3 end}
do sql \ {case} express fall through nope.
what are the granular of lock avail in sql tabl lock memori page lock row lock
what are the common concurr strategi implement in sql server read lock amp; write lock write lock amp; version to preserv read consist
what a transact in sql a group of queri that are either all executed, or none of them are.
what are the two most common transact creation strategi in sql implement transact are equival to databas session user are in auto-commit mode until they explicit start a transact
what should you alway do when log into an sql databas disabl auto-commit mode!
in what six scenario are transact common termin by most sql server on \ {commit} on \ {rollback} on shutdown (caus a rollback on restart) on schema chang (caus a commit) on creation of a new transact (caus a commit) on deadlock detect (caus a rollback)
whi do schema chang end a transact in most sql implement becaus they can't be roll back.
what a storag engin in sql the program respons for low-level databas iti like retriev a particular row or issu lock
how can you creat a savepoint in a transact in sql \ {savepoint savepoint\_name;}
how do you return to a savepoint in sql \ {rollback to savepoint savepoint\_name;}
how do you start a transact in mysql \ {start transaction;}
how are row order in an sql tabl in no particular order; new row will be insert into the first free space.
how do you creat an index for a column in sql \ {creat index a\_x\_idx on a (x);}
how do you get a list of the index for a tabl in sql \ {show index from a;}
when are index are automat creat in sql in most implementations, a uniqu index \ {primary} on the primari key (if specified) foreign key and uniqu constraint can creat index in some implementations, but not other
what the default datastructur under an sql index a b-tree
how do you remov an index from a tabl in sql \ {drop index index\_name;}
how do you creat a uniqu index in sql \ {creat uniqu index a\_x\_idx on a (x)}
what the effect of a uniqu index in sql compar to a regular index it enforc a uniqu constraint
what import about the order of column in a compound index in sql the index will onli be use for lookup use the head of the column list so if you build an index on \ {(x, y, z)}, it use for look up \ {(x), (x, y), (x, y, z)}
what index datastructur are common avail in sql b-tree index bitmap index hash index text index
when should you use each of the sql index datastructur b tree index in general bitmap index when there are onli a few possibl valu for the column (low-cardin data) text index when the data is compris of document
how do you creat a bitmap index in sql \ {creat bitmap index a\_x\_idx on a (x);}
how do you view the execut plan for a queri in mysql \ {explain /* queri */}
what'r the problem with sql index everi time a tabl is modified, index on it will be modifi too. they take up disk space they entail extra mainten time
what the standard strategi as to which column to index in sql primari key column referenc as foreign key column that'll be frequent use to fetch data (such as dates)
what are the four most common kind of constraint in sql primari key constraint foreign key constraint uniqu constraint check constraint
how do you remov a primari key constraint from a tabl in sql \ {alter tabl a drop primari key;}
how do you remov a foreign key constraint from a tabl in sql \ {alter tabl a drop foreign key fk\_a\_name;}
what a cascad updat in sql one that propag through foreign key constraint rather than throw an error
how do you creat a cascad updat constraint in sql \ {alter tabl a add constraint fk\_a\_x foreign key (x) refer a (x) on updat cascade;}
how do you creat a cascad delet constraint in sql \ {alter tabl a add constraint fk\_a\_x foreign key (x) refer a (x) on delet cascade;}
what the main use of cascad constraint when use self-referenti foreign key constraint sinc you won't be abl to insert a row otherwis
what are the four main reason to use view in sql data secur data aggreg hide complex join partit data
how can view help with data secur in sql by grant user \ {select} permiss on certain view but not on the under tables, you can conceal sensit data. this doesn't sound like a veri solid strategy.
are view updat in sql they can be, but the condit under which they are are complex and implementation-depend
how can you access metadata about a databas in sql by queri the \ {in ation\_schema} tabl
how can you generat databas compon automat in sql you can do it use sql alone, but it a pain. better to use a procedur languag (pl/sql, transact-sql, java, etc)
what'r pl/sql and transact-sql pl/sql is an extens of sql for oracl product transact-sql is an extens of sql for microsoft product
what a cache-oblivi algorithm one that abl to take advantag of a memori hierarchi without know the size of the level
what are the five common implement of a prioriti queue sort array or list binari heap bound height pqueue binari search tree fibonacci/pair heap
what the bound height implement of a prioriti queue creat an array of link lists, one for each possibl key valu keep a pointer to the least non-empti cell.
when is the bound height prioriti queue implement optim when there onli a small, discret rang of possibl key
when are fibonacci or pair heap appropri for a pqueue implement when decrease-key oper need to be speed up.
what the structur of a pair heap a pair heap is either an empti heap, or a pair consist of an element and a possibl empti list of pair heaps. in the latter case, the element is no larger than the root in the list
describ the \ {merge-pairs(l)} oper on a pair heap. if \ {l} is an empti list, return an empti heap if \ {l} has one element, return it if \ {l} has more than one element, return \ {merge(merge(l[0], l[1]), merge-pairs(l[2..]))}
describ the \ {merge(g, h)} oper of a pair heap if one of the heap is empty, return the other els append the one with the smaller root element to the list of subheap of the larger one
what'r the nontrivi oper on pair heaps, and how do they work \ {delete-min} and work by return \ {merge-pairs(heap.subheaps)} if the heap isn't empti \ {decrease-key} chang the key of a node, and if necessari cut it out of the parent node list and then re-merg it.
what'r the advantag of the pair heap it veri simpl in practice, despit have wors bound on \ {decrease-key} than a fibonacci heap (log vs constant), it faster
what a veb tree a van emd boa tree, a tree that implement an associ array on $m$ bit key all oper run in $o(\lg m)$ time (so $o(\lg \lg n)$ time in the size of the largest key), includ next/previ
in practice, what usual the optim implement of a prioriti queue a binari heap over an array. it competit with pair heap up to a larg number of elements.
what a suffix tree a radix tree of all the suffix of a string.
what the differ between a trie and a radix tree tri can onli have one letter per node radix tree can have substr in each node
how much space doe a suffix tree take $o(n)$ in the size of the sourc string
how do you construct a suffix tree in linear time use ukkonnen algorithm or one of it successors. high nontrivial.
what are the two most popular use of suffix tree find substr find longest common substr
what a suffix array an array of integ provid the start posit of suffix of $s$, sort in lexicograph order
what'r the advantag of suffix array over suffix tree they use less memori linear-tim construct of sarray is much simpler than stree they exploit cach local ani algorithm use suffix tree can be adapt to use a suffix array and (if necessary) a lcp array while maintain their time bound
what a lcp array a longest common prefix array, an auxilliari structur to a suffix array it store the length of the longest common prefix between consecut element of the suffix array
what a compress suffix array a variant of the suffix array that use $o(n \lg \sigma )$ space rather than sarray $o(n \lg n)$ space. use wavelet tree to carri out the compression.
what the bwt algorithm the burrows-wheel tran , which is a revers rearrang of a string into run of similar charact the rearrang string can then be more effici compress
what'r the usual data structur for repres hypergraph incid matricies, with \ {m[i, j] == 1} if vertex $i$ appear in edg $j$ incid lists, which contain the element $(i,j)$ if vertex $i$ appear in edg $j$
what a static graph represent a data structur for graph which won't be modifi dure the cours of an algorithm ex: use a 2d array to implement an adjac list.
how are extrem larg graph common implement as hierarchies, with subgraph repres as node in a parent graph a natural/domain-specif partit is prefer to a naiv heurist
what are dynam graph algorithm algorithm that can effici recomput some properti of a graph after insert or delet of some edg
what sparsif in the context of graph algorithm a general approach to design dynam graph algorithms.
what are the usual represent of set arbitrari contain bit vector hash set bloom filter
what'r the common implement of a set partit datatyp collect of arbitrari contain general bit vector dictionari onto a set id attribut union-find
how are kd tree usual construct recurs partit a set of point into the point less than and the point greater than the median along a select axi
how is the axi to partit point by in a kd tree node usual select either by cycl through the dimens or by select the dimens with the greatest range, which encourag cube-shap box
what rang of dimens are kd tree best in 2-20. abov that the number of neighbour get too high
how is bandwidth minim relev to store graph by pick a bandwidth-minim permut of the graph vertices, local of access can be improv
rough how larg doe a matrix have to be befor strassen beat naiv multipl about $100 100$
what strassen algorithm a $o(n^{2.81})$ algorithm for matrix mulipl
what the cooley-tukey algorithm the most common algorithm for calcul the fft, which use divide-and-conqu and the root of unity.
what a common optim in chain matrix multipl matrix multipl is associative, so use a dp algorithm to order the multipl
what the polytim algorithm for comput the determin calcul the lu factor and take the product of the diagon element
what a linear congruenti generat a veri simpl and (with the right constants) reason good prng: $r_n = (ar_{n-1} + c) \mod m$
what the main problem with linear congruenti generat low period. a 32-bit lcg can repeat insid a few minut on modern machines.
what the simplest way to approxim arbitrari random number distribut accept-reject methods: pick a valu of $x$ uni ly, then a $y [0, 1]$ uni ly. if $f(x) y$, accept it, els reject and tri again.
what karatsuba algorithm a $o(n^{1.6})$ algorithm for multipl
when doe karatsuba algorithm beat out the naiv approach somewher under 100 digit
when doe bucket sort make sens when the input is known to be uni ly distribut
what the worst-cas runtim of a radix sort $o(kn)$ on $k$-charact key and $n$ item
how doe a radix sort work for each digit, go from least to most significant, group the input by the digit value, make sure to preserv the order within a group. a bucket or count sort can be use for the group stage.
what the function differ between a lsb-first and msb-first radix sort lsb is stable; msb isn't. msb will stop rearrang a key after reach it uniqu prefix
how doe a count sort work if the maximum key is $m$, creat an array of length $m$. sweep the input, count how mani of each key there is calcul the partial sum of the array insert the input into the output array use the partial sums, increment a sum each time a valu is insert into the rang follow it
what the best way to sort a small rang of distinct integ creat a bit vector the size of the rang sweep the input, flip the correspond bit. sweep the bit vector to get the sort output.
what an optim binari search tree a binari search tree that been adjust to repres the access distribut
at a high level, how is an optim binari search tree construct by dynam program and by observ that each possibl root node partit the space of key into two smaller ranges, and that each subrang should be repres by it own optim binari search tree.
at a high level, how doe the stout-warren algorithm work creat a pseudo-root with the real root as it right child. conduct tree rotat until the tree s a right chain, then rotat it back into a balanc tree.
what the best algorithm for find the $k$th order statist recurs use median of median to partit the input and select the partit in which the $k$th order statist lies. by the master theorem, this is linear time.
what the lower bound on the runtim of ani algorithm for comput the mode $o(n \lg n)$, as element uniqu reduc to it
what are rank and unrank function in the context of permut generat given a permut generat scheme, $\text{rank(p)}$ give the posit of permut $p$ in the generat order $\text{unrank(m, n)}$ give the permut in posit $m$ in the generat order when permut $n$ element
what the use of rank amp; unrank function in generat permut onc they'r defined, sever use oper can be per ed: find the permut after a given permut generat a random permut keep track of a set of permut
what are the two general approach to generat permut for small $n$, defin rank amp; unrank function for larger $n$, defin previous amp; next function
what'r the step in the johnson-trott algorithm for each element, defin a direction: $-, 0, +$. initi the first element with direct $0$, and the other with $-$. at each step, find the largest element with a nonzero direct and swap it in that direction. if this caus the element to reach the end of the permutation, or if the next element in the same direct is larger than the chosen element, set the direct of the chosen element to $0$. now set the direct of all element larger than the chosen element to $+$ if they'r to the left of it and $-$ if they'r right of it.
what the best algorithm for generat permut in sequenc johnson-trott aka chang ring
what are the three main approach to generat all subset of a set lexicograph order (hard) gray code binari count
what the best way to generat subset contain exact $k$ element lexicograph order.
what an effici way of generat gray codeword in sequenc start with the all zero codeword. at step $i$, find the posit $j$ of the least signific $1$ in the binari represent of $i$ and flip bit $i$ in the codeword.
what the best way to generat all integ partit of a number use lexicograph decreas order start with the integ to be partitioned, $n$ subtract 1 from the smallest part that is $ 1$ then gather all the 1s so as to match the new smallest part $ 1$
how can you generat integ partit uni ly at random the partit function $p_{n,k}$, the number of partit of $n$ with largest part at most $k$, has a simpl recurs . this can be use recurs to select the largest part of a random partit with the correct probabl
what a restrict growth string in the context of set partit generat a sequenc $a_1, \dotsc, a_n$ such that $a_i 1+\max(a_1, \dotsc, a_{i-1})$
how do restrict growth string correspond to set partit each distinct number in the string correspond to a of the partit so $0, 1, 1$ correspond to $\{\{0\}, \{1, 2\}\}$
how do you generat a set partit uni ly at random stirl number of the second kind ${n \brace k}$ count the number of partit of an $n$ element set with $k$ s. this can be use in a recurs manner to generat partit uni ly at random.
what are integ composit a represent of the possibl assign of $n$ indistinguish ball to $k$ box
what are the three strategi common use to generat random graph includ edg with a specifi probabl includ $m$ random edg select neighbour for each node in proport to how mani neighbour the target current have (power law graphs)
what a prufer code they'r a way to rank amp; unrank label tree by biject each tree onto a $n-2$ length string $s$ on the alphabet $\{1, \dotsc, n\}$
how do you generat the prufer code associ with a tree for $i \{1, \dotsc, n\}$, pick the vertex $v$ incid on the leaf with the lowest label. set $s_i = \text{label}(v)$ and remov the leaf.
how do you generat the tree associ with a prufer code given a code $s$, the lowest-label leaf will be the smallest integ $u$ miss from $s$. the first edg will be from $u$ to the vertex $v$ correspond to $s_1$. proceed recurs from this.
how can you generat a graph correspond to a specif degre sequenc given degre sequenc $\{p_1, \dotsc, p_n\}$ go from highest to lowest connect vertex $v_1$ to $v_2, \dotsc, v_{p_1+1}$ reduc the degre count and repeat. if ani degre count becom negative, no graph correspond to this degre sequenc (there a simpler condit to test this)
at a high level, what need to be done to implement calendr calcul pick an epoch to count from. creat a function that convert a count sinc epoch to a date in your given calendar system. creat a function that convert a date in your given calendar system into a count sinc epoch. don't actual do this
what'r the step in kosaraju algorithm given a digraph $g$ and an empti stack $s$ while $s$ doesn't contain all vertices: pick an arbitrari vertex $v \not s$ and per a dfs from $v$. when a vertex $u$ has been process and it children all dfsd, push $u$ into $s$ revers the direct of all arc to get the transpos graph. while $s$ is nonempty: pop the top vertex $v$ and per a dfs from it. record all visit vertic - this is the strong connect compon of $v$. remov all these vertic from the graph and stack.
what the best algorithm for find strong connect compon in a digraph kosaraju algorithm. it easier to program than tarjan's.
what a feedback arc set in graph theori a set of edg that can be remov from a graph in order to convert it to a dag.
what a linear extens in graph theori anoth name for a topolog sort of a graph.
what an easi way to generat a linear extens of a graph uni ly at random take ani topolog sort and tri to swap a pair of vertices. if the ing permut is still a topolog sort, this is a random linear extension. a larg number of iter will converg on a uni distribut over the linear extens of the graph.
what the fastest (reason simple) algorithm for calcul msts in practic prim over an array heap.
what a particular fast random algorithm for minimum cut karger algorithm: repeat pick an edg at random and contract it until there'r onli two vertic left. the number of edg between these vertic is the candid minimum cut. repeat this a whole pile of time and take the minimum.
what are the two main class of algorithm for network flow problem augment path method (ford-fulkerson, etc) preflow-push method
how do preflow-push method work in the solut of network flow problem flow are push from one vertex to it neighbours, ignor the zero-sum-flow-at-nod constraint.
how do preflow push method compar to augment path method in the solut of network flow problem preflow push method are general faster than augment path method
what the best known runtim for graph planar test problem linear, though the algorithm use are complicated.
what the pdf for a multivari gaussian $n(x x, \sigma) =\exp \frac{1}{\sqrt{(2\pi)^d \sigma }} \left[-\frac{1}{2}(x-\mu)^t\sigma^{-1}(x-\mu)\right]$
what'r the isoclin of a multivari gaussian distribut they'r ellips with axe $u_1, u_2$, coeffici $\sqrt{\lambda_1}, \sqrt{\lambda_2}$ and center $\mu$. where $u_i$ are the eigenvector of $\sigma$ and $\lambda_1$ are the eigenvalu
what the definit of the mahalanobi distanc $d(x, y) = \sqrt{(x-y)^t s^{-1} (x-y)}$
what the use of the mahalanobi distanc it a statist intend to gaug similar between two sampl
what distinguish the mahalanobi distanc from the euclidean distanc take into account correl in the data is scale-invari
what'r the two possibl estim for a normal distribut variance, and what'r the advantag of each $s^2 = \frac{1}{n-1} \sum(x_i - \bar x)^2$ is unbiased. $\hat \sigma^2 = \frac{1}{n} \sum(x_i - \bar x)^2$ is bias (underestim $\sigma^2$) but has slight lower mse.
what bessel correct in statist use $n-1$ rather than $n$ in estim for the sampl varianc and sampl standard deviat
what'r the caveat attach to bessel correct it give an unbias estim of the varianc but not of the standard deviat it general increas the mean squar error
whi doe bessel correct still yield a bias estim for the standard deviat becaus squar root is a concav function.
what the trace trick in linear algebra $x^t a x = \text{tr}(x^t a x) = \text{tr}(axx^t)$
what the mle for the mean of a multivari gaussian the sampl mean
what the mle for the varianc of a multivari gaussian the sampl varianc
what doe gda stand for in machin learn gaussian discrimin analysi
what gda equival to when the covari matrix is the ident naiv bay classification.
what gda equival to under the mahalanobi distanc a nearest centroid classifi
what the differ between a generat and a discrimin model in machin learn generat model can generat datapoint from some joint distribut (typic with hidden parameters) discrimin model must work with a suppli dataset onli
what qda stand for in machin learn quadrat discrimin analysi
what quadrat discrimin analysi assum the prior probabl for a class label $c$ is $\pi_c$ and assum $p(x y=c, \theta)$ is a multivari gaussian distribut then use bay to construct the class posterior $p(i = c x, \theta)$ threshold the ant ula yield quadrat discrimin analysi
what lda stand for in machin learn linear discrimin analysi latent dirichlet alloc
what linear discrimin analysi take qda and assum $\sigma_c = \sigma$ for all $c$. then in the expon throw away the quadrat term in $x$ (becaus it independ of $c$) defin $\beta_c$ to be the linear coeffici of $x$ defin $\gamma_c$ to be the constant. then let $\eta = [\beta^t_1 x + \gamma_1, \dotsc, \beta^t_c x + \gamma_c]$ and we can write $p(y=c x, \theta) = \mathcal{s}(\eta)_c$ use the softmax function
what the definit of the softmax function $\mathcal{s}(\eta)_c = \frac{e^{\eta_c}}{\sum_{c^\prime} e^{\eta_{c^\prime}}}$
what the asymptot behavior of the softmax function $\mathcal{s}(\eta/t)_c$ concentr on $\text{argmax}_{c^\prime} \eta_{c^\prime}$ with valu 1 as $t \rightarrow 0$
what anoth function veri similar to the softmax function the boltzmann distribut
what doe lda degener to in the case of two class $p(y=1 x, \theta) = \text{sigm}(w^tx + x_0)$ for some $w$, $x_0$ that can be deriv from $\sigma, \mu_0, \mu_1$ and $\pi$
what the definit of d-prime in machin learn $d^\prime = \frac{\mu_1 - \mu_0}{\sigma}$
what doe the d-prime of a pair of distribut repres in machin learn discrimin of a signal from background nois larg $d^\prime$, easier to discrimin
how do you fit a gda model use the mle factor the log-likelihood into a class prior term and $c$ log-gaussian, one for each class $c$ estim the class prior $\pi_c$ as $n_c / n$ partit the data accord to class label and estim $\mu_c, \sigma_c$ for each class independ use the sampl mean and varianc
what the definit of the sampl varianc for a multivari distribut $\hat \sigma = \frac{1}{n} \sum (x_i - \hat \mu)(x_i - \hat \mu)^t$
when will the mle for a full covari matrix be singular when the number of datapoint is less than the dimens it can be ill-condit even when there are more datapoint than the dimens though!
what doe it mean for a matrix to be ill-condit in machin learn mean it close to singular.
what are six strategi to use when overfit occur with the mle in gda use a diagon covari matrix share the covari matrix across class (ie lda) use a diagonal, share covari matrix (ie diagon covari lda) impos a prior then integr it out (ie bayesian naiv bayes) fit a full or diagon covari matrix use map estim instead project onto a lower dimens
what regular discrimin analysi it lda with a iw prior enforc on the covari matrix and integr out
what the definit of the pool empir varianc $s^2 = \frac{1}{n-c} \sum_c \sum_{i : y_i = c} (x_i - \bar x_{c})^2$
what done in diagon lda take lda and enforc a diagon covari matrix and take the varianc to be the pool empir varianc of the set
what the idea in nearest shrunken centroid classif write $\mu_{cj} = m_j + \delta_{cj}$ and put a sparsity-promot prior (like a laplac prior) on the $\delta_{cj}$ comput a map estimate. if for some featur $j$ all $\delta_{cj} = 0$, that featur is not discrimin and will be automat ignored.
given a gaussian joint distribut $p(x, y)$, what'r the margin $x \mathcal{n}(\mu_x, \sigma_{xx})$ $y \mathcal{n}(\mu_y, \sigma_{yy})$
given a gaussian joint distribut $p(x, y)$, what the precision-mean of $p(x y)$ $\lambda_{x y}\mu_{x y} = \lambda_{xx}\mu_x - \lambda_{xy}(i - \mu_y)$
given a gaussian joint distribut $p(x, y)$, what the precis matrix of $p(x y)$ $\lambda_{x y} = \lambda_{xx}$
given a gaussian joint distribut $p(x, y)$, what the distribut of $p(x y)$ $x y \mathcal{n}(\mu_{x y}, \sigma_{x y})$ can be use when $x_1, x_2$ are vectors, as can the other, similar s
what'r the moment paramet of a gaussian distribut $\mu$ $\sigma$
what'r the canon paramet of a gaussian distribut $\lambda = \sigma^{-1}$ $\xi = \sigma^{-1}\mu$
what anoth name for the natur paramet of a gaussian distribut the canon paramet
what'r the advantag of the canon parameter of the gaussian condit distributions, and product distribut have a much simpler than they do in the moment parameter
what are two key mathemat tool for use in invert partit matrici schur complement sherman-morrison-woodburi ula
what a linear gaussian system a system with the $p(x) = \mathcal{n}(x \mu_x, \sigma_x)$ $p(i x) = \mathcal{n}(i ax+b, \sigma_y)$
given a linear gaussian system with condit $p(i x)$, what the distribut of $p(x y)$ $p(x y) = \mathcal{n}(x \mu_{x y}, \sigma_{x y})$
given a linear gaussian system with condit $p(i x)$, how is the $\lambda_{x y}\mu_{x y}$ of $p(x y)$ defin $\lambda_{x y}\mu_{x y} = \lambda_x\mu_x + a^t\lambda_y(y-b)$
given a linear gaussian system with condit $p(i x)$, how is the precis of $p(x y)$ defin $\lambda_{x y} = \lambda_x + a^t\lambda_i a$
given a linear gaussian system with condit $p(i x)$, what the distribut of $p(y)$ $p(y) = \mathcal{n}(i a\mu_x + b, \sigma_i + a\sigma_x a^t)$
what sensor fusion in machin learn use read from multipl noisi sensor to predict a paramet
how do you revert all unsav chang to a file in vim \ {:e!}
how do you overwrit a file in vim \ {:w!}
how do you get vim to automat insert newlin \ {:set wrapmargin=10} will insert them ten charact befor the right border
how do you enabl line number in vim \ {:set number}
how do you move back a word in vim \ {b}
what the differ between chang and replac in vim chang will delet a portion of text then stick you in insert mode to replac it replac mode will let you overwrit the text as you go
what the general of a vim command \ {(command)(number)(object)} command is optional; without it you just get a movement number is optional; unspecified, it default to 1 object is option for some command
how do you chang a whole line in vim \ {cc} shorthand for \ {c\_}
what the whole-lin movement in vim \ {\_}
how do you chang up to the end of the line in vim \ {c} shorthand for \ {c }
how do you delet until the end of the line in vim \ {d} shorthand for \ {d }
how do you give repeat count to vim command that have single-charact effects, like \ {c} \ {(number)(command)(object)}
how do you chang the case of a charact in vim \ {\~{}} will e between uppercas and lower case
how do the default delet buffer work in vim the last 9 whole delet line are store in number buffers, and can be recal with \ {"3p} or similar word and charact are store in a temporari buffer that'll be overwritten with the next change/delete/yank/etc command
how do you transpos two letter in vim \ {xp} (\ {x} delet the charact and stuff it in the buffer, \ {p} past it after the new charact under the cursor)
how do you repeat your previous command in vim \ {.}
how do you enter insert mode at the begin of a line in vim \ {i}
how can you join two consecut line in vim \ {j} while in the first line
how do you scroll down by a half-screen in vim \ {ctrl-d}
how do you scroll up by a half-screen in vim \ {ctrl-u}
how do you move the current line to the top of the screen in vim \ {z}
how do you move the current line to the center of the screen in vim \ {z.}
how do you move the current line to the bottom of the screen in vim \ {z-}
how do you move a specif line number to the top of the screen in vim \ {200z}
how do you move the cursor to the top of the screen in vim \ {h} (for home)
how do you move the cursor to the middl of the screen in vim \ {m}
how do you move the cursor to the bottom of the screen in vim \ {l}
how do you move the cursor to the 5th line down the screen in vim \ {5h}
how do you move to the first charact of the next line in vim enter.
how do you move to the first charact of the previous line in vim \ {-}
how do you move to the 50th column in vim \ {50 }
how do you move to the end of the word, ignor punctuat in vim \ {e}
how do you move to the end of a sentenc in vim \ {(} and \ {)}
how do you move to the end of a paragraph in vim \ {\{} and \ {\}}
how do you move to the end of a code in vim \ {[\{} and \ {]\}}
how do you move to the end of a parenthes express in vim \ {[(} and \ {])}
how do you search backward in vim \ { pattern}
how can you repeat the previous forward search in vim \ {/}
how do you delet text up to and includ a specif word in vim \ {d/pattern}
how do you move the cursor to the first occur of \ {z} in a line \ {fz} (\ {f} for find)
how do you move the cursor to one befor the charact \ {z} in a line in vim \ {tz}
how can you repeat a find command in vim \ {;}
how can you repeat a find command in vim, but in the other direct \ {,}
how do you find the previous occur of the charact \tt{z} in vim \ {fz}
how do you return the cursor to a posit it was in when you issu a command in vim \ {``}
how do you return the cursor to the start of the line it was in when you issu a command in vim \ {''}
from the commandline, how do you open a file in read-on mode in vim \ {vim -r filename}
how do you get a list of autosav file from vim \ {vim -r}
how do you recov an autosav copi of a file in vim \ {vim -r filename}
how can you search through the number yank buffer in vim \ {"1p} then \ {u} and \ {.} to auto-incr and put the previous buffer
how do you yank a line into a name buffer in vim \ {"kyy} will yank into buffer \ {k}
how do you put befor the cursor in vim \ {p}
how do you yank a whole line in vim \ {yy}
how do you append a line to a name buffer in vim \ {"kyy} will append to the k buffer
how do you set a name bookmark in vim \ {mk} will set a bookmark call k
how do you move to the line contain a name bookmark in vim \ {'k} will move to bookmark k
how do you move to a name bookmark in vim \ {`k} will move to bookmark k
what \ {ex} in unix the line editor that \ {vi} is built on
which vim command start with a colon the one that'r actual command to the under \ {ex} line editor
how do you invok \ {ex} from vim \ {q}
how do you get from \ {ex} in vim back to vim give the command \ {vi}
in \ {ex} commands, how can you refer to the current line number \ {.}
in \ {ex} commands, how can you refer to number of the last line of the file \ { }
in \ {ex} commands, how can you refer to all line \ { }
in \ {ex} commands, how can you refer to the number of the 20th line after the current line \ {.+20} \ {+20}
in \ {ex} commands, how can you refer to the number of the line 20 line befor the current line \ {.-20} \ {-20}
in \ {ex} commands, how can you refer to the before-the-first line in a file \ {0}
when address rang of line in \ {ex}, how can you give the second line number relat to the first \ {:100;+5}
how do you combin command in \ {ex} \ {command1 command2}
what the differ between \ {wq} and \ {x} in vim \ {x} will onli write to the file if it been modified. use with make, which rebuild file if the last modifi time has chang
how do you refer to the current filenam in vim \ { }
how do you refer to the filenam of the previous file in vim \ { }
how do you switch to the previous file in vim \ {ctrl+\^{}}
how do you repeat the last substitut command in vim \ {:s}
how do you appli a command to everi line in a rang in \ {ex} \ {:143,256g} follow by the command befor each execut of the command, \ {.} will be set to the current line
what the problem with rang like \ {[a-z]} in regex pattern they don't local well: é isn't match for example. use posix bracket express like \ {[[:lower:]]} instead
in the replac pattern of a vim substitut command, how do you refer the entir search pattern \ { amp;}
in the replac pattern of a vim substitut command, how do you refer the text found in the previous search \ {\~{}}
in the replac pattern of a vim substitut command, how do you uppercas the first letter of an express \ {\textbackslash uexpr} ie \ {\textbackslash u} is the oper and \ {expr} is the express
in the replac pattern of a vim substitut command, how do you lowercas the first letter of an express \ {\textbackslash lexpr} ie \ {\textbackslash l} is the oper and \ {expr} is the express
in the replac pattern of a vim substitut command, how do you uppercas a sequenc of letter in an express \ {\textbackslash uexpr\textbackslash e} ie \ {\textbackslash u} is the oper and \ {expr} is the express if \ {\textbackslash e} is missing, it'll just run to the end of the replac
in the replac pattern of a vim substitut command, how do you lowercas a sequenc of letter in an express \ {\textbackslash lexpr\textbackslash e} ie \ {\textbackslash l} is the oper and \ {expr} is the express if \ {\textbackslash e} is missing, it'll just run to the end of the replac
how can you do a vim search amp; replac on a pathnam contain \ {/} use anoth charact like \ {;} as the delimit instead: \ {: s;/pattern;/replacement;g}
what are buffer and window in vim a buffer hold text a window is a view into a buffer
how do you open a file in binari mode in vim \ {vim -b filename}
how do you enabl increment search in vim \ {:set incsearch}
how do you diff file in vim \ {vim -d file1 file2 file3 file4}
how do you denot a comment in a .vimrc file start with \ {"}
in vim visual mode, how do you select up to and includ the space after the next word \ {aw}
in vim visual mode, how do you select up to the end of the next word \ {iw}
what'r the two way to creat a new horizont window in vim \ {:split}, \ {ctrl+ws}
what'r the two way to creat a new vertic window in vim \ {:vsplit}, \ {ctrl+wv}
how do you open a file in a new window in vim \ {:split filename}
how do you split off a horizont window of a specif size in vim \ {:15split} will creat a 15-line window
how do you creat a new window in vim and do housekeep \ {:new}
how do you open a window for a new file in vim onli if the file exist \ {:sfind filename}
how do you open a new readon window in vim \ {:sview filename}
how do you move between window in vim \ {ctrl+wh, ctrl+wj, ctrl+wk, ctrl+wl}
how do you move to the previous window in vim \ {ctrl+wp}
how do you rotat window in each direct in vim \ {ctrl+wr}, \ {ctrl+wr}
how do you exchang window in vim \ {ctrl+wx}, \ {ctrl+wx}
how do you move a window to the edg of the screen in vim \ {ctrl+wh, ctrl+wj, ctrl+wk, ctrl+wl}
how do you resiz window to equal size in vim \ {ctrl+w=}
how do you increas and decreas a window height in vim \ {ctrl+w+}, \ {ctrl+w-}
how do you increas and decreas a window width in vim \ {ctrl+w }, \ {ctrl+w }
how do you maxim a window width in vim \ {ctrl+w }
how do you store a set of fold in vim \ {:mkview}
how do you load a previous save set of fold in vim \ {:loadview}
how do you e a fold in vim \ {za}
how do you delet a fold in vim \ {zd}
how do you creat a fold in vim \ {zf} follow by a movement command
how do you recurs e a set of fold in vim \ {za}
how do you reduc the fold level in vim \ {zm}
how do you increas the fold level in vim \ {zr}
how do you enabl the fold margin in vim \ {:set foldcolumns=4} to 4 level of fold
how do you e the case of all charact in a line in vim \ {\~{}\~{}}
how do you enabl indent-bas fold in vim \ {:set foldmethod=indent}
how do you delet all fold in vim \ {zd}
how do you enabl c- indent in vim \ {:set cindent}
how do you autocomplet a line in vim \ {ctrl+x, ctrl+l}
how do you complet by keyword within a file in vim \ {ctrl+x, ctrl+n} \ {ctrl+p} to skip back a match; enter to accept
how do you complet by keyword, includ includ file in vim \ {ctrl+x, ctrl+i} \ {ctrl+p} to skip back a match enter to accept
how do you autocomplet with the omni function in vim \ {ctrl+x, ctrl+o} \ {ctrl+p} to skip back a match enter to accept
how do you autocomplet by filenam in vim \ {ctrl+x, ctrl+f} \ {ctrl+p} to skip back a match enter to accept
how do you autocomplet by tag in vim \ {ctrl+x, ctrl+]} \ {ctrl+p} to skip back a match enter to accept
what omni autocomplet in vim a user-defin filetype-depend autocomplete.
how do you navig to a tag locat in vim \ {ctrl+]} with the cursor over an identifi keyword
how do you return from a tag locat to your previous locat in vim \ {ctrl+t}
what'r exuber tag in vim an extens of \ {ctags} that make it use for other languag
how do you enabl syntax highlight in vim \ {:syntax enable}
how do you search through local file in vim \ {:vimgrep pattern filepattern}
how can you save session state in vim \ {:mksession} restor it with \ {:so session.vim}
how can you open vim command histori \ {ctrl+f}
how do you defin a macro in a makefil \ {name=value}
how do you use a macro in a makefil \ { (name)}
what the default behavior when \ {make} is call it'll process the first recip in the file call \ {makefile} not start with \ {.}
what are make two most impor implicit rule target end in \ {.o} are automat deriv from the file with the same filenam but end \ {.c/.cpp/etc} target without an extens are automat deriv from the file with the same name but end in \ {.o}. other automatically-gener \ {.o} file are link in.
what the variabl for set the c++ compil in make \ {cxx}
what the variabl for defin flag for the c++ compil in make \ {cxxflags}
what the variabl for defin librari to be given to the linker in make \ {ldlibs}
what the variabl for defin flag for the linker in make \ {ldflags}
what distinguish java stringbuff and stringbuild stringbuild is faster stringbuff is concurrency-saf
what the boolean type in c++, and where is it found \ {bool}, and it includ automat
what the bit at of a ieee754 doubl in most languag from most signific to least: 1 bit sign 11 bit expon 52 bit mantissa
how is the expon of a doubl encod in ieee754 use offset binary: 1 correspond to an expon of -1022 2046 correspond to an expon of 1023
how can you calcul the $i$th gray codeword in c \ {(i 1) \^{} i}
how do you calcul the sign of a permut count the number of invers ($i j$ st $\pi_i \pi_j$) in the permutation. an odd number impli a sign of -1, an even number a sign of +1
what the use of the \ {xargs} command in unix it execut the given command with whitespace-separ token taken from standard input
how can you control the number of argument \ {xargs} read in dure each execut \ {xarg -n 1} will read 1 argument at a time
what the general outlin of a problem amen to binari search you have a monoton function $f(x)$ and a predic $p(y)$, and you want to search a finit domain for an argument $x$ which caus $p(f(x))$ to be true.
what are the common variant of problem amen to binari search given a monoton function $f$ and a monoton predic $p$, upper bound: find the maximum $x$ such that $p(f(x))$ is true lower bound: find the minimum $x$ such that $p(f(x))$ is true
how would you code up the upper-bound binari search variant given a target \ve£target£ defin a predic \ve£p: v - (v = target)£ defin \ve£lo = 0, hi = length - 1£ defin \ve£best_so_far = null£ \ {while lo = hi ....x = (lo + hi)/2 ....if p(arr[x]) ........best\_so\_far = x ........lo = x + 1 ....els ........hi = x - 1} then check if \ve£best_so_far£ match \ve£x£
how do you connect to a databas in psql \ {psql databasename}
how do you get help in psql \ {\textbackslash h commandname} for sql command \ {\textbackslash commandname} for psql command
when creat a tabl in sql, how can you implicit creat a foreign key constraint \ {creat tabl citi ( country\_cod char(2) refer countri );}
in the select claus befor a join, how can you retriev onli the attribut of one contribut tabl \ {select a.* from a inner join b;}
what crud with respect to databas creat read updat delet ie the four fundament oper on persist storag
in a sql execut plan, what are the cost measur in arbitrari units, but by convent a singl disk page fetch will cost 1.0.
in a psql queri plan, what are the two cost that appear on each row the cost for ani preprocess need and the total cost
how can you get more detail in ation on the runtim of a queri plan in psql \ {explain analyz query} will actual run the queri and a summari of how much time was spent on each compon of the plan
how do you set a default valu for a column in sql \ {colnam coltyp default defaultval'}
what an import choic to make when enforc a compound foreign key constraint whether to allow some compon of the key to be null
how do you instruct psql whether to allow null in a foreign key constraint \ {foreign key (a, b) refer c(d, e) match simple} to allow null \ {foreign key (a, b) refer c(d, e) match full} to disallow null
how do you write a function in pl/pgpsql \ {creat or replac function fname(argname1, argtype1, argname2, argtype2) return rettyp as declar ....variabl declar begin ....fn bodi end; languag plpgsql;}
how do you declar variabl in a psql function definit \ {declar varname1 vartype1; varname2 vartype2 := initialval;}
how do you set a variabl in psql function \ {varnam := val;}
how do you import a function from a file in psql \ {\tbs i filename.sql}
how do you run a function in psql \ {select fnname(argval1, argval2);}
in sql, what a store procedur a subroutin store on the databas which queri can call
in sql, what a trigger a routin that'll be call befor or after certain oper on the databas
what'r the common trigger event in psql befor or after insert befor or after updat
how would you write a trigger in psql to execut for each row after an updat to a tabl \ {creat trigger triggernam after updat on tablenam for each row execut procedur procname();}
how do you add a column to a tabl in sql \ {alter tabl tablenam add colnam coltype;}
what a rule in psql a descript of how to alter a queri ast
how do you creat a rule in psql that'll execut when a row is insert into a tabl \ {creat rule rulenam as on insert to tablenam do instead queri }
when construct row-level trigger in psql, how do you refer to the row be insert or the row be replac \ {new} and \ {old} resp.
what the function of a pivot tabl cross-tabulation; count the number of datapoint with each pair of possibl valu in two attribut
what the levenshtein distanc anoth name for the string edit distanc
what a trigram text search break each string in the databas into trigram and do the same for the pattern string then calcul the best match against a pattern by count the number of match trigrams.
in natur languag processing, what a stop word a word that so common that it useless to search against, and is filter out from ani search query. ie "the"
what the purpos of the metaphon algorithm an algorithm for index word by their pronunciation, by convert the input string to a standard approxim of the input pronunci
what kind of data scale are modern sql databas viabl up to sever terabytes.
when is a sql databas prefer over other databas type when data is fair homogen when the data con s to an structur schema
when is a sql databas inappropri when greater flexibl than a rigid schema is need when signific horizont scale is need when veri high volum read and write are need when onli larg blob of data need to be store
what the effect of the unix \ {cat} command print the file content to standard output
what the conjug prior of the bernouilli distribut the beta distribut
what is $\gamma(n)$ equival to when $n$ is a posit integ $\gamma(n) = (n-1)!$
what the simplest way to do featur select in machin learn calcul the mutual in ation between each featur $x_j$ and the class label $y$ discard all but the $k$ featur with the highest mi
what the definit of the logit function $\ln \frac{x}{1-x}$
how are the logist and logit function relat they'r invers
what the chang of variabl ula in probabl if $y = f(x)$ is monoton (and henc invertible), $p_y(y) = p_x(x) \frac{dx}{dy} $
what the conjug prior of the uni distribut the pareto distribut
what the in ation of a gaussian distribut when it written in term of the canon paramet
what the product of two gaussian distribut $xi n(\lambda_x \mu_x + \lambda_i \mu_y, \lambda_x + \lambda_y) $
what'r the paramet of the wishart distribut $s$, the scale matrix $\nu$, the degre of freedom
what the wishart distribut a general of the gamma distribut general to posit definit matrici
what the usual use of the wishart distribut to model uncertainti in precis matrici
what the definit of the wishart distribut $wi(\lambda s, v) \propto \lambda ^{\frac{\nu-d-1}{2}} e^{-\frac{1}{2} \text{tr}(\lambda s^{-1})}$
when is the normal constant for the wishart distribut well-defin when $\nu d+1$
in machin learning, what the scatter matrix of a seri of observ $s = \sum x_i x_i^t$
what the mean of a wishart distribut $\mu = \nu s$
what the mode of a wishart distribut $(\nu - d - 1)s$ which onli exist if $\nu d + 1$
if $x_i \mathcal{n}(0, \sigma)$, what the distribut of the scatter matrix $s \text{wi}(\sigma, n)$
how is the wishart distribut relat to the invers wishart distribut if $\lambda wi(s, \nu)$ then $\sigma iw(s^{-1}, \nu + d + 1)$
what doe mvn stand for in machin learn multivari normal (distribution)
what the map estim of a gaussian covari matrix given an invers wishart prior $\hat\sigma = \frac{s_0 + s_\mu}{n_0 + n}$ where $s_\mu$ is the scatter matrix of the observ relat to $\mu$ $n_0 = \nu_0 + d + 1$ is the strength of the prior $s_0$ is the scale matrix of the prior
what a common altern parameter to the invers gamma distribut the invers chi squar distribut
what are the problem with the invers gamma distribut when use as a gaussian varianc prior the strength of the distribut is encod in both paramet the convers from the wisart distribut to the gamma distribut introduc a lot of ugli $\frac{1}{2}$ constant
what the natur conjug to the normal distribut on $(\mu, \sigma)$ the normal invers wishart
what the definit of the normal invers wishart distribut $\text{niw}(\mu, \sigma m, \kappa, \nu, s) = \mathcal{n}(\mu m, \frac{1}{\kappa}\sigma) \text{iw}(\sigma s, \nu)$
what are the paramet of the normal invers wishart distribut $m$ is the prior mean for $\mu$ $\kappa$ is the strength of $\mu$ prior $s$ is (proport to) the prior mean for $\sigma$ $\nu$ is the strength of $\sigma$ prior
how mani free paramet doe the normal invers wishart distribut have three; confid in the mean is invers proport to the varianc
what are the problem with the map estim doesn't indic confid use it will overestim confid inher an atyp point not invari wrt reparameter
what a credibl interv a $100(1-\alpha) $ credibl interv is a contigu region of the (one dim) paramet space that contain $1-\alpha$ of the posterior probabl mass
what a central interv a credibl interv for which the left and right tail have equal mass
what hpd stand for in machin learn highest posterior densiti (regions)
what'r the highest posterior densiti region in machin learn the $100(1-\alpha) $ hpd region are defin by the set $\{\theta : p(\theta \mathcal{d}) p^*)$ where $p^*$ is defin by $1 - \alpha = t_{\theta : p(\theta \mathcal{d}) p^*} p(\theta \mathcal{d})d\theta$ ie it the threshold such that the hpd region contain $1-\alpha$ of the probabl mass
how do you calcul the hpd threshold imagin the distribut immers in water: we lower the water level until $1-\alpha$ of the distribut is revealed. this lend itself to binari search if the invers cdf is easili comput or search over sort data point if it isn't.
what the model select problem in machin learn decid how to set the model paramet to avoid overfit and underfit
what are the two main approach to the model select problem cross-valid with one set of the paramet in each fold. select the set which minim the general error. bayesian model selection. construct the posterior of the model given the data and pick the map model.
what the margin likelihood in machin learn $p(\mc{d} m) = t p(\mc{d} \theta) p(\theta m)d\theta$
what'r two other name for the margin likelihood in machin learn integr likelihood evid
if there a uni prior over the models, what doe bayesian model select reduc to pick the model that maxim the margin likelihood
what the bayesian occam razor effect when use bayesian model select and comput the margin likelihood for each model model with a larger number of paramet do not necessarili have a higher margin likelihood, protect us from overfit this is in contrast to use $p(\mc{d} \hat \theta_{mle(m)})$ or $p(\mc{d} \hat \theta_{map(m)})$ to evalu the models, which would lead to a prefer for more paramet
what the conserv of probabl mass principl a pdf alway sum to 1 so a complex model which can predict mani thing will spread it mass thinly, so on a specif dataset it won't obtain as larg a probabl dure bayesian model select as a simpler model.
when use a conjug prior, how can you calcul the margin likelihood extract the normal constant as so: $p(\theta) = q(\theta)/z_0$ $p(\mc{d} \theta) = q(\theta)/z_l$ $p(\theta \mc{d}) = q(\theta)/z_n$ then $p(\mc{d}) = \frac{z_n}{z_0 z_l}$
what the bic in ml bayesian in ation criterion
how is the bic calcul in ml $\text{bic} = \ln p(\mc{d} \hat \theta_{mle}) - \frac{\text{dof}(\hat \theta_{mle})}{2}\ln n$ where $\text{dof}$ is the number of degre of freedom in the model
what the intuit interpret of the bic in ml it effect the minimum descript length of the data use that model: it has a term for how well the model describ the data, minus the amount of in ation need to describ the model itself
what doe the bic approxim the log maring likelihood: $\text{bic} \ln p(\mc{d})$
what the use of the bic in ml often the margin likelihood can be hard to comput and in that case the bic serv as a decent approxim
what the bayesian approach when it not clear how to set the paramet of the prior put a hyper-prior on the prior: $p(\mc{d} m) = t t p(\mc{d} \theta)p(\theta \alpha, m) p(\alpha m) d\theta d\alpha$ the hyper-prior can usual be unin ative, as the higher in the bayesian hierarchi you go, the less sensit the s are to the parameters.
what empir bay when conduct model select with an unknown paramet $\alpha$ in the prior, rather than integr it out just optim it wrt the margin likelihood
what the bay factor the ratio of posterior probabl between two possibl models: $bf_{1, 0} = \frac{p(\mc{d} m_1)}{p(\mc{d} m_0)}/\frac{p(m_1)}{p(m_0)}$
what distinguish the bay factor from the likelihood ratio in the bf, the paramet have been integr out allow model of differ complex to be compar
how should the bay factor be interpret $bf 10$ is strong evid for it numer $bf 0.1$ is strong evid for it denomin
what the takehom of the jeffreys-lindley paradox if you use improp prior dure model selection, the posterior can chang arbitrarili improp prior are onli allow over the paramet that the compet model share
how the haldan prior defin $beta(0, 0)$
what the use of the haldan prior it an altern to $\text{beta}(1,1)$ as an unin ativ prior which won't alter the mle of say the uni distribut
what the problem with the haldan prior it improper. the posterior will onli be proper on the uni distribut if at least one head and one tail are observ
what sensit analysi in machin learn check how much you conclus chang when the model assumpt are perturb
when are read not atom in java when it a read of a non-volatil long or a double.
what a defens copi in concurr program return a copi of intern state rather than the state itself so no-on els can possibl mutat it.
what are java callabl a \ {callable} is an altern to \ {runnable} that return a .
how are java callabl use implement \ {callabl t }, where \ {t} is the return type call \ {executorservice.submit(callable)}, and get a \ {futur t } back call \ {future.get()} which will return the when it done.
how do you implement a java \ {callabl t } implement \ {call()} which return a \ {t}
what the class of java atom framework class like \ {atomicinteger, atomicreference} class like \ {atomicintegerarray, atomicreferencearray} class like \ {atomicintegerfieldupdat t , atomicreferencefieldupdat t, u } class like \ {atomicmarkablereference} and \ {atomicstampedreference}
what the use of java \ {atomicreferencefieldupdat t, v } allow atom updat to ani \ {volatile} field of type \ {v} of ani object of type \ {t}.
what the use of java \ {atomicstampedrefer v } maintain an object refer along with an integ stamp that can be updat atomically.
what are the most import method in java \ {atomicinteger} class \ {addandget(int delta)} \ {compareandset(int expect, int update)} \ {get()} \ {set(int newvalue)} \ {getandset(int newvalue)}
what java basic lock class \ {reentrantlock}, which implement the \ {lock} interfac
how do java semaphor work \ {acquire()} get a permit and decrement the semaphor count. \ {release()} releas a permit back to the semaphor
what a \ {cyclicbarrier} in java a synchron object that will caus thread to until $n$ thread have arriv at it. can also be given a barrier action for the last enter thread to execut when the barrier trip call `cyclic becaus it can be reset
what the basic way to use thread in java pass a \ {runnable} to a \ {thread} constructor call \ {thread.start()}
what a java \ {threadgroup} a way to organ thread into a tree. creat it by call the \ {threadgroup} in a name and possibl a parent \ {threadgroup} then pass the \ {threadgroup} to a thread constructor, and the thread can use it to access the other thread in the group.
what the statist power of a test the probabl of correct reject the null hypothesi when the null hypothesi is false.
what a type ii error in statist when the null hypothesi is fals but is accept anyway
what a type i error in statist when the null hypothesi is true but is reject anyway
what anoth name for the power of a statist test it sensit
what'r intrins lock in java everi java object can implicit act as a lock
what a monitor lock in java anoth name for an intrins lock
what'r the two built-in lock construct in java method mark \ {synchronized} block of the \ {synchron (lockobject) \{ .. \}}
what a reentrant lock one that can be acquir again by the thread that hold it.
are java intrins lock reentrant yup
what a live failur in concurr program when the program get into a state such that it perman unabl to make forward progress
how much overhead is involv in a context switch between thread in linux about 1us
how long is a linux timeslic typic about 10ms
how much overhead is there in a context switch between process in linux about 10us
what the visibl problem in concurr program when a variabl is altered, it might onli be immedi alter in the cpu own cach publish it to main memori can sometim take a deliber action
when are reorder optim permit by the java spec when the reorder would make no differ to that specif thread
how can you make read of java 64 bit doubl amp; long atom by mark them \ {volatile} by guard them with a lock
how do java intrins lock affect variabl visibl the valu of the variabl visibl to a thread immedi befor releas an intrins lock will be visibl to the next thread that get the lock
what the effect of java \ {volatile} keyword field decor with it will not have their valu cach by the cpu will not have their access reorder with respect to other memori oper
what public in concurr program make an object avail outsid of it current scope
what are the three condit on \ {volatile} use in java write to the variabl do not depend on it current value, or altern onli a singl thread updat it valu the variabl is not part of ani invari which concern other state variabl lock isn't requir for ani other reason while the variabl is be access
what the danger in publish mutabl state from a concurr perspect if a refer to mutabl state is published, then it can be mutat without have to partak in the owner synchron polici
what doe it mean for an object to escap in concurr program the object can be referenc by some method whose behaviour isn't fulli specifi by the owner of the object
what an alien method in concurr program a method whose behaviour isn't fulli specifi by the owner of an object it act upon
what the danger with publish \ {this} from a constructor even if \ {this} is publish as the last statement in the constructor, it could give access to the object befor it fulli construct
whi is it danger to start a thread from a constructor becaus it veri often allow a refer to \ {this} to escape, either by explicit pass \ {this} to the new thread or by pass an inner class of type \ {thread} or \ {runnable}, which has an implicit refer to the contain class
what the best way to avoid problem with publish \ {this} from a constructor in java use a privat constructor and a public factori method
what thread confin in concurr program issu with share mutabl data can be avoid by not share it
what the usual way to implement thread confin in java use the \ {threadlocal} class
how do you set the initi valu of a java \ {threadlocal} variabl subclass it and overrid \ {initialvalue()}, a method which determin what \ {get()} will return prior to \ {set()} be call
what stack confin in concurr program thread confin by way of onli store mutabl state in local variables.
what the import of java volatil keyword when it come to publish object publish an object via a volatil variabl constitut safe public
what the import of \ {final} variabl from the perspect of concurr publish an object via a final field constitut safe public
what effect immut in concurr program an object which is technic mutable, but which isn't modifi after initialization.
what the advantag of effect immut object in concurr program they can be use amp; treat as if they were immut
what are the suffici condit for an object to be safe share oper on the object must all be thread-saf the object must be safe publish
what safe public in concurr program make an object state and a refer to the object visibl at the same time.
what a synchron polici in concurr program the definit of how an object coordin access to it state so as to preserv invari and maintain postcondit
what instanc confin in concurr program encapsul state so that all access to it has to go through a specif object
what the java monitor pattern encapsul all the mutabl state of an object and guard it with the object own intrins lock
what'r the disadvantag of a public lock in concurr program verifi that a public lock is proper use requir analyz the whole program rather than a singl object
what'r the advantag of public lock in concurr program allow client-sid lock
what the best way to publish mutabl state without let a client modifi it publish a copi of the state.
what client-sid lock in java if an object synchron on a public lock, then to conduct sever oper on the object in an atom manner, you can acquir the public lock first
how do java synchron collect class work they hold a collect in a synchron wrapper.
what are java concurr map type \ {concurrenthashmap} \ {concurrentskiplistmap}, which is a \ {navigablemap}
what are java concurr queue interfac \ {blockingdeque} \ {blockingqueue} \ {transferqueue}, which allow produc to wait on consum
what java \ {concurrenthashmap} synchron polici lock stripe weak consist iter approxim global oper (like \ {size} or \ {isempty})
what a weak consist iter in java it'll travers element as they exist when the iter was creat but may or may not updat to those element
what java concurr list type \ {copyonwritearraylist}
what java concurr set type \ {copyonwritearrayset}
what are the six main concurr queue implement in java \ {arrayblockingqueue} \ {linkedblockingqueue} \ {priorityblockingqueue} \ {synchronousqueue} \ {delayqueue} \ {linkedtransferqueue}
what a java \ {synchronousqueue} a concurr queue where each remov op wait on an insert op
what a java \ {delayqueue} a concurr queue where element cannot be taken until a delay has expir
what a java \ {linkedtransferqueue} a queue where produc can wait for consum to receiv element
what serial thread confin in concurr program transfer an object to anoth thread and ensur that it not use by the origin owner after the transfer
at an abstract level, what the easiest way to implement serial thread confin use a ing queue
what work steal in concurr program a producer-consum pattern where consum take work from the back of other consum dequ when they run out of their own
what are java implement of \ {blockingdeque} \ {concurrentlinkeddeque} (unbounded, lock-free) \ {linkedblockingdeque} (option bounded, locking, preferred)
what are the two basic way to respond to an \ {interrupt} request in java propag the \ {interruptedexception} catch the \ {interruptedexception} and call \ {interrupt} on the current thread
what java latch type \ {countdownlatch}
how doe a latch work in concurr program it a synchron which delay thread until it reach it termin state
what a synchron in concurr program ani object that coordin the control flow of thread base on it state
how do you repres asynchron comput in java \ {future}s, usual implement by \ {futuretask}
what java semaphor type \ {semaphore}
what are the four standard \ {executor} implement in java \ {executors.newfixedthreadpool()} \ {executors.newcachedthreadpool()} \ {executors.newsinglethreadexecutor()} \ {executors.newscheduledthreadpool()}
what doe \ {executors.newcachedthreadpool()} produc an \ {executorservice} which can dynam add and reap thread as need
what doe \ {executors.newscheduledthreadpool()} do creat an \ {executorservice} with a fix number of thread that support delay and period task execution, similar to \ {timer}
what the differ between java \ {executor.execute()} and \ {executorservice.submit()} \ {executorservice.submit()} return a futur that can be use to track execut
what are the \ {invoke} method of java \ {executorservice} \ {invokeall()} take a collect of callabl and return a collect of futures, one for each callabl \ {invokeany()} take a collect of callabl and return the of the first that complet success
how do you shut down a java \ {executorservice} \ {service.shutdown()}, then \ {service.awaittermination()}
how do you schedul asynchron task in java use \ {executors.newscheduledthreadpool()} to get a new \ {scheduledexecutorservice} then call \ {service.schedule(runnable, delay)}
what java \ {completionservice} a combin of \ {executor} and \ {blockingqueue} \ {callables} can be submit to it and the *completed* \ {future} will be queu for access by \ {take()} ( s) and \ {poll()} (return null)
what java standard implement of \ {completionservice} \ {executorcompletionservice}
what a common problem with \ {threadlocal} in java and thread pool the lifespan of ani object use a \ {threadlocal} variabl must not exceed the lifespan of the task els it might pollut the next task the same thread execut
when do thread pool architectur work best when thread are homogen when thread are independ
what thread starvat deadlock when a task creat amp; wait on anoth task that'll need a resourc but the pool of such resourc is alreadi consum
what doe it mean for a thread pool to becom clog when there are too mani long-run task compar to the number of thread avail in the pool all the thread end up work on the long-run task and the respons time for ani shorter task suffer
how do you find out how mani processor a system has in java \ {runtime.getruntime().availableprocessors()}
what the optim number of thread for an $n$ cpu system usual $n$ or $n+1$ if it comput bottleneck $(1 + \frac{\text{wait time}}{\text{comput time}})n$ for io bottleneck task
how doe a standard java \ {executorservice} decid whether to reap a thread it'll reap the thread when it idl time exceed the \ {keepalivetime}, a valu pass to the \ {threadpoolexecutor} constructor
what a satur polici in concurr program what to do when a resourc (like space in a bound queue) have all been consum
how do you control in what order task are execut by a java \ {executorservice} pass a \ {blockingqueu runnabl workqueue} to the \ {threadpoolexecutor} constructor that implement the order heurist you want.
how do you alter the satur polici of a standard java \ {executorservice} custom the \ {rejectedexecutionhandler} pass to the \ {threadpoolexecutor} constructor standard custom are provid as subclass of \ {threadpoolexecutor}
what are the standard satur polici avail for java \ {executorservice} \ {abortpolicy}, which caus an except to be thrown \ {callerrunspolicy}, which caus the task to be run on the caller thread \ {discardpolicy} \ {discardoldestpolicy}
how can you stop a java \ {executorservice} from be modifi after creation wrap it in an \ {unconfigurableexecutorservice} object.
what the easiest way to add log or monitor to a java thread pool extend \ {threadpoolexecutor} and overrid \ {beforeexecute}, \ {afterexecute} and similar.
what the best way to parallel recurs task in java use a \ {forkjoinpool} and pass it subclass of \ {forkjointask}
what are the subclass of a \ {forkjointask} \ {recursivetask}, which return a \ {recursiveaction}, which doesn't.
how do you usual subclass java \ {recursivetask} you overrid \ {compute()}, use the inherit \ {invoke}, \ {fork} and \ {join} method to start subcomput use the same pool
what'r the four approach to safe public in java volatil variabl final variabl static initi lock-guard variabl
what a phaser in java \ {phaser} is a general of \ {cyclicbarrier} and \ {countdownlatch}
how doe java \ {phaser} compar to it predecessor allow the threshold to be modifi dynam has \emph{generations}, which track how mani time the barrier has been broken has tiering, which allow phaser to be structur into a tree for use with larg number of thread has monitoring, which mean the phaser state can be track
what the best way to provid random number to thread in java \ {threadlocalrandom}, which elimin the content issu that global prngs have
what doe dma stand for in comput architectur direct memori access
what the front side bus in most comput architectur the compon that link the cpus to the north bridg elimin as a discret compon in modern architectur in favour of an on-chip solut
what the northbridg in comput architectur the motherboard compon that manag communic between the cpus and the `fast system compon (like the ram) and the southbridg elimin as a discret compon in modern architectur in favour of on-cpu memori manag
what the southbridg in comput architectur manag communic between the northbridg and the `slow part of the system elimin in modern architectur when the northbridg was elimin
what direct memori access in comput architectur the abil for certain devic to access system memori without involv the cpu
what the standard intel chipset architectur nowaday core communic via qpi each core has access to memory, graphic and plat control hub pch with access to the rest of the system
what qpi stand for in comput architectur quickpath interconnect, intel core communic tech
what pch stand for in comput architectur plat control hub, intel replac for the southbridg
at a high level, what distinguish sram and dram static ram is fast and expens dynam ram is slow and cheap
what the electr architectur of a sram cell six transistors; four a bistabl switch and the other two allow the current state of the switch to be read/written
what the electr architectur of a dram cell a transistor and a capacitor.
what leakag in dram the logic valu is store on a capacitor, and the charg held there disapp veri quick
how is leakag remedi in dram refresh cycles. read the memori row then write it back.
whi doe read a dram cell take much longer than an sram cell need to wait for the capacitor to discharg need to amplifi the signal need to write the valu that was read out back to the capacitor
how is demultiplex size explos limit in ram by transmit the address to the ram in two parts: first the row address is sent, demultiplexed, and the correspond row line ate amp; read out. then the column address is sent, demultiplexed, and the bit correspond to the column line is sent to out output then both column amp; row address can be reset
where is sram usual use for on-di cach
what sdram synchron dram
what ddr in comput architectur doubl data rate dram, a successor to sdram
what the cas latenc in comput architectur how mani cyce it take the ram to read out a valu onc the column address select has been receiv
what the word size of most modern sdram 64 bit
what doe it mean for comput memori to be double-pump it'll transmit two word per cycl
how doe modern dram optim row address select amp; column address select the memori control can ask for the next 1, 2, 4, 8 word follow the first address the memori control can `keep the row open and submit a new column address selector without have to respecifi the row address selector.
what the row precharg time in comput architectur the time it take to delatch the previous row and prepar to accept a new row address selector
what the e to precharg delay in comput architectur the time need after a row address select befor the next row address select can be precharg usual two or three time the row precharg time
what the memori command rate in comput architectur how mani cycl it take the memori control to issu a new command
how often must a dram cell be refresh usual onc everi 64ms
how are dram cell read everi cell on a row is read simultaneously, and all are subsequ written/refresh simultaneously. onli the bit specifi by the column address selector will be output though
what distinguish ddr from sdram sdram transmit data on the lead edg of the clock signal ddr transmit data on the lead and fall edg of the clock signal
what the differ between ddr and ddr2 the memori intern frequenc is twice that of the extern frequency.
in comput architecture, what are the l1i and l1d cach l1 instruct cach l1 data cach
whi are instruct cach use by most processor decod instruct is veri slow
how is the l2 cach split between code and data it usual isn't. both the l1i and l1d cach treat it as the next level of the memori hierarchi though
what a cach line (usually) 64 byte of contigu words, the unit of cach in most modern architectur
what a cach tag in comput architectur the portion of the memori address that a cach line correspond to (on a 64 byte cach line for example, the low six bit will be zeroed)
how mani data transfer doe a cach line correspond to in modern architectur a 64 byte cach line and a 64 bit word correspond to 8 trasfer
how doe a memori address correspond to a locat in most modern cach the lead bit the tag the middl bit identifi the cach set the trail bit an offset into the cach line
what'r the alias of a cach line the address of all possibl memori locat that could be store in that cach line
what an exclus vs an inclus cach in comput architectur exclus cach copi a line back to the next level of the hierarchi when they'r evict from the current level inclus cach store ani line in the current level in all higher levels, so evict onli entail a delet rather than a copi
what a dirti cach line in comput architectur a cach line whose modif haven't been copi back to main memori
what cach coher in comput architectur the fact that differ processor with differ cach should all have the same uni view of memori
how is cach coher maintain in most multiprocessor architectur processor watch eachoth memori request dirti cach line are not allow to be present in ani other processor cach clean cach line can resid in arbitrarili mani cach implement by a protocol call mesi
how doe random write time react to chang in work set size it a staircase, with a lead edg whenev the work set exceed the size of a cach
what a fulli associ cach one where each cach line can hold a copi of ani memori locat
what the problem with a fulli associ cach the processor has to check everi cach line to see if the memori locat it look for has been cach this requir a lot of circuitri to do quick
what the most common use of a fulli associ cach tlb cach of a few dozen entri on some intel processors.
what a direct map cach one where each memori locat correspond to a singl cach line
what the problem with direct-map cach high evict rate if you'r unlucki larg multiplex need
what a set associ cach the memori address is broken into a tag (highest bits) and set address (middl bits) all entri in the correspond cach set have their tag compar against the tag simultaneously, and their data read out simultaneously. the success match has it data output
what doe it mean for a cach to be 8-way set associ it a set-associ cach where each set contain 8 tag
how is cach size relat to associ in a set associ cach $\text{cach size} = \text{cach line size} \text{associativity} \text{numb of sets}$
what the most common kind of cpu cach in 2014 an 8-way set-associ cach
what a processor prefetch if the processor can predict which memori locat will be need next, it'll start the memori access earli
what a write-through cach implement one where chang to a cach line are immedi written to main memori
what a write-back cach implement when a cach line is written to, it mark as dirty. when a dirti cach line is evicted, it written back to memori
what the problem with write-back cach they damag cach coher delet take some time
what the mesi cach protocol a protocol for maintain cach coher in write-back cach a cach line can be mark as modified, exclusive, share or invalid. these repres whether the cach line is to be written to and whether ani other processor has a copi of the line. processor snoop on eachoth write to share line and invalid their own cach version of a line if anoth processor write to it.
in the mesi protocol, what an rfo a request for ownership, which request the other processor invalid their cach line so the sender can write to their
what the problem with hyperthread technolog they split the cach between them, possibl dramat lower the cach hit rate
what'r hyperthread thread that share almost all the resourc of a cpu except the register. this allow one thread to take advantag of spare cpu resourc while the other is wait on reads, and allow read to be execut concurr
are l1 cach general virtual or physic address virtual addressed, so they have to be flush when the page tabl chang
are l2 and l3 cach general virtual or physic address physically, as their latenc give plenti of time for the virtual to physic convers to be done
from a cach perspective, what the problem with self modifi code the l1i cach assum that code page are immut so if chang are detect a lot of pessimist assumpt have to be made
what the critic word protocol in comput architectur if the word the processor need to fetch from memori won't be the first in it cach line the memori control can ask for that word - the critic word - first, and then backfil the cach line afterward
what the job of a cpu memori manag unit to take a page tabl provid by the os and use it to implement virtual address
how mani level do modern page tabl have at most 4
how is a page tabl use if the page tabl has (say) 4 levels, the address is split into 5 part the lead bit are use for an index into the level 4 directori which give a pointer to a level 3 directori that the second group of bit index into, etc and the level 1 directori give a pointer to a memori page that the last group of bit index into
how are page tabl tree assign to thread each process get it own page tabl
what the standard page size in most comput 4kb
what aslr in comput architectur address space layout randomization, where the code, heap, stack, librari of an execut are map to random locat for secur reason
what the tlb cach a cach (hierarchy) for page tabl lookup usual small and fulli associative, but more recent larger and set associ
how is the tlb use the virtual address is broken into a tag, which is an address into the page table, and an offset the tag is look up in the tlb and the offset is ad to it
whi can't tlb entri be prefetch by processor becaus the processor might prefetch an invalid tage tabl walk
what are the cach in a modern tlb hierarchi (instruction) itlb (data) dtlb and a unifi l2tlb
what the structur of a haswel l1 dtlb 100 entri over 4kb, 2mb and 1gb page all 4-way set associ
what the structur of haswel l2 tlb 1024 entri share over 4kb and 2mb page 8 way associ
are tlbs per core or per processor per core
what are the two approach to deal with the tlb across multipl process either flush the tlb whenev the page tabl tree chang or extend the tlb tag to uniqu id the page tabl tree they refer to
what the two main advantag of extend tlb tag a lot of context switch are to the kernel and/or virtual memori manag for a veri short time. without extend tags, this lead to two flush of the tlb. switch between thread within a process also requir a trip to the kernel. again, this would lead to two flush without tags.
what the advantag of larg page size fewer entri in the page tabl lower chanc of a tlb miss for contigu data if it built into the os, the page tabl depth can be reduc
what the downsid of larg page size a lot of memori oper requir align to page boundaries, so larg page wast more space larg page on most system need to be compris of mani smaller pages, and find contigu region of free space can be difficult.
what the standard structur of a x86-64 page tabl four level each dispatch 9 bit and a offset of 12 bit
what the size of the standard virtual address 48 bit
what are extend page tables/nest page tabl an intel/amd virtual technolog which allow the page tabl on the host machin to be extend to provid virtual address to process in the guest machin
what the effect of virtual on page tabl lookup more level have to be search through the tlb can't tag individu process in the guest machin
when a hardwar prefetch usual trigger tradit when there'r two cach miss to adjac cach line but nowaday larger `stride of up to 256 byte might also be recognis
how mani memori access can modern cpus have go simultan 8 or 16
what'r the main limit of hardwar prefetch it won't be trigger by miss on differ page it'll onli spot veri simpl access pattern
what simd in comput architectur singl instruction, multipl data ie parallel instruct that can be found on some processor
how do you group subexpress in a regex without captur them into a variabl \ {( :abc)} will group without captur into a \ {\tbs i} variabl
what an import distinct with thing like regex \ {\tbs t} use in a regex it a regex metacharact that'll match against a tab charact use in a string it a string metacharact that'll insert a tab charact
what the regex lookahead oper \ {( =abc)} will tri to resolv to a posit that is follow by \ {abc}
what the regex lookbehind oper \ {( =abc)} will tri to resolv to a posit that follow \ {abc}
what the regex negat lookahead oper \ {( !abc)} will tri to resolv to a posit that isn't follow by \ {abc}
what the regex negat lookbehind oper \ {( !abc)} will tri to resolv to a posit that isn't after \ {abc}
where doe a regex usual start when search for the next match the end of the last match
what a multilin mode in regex a switch that'll caus \ { } and \ {\^{}} to be interpret as start- and end-of-lin charact rather than start- and end-of-str characters.
how do you creat a case-insensit regex in java \ {pattern.compile("expression", pattern.case\_insensitive)}
use a singl line of code, how can you test whether a regex appear in a string in java \ {pattern.matches("regex", "string")} will return true or fals
how do you insert comment into a java regex \ {pattern.compile(" comment \tbs n anoth comment", pattern.comments)}
how do you match against a whitespac charact in java regex \ {pattern.compile("\tb \tbs s")} first backslash escap the second
how do you view a java regex as it was receiv by the function \ {system.out.println(compiledpattern.pattern().tostring())}
how do you search and replac with regex in java \ {pattern.compile("pattern")} to get a \ {pattern} \ {pattern.matcher("text")} to get a \ {matcher} \ {newtext = matcher.replaceall("substitute")} to replac all occur
how do you pass multipl bit flag to a function in java \ {pattern.compile("text", pattern.case\_insensit pattern.comments)}
how do you write a regex over sever line in java \ {pattern.compile("line1 \tbs n" + "line2", pattern.comments)} \ {comments} flag caus the pattern to ignor whitespac
how do you match against a tab charact in a java regex \ {pattern.compile("\tb t")} onli one backslash need as java string pars will convert it to a tab charact befor pass it to the regex engin
what a unicod code point the number (usual written in hexadecim and prepend by \ {u+}) that correspond to a certain charact
how do you escap a unicod code point in java \ {\tbs u2345}
what do unicod regex consid to constitut a charact a singl code point. so combin charact are their own charact
what the shorthand for match against a digit in java regex \ {\tbs d}
what the shorthand for match against a non-digit in java regex \ {\tbs d}
what the shorthand for match against a whitespac charact in java regex \ {\tbs s}
what the shorthand for match against a non-whitespac charact in java regex \ {\tbs s}
what the shorthand for match against a word charact in java regex \ {\tbs w}
what the shorthand for match against a non-word charact in java regex \ {\tbs w}
how do you match against a unicod charact with a specif qualiti in a java regex \ {\tbs p\{quality\}}
how do you match against a unicod charact without a specif qualiti in a java regex \ {\tbs p\{ qualiti \}}
what java regex qualiti for ascii punctuat \ {\tbs p\{punc\}}
what java regex qualiti for ascii lower case letter \ {\tbs p\{lower\}}
what java regex qualiti for ascii upper case letter \ {\tbs p\{upper\}}
how do you match against an intersect of two charact class in java regex \ {[[a-d] amp; amp;[b-e]]}
how do you match against the union of two charact class in java regex \ {[[a-d][b-e]]} \ {[a-db-e]}
how do you match against a word boundari in java regex \ {\tbs b}
how do you match against the begin of the input in java regex \ {\tbs a}
how do you match against the end of the input in java regex \ {\tbs z}
how do you match against the end of the previous match in java regex \ {\tbs g}
what java restrict on what can appear in a regex lookbehind it can onli match a finit amount of text
how do you turn case sensit on and off within a java unicod regex \ {( i)} and \ {( -i)} will turn it on/off for the durat of the parenthes express it sit in \ {( i:pattern)} and \ {( -i:pattern)} will turn it on/off for the pattern follow
how do you enabl multilin mode in java regex \ {pattern.compile("pattern", pattern.multiline)}
doe \ {.} match line termin in java regex no, but it can be enabl with the \ {pattern.dotall} switch
how do you e liter mode in a java regex \ {\tbs q} to start a liter segment \ {\tbs e} to end the liter segment
what liter mode in java regex a portion of the regex where no charact other than the liter termin charact (usual \ {\tbs e}) is consid a metacharact
how can you fetch a subexpress of the current regex match in java from outsid the regex string \ {matcher.group(4)} to get the 4th parenthes subexpress
how do you captur a name subexpress in java regex \ {( name pattern)}
how do you recal a name subexpress in a java regex \ {\tbs k name }
how do you access a name group from the current match in java, from outsid the regex \ {matcher.group("name")}
what atom group in a regex a parenthes subexpress that match as much as it can and then becom immut - it won't back off later to allow later express to match against the text it cover
how do you construct an atom group in java regex \ {( pattern)}
what are the reluct quantifi in java regex \ {* }, \ { }, \ {\{3, 5\} }, etc they'll match as littl as possibl
how do you match against an express at least 7 time in java regex \ {pattern\{7,\}}
how do you match against an express exact 7 time in a java regex \ {pattern\{7\}}
what are java regex possess quantifi \ {*+}, \ {++}, \ {\{3, 5\}+} syntact sugar for atom groupings; onc they'v match they won't back off
what java under regex engin tradit nfa
in an altern in an nfa regex, which altern will be select if sever could fit the first one that fit
what are the two univers rule about how a regex will select match match that begin earlier are alway prefer the standard quantifi (\ {*, , +, \{m, n\}}) are greedi
what do nfa and dfa stand for in the context of regex nondeterminist finit automaton determinist finit automaton
intuitively, how do nfa regex algorithm work they crawl the string until they get a match against the first compon of the pattern and then tri to match the next compon iteratively, backtrack if need be
intuitively, how do dfa regex algorithm work they scan the string, maintain a list of all the possibl match to the pattern
what a common mistak with the zero-or-mor quantifi in regex have a regex of the \ {a*} not realiz it'll match the empti string on a text \ {"b"}
what a subtl distinct between the charact match by \ {[\^{}a]} and \ {.} in most regex \ {[\^{}a]} will match a newlin \ {.} won't
how do you get the match text from a regex in java after a match operation, \ {matcher.group()}
how do you see whether a regex match some part of a string in java \ {matcher.find()} will return a boolean (and advanc the match pointer)
how can you restrict the region of a string target by a java regex \ {matcher.region(int start, int end)}
what doe java transpar bound regex flag do it allow lookaround oper to check outsid the region of text to be match against
how can you test whether pass addit input to a regex could creat anoth match call \ {matcher.find()} and then \ {matcher.hitend()} to see whether the matcher hit the end of the string while tri to match
how do you get the start amp; end indic of a java regex match \ {matcher.start()}, \ {matcher.end()}
how do you store a match from a java regex \ {matcher.tomatchresult()}, which will return a \ {matchresult} object
how do you test whether a java regex exact match a region of text \ {matcher.matches()}
how do you test whether a java regex match the start of a region of text \ {matcher.lookingat()}
how do you move to the next match of a java regex \ {matcher.find()} will return fals if there are no further match
how do you get the indic delimit the text match the $i$th subexpress of a java regex \ {matcher.start(i)}, \ {matcher.end(i)}
how do you replac the first instanc of a java regex in a text \ {matcher.replacefirst("replacement")}
how do you refer subexpress in a regex from a replac string in java \ { 1} match the first subexpression, \ { 2} the second, etc
how do you refer the text match a java regex from a replac string \ { 0}
what the most effici way of do a lot of search amp; replac on java string use regex creat a \ {stringbuffer} and repeat call \ {matcher.find()}, \ {matcher.appendreplacement(buffer, "replacement")}, which will copi the text up to the start of the match then append the replac string. then call \ {matcher.appendtail(buffer)} to append the remain text after the final match.
how can you test whether pass addit input to a java regex would stop the previous match from succeed \ {matcher.requireend()}
what a context switch in comput architectur can mean a switch between thread can mean a switch between process can mean a domain transit
what a trap in comput architectur an event that caus a transit from user privileg to supervisor privileg if a trap occur while alreadi in supervisor privilege, it doesn't caus a domain transit
what doe a process switch entail chang from user to supervisor protect domain chang stacks: switch from user stack to a kernel stack save user state on the kernel stack do kernel stuff kernel thread switch restor user state chang from supervisor protect domain to user
what are common caus of trap in comput architectur trap calls, explicit system call to trigger a trap exceptions, like access invalid memori extern event like packet arriv or disk oper complet
when can context switch occur onli when the comput in privileg mode
what'r two other name for kernel mode supervisor mode priveleg mode
what doe yegg think googl doe wrong it focus on product over plat s
whi it a better idea to build a plat than a product becaus plat s can be extend without the owner interfer
what the goal in build a plat to encapsul some comput and data into someth that can be treat as a servic
what avail in the context of cap ani consum can reach some copi of the data it want
what an exampl of a ca but not p distribut system databas which provid distribut transact
what an exampl of a cp but not a distribut system an acid databas which prevent transact while there a partit
what an exampl of an ap but not c distribut system a system which use cach in the advent of an avail failur
what the weak cap conjectur that to improv a system c, a, or p, you have to trade off in one of the other two
what yield in the context of distribut system the probabl of a request be complet the probabl of an updat be appli
what harvest in the context of distribut system the complet of an answer to a queri the visibl of an updat
what yield do good distribut system aim for four or five nines.
what the simplest strategi for trade harvest for yield in distribut system spread data over mani nodes, and answer a queri use whatev data is visibl within a timeout
what'r orthogon mechan in distribut system compon that have veri littl or no runtim interfac to other mechan so if they fail, they don't affect anyon els
what the typic rang of the server-to-administr ratio 2:1 on less autom servic 2,500:1 on ideal high autom servic
what are three simpl tenet to design web-scal servic expect failures. ani compon may crash or be stop at ani time. keep thing simple. few dependencies, simpl installation. autom everyth possible.
how should the development, test and oper team of a webscal servic be structur they should work as close togeth as possible.
what the best way to test the failur path of a distribut system never shut it down normally, just hard-fail it and see how it recovers.
what the acid test as to whether a distribut servic is suffici fault toler whether the op team is will to bring down ani server at ani time without drain it workload first
how doe power consumpt scale with clock frequenc what the relev to distribut system design cubically. so lot of low-pow server might be a better idea than a few high-pow one
what should be the version polici for web-scal softwar one support version at ani one time one support hardwar at ani one time. but *tolerate* multipl concurr version and hardwar
what'r the prerequisit for develop web-scal softwar in a full environ a way to deploy the system to a singl test server a way to quick check the system health
how should protocol for human intervent in distribut system be written as scripts, not as multi-step documents!
what the rule of thumb for a complex optim be worth it compar to a simpl one in distribut system when it bring order-of-magnitud improv to per anc
where should throttl be implement in distribut system at all major compon boundari
how should the partit of a distribut system be design to be infinit fine-grained, usual mean a hash tabl ex: if you partit data over node by name prefix for example, eventu all name start with p won't fit on one server.
at rough what rate doe emerg human intervent in a system go wrong about 20 of the time
what are the four strategi for deal with latenc when design distribut servic suppress coupl between compon asynchron interact from the begin horizont data decompositiion from the begin design for e/ e configur
what an e/ e configur in distribut system one where all node partak in servic requests, and in case of failur traffic is move onto the other node contrast with e/passive, where one node is dormant until the e one fail
rough how far is a 10ms rtt equival to in fiber 1000km
what the speed of light in a fiber about $ $200,000km/s
what the averag round-trip latenc of the last mile of an internet connect 20-50ms
what the typic round trip latenc for the first hop of a mobil connect 100-1000ms
what the basic idea of mapreduc user specifi a map function that process each key-valu pair in the data to produc an intermedi set of key-valu pair a reduc function that take an intermedi key and the intermedi valu with that key, and return anoth (usual much smaller) set of valu
how is a mapreduc invoc distribut across hardwar input data is partit into split of $ $64mb each, and a singl node comput the map function for each split, store the on a local disk. intermedi key space is partit by the hash of the key, and a singl node comput the reduct for one partit of the space the locat of the for each key for each map worker is pass back to the master, who pass all the locat for a given partit of the key space to the relev reduc worker. when the reduc worker completes, it sort the output and write it to a temporari file in the output directori and then atom renam the file to the final name
what in ation doe the master track about execut in mapreduc the state of each map task and reduc task (idle/in progress/completed) the ident of the worker machin the locat of the intermedi file
how doe mapreduc deal with worker failur worker are ping regularly. if a map worker fail to respond to a ping, their work is re-execut and all worker are notifi of the failur if a reduc worker fail to respond to a ping befor they'v report their work complete, their work is re-execut
how doe mapreduc respond to master failur it abort execution. client can retri if they desire.
how mani partit of the intermedi key space are typic use in mapreduc a small multipl of the number of worker that'r expect to be use
how doe mapreduc deal with straggl task when a mr oper is close to completion, the master schedul backup execut of the last few task
how doe mapreduc deal with bad record befor each execut of map or reduce, the worker store the sequenc number of the input if a worker encount an error, it send a last-gasp packet to the master with that sequenc number when the master has seen the same sequenc number more than once, it tell the next worker attempt the task to skip that record
what the problem solv by a consensus algorithm a collect of process can each propos a valu we want to select a singl valu from them, if there are ani propos process should be abl to learn the propos valu onli a valu that has been propos can be chosen onli a singl valu is chosen no process learn that a valu has been chosen unless it actual has been.
what are the three role in a consensus algorithm propos acceptor learner a singl process can act in more than one role though
what are the communic assumpt of most consensus algorithm messag can take arbitraril long to be delivered, can be duplic can be lost but can't be corrupt
what are the node failur assumpt of most consensus algorithm node can stop node oper at arbitrari speed node can restart node can rememb in ation across restart
what the propos algorithm in the first phase of the paxo when choos a valu a propos select a propos number $n$ and send a prepar request with number $n$ to a major of acceptors.
what the acceptor algorithm in the first phase of the paxo when choos a valu if an acceptor receiv a prepar request with a number $n$ greater than that of ani prepar request to which it has alreadi responded, then it respond with a promis not to accept ani more propos number less than $n$ the highest-numb propos it accept (if any)
what the propos algorithm in the second phase of the paxo when choos a valu if the propos receiv a respons to it prepar request for number $n$ from a major of acceptors, then it respond with an accept request to each of them with number $n$ and valu $v$, where $v$ is the highest-numb valu amongst the responses, or ani valu if the respons report no propos
what the acceptor algorithm in the second phase of the paxo when choos a valu if an acceptor receiv an accept request for a propos number $n$, it accept the propos unless it has alreadi respond to a prepar request have a number greater than $n$
how do learner learn the accept valu in the paxo algorithm the trivial approach is to have each acceptor tell each learner when it accept the propos a refin approach is to select a distinguish learner (usual the leader) who propag the accept propos to the listen
how is live ensur in paxo by select a distinguish propos (usual the leader) who make all the proposals.
what the fischer-lynch-paterson imposs that a reliabl algorithm for elect a leader must make use of either random or time
how doe the paxo algorithm ensur two propos aren't issu with the same number differ propos choos their number from disjoint set and each propos rememb in stabl storag the highest number she tri to issu so far
what the bulli algorithm for elect a leader on each node the node broadcast an elect messag to all other node with higher id if it doesn't get ani response, it win the elect and broadcast it victori if it hear from a node with a higher id, it wait for a certain amount of time for ani process with a higher id to broadcast itself as the winner. if it time out, it rebroadcast the elect message. if it get an elect messag from a node with lower id, it respond with an aliv messag if it get a victori messag from a node with lower id, it initi a new elect
how doe the first phase of the ring algorithm for elect a leader start everi node start as a non-particip if a node notic a lack of leader, it creat an elect messag contain it id and send it clockwise.
in the first phase of the ring algorithm for select a leader, what doe a node do if it a particip when it receiv an elect messag if the receiv id is smaller than it own the messag is discard if the receiv id is larger than it own the messag is forward if the receiv id is the same as it own, the node start act like a leader.
in the first phase of the ring algorithm for elect a leader, what doe a process which is a non-particip do when it receiv an elect messag it mark itself as a particip and forward the messag with the id sub'd for the larger of it own and the receiv id.
what the second phase of the ring algorithm for elect a leader the leader node mark itself as a non-particip and send a victori messag out with it id when a non-lead node receiv a victori messag it mark itself as a non-particip and forward the messag when the leader node receiv the victori messag it discard the messag
defin residu the differ between the measur valu of a datum and it estim valu in a regress model
what demand page in comput architectur the os onli copi a disk page into physic memori if an attempt is made to access it
at a low level, how doe demand page work an attempt is made to access a page if the page is valid if the page is invalid, a page-fault trap occurs, in which if the refer is a valid refer to secondari memory, page it in by schedul the necessari disk oper resum the interrupt instruct
what an invalid page in comput architectur a page which current resid in secondari memori
what the differ between \ {compareandset} and \ {weakcompareandset} in java atom the \ {weak} version doesn't impos a happens-befor order on the variabl this make it more effici on some plat s, but also might make the compar fail for no appar reason don't use it
how do you implement a custom unmodifi collect in java deriv from \ {abstractcollection} implement \ {iterator()} without it \ {remove()} method implement \ {size()} advis to implement a no-arg constructor and a constructor that take a collect
how do you implement a custom modifi collect in java deriv from \ {abstractcollection} implement \ {iterator()} with it \ {remove()} method implement \ {size()} and \ {add()} overrid ani other method you want advis to implement a no-arg constructor and a constructor that take a collect
how do you implement an \ {iterator} in java implement \ {hasnext()} and \ {next()} and option \ {remove()}
what doe java \ {==} do compar refer equal
how do you overrid \ {equals(object obj)} in java test if \ {obj == null} test if \ {this == obj} test if \ {obj instanceof class} cast to \ {class} test hash code test member equal
when should you consid overrid equal when it an immut type.
what are the core compon of java nio channel buffer selector
what are the four primari channel in java nio filechannel datagramchannel (udp) socketchannel (tcp) serversocketchannel (listen for tcp connections)
what are the core buffer implement in java nio bytebuffer, charbuffer, etc - one for each primat
what the use of a selector in java nio it allow a singl thread to handl multipl channel
what the differ between nio channel and stream channel are bidirect channel are asynchron channel alway read/writ to a buffer
how do you get an input channel for a file in java creat a \ {fileinputstream("name")} call \ {fileinputstream.getchannel()}
how mani bit do you need in a bloom filter for a 1 error rate $ $10 bit per element
what the time complex of a radix sort $o(kn)$, where $k$ is the length of the key
what googl bigtabl a sparse, distributed, multi-dimension sort map
what bigtabl map index by row key (string) column key (string) timestamp
what a tablet in bigtabl a rang of row that constitut a unit of load balanc
what type are the valu in bigtabl strings.
what are column famili in bigtabl a group of column key that the basic unit of access control and memori account
how is version done in bigtabl version of a cell are store in descend timestamp order client can specifi how old a cell must be befor it garbag collect
what kind of indic do you use when write a binari search a close range!
when should a binari search loop termin when the lower index is strict larger than the upper index
how should you updat the lower or upper index after a binari search loop \ {lower = midpoint + 1} \ {upper = midpoint - 1}
where should you test for equal in a binari search insid the loop!
in a binari heap over an array, where will you find the children of node $i$ at $2i+1$ and $2i+2$
how do you insert an element into an array heap add it in the last posit and bubbl it up
how do you delet an element from a binari heap over an array swap the element with the last element shorten the array by one bubbl the old-last element down.
what the idea in the join-idle-queu algorithm token for idl worker are load balanc across the dispatch
what the problem the join-idle-queu algorithm solv how to distribut work from mani dispatch across mani worker while keep respons time low
how doe join-idle-queu distribut it idl worker either by chose a dispatch at random to notifi of it idl or sampl $d$ dispatch and choos the one with the least number of queu idl
what doe $-1$ correspond to in two compliment all ones.
how do you detect an overflow when ad two two compliment number if the two highest carri bit are 10 or 01
what the simplest solut to the dine philosoph problem number the fork and insist the philosoph pick up the lowest number fork first
how do you call the default \ {hashcode} implement in java, even if it been overridden \ {system.identityhashcode(obj)}
if you induc an order on lock use their hash codes, how can you deal with the case where two lock have the same hash code add a third tie-brak lock. have to get the tie-break lock befor you get the other two lock
when should you be wari of call alien method when you'r hold a lock, sinc it imposs to know what other lock the alien will tri and get
what an open call in concurr program a method call made without hold a lock
what the best way to diagnos deadlock identifi all locat in the code where more than one lock is taken simultan check that the lock order is consist across all these locat
how do you get a lock in an interrupt manner in java \ {obj.lockinterruptibly()} throw \ {interruptedexception}
how do you trigger a thread dump from the command line on unix \ {ctrl+\tbs}
what a thread dump like a stack trace, but for thread and their lock in ation
what \ {jstack} a command line tool which when fed a java process id will print the thread dump
how do you list all process in unix \ {ps -e}
what the usual solut to livelock problem introduc randomness, usual a random backoff
whi doe frequent lock content degrad per anc so bad caus extra context switch when a thread s, it wast the rest of it quantum
what lock elis in concurr program if a compil notic that an object is onli visibl to one thread (ex: it in local scope) it'll optim out ani lock acquisit on that object
when is spin-wait prefer to suspens spin wait is prefer when the thread will onli for a short time suspenson is prefer when the thread will for a long time
what lock stripe in concurr program replac a lock that guard multipl variabl with multipl lock to guard subset of those variabl
when it advantag to split a lock in concurr program when the lock suffer moder content
what the downsid to lock stripe it harder to acquir all the locks.
how do you acquir an arbitrari set of intrins lock in java through recurs
what a hot field in concurr program a field that everi mutat method on an object has to acquir the lock for (like the \ {size} of a collection)
what the advantag of \ {atomic} java primit over explicit lock they use veri fine grain processor primat to conduct the locking.
what the problem with object pool in java in modern jvms, \ {new} is veri fast - ten instruct or less so object pool is a serious loss for all but the most heavyweight object
what the foundat of the miller-rabin test if $p$ is prime, there are no squar root of 1 mod $p$ other than $1, -1$ fermat littl theorem $z/pz$ is generat by the element between 2 and $2(\ln n)^2$
what fermat littl theorem $a^{p-1} _p 1$ for all $a$
what the differ between \ { } and \ { } in java \ { } is the unsign right shift, introduc a zero on the left \ { } is the sign right shift, replic the sign bit on the left
do shift or bitwis oper have higher preced in java shift oper
do arithmet or shift oper have higher preced in java arithmet operators.
how can you encourag thread interleav when test concurr java code use more thread than there are processor judici use of \ {thread.yield()} (though this might be a no-op on some plat s!)
what the best way to time thread when test concurr java code use a \ {cyclicbarrier} and have it barrier action start/stop a timer get time with \ {system.nanotime()}
what dynam compil in java if a method is use often enough, the jvm might compil it on-the-fli to machin code
what the differ between \ {thread.start()} and \ {thread.run()} in java thread start caus the jvm to call \ {run()} on a new thread thread run just run execut the method
what the idiom for use an explicit lock in java acquir the lock immedi afterward insid a tri block do some work insid the final block releas the lock
what hand over hand lock have a lock for each node in a link list the lock guard both the data and the next pointer so you have to acquir the lock befor you can progress down the list
what the problem with fair lock in java it signific reduc throughput. onli use it when a lock will be held for a long time.
what the easiest way to implement a read-writ lock in java use \ {readwritelock} and \ {reentrantreadwritelock}
how do you use a \ {readwritelock} in java \ {rwlock.readlock()} get the read lock \ {rwlock.writelock()} get the write lock
how is the fisher in ation defin $i(\phi) = \mbb{e}\left[\left(\frac{d \ln p(x \phi)}{d\phi}\right)^2\right]$
what the intuit interpret of the fisher in ation it a way of measur how much in ation a rv $x$ carri about a paramet $\phi$
what are two altern definit of the fisher in ation varianc of the score expect of the observ in ation
what the definit of the score in statist $\frac{d \ln p(x \phi)}{d\phi}$
what the intuit interpret of the score in statist indic how sensit the likelihood is to a paramet
what the definit of the observ in ation in statist $\mathcal{j}(\theta^*) = -h(\ln p(x \theta)) _{\theta^*}$ where $h$ is the hessian usual evalut at the mle
what the altern way of calcul the fisher in ation under certain regular conditions, $-\mbb{e}\left[\frac{d^2}{d\phi^2} \ln p(x \theta)\right]$
what the jeffrey prior for the dirichlet distribut $p(\theta) \propto \text{dir}(\frac{1}{2}, \dotsc, \frac{1}{2})$
what the translat invari jeffrey prior $p(x) \propto 1$
what the scale invari jeffrey prior $p(s) \propto 1/s$ for some constant $s$
what a robust prior a prior which has veri heavi tail so as not to have too much of an influenc on the
what the advantag of use a mixtur of conjug prior a mixtur of conjug prior is also conjug a mixtur of conjug prior can approxim ani prior
what doe it mean to `borrow statist strength if you infer a hyperparamet from the data then volum of the independ variabl space contain mani datapoint can help deduc the paramet for volum of the variabl space contain few datapoint
what gdi in window graphic devic interface, the technolog under (and preceding) winform
at what point dure construct are xaml event handler attach after the \ {name} properti has been set but befor ani other properti is set
what the purpos of the \ {xmlns="longstring"} properti in xaml it identifi the namespac to be use to qualifi that element and it children, allow xaml element to be map to framework element
how do you declar multipl xml namespac in the same tree they need to be declar with differ prefix to be use with ani identifi from that namespac typically: \ {xmlns:x="longstring"}
what winrt an unmanaged, low-level api for windows. replac for the ancient winapi
what are the project of the api expos by winrt \ {windows.ui.xaml} for xaml app \ {windows.ui.webui} for html app \ {system} for .net \ {windows} for general purpos winrt
what the window app packag manifest the \ {package.appxmanifest} file that describ the app
how do you exit the splash screen in window 8 app call \ {window.current.activate}
what are win 8 app capabl the permiss grant to an app
what are win 8 contract a way to cooper with other app or with window everi contract has a sourc that initi a task and a target that complet it
what properti of a window 8 app doe the packag manifest describ application-level properti (name, orientation, etc) capabl (permissions) declar (contracts) content uri (url whose js can be called) packag
whi are c class that back xaml mark \ {partial} becaus the definit in the \ {.cs} file is merg with a \ {.cs} file generat from the xaml
how do you name an xaml element \ {name="nme"} attribut
how do you caus a method to trigger when an xaml element is click on use the \ {click="methodname"} attribut
what method should alway be in a code class that back an xaml document a public constructor that call \ {this.initializecomponent()}
what doe \ {this.initializecomponent()} do in xaml back code associ the xaml-defin content with the instanc of the class
what the xaml applic definit the \ {app.xaml} and \ {app.xaml.cs} file that handl application-level task
what the entrypoint to an xaml applic by default, the constructor of the \ {app} class
what the default-gener \ {assemblyinfo.cs} file do for xaml app it contain a pile of in ation that can be manag elsewhere. don't use it.
what an extend splash screen in a win 8 app a splash screen that has more function (like a load bar) than the standard dumb imag
what global in app develop design your app so it adapt to differ market without ani need for manual custom
where is winrt support for global found in \ {windows.globalization}
what local in app develop the explicit iti of custom an app for a specif market
how do you support local in win 8 app identifi xaml element with \ {x:uid="uniqueid"} add a project folder with the relev languag code (ie \ {en-gb, en-ca}) add a resourc file to the folder, where you can defin custom \ {uniqueid.propertyname}, \ {value} pair for that languag code
how do you test the localiz of a win 8 app use the pseudo languag of vs multilingu app toolkit
what'r xliff file an xml standard for localiz data
what the c equival of declar a xaml object element instanti a class with it default constructor
what the c equival of defin an attribut in a xaml element set a properti of an object or hook up an event handler
what are the typic namespac in use in xaml no prefix: standard ui \ {x} prefix: xaml lang \ {local} prefix: a custom namespac ad with \ {using:} \ {d} prefix: design-tim in ation \ {mc} prefix: markup compat
what an xaml properti element one of the \ { classname.propertynam }, within a \ { classnam } element. it content correspond to assign to that properti
what'r type convert in wpf a properti with a type deriv from \ {typeconverter} and decor with \ {[typeconverter]} can be set up to automat convert certain other type when they'r given as valu to the properti in xaml
what'r markup extens in xaml attribut string of the \ {"\{markupextensionnam positionalparameterval, namedparameter=namedparameterval\}"} which allow you to instanti an instanc of the \ {markupextension} class which will pass out a valu via \ {markupextension.providevalue}
in xaml markup extensions, how do you refer to the current element \ {self}
in xaml, how do you escap a string which would otherwis be pars as a markup extens use an empti pair of cur braces: \ {\{\}\{...body...\}}
how do you add an item to a dictionari in xaml \ { valuetypenam x:key="key" val /valuetypenam }
what'r the xaml rule for process object element children if the type implement \ {ilist}, call \ {add(child)} if the type implement \ {idictionary}, call \ {add(key, child)} if the parent has a `content property, tri to set \ {parent.cont = child} if the child is plain text, a type convert exist to tran the child into the parent type, and the parent element has no properti set, feed the child into the convert and use the output as the parent object instance. rais an error.
how do you manual pars xaml use \ {windows.ui.xaml.markup} \ {xamlreader.load} method
how do you associ a xaml element with a code-behind file \ {x:class="namespace.classname"}
how do you refer a resourc file from xaml \ {"ms-appx:///path/to/file"}
how do you add resourc file to a window 8 app in vs add the file to the project with a build action of content
what are .xbf file in window 8 xaml binari files; the compiled, streamlin of xaml file
what are panel in xaml ani element that support the arrang of multipl children
what the root of xaml layout class inherit hierarchi \ {uielement}
how can you get a win8 app \ {frameworkelement} actual size the \ {actualheight} and \ {actualwidth} properti but these should onli be access from a \ {layoutupdated} event handler
what the onli public subclass of \ {uielement} in win 8 app \ {frameworkelement}
what the unit of measur for length properti in xaml logic pixels, which might be differ from physic pixel depend on size, resolut and dpi (window will scale the quantiti automatically)
what'r the possibl set for \ {horizonalalignment} in xaml left center right stretch
what content align in the xaml uif \ {horizontalcontentalignment} control how the content of an element are placed, as oppos to the element itself
how do you rotat an element in an xaml layout set the \ {rendertran } properti of the element to a \ {rotatetran } set the \ {rendertran origin} or \ {centerx, centery} properti
how can you procedur get a xaml element final posit and size after tran ation have been appli get the total tran with \ {element.tran tovisual(null)} then appli \ {tran .tran bounds(new rect(0, 0, element.actualwidth, element.actualheight))}
how can you construct `3d tran s in xaml use the \ {planeprojection} type
how do you get the current size of the window in the xaml uif use \ {window.current.bounds}, prefer from insid a \ {window.current.sizechanged} event handler
what a common mistak with \ {sizechanged} event handler in xaml uif forget to detach the handler when remov a page, which will stop it from be gc'd
what are the standard panel of the xaml uif canva stackpanel grid variablesizedwrapgrid
how do you use the \ {canvas} xaml uif panel set the \ {canvas.left} and \ {canvas.top} attribut in it child element
what are attach properti in xaml attribut of the \ {providertype.method} whose valu are pass to a static method of the specifi type
how do you control the z-index of children of an xaml uif element either alter the order they'r list in or set their \ {canvas.zindex} properti
what the use of xaml uif \ {canvas} panel allow precise, lightweight placement of elements.
what the use of xaml \ {stackpanel} element allow vertic or horizont `stack of it child element
naively, how do you use the xaml uif \ {grid} element use \ { grid.rowdefinit } contain \ { rowdefinition/ } to add row use \ { grid.columndefinit } contain \ { columndefinition/ } to add column use the \ {grid.row="3"} and \ {grid.column="4"} attach attribut to place element in the grid.
how do you get an xaml uif element to span \ {grid} row \ {grid.rowspan="2"}
what'r the size option for xaml uif \ {grid} row and column absolut autosizing: \ {auto} use as littl space as possibl proportional: \ {3*} will split the avail space between everi row use proport sizing, use the number to weight the split
what xaml uif \ {variablesizedwrapgrid} basic a \ {stackpanel} (not a \ {grid}!) which wrap element to the next row/column when the current row/column run out of space
what are the five strategi for deal with content overflow in xaml uif clip scroll scale wrap trim (clip with ellips for text)
in xaml uif, are overflow strategi appli befor or after tran s before. so an item will be clip befor it scaled.
what the easiest way to set up a scrollbar in xaml uif use a \ {scrollviewer} element and put your content insid it
what are snap point in xaml uif a featur of \ {scrollviewer} that allow you to `snap to content when touch-scrol end
what'r the easiest way to scale in xaml uif as a way of avoid overflow use the \ {viewbox} element to zoom to fit automat use the \ {scrollviewer} pinch-to-zoom function
what the symbol for bit and what the symbol for byte 4kb: 4 kilobit 4kb: 4 kilobyt
what a depend properti in xaml uif a way to (almost) arbitrari associ data with uif objects. call \ {elem.setvalue(typename.propname, val)} effect creat an attribut on \ {elem} call \ {typename.propname} with valu \ {val}
what the c equival of a xaml uif depend properti a custom public, static field with the suffix \ {property} whose type is \ {dependencyproperty} which is regist with \ {dependencyproperty.register}
what a properti wrapper in xaml uif instanc accessor for a static depend properti
whi should xaml uif properti wrapper not have ani logic becaus they'r onli there for conveni at compil time - the \ {getvalue}, \ {setvalue} method will be call direct at runtim
what are the three featur that xaml uif depend properti enabl notif when their valu chang properti inherit multipl valu provid
what'r the prioriti of the most common provid to xaml uif depend properti activ anim local valu (set with assignment) templat properti style setter properti inherit default valu (the initi valu the properti was regist with)
how do you clear the locally-set valu from a depend properti in xaml uif use \ {elem.clearvalue(elemtype.propname)}
how are xaml uif attach properti relat to depend properti attach properti are a of depend properti set \ {provider.attachedprop = val} on an element \ {elem} call \ {elem.setvalue(provider.attachedprop, val)}
how do you creat a new xaml uif attach properti use \ {dependencyproperty.registerattached}
what a rout event in xaml uif an event which when rais by an element, bubbl up the element tree to it root, get rais by each element on the way
what the visual tree in the win 8 uif the al tree describ the relationship between object and their children
whi shouldn't you manual alter the visual tree in the win 8 uif becaus a later restyl of control can alter the visual tree
how are depend properti relat to rout event in the win 8 uif they use the same basic structur as depend properties, but with \ {routedevent} field rather than \ {dependencyproperty} fields.
what the most import differ between rout event and depend properti in the win 8 uif you can't defin your own rout event
how do you attach a handler to a rout event in the win 8 uif in the xaml file: \ { elemnam eventname="handlername" } in the code-behind file: \ {void handlername(object sender, eventnamearg e)}
how do you halt bubbl in win 8 uif rout event set \ {eventargs.handl = true}
whi might event on some win 8 uif control not bubbl up the control might be design to swallow certain event (like \ {pointerreleased} on a \ {button})
what a common confus with rout event in the win 8 uif there are onli 23 actual rout events, but there are *many* more event that pass a \ {routedeventargs} (or derived) instanc to their handler. this was done for compat with silverlight. they don't actual bubbl up.
what a common mistak when handl rout event in the win 8 uif set \ {eventargs.handl = true} onli provid the *illusion* that they'v stop bubbling. ancestor can still receiv rout event if they go out of their way to.
at a high level, what are command in the win 8 uif they'r version of event intend for use with interface-independ action (like \ {open} or \ {refresh})
how is a command defin in the win 8 uif by implement \ {icommand}, which has method \ {execute} \ {canexecute}, which indic whether the command is enabl \ {canexecutechanged}, an event which is rais when \ {canexecute} chang
what the advantag of win 8 uif command over event mani uif control have a \ {command} properti that can be set to ani \ {icommand} and when the control is clicked, the command \ {execute} method is call and the control \ {isenabled} properti reflect the command \ {canexecute} properti
what'r the guidelin on deal with input in win 8 uif app use the built-in interact wherev possibl when you can't, code for touch becaus you'll get the mous amp; pen behaviour for free optim for specif input devic if it make sens
what are the three categori of touch event in the win 8 uif pointer (lowest level events) gestur (motion calcul from pointer events) manipul (prolong gestur which in a continu change)
how do you discov what pointer devic are avail in the win 8 uif use the \ {pointerdevice} class
what are the basic class associ with pointer input in the win 8 uif \ {pointer}, which describ a pointer (like a finger or a cursor) \ {pointerpoint}, which describ the state of a pointer \ {pointerpointproperties}, which give more detail in ation about the pointer state
with win 8 uif touch input, what are intermedi point collect of \ {pointerpoints} that might have accumul between success \ {pointermoved} event
what a common mistak when interpret \ {pointerpressed} event in the win 8 uif assum they'll alway be pair with a \ {pointerreleased} event
what it mean to captur a pointer in the win 8 uif \ {elem.capturepointer(ptr)} will caus \ {elem} to receiv all event from \ {ptr} irrespect of whether it lie in the element boundari
what doe it mean for a win 8 uif control to be hit-test it possibl for the control is receiv pointer event
how do you find out which element overlay a point in the win 8 uif use \ {visualtreehelper.findelementsinhostcoordinates}
what are the basic gestur in the win 8 uif tap press amp; hold doubl tap cross slide edg gestur
how do you manual interpret gestur event in win 8 uif creat a \ {gesturerecognizer} set \ {gr.gesturesettings} to the gestur you want regist handler for those gestur forward pointer event from the element to the gestur recogn
what a common mistak with the win 8 uif \ {gesturerecognizer} it onli intend for use with simpl shape or custom controls; built-in control like \ {button} can mess with it
what the simplest way of receiv gestur event in the win 8 uif regist with the \ {uielement} simpl gestur event
what are the common manipul in the win 8 uif swipe turn pinch/stretch
what the simplest way of receiv manipul event in the win 8 uif use the \ {manipulationxxx} event on \ {uielement} and set the \ {manipulationmode} properti
how do you set a flag enum in xaml use comma-separ values: \ {manipulationmode="rotate, scale"}
what a common problem with manipul in the win 8 uif they use the ui thread, so can be laggy. use the \ {scrollviewer} if possible, which use a second thread, and use the manipul event just to keep other compon in sync
what pointer devic are well-support by win 8 uif touch pen mous
what are the thread in a win8 app one ui thread for each window as mani background thread as you want
what special about ui thread in win 8 app they'r the onli one that can creat or call ui object they'r general not re-entrant: a call from a ui thread can make a call back to the ui thread without worri about deadlock they can't call other ui thread
what are agil object in win8 app winrt object which can be creat and use by ani thread, as oppos to just ui thread
what an asta thread in win 8 app app single-thread apart thread, anoth name for a ui thread
how are winrt async oper repres as \ {iasyncoperation} and \ {iasyncaction} which can be freeli convert to and from .net \ {task}
how do you use async/await in c mark a method \ {async} with a return type of \ {task}, \ {task int }, or (if necessari \ {void} \ {await} some \ {task} insid the method.
how can you use the c \ {async} control flow for api that don't return a \ {task} or \ {iasyncxxx} wrap the api call use a \ {taskcompletionsource}, which has a \ {task} properti you can return
when an event is rais on a xaml object, which thread is it run on the thread which creat the object
how can you call back to a win 8 ui thread from a worker thread use the \ {window} \ {window.dispatcher.runasync} method to run a deleg on the ui thread
what a view in winrt terminolog a window along with it ui thread
how do you creat a new window in the win 8 uif call \ {coreapplicationview.createnewview()} wait for \ {application.onwindowcreated()} to be called, which will happen on the new ui thread initi the new window on the new ui thread use the \ {applicationviewswitcher} on the old ui thread to show the new window best done use an async wrapper method
what the root element of a win8 uif window a \ {frame}
how do \ {page} relat to \ {frame} in win 8 uif app all \ {page} are contain in a \ {frame} the \ {frame} enabl page-to-pag navig
how do you navig between page in a win 8 uif app call the frame \ {navigate} method with the type of page you want to move to, and possibl a config object too the page will be instanti and navig to, call \ {onnavigatedto} with an event arg paramet that contain the config object that was pass
how doe a win 8 uif \ {frame} manag it page as a backward stack and a forward stack, allow you to move forward and backward between page by default though, they are *not* kept alive!
how do you make a win8 app \ {frame} keep a page aliv while it in e set the page \ {navigationcacheproperty}
how do you maintain navig histori as part of session state in a win 8 app use the \ {navigationhelper} class
how do you chang the app theme in win 8 app chang the \ {application.requestedtheme} properti
what are content control in the win 8 uif control which can onli contain a singl item. most buttons.
what the differ between a click and a tap in the win8 uif ani ui element can rais a tap in respons to a pointer event a button rais a click in respons to a pointer event or some keyboard event
how do you autom control in a win8 uif app use the \ {automation.peers} namespac
what a flyout in the win8 uif a dialog box with a light dismiss behaviour: click outsid it dismiss it
what the usual sourc of icon for win 8 app the \ {symbolicon} class, which repres glyph from the sego ui font
how do you creat a win 8 uif icon from arbitrari text use the \ {fonticon} class.
what an \ {appbarbutton} in the win 8 uif a circular button with an icon and a label.
what a \ {hyperlinkbutton} in the win8 uif a text button that can be use to launch a uri
what the win8 uif \ {repeatbutton} a button which will rais \ {click} event as long as it press
what the win8 uif \ {appbartogglebutton} a e button in a circl with an icon and a label.
what'r the guidelin for win 8 app bottom bar they should have context-specif command they should prefer to be close to the edg
what'r the design guidelin for win 8 app top bar navig is the usual use, but it can be anyth general taller than the bottom bar
how do you implement app bar in the win 8 uif \ {appbar} for the top bar \ {commandbar} for the bottom bar
what are item control in the win8 uif control which contain a collect of item
what a selector in the win8 uif a subtyp of \ {itemcontrol} which allow the select of object
what happen when you add an arbitrari object to a win8 uif item control content it wrapped. in the case of a \ {listbox}, it place in a \ {listboxitem} element
what ui virtual in the win8 uif when deal with a larg number of object, item control onli wrap object while they'r on screen when an object pass off screen, it contain is recycl for use by anoth item
what a combo box in the win8 uif an item control which implement a drop-down menu.
what a list box in the win8 uif an item control which implement a scroll multi-select list
what a list view in the win8 uif an item control which is basic a prettier \ {listbox} with drag-and-drop support
what a grid view in the win8 uif a two-dimension \ {listview} with wrap
what a flip view in the win8 uif an item control which allow the user to flip through one item at a time
what a semant zoom in the win8 uif a kind of zoom that switch between higher and lower level view of some content
what a menu flyout in the win8 uif a more restrict dropdown menu than \ {combobox} meant for menus
what are the control for ing chunk of text in the win8 uif \ {textblock} \ {richtextblock}
what a rich text in the win8 uif a text which support in-text ui element overflow to separ element
what a \ {textbox} in the win8 uif a control that allow for text entri
what an input scope in the win8 uif it a properti of a \ {textbox} that allow you to alter whether the softwar keyboard ed should be the one for urls, for emails, for numbers, etc
what a \ {richeditbox} in the win8 uif a thin wrapper over the rich text at api of winrt which allow d text input
how do you construct absolut uri in win 8 app \ {new uri(this.baseuri, "relative/uri")}
what the of win 8 uri becaus the of the uri is \ {scheme://domainname/path} but omit \ {domainname} will impli the current app full name
what a nine-grid in the win 8 uif a featur of the \ {image} type that allow you to dynam modifi an imag for use as a border
what the easiest way to construct an imag pixel-by-pixel in the win 8 uif use a \ {writablebitmap} as an \ {image} \ {source} use {writablebitmap.pixelbuffer.asstream()} to push in byte
what the problem with frequent call to window api in the win 8 uif winrt is unmanaged, and managed-to-unmanag call are much more expens than managed-to-manag call
how do you deal with automat scale of imag in win 8 app provid three images: \ {imagename.scale-100.jpg} \ {imagename.scale-140.jpg} \ {imagename.scale-180.jpg} and in the app, just load \ {imagename.jpg} can also put them in three appropri name folder
what are automat resourc qualifi in the win8 uif special filenam modifi (like \ {scale-140}) which the app will consum automat
how can you get or modifi the raw in ation in an imag from the win8 uif use the \ {bitmapdecoder} class
what the best way to per standard tran ation on imag data from the win 8 uif use the \ {bitmaptran } class
which is general prefer of \ {datetime} and \ {datetimeoffset} in .net \ {datetimeoffset}
what'r media extens in the win 8 uif anoth name for media foundat compon
what the simplest way of play video or audio in the win 8 uif \ {mediaelement} possibl with \ {aretransportcontrolsenabled} enabl
how do you add or get the marker associ with a video from the win8 uif \ {mediaelement.markers}
what the advanc way to video in the window 8 uif use the \ {mediaplayer} class of the player framework
what'r the two way to deal with custom media stream in the win 8 uif make a custom media extens use the \ {mediastreamsource} class
what are the usual way of captur video in win 8 app \ {cameracaptureui} for a self-contain solut \ {captureelement} for a lower-level solut
what are the three main class use for vector graphic in the win8 uif \ {shape}, a 2d draw \ {geometry}, an abstract represent of a draw \ {brush}, which repres a way of fill an element
how can you amelori per anc issu with complex vector graphic in the win 8 uif use \ {uielement} \ {cachemode} to enabl cach composition, which caus updat to vector graphic to onli redraw the `dirti part
what gpu overdraw when the same pixel is drawn more than onc for the same scene
what are s in the win8 uif they'r to xaml as css is to html
how do you creat a in the win8 uif in the resourc properti of a control, creat an \ { style x:key="stylename" targettype="targetname" } element in the element, creat \ { setter property="propertyname" value="val"/ } elements.
how do you use a in the win8 uif from within subtre of the elemen the was defin on: \ { targetnam style="\{staticresourc stylename\}" }
how do you implement inherit in s of the win8 uif set the \ {basedon} attribut of a element
what an implicit in the win8 uif a without a \ {x:key} or \ {x:name} attribut the then appli to all element of it target type
how do win8 uif s account for theme there a \ {themeresource} properti that can store multipl \ {resourcedictionary}
how doe the \ {staticresource} markup extens locat s in the win8 uif it walk up the tree until it find a resourc with a match name
what a control templat in the win8 uif a way to defin a custom tree of element for ani control
how do you defin a templat in the win8 uif popul a \ { controltempl } element with the visual tree you want and use the markup extens \ {\{templatebind dependencypropertyname\}} to get the valu of depend properti from the target control
what a \ {contentpresenter} in the win8 uif a lighter-weight \ {contentcontrol} meant for use with templat
what are visual state in the win8 uif a type \ {visualstate} which repres the state of a control, allow templat to take them into account
what are state group in the win8 uif \ {visualstate} are group into \ {visualstategroup}s, and the control is alway in one state from each group
how do templat make use of visual state anywher insid a template, defin a \ { visualstatemanager.visualstategroup } properti element and within that, defin \ { visualstategroup x:name="groupname" } element and within that, defin \ { visualst x:name="statename" } element
what the \ {binding} markup extens in the win8 uif a way to declar tie the valu of two properti togeth
how do you write a simpl ing in the win8 uif on the sourc element, add a \ {name="sourcename"} attribut on the targ element, add a \ {targetprop="\{bind elementname=sourcename, path=sourceprop\}"} attribut
how can you add default valu to a ing in the win8 uif add a \ {fallbackvalue} properti to the \ {binding} extens
how can you data to a properti which cannot be set to null in the win8 uif use the \ {targetnullvalue} attribut of the \ {binding} extens
when ing data in the win8 uif, how can you select the sourc element relat to the target one use the \ {relativesource=\{relativesourc relation\}} attribut of \ {binding}
what the restrict on what can be bound in the win8 uif the target properti must be a depend property.
how can you data in both direct simultan in the win8 uif set the \ {bindingmode} of the \ {binding} to \ {twoway}
when ing data from a text box in the win8 uif, how can you stop it from updat until type complet set \ {updatesourcetrigger=explicit} on the \ {binding} then explicit call \ {updatesource} when the \ {textbox} lose focus
what a data context in the win8 uif a way to implement an implicit data sourc for ing within a tree
how do you implement a data context in the win8 uif set the \ {datacontext} of an element to the desir sourc object ani ing in the tree without sourc object specifi will search up the tree until they find a non-nul \ {datacontext}
what are common shorthand for \ {binding} that use \ {datacontext} in the win8 uif the first posit paramet is alway the sourc properti name, ie \ {\{bind length\}} will to the \ {length} properti of the \ {datacontext} if no sourc properti is named, then the properti is assum to be the whole object, ie \ {\{binding\}} s to the whole \ {datacontext} object
how do you data to an item control in the win8 uif set the \ {itemssource} properti to a \ {binding}
when ing data to a collect in the win8 uif how can you ensur the collect view stay updat make the collect implement \ {inotifycollectionchanged}, usual via \ {observablecollection}
what the use of a \ {datatemplate} in the win8 uif a way to tran data into a visual represent
how do you implement a data templat in the win8 uif in a control \ {xxxtemplate} property, declar a \ { datatempl } element, contain ani old tree of control and in it make ani ing need to a data context that'll be provid by the host element
how can you chang templat at runtim in the win8 uif use a \ {datatemplateselector}, if the parent control allow them
what a valu convert in the win8 uif a way to tran a data sourc to the target type when use ing
how do you implement valu convert in the win8 uif implement \ {ivalueconverter} and it \ {convert, convertback} method add an instanc of the convert to the element \ {resources} with \ { local:converternam x:key="convertername" } refer it in the \ {binding} with \ {converter=\{staticresourc convertername\}}
what the easiest way to a group view of a collect in the win8 uif make the \ {datacontext} a \ {collectionviewsource} pass it a group collect give the item control a \ {groupstyle}
what the best way to manag select of item in a data-bound collect in the win8 uif use a \ {collectionviewsource} and it \ {currentitem} properti
what are two win8 uif featur that can help with ing larg collect scroll placehold increment render
what'r major and minor type in media foundat major type are generic type of data - video, audio, caption minor type are at - usual identifi a codec
what are the three categori of media foundat compon sourc tran s sink
what a media sampl in microsoft media foundat a packet of data which is pass through the pipelin
what'r data buffer in microsoft media foundat the part of a media sampl that hold frame in ation (or similar) are fetch from the under mf system, which recycl them for per anc reason
what a topolog builder in microsoft media foundat a compon that take a partial topolog contain onli sourc and sink and which chain tran s it find in the registri until some combin fit
what a media session in wmf it hold all the mf components, and pull sampl from the sourc through the mfts and send them to the sink
what a hierarch bay model a model where the paramet are distribut by some hyperparameters, and mayb the hyperparamet are distribut accord to some hyper-hyperparameters, etc
what a multi-level model anoth name for a hierarch bay model
how empir bay usual use to get a point estim of some hyperparamet by appli a uni prior to them and take the maximum posterior estim (equiv. mle)
what `the evid procedur anoth name for empir bay
whi is empir bay usual safe to appli to hyperparamet becaus the hyperparamet typic have a much lower dimension than the parameters, so are less suscept to overfit
what a varianc stabil tran in ml if $var[x] = \sigma^2(\mu)$ and $y = f(x)$ then $f$ is a varianc stabil tran iff $var[y] f^\prime(\mu)^2 \sigma^2(\mu)$ is independ of $\mu$
how is the definit of a varianc stabil tran deriv use a taylor seri expans of the tran ed variabl
how are statist decis problem usual aliz as a game against nature: natur has a state $y \mathcal{y}$ and generat an observ from it $x \mathcal{x}$ base on $x$ we pick an action $a \mathcal{a}$ and incur a loss $l(y, a)$ due to it the goal is to devis an optim decis polici $\delta : \mathcal{x} \rightarrow \mathcal{a}$ that'll minim the expect loss
what anoth name for the maximum expect util principl ration behaviour
what the bay decis rule in bayesian decis theory, the goal is to make the decis that'll minim the posterior expect loss
given a 0-1 loss function in bayesian decis theory, which estim should be use the map estim
what a 0-1 loss function a loss function which is 0 when the action chosen correspond to the under state and 1 otherwis
given a $l_2$ loss function in bayesian decis theory, which estim will minim the loss the posterior mean
what the $l_2$ loss function $l(y, a) = (y - a)^2$
what the minimum mse estim in bayesian decis theori the posterior mean of the distribut
given a $l_1$ loss function in bayesian decis theory, what the optim estim the posterior median
what the $l_1$ loss function in bayesian decis theori $l(y, a) = y - a $
what the roc in roc curv stand for receiv oper characterist
what the definit of a roc curv a plot of true posit rate (y axis) vs fals posit rate (x axis) of a famili of model
what auc stand for in machin learn area under the roc curv
what eer stand for in machin learn equal error rate
what the definit of the equal error rate in machin learn given a one-paramet famili of models, it the rate at which the true posit rate equal the fals posit rate
what the cross over rate anoth name for the equal error rate
what recal anoth name for in machin learn the sensit
what a common altern to a roc curv when deal with veri rare event a precision-recal curv
what the problem with roc curv when deal with veri rare event the larg number of negat mean the curv will be compress over to the extrem left
what the summari statist precis anoth name for in machin learn posit predict valu
what doe the statist `averag precis at $k$ mean it the expect precis of the first $k$ element to be recal
what the f1 score in machin learn $f_1 = \frac{2}{1/p + 1/r}$ where $p$ is the precis of a model and $r$ is it recal
how is the $f_1$ score general to multi-class model macro-averag $f_1$ micro-averag $f_1$
what the macro-averag $f_1$ score $\frac{1}{c}\sum f_1(c)$
what the micro-averag $f_1$ score the $f_1$ score calcul by pool all class count into a singl conting tabl equivalently, the score calcul by class-weight each class $f_1$ score
what the differ between the macro and micro $f_1$ score macro assign equal weight to each class micro weight class by their size
what the goal of invers reinforc learn to infer a util function from some set of behaviour
what the multi-arm bandit problem given a bank of one-arm bandit with unknown payoff functions, which arm should you pull and in one order
what'r gitten indic an approach to optim the exploration-exploit tradeoff implicit in the multi-arm bandit problem
what the contextu bandit problem it like the multi-arm bandit problem, but each bandit - and the player - has a featur vector, give you some knowledg about them
what the thompson sampl approach to multi-arm bandit model at each step, pick the action with a probabl equal to the probabl with which that action is the optim action
what the upper confid bound approach to the multi-arm bandit problem you should either pick action that are believ to be optim benefici or action for which there current a larg uncertainti in the outcom
what the sampl distribut of an estim the distribut of an estim $\hat \theta = \delta(\mathcal{d})$ over mani sampl dataset $\mathcal{d}$ from the same model
what a parametr bootstrap the unknown distribut paramet are estim with $\hat \theta(\mathcal{d})$ and a sampl distribut is generat use these paramet
what a non-parametr bootstrap sampl with replac from the dataset $\mathcal{d}$ to generat the sampl distribut
intuitively, how doe the fisher in ation matrix carri in ation about the varianc of a distribut if a distribut peak is `sharp (so it second deriv is larg there), the varianc must be quit low
under common regular conditions, how is the mle of a paramet $\theta$ distribut as the number of sampl approach infin $\hat \theta \rightarrow \mathcal{n}(\theta^*, \mathbf{i}_n(\theta^*)^{-1})$ where $\theta^*$ is the true valu of the paramet and $\mathbf{i}_n$ is the fisher in ation matrix
what'r the standard error of a sampl distribut on the vector-valu paramet $\theta$ $\hat{se}_k = \frac{1}{\sqrt{\mathbf{i}_n(\hat \theta)_{kk}}}$ where $\mathbf{i}_n$ is the fisher in ation matrix and $\hat \theta$ is the mle
what the risk in frequentist decis theori $r(\theta^*, \delta) = \mbb{e}_{\tilde{d} \theta^*} [ l(\theta^*, \delta(\tilde{d})) ]$ where $\tild d$ is natur distribut and $\delta$ is the decis procedur
what the definit of the posterior expect loss in bayesian decis theori $\rho(\delta d) = e_{\theta d} [ l(\theta, \delta(x)) ]$ where $\delta$ is the decis procedur and $\theta$ is natur state
what the differ between bayesian posterior expect loss and frequentist risk pel averag over the unknown paramet $\theta$ and condit on the data $d$ risk averag over the data distribut $\tild d$ and condit on unknown paramet $\theta^*$
what the bay risk in frequentist statist $r_b(\delta) = \mbb{e}_{\theta^*} [r(\theta^* \delta)]$ where $p(\theta^*)$ is a prior on $\theta^*$
what'r two other name for the bay risk in frequentist statist integr risk preposterior risk
what a bay estim in frequentist statist a decis rule $\delta_b$ that minim the bay risk $r_b(\delta)$
how are bay estim connect to the posterior expect loss if you minim the (bayesian) posterior expect loss for each observ $x$, you get a (frequentist) bay estim
what anoth name for a bay estim a bay decis rule
what the most common altern to bay risk minimax risk: pick the decis rule that minim the maximum risk you'r expos to over all possibl set of natur paramet
what the problem with use the minimax risk in frequentist statist it over pessimist
what doe it mean for a frequentist estim to be admiss there is no other estim which lead to strict lower risk over all set of natur paramet
what it mean for an estim to be consist in frequentist statist if $\hat \theta(\mathcal{d}) \rightarrow \theta^*$ as the size of $\mathcal{d}$ goe to infin
what the definit of the bias of an estim in frequentist statist $\text{bias}(\hat \theta) = e_{p(\mathcal{d} \theta^*)}[\hat \theta (\mathcal{d}) - \theta^*]$
what the cramer-rao inequ if an unbias estim $\hat \theta$ is appli to a size-$n$ sampl from a distribut with paramet $\theta^*$, then $\text{prec}(\hat \theta) n i(\theta^*)$ where $i$ is the fisher in ation matrix (under certain regular conditions)
what the bias-vari tradeoff in frequentist statist $\text{mean squar error} = \text{variance} + \text{bias}^2$ so if you want to minim the mse, sometim it pay to use a bias estim
how is the risk of an estim usual assess in frequentist statist use a valid set
what regular risk minim in frequentist statist when you add a complexity-penalti term to the risk function to discourag overfit
what the one standard error rule in frequentist statist given a set of model pick the simplest model whose risk is at most one standard error abov the risk of the best model
intuitively, what the use of the vc dimens in statist it a measur of the size of a real-valu paramet space
give a bound on the error in estim the risk of a hypothesi pick from a finit hypothesi space. $p\left(\max_{h \mathcal{h}} r_{emp}(\mathcal{d}, h) - r(p^*, h) \epsilon \right) 2 \frac{ \mathcal{h} }{e^{2n\epsilon^2}}$ where $n$ is the size of the dataset $\mathcal{d}$ and $p^*$ is the true distribut
what the empir risk in frequentist statist it the expect of the loss function when taken over the observ data (the empir distribution)
what a surrog loss function some common loss function (like 0-1) can be hard to optim so they'r often substitut for friendlier functions, often a convex upper bound on the unfriend function
what the stop rule problem with p-valu their calcul can differ on when the experi was chosen to termin (eg after $n$ trial or after $n$ success trials) even if the observ data is the same
what the likelihood principl in statist infer should be base on the likelihood of the observ data, not on hypothet futur data that hasn't yet been observ
what order do pixel general appear in bitmap file blue, green, then red
what edg trace in comput vision collect the pixel belong to an edg into a list
what are the three main categori of edg detect algorithm deriv oper template-match (sobel, kirsch) edg model similar
what the differenti oper approach to edg detect calcul the central differ across each pixel, $\delta_2(x, y)$ threshold $ \delta_2(x, y) $
what local edg coher in comput vision a measur of how well a predict edg pixel is continued, calcul from the chang in predict edg direct between neighbour pixel minus a penalti for edg of greater than unit width
how doe sobel edg detect work two $3 3$ matrici are convolv with the imag the matrici approxim a kind of deriv in the $x$ and $y$ direct respect their sum-squar is then threshold
how doe kirsch edg detect work eight $3 3$ matrici are convolv with the imag each repres an edg orient and the maximum of the eight s is taken for each pixel
at a high level, how doe the marr-hildreth edg detector work convolv the imag with a log oper find the zero-cross
what the log oper in comput vision laplacian of the gaussian oper common in edg detect
how doe marr-hildreth detect edg at differ scale log with differ varianc are used, and the union of their output is taken
what'r the main stage of the canni edg detect algorithm a filter emul the first deriv of the gaussian is appli then pixel which aren't local maxima are remov then a hysteresi step is appli
how doe the canni algorithm suppress non-loc maxima use the valu of the surfac at the two point where the gradient vector at the pixel intersect the boundari of the pixel the valu of the surfac at a point is calcul use linear interpol between the nearest two neighbour
what hysteresi in edg detect an upper and lower threshold are defin and ani pixel which is either abov the upper threshold or abov the lower threshold \emph{and} connect to a pixel abov the threshold are retain
what the definit of the filter use in the shen-castan edg detector the infinit symmetr exponenti filter $a\cdot e^{-p( x + y )}$ with paramet $a, p$
how is the isef usual realis in code as a recurs filter, calcul in each direct independ
what'r the stage in the shen-castan algorithm appli the isef to an imag $i$ to get a filter imag $s$ calcul $b = s-i$, the band-limit laplacian set all posit pixel to 1 and all negat to -1 to get the binari laplacian imag clean the bli up with fals zero-cross suppression, adapt gradient and hysteresi method
what the adapt gradient method in comput vision if a pixel has been detect as an edg then in a small window around that pixel in the origin image, the direct it indic should have two differ gray level on either side, divid by the vector of the edg
what fals zero cross suppress in comput vision a fals zero cross is one where either the laplacian chang from posit to negative, but the gradient is negat the laplacian chang from negat to positive, but the gradient is posit these case are consid spurious in edg detect
what the bli in comput vision the binari laplacian imag which is the laplacian with all posit pixel map to 1 and all negat pixel map to 0
what isef stand for in comput vision infinit symmetr exponenti filter
what are three common color tran s to use when carri out edg detect averag all three channel $t_1 = \frac{r}{r+g+b}$ or $t_2 = \frac{b}{r+g+b}$ hue of hsv
what a good tool for debug depend properti in wpf \ {dependencypropertyhelper.getvaluesource}
what'r the five step in calcul the valu of a depend properti in wpf determin the base valu if an express has been given, evalu it appli anim coerc (appli a tran if one been provided) valid (appli a valid if one been provided)
how can you chang the valu of a depend properti without chang the valu of it sourc in wpf \ {dependencyobject.setcurrentvalue}
what an attach properti in wpf a depend properti that can be attach to arbitrari object (like \ { stackpanel textelement.fontsize=30 })
how do you programmat set the valu of depend properti in wpf \ {element.setvalue(propertyname, value)}
what the easiest way to attach custom data to a wpf element by set it \ {tag} depend properti
what a panel in wpf an element that support the arrang of multipl children
what the restrict on use \ {actualheight} and \ {actualwidth} in wpf the layout process is asynchronous, so the should onli be readi from a handler for the \ {uielement.layoutupdate} event
what are the two categori of 2d tran s in wpf \ {layouttran }s, which are appli befor the layout process \ {rendertran }s, which are appli after the layout process and immedi befor the element is render
what are two common mistak with tran s in wpf they can onli be appli to nativ wpf content tran s don't affect the \ {actualheight}/\ {actualwidth} of an object ever
what are virtual panel in wpf panel which automat discard offscreen element to optim per anc
what a use tool for debug \ {grid} panel in wpf set \ {showgridlines} to true.
how do you give a \ {grid} cell a background in wpf add a \ {rectangle} with the appropri \ {fill}. it'll automat stretch to fill the cell, and the content can sit on top of it
what star size in wpf set a \ {grid} \ {rowdefinition} width to \ {2*} or similar will assign it a width as a weight fraction of the panel width
what are rout event in wpf event that'll move through the visual tree, trigger ani handler instal on the element they pass through
what are the rout strategi avail for wpf rout event bubbl (move up tree) tunnel (move from root down tree to source) direct (on rais on sourc element)
what doe a \ {preview} prefix on a wpf event mean it a tunnel event that preced a bubbl event (or vice versa)
what the use of \ {previewxxx} event in wpf by handl a \ {previewxxx} event on it way down the tree, an element can prevent the \ {xxx} event from be rais and bubbl up
what'r attach event in wpf element can regist handler for event they don't defin themselv like \ { window listbox.selectionchanged="handler" }
what a \ {weakeventmanager} in wpf it allow you to attach weak event handler to rout event which mean that an object rais the event doesn't hold a strong refer to it weak listen
what doe it mean for a wpf element to `captur an input devic it'll receiv all event from that input devic regardless of where it is on the screen
what are command in wpf version of event intend for thing divorc of specif io like cut/copy/past
what an \ {inputbinding} in wpf a way to associ sever differ input combin with the same command
what are control in wpf standard but flexibl ui compon ex: drop down menus, tabs, etc
what a common mistak with text render in wpf not set the \ {textoptions.textformattingmode} and \ {textoptions.textrenderingmode} properti to optim valu for the size of the text
what'r \ {flowdocument} in wpf control intend for larg chunk of text which requir complex at
what are the two main way to includ binari resourc in wpf applic with the build action \ {resource}, which emb it in the assembl with the build action \ {content}, which leav it loos but record the relat locat of the file in the assembl
how can you refer compile-tim wpf resourc in xaml \ { imag source="foldername/filename.gif" } onli work for embedded/regist resources!
what the `site of origin in wpf some sort of locat against which xaml uri of the \ {pack://siteoforigin:,,,,/filename.jpg} are resolv
how do you access compile-tim resourc from c in wpf embed resources: \ {new uri("pack://application:,,,/resourcename.jpg")} loos resources: \ {new uri("pack://siteoforigin:,,,/resourcename.jpg")}
how do you defin a logic resourc in xaml in wpf under \ { ancestorname.resourc } creat an element which has a \ {x:key="keyname"} attribut
how do you refer a logic resourc from xaml in wpf \ {attributename="\{staticresourc keyname\}"}, which mean the resourc will be appli onc \ {attributename="\{dynamicresourc keyname\}"}, which mean the resourc will be reappli everi time it chang
how can you share logic resourc dictionari across file in xaml put the resourc directori in it own file se the \ {resourcedictionary.mergeddictionaries} element to includ it
how can you caus a wpf logic resourc to be copi to each locat it referenc from instead of share \ {x:shared=false}
what the main restrict on static logic resourc in xaml they have to be declar befor they can be referenc so to refer the resourc from the same element, use property-el syntax
what properti element syntax in xaml rather than set an attribut within a xaml element, you can set it as the content of a subel with the attribut name ie \ { window.background valu /window.background } vs \ { window background=valu }
what the main restrict on dynam logic resourc in xaml onli depend properti can be dynam bound
how can you static refer a wpf logic resourc from c call \ {elementname.findresource("resourcename")}
how can you add a dynam refer to a logic resourc from c in wpf \ {elemname.setresourcereference(classname.propname,"resourcename")}
how should you refer system set from a wpf applic use dynam refer to the logic resourc (\ {xxxkey}) in \ {systemparameters}, \ {systemcolors}, \ {systemfonts} becaus they might chang while your app is run
how do you creat a wpf data ing from procedur code creat a \ {binding} object set \ { ing.source} and \ { ing.path} attach it to a target properti with \ {elemname.setbinding(classname.propname, ing)}
how can you data to object that don't have a \ {setbinding} method in wpf \ {bindingoperations.setbinding(targetobj, propname, ing)}
how do you data from a name xaml object in wpf \ {attributename="\{bind elementname=name, path=propname.otherpropname\}}
how do you data from a relat defin sourc in xaml \ {attrname="\{bind relativesource=\{relativesourc findancestor, ancestortype=\{x:typ desiredtype\}\}"} it pretti power
how do you data from arbitrari .net object in xaml add the object to some ancestor element resourc dictionari then \ {attrname="\{bind source=\{staticresourc resourcename\}, path=pathname\}"}
when ing data from a plain .net object, how can you get the target to updat when the sourc doe implement \ {inotifypropertychanged} on the sourc object (or inherit from one of the \ {observable} class that doe it for you)
what a data context in wpf by set the \ {datacontext} properti of an element, all ing without sourc in the element subtre will automat use the nearest context as a sourc
what doe it mean when a data ing in wpf doesn't have a \ {path} attribut the ing is to the entir sourc object
how do you alter the at of a data bound string in xaml \ {\{bind stringformat=\{\}\{0\} thing(s), source=sourcename, path=pathname\}} where ani old at string can be use
what a data templat in wpf a chunk of ui which is use to render an arbitrari .net object
how do you defin a data templat in xaml creat a \ { datatempl } element and in it use implicit ing \ {\{bind path=pathname\}} which will be provid a sourc object via data context when the templat is use
what are templat selector in wpf element with \ {xxxtemplate} properti also have \ {xxxtemplateselector} properti which all you to insert custom code that'll select the templat to be use
what a valu convert in wpf an object implement \ {ivalueconverter} that can be plug into data ing via the \ {converter} paramet in order to adapt a sourc type to a target type
what a collect view in wpf when you against an \ {ienumerable} in wpf, a \ {collectionview} object is automat insert that support thing like sorting, select and group
how can you data from the current select item in wpf a forward slash in the \ {path} attribut of a ing correspond to the current select item
how can you synchron the select item across sever represent of the same collect in wpf set \ {issynchronizedwithcurrentitem} to true
what'r the two data provid in wpf \ {xmldataprovider}, which allow ing against the content of chunk of xml \ {objectdataprovider}, which allow more complex oper on .net object like parameter instanti and ing from method
what the best way to do input valid in wpf set the ing \ {validationrule} paramet to an \ {validationrule}-deriv object
how do you make updat to a ui field chang the ing sourc in wpf set the \ {bindingmode} of the ing to \ {twoway} or \ {onewaytosource}
how can you modifi when a two-way ing target will modifi it sourc in wpf set the \ {updatesourcetrigger} paramet on the ing
what are some advanc multi- ing featur in wpf \ {compositecollection}, which aggreg multipl collect \ {multibinding}, which aggreg the s of multipl ing \ {prioritybinding}, which process multipl ing in order and updat it valu as they complet
how can you creat an asynchron data ing in wpf set the ing \ {isasync} attribut to true (gener though this should be avoid in favour of asynchron background code)
what is the product rule of probabl for event a and b p(ab) = p(a b)p(b) = p(b a)p(a) where p(ab) mean the probabl of the intersect of a and b
what is the general sum rule of probabl for event a and b p(a+b) = p(a) + p(b) - p(ab) where p(ab) mean both a and b simpl sum rule, when a and b do not overlap jaynes; deriv from the product rule and basic logic appli repeat
what is the principl of indiffer in probabl how doe jayn deriv it the probabl of n equival possibl is each 1/n; if a reason system led to ani other than this, then a mere permut of label could chang their probabl jayn ptlos, uniqu follow keyn 1921
accd to jaynes, what must be true of the function for a probabl under standard notat it must be a monoton increas function btwn 0 and 1 jayn
accd to jaynes, whi is probabl subject and whi is it object it is subject becaus it must begin from a particular state of knowledge, while it is object becaus onc that state of knowledg has been established, the rule of probabl will lead to the same mathemat estim jayn
accd to jaynes, is random a properti of the world if so, whi if not, what is it no; n/a; the belief that random is a real properti exist in natur is a of the mind project fallaci which says, in effect, "i don't know the detail caus -- therefor -- natur doe not know them" jayn quot p 74 ptlos; eg "random in shake a set of ball in an urn doe not affect natur work in ani way, it onli ensur that no human is abl to exert ani willful inlfuenc on the "
accd to jaynes, what is a sampl distribut a system for reason from some specifi hypothesi to potenti observ data jayn p 84
how doe jayn describ the logic relationship between prior and posterior probabl "one man prior probabl is anoth man posterior probability" classic quot jayn p 89; "there is realli onli one kind of probability; our differ name for them refer onli to a particular way of organ a calculation"
what doe jayn think about describ the probabl distribut of some paramet whi it is misleading; that verbiag impli that the paramet itself is "distributed" in some way, wherea it is the probabl for the possibl valu of that paramet that is distribut over a rang of valu type of mind project fallacy; jayn p 108
what is varianc a properti of with respect to some paramet that is measur onc in an experi varianc is a properti of the probabil distribut function for that measur , not of the measur itself, which will onli take one valu upon measur jayn p 113; vs varianc which can also be defin for a frequenc distribut of sampl measurements, but onli if it is measur more than onc (actual = 3 times, iiuc)
what are the most common method of go from the posterior distribut function to estim that jayn discuss (3) 1) mean of the posterior pdf, via minim the mean squar error of the estim (gauss, and the lowest mse possibl is the variance) 2) median of the posterior pdf, via minim the sum of the error (laplace) 3) mode of the posterior pdf (if given a constant prior, this is equival to the "maximum likelihood estimate" as name by fischer, even though gauss and laplac use this too and just call it the "most probabl value") jayn p 173; each of these had benefit in various situations, depend on which type of error are relat wors in the scenario you are work under; eg, mse depend on the scale of the parameters, and it weight outlier as more import
what is the relat import of remov bias and minim varianc of an estim whi they are equal important; both can contribut to the error of the estim over the sampl distribut jayn ptlos 511 - 514; "whi do orthodoxian put such exagger emphasi on bias we suspect that the main reason is simpli that they are caught in a psychosemant trap of their own making. when we call the quantiti ([b] - a) the "bias", that make it sound like someth aw represensible, which we must get rid of at all costs. if it had been call instead the "compon of error orthogon to the variance", as suggest by the pythagorean of (17.2), it would have been clear to all that these two contribut to the error are on an equal footing; it is folli to decreas one at the expens of increas the other. this is just the price one pay for choos a technic terminolog that carri an emot load, impli valu judgments; orthodoxi fall constant into this tactic error."
what a panchromat imag in imag process one where the bright at a point $x, y$ is given by a singl number $f(x, y)$
what a pel in imag process a pictur element anoth name for a pixel
what the checkerboard effect in imag process the effect you get from reduc the resolut of an imag
what a point spread function in imag process a descript of how an oper tran s it subject, characteris by it effect on a point sourc
what fals contour in imag process the effect you get from reduc the number of grey level in an imag
when can you get away with reduc the number of grey level in an imag when it veri detail so doesn't have ani shallow gradient
what'r simpl suffici condit for the wavelet tran admiss condit to be satisfi the wavelet is square-integr the wavelet has zero mean
what the intuit interpret of a squar integr function it a function with finit energi
whi are function of finit power of interest in wavelet theori becaus they effect have finit energi when a window of finit durat is appli
what doe $x[n]$ repres in wavelet theori when $x$ is a continu function the function $x(t)$ convert to a discret time seri by sampl everi $\delta t$
what $l_2(\mathbb{z})$ in wavelet theori the space of discret time signal of finit energy, $\sum x[n] ^2 fty$
what a banach space a complet norm vector space
what a complet metric space a space such that everi cauchi sequenc in the space converg to a limit in the space
what a cauchi sequenc a sequenc such that for ani $\epsilon$, there a $n$ such that for all $n n$, $ x_{n+1} - x_n \epsilon$
what a hilbert space a complet inner product space.
what a separ space a space which contain a countable, dens sequenc of element such that everi nonempti open subset of the space contain an element of the sequenc
how are the rang amp; null space of a linear oper and it adjoint relat if $\text{range}(t^*)$ and $\text{range}(t)$ are both closed, $\text{range}(t)^ = \text{null}(t^*)$ $\text{range}(t^*)^ = \text{null}(t)$
what the neumann expans for linear oper if $ t 1$ under the $p$-norm, $(1 - t)^{-1} = \sum_{k=0}^ fti t^k$
what the evalu function in wavelet theori $\mathcal{e}_t [x] = x(t)$
what the frechet-riesz theorem ani bound linear function $f$ on a hilbert space can be uniqu repres as $f[x] = \langl \phi, x \rangle$ for some $\phi$
how can the reproduc kernel of a hilbert space with a bound evalu function be defin write the evalu function as an inner product $\mathcal{e}_t[x] = \langl e_t, x \rangle$ with some $e_t$ the reproduc kernel is defin as $k(t, t^\prime) = \langl e_{t^\prime}, e_t \rangle$
in term of it evalu functional, when doe a hilbert space have a reproduc kernel when the evalu function is bound
what'r some use properti of reproduc kernel if it exist in a hilbert space, it uniqu $x(t) = \langl k, x \rangle_t$
what the space of test function of rapid decay, $\mathcal{t}$ the space of function which are absolut integr over $\mathbb{r}$ infinit differenti everywher on $\mathbb{r}$ domin by everi negat power of $t$ as $t \rightarrow fty$
what usual consid to be the dual of a function space the space of linear function on that space
how is the dual of a function space usual denot if the space is $\mathcal{t}$, the dual is $\mathcal{t^\prime}$
what an analyt function one which is local given by a converg power seri
what a regular function one which is analyt and single-valu
when is a project in a hilbert space orthogon when it rang and null space are orthogon and exact when it self-adjoint
what are the properti need for a set of subspac to a multi-resolut analysi set nesting: $\mathcal{v}_m \subset \mathcal{v}_{m-1}$ completeness: $\bigcup \mathcal{v}_m = l_2(\mathbb{r})$, $\bigcap \mathcal{v}_m = \{0\}$ multi-resolution: $x(t) \mathcal{v}_m \iff x(2t) \mathcal{v}_{m-1}$
when will an infinit combin over an orthonorm basi converg $\sum c_n \phi_n$ converg iff $c l_2(\mathbb{z})$
what'r three way of character complet orthonorm set in hilbert space $\{\psi_n\}$ is complet iff the onli vector orthogon to it is 0 iff all linear combin over the set with coeffici in $l_2$ s the entir space iff parsev relat hold for everi member of the space
what parsev relat for vector given a complet orthonorm set $\{ \phi_n \}$ in a hilbert space, for ani $x$, $ x ^2 = \sum \langl \phi_n, x \rangl ^2$
how can you write the reproduc kernel of a space in term of a complet orthonorm basi set $\{\phi_n\}$ $k(t, t^\prime) = \sum \phi_n^*(t) \phi_n (t^\prime)$
\emph{when} can you write the reproduc kernel of a space in term of a complet orthonorm basi set $\{ \phi_n \}$ when $\sum \phi_n(t) ^2 fty$ for all $t$
what the einstein integr convent all repeat continu variabl occur in bras and ket are integr over
how do you write $\phi(t)$ in bra-ket notat $\langl t \phi \rangle$ use the reproduc kernel
how do you write $\phi^*(t)$ in bra-ket notat $\langl \phi t \rangle$ use the reproduc kernel
what doe $\omega$ repres in wavelet theori $\omega(t) = \frac{1}{\sqrt{2\pi}}e^{-iwt}$
how do you write the fourier analysi and synthesi equat in dirac notat $x(\omega) = \langl \omega t \rangl \langl t x \rangl = \langl \omega x \rangle$ $x(t) = \langl t \omega \rangl \langl \omega x \rangl = \langl t x \rangle$
what doe $t$ repres in $\langl t $ an evalu function element $e_t$ such that $x(t) = \langl e_t, x \rangle$
in fourier analysis, what are the complet and orthonorm relat for a basi completeness: $ \omega \rangl \langl \omega = \mathbf{1}$ orthonormality: $\langl \omega \omega^\prim \rangl = \delta(\omega - \omega^\prime)$
what a complet orthonorm basi for $l_2[0, t_0]$ $\phi_n(t) = \frac{1}{\sqrt{t_0}}e^{in\omega_0t}$ with $n \mathbb{z}$ and $\omega_0t_0 = 2\pi$
intuitively, what doe the riesz-fisch theorem say if you a linear combin of $\phi_n(t) = \frac{1}{\sqrt{t_0}}e^{in\omega_0t}$ use ani sequenc of coeffici $\{c_n\} l_2(\mathbb{z})$, it'll converg to a lebesgu measur function on $[0, t_0]$
what the poisson summat ula for $x[n]$ and $x(\omega)$ defin so that the $\frac{1}{2\pi}$ coeffici is entir in the reconstruct ula $\sum_{\mathbb{z}} x(t + 2n\pi) = \sum_{\mathbb{z}} x(n) e^{int}$
what a fourier basi for $l_2[0,n)$ $p_{kn} = \frac{1}{\sqrt{n}}e^{2i\pi kn/n}$ with $0 k, n n$ ($k$ is the frequenc parameter, $n$ is the time parameter)
intuitively, what parsev relat it say that the energi of the fourier tran of a function is the same as the energi of the function
what a band-limit function one whose fourier tran has support limit to $[-\omega, \omega]$
what $l_2^\omega(\mathbb{r})$ the space of $\omega$-band-limit function with finit energi
what the reproduc kernel of $l_2^\omega(\mathbb{r})$ $k(t, t^\prime) = \frac{ [\omega(t - t^\prime)]}{\pi(t - t^\prime)}$
what the invers fourier tran of $\mathbf{1}_{[-\omega, \omega]}$ $\frac{1}{\pi t} \omega t$
what'r a set of orthonorm basi function for $l_2^\omega(\mathbb{r})$ $p_{n}(t) = \frac{\omega}{\pi}\text{sinc}\left(\frac{\omega}{\pi}t - n\right)$
what the linear bandwidth variabl for a band-limit function $b = \frac{1}{2\pi}\omega$
what'r the orthonorm basi function for $l_2^\omega(\mathbb{r})$ in term of the linear bandwidth variabl $p_n(t) = \sqrt{2b} \,\text{sinc}(2bt - n)$
intuitively, what the sampl theorem the sampl frequenc should be twice the highest frequenc in order to perfect reconstruct a function
what the $l_2(\mathbb{r})$ basi analysi oper for a given basi $\{\phi_n(t)\}$, $t_\phi : l_2(\mathbb{r}) \rightarrow l_2(\mathbb{r})$ $t_\phi \{x(t)\} = [\cdots, \langl \phi_n x \rangle, \cdots]^t$
what the $l_2(\mathbb{r})$ basi synthesi oper for a given basi $\{\phi_n(t)\}$, $t^+_\phi : l_2(\mathbb{r}) \rightarrow l_2(\mathbb{r})$ $t^+_\phi \{c\} = \sum_\mathbb{z} c_n \phi_n(t)$
given a basi $\{\phi_n\}$, what the basi oper on $l_2(\mathbb{r})$ $t^+_\phi t_\phi : l_2(\mathbb{r}) \rightarrow l_2(\mathbb{r})$ $t^+_\phi t_\phi = \phi_n \rangl \langl \phi_n $
what the condit number of a normal linear oper $\kappa = \left \frac{\lambda_{max}}{\lambda_{min}} \right $
what the condit number of an arbitrari (possibl non-normal) linear oper $\kappa = \frac{\sigma_{max}}{\sigma_{min}}$, where $\sigma_i$ are the singular valu of the matrix
what doe it mean for two set of vector $\{v_i\},\{w_j\}$ to be biorthogon $\langl v_i, w_j \rangl = \delta_{ij}$ but neither the $v_i$ or $w_j$s need be orthogon set
how are riesz base defin given a complet orthonorm basi $\{\phi_n\}$ and an invert and continu linear oper $l$ defin $\xi_n = l \phi_n \rangle$, $\chi_n = (l^{-1})^* \phi_n \rangle$
what the most import properti of riesz base $\{\xi_n\}$ and $\{\chi_n\}$ are biorthogon
what'r the complet relat for biorthogon base $ \xi_n \rangl \langl \chi_n = \mathbf{1}$ $ \chi_n \rangl \langl \xi_n = \mathbf{1}$
what'r the expans ula for biorthogon base $ x\rangl = \langl \xi_n x \rangl \chi_n \rangle$ $ x\rangl = \langl \chi_n x \rangl \xi_n \rangle$
given a linear oper $a : \mathbb{c}^n \rightarrow \mathbb{c}^m$, $m n$, how do you recov the solut $\hat x$ to the equat $ax = y$ with minim norm $\hat x = a^+ u$, where $u$ is ani solut to $aa^+u = y$ if $a$ is nondegenerate, then this reduc to $\hat x = a^\dagger_r y$
what the right pseudoinvers of a linear oper $a$ $a^\dagger_r = a^+ (aa^+)^{-1}$ (such that $aa^\dagger_r = \mathbf{1}$)
given a linear oper $a : \mathbb{c}^n \rightarrow \mathbb{c}^m$, $m n$, how do you recov the vector $\hat x$ which minim $ ax - y $ $\hat x = a^\dagger_l y$
what'r the frame inequ on a finit vector space if linear oper $a$ has singular valu $\sigma_i$ $\sigma_{min} y ^2 \langl aa^*y, y \rangl \sigma_{max} y ^2$ for all $y$
what a frame in wavelet theori ani set of vector $\{c_j\}$ such that, when interpret as column of a matrix $a$, the frame inequ are satisfi
in a finit dimension space, what are the frame bound equal to given a frame $\{c_i\}$ and matrix ed from it $a$, the frame bound are $\sigma_{max}$, $\sigma_{min}$, the extrem singular valu of $a$
what it mean for a frame to be `tight in wavelet theori the frame bounds, $a, b$, are equal so it mimic an orthogon matrix
how can you construct a tight frame from a non-tight frame choos $c^\prime_k = \frac{1}{\sqrt{aa^+}} c_k$
what an exact basi one where remov an element would make it incomplet
what an uncondit basi a basi where there are constant $a$, $b$ such that $a \frac{ \sum c_n \phi_n ^2}{ c ^2} b$ for everi $c l_2(\mathbb{z})$ in the space
what a frame in $l_2(\mathbb{r})$ a set of function $\{\phi_n\}$ such that there are constant $a, b$ (the frame bounds) satisfi $a x ^2 \sum \langl \phi_n, x \rangl ^2 b x ^2$ for everi $x l_2(\mathbb{r})$
when is a frame a riesz basi when it exact
what the frame oper on $l_2(\mathbb{r})$ $t^*_\phi t_\phi \{x(t)\} = \sum \langl \phi_n, x \rangl \phi_n (t)$
what the frame redund ratio given frame bound $a, b$, it $b/a$
what the use of the frame redund ratio it approxim the condit number of the frame oper a finit redund ratio that not too larg or small is need for stabl synthesi or recoveri
how do you defin a dual frame given a frame $\{\phi_n\}$ with an invert frame operator, the dual is defin as $\xi_n = (t^+_\phi t_\phi)^{-1} \phi_n$
how do the frame bound of a dual frame relat to those of the primal if the primal has bound $a, b$, the dual has bound $b^{-1}, a^{-1}$
what'r the dual frame reconstruct equat $x(t) = t^+_\xi t_\phi \{x(t)\}$ $x(t) = t^+_\phi t_\xi \{x(t)\}$
given a frame $\{\phi_n\}$, of all the $c$ that minim the differ between the lhs and rhs of $x = \sum c_n \phi_n$, what the solut with minimum norm $c_n = \langl \xi_n, x \rangle$ where $\{\xi_n\}$ is the dual frame to $\{\phi_n\}$ work for the dual too
how do you write the reproduc kernel in a hilbert space that has dual frame $\{\phi_n, \xi_n\}$ $k(t, t^\prime) = \sum \xi^*_n(t)\phi_n(t^\prime)$
when can a function $x(t) l_2(\mathbb{r})$ be uniqu reconstruct from coeffici $\langl \phi_n, x\rangle$ or $\langl \xi_n, x\rangle$ when $\phi_n, \xi_n$ dual frame
given a frame, how can you construct the dual if the frame redund ratio is close to 1, there an iter algorithm that can approxim the dual.
what'r the advantag of frame over base redundancy: better approxim of cwt coeffici robust to quantize effect fewer restrict on choic of analyz basi function signal can be reconstruct even with miss coeffici
what'r the problem with use frame vs base difficult to use for signal reconstruct sinc the dual frame has to be computed, which can be slow and complex
what the convolut theorem in fourier analysi $\mathcal{f}[x * y] = \mathcal{f}[x]\mathcal{f}[y]$
what the invers convolut theorem in fourier analysi $\mathcal{f}[xy] = \mathcal{f}[x]*\mathcal{f}[y]$
how is convolut defin $x*i = t_d x(s)y(t-s) ds$
what the intuit interpret of convolut suppos a linear, time-invari system has a respons $y(t)$ to an impuls $\delta(0)$ and write a signal $x$ as $x(t) = t x(s)\delta(t-s) ds$ then the system respons to $x$ is $(x*y)(t) = t x(s)y(t-s)ds$
what the of convolv a sequenc of length $m$ with a sequenc of length $n$ a sequenc of length $m+n-1$
how do you use standard dfts to comput the convolut of two sequenc of length $m$ and $n$ pad the two sequenc at the back with zero to $m+n-1$ take the dft of each sequence, multipli the two, then take the invers dft
how do you write a convolut of finit sequenc as a matrix equat $y = h * x$ is written as $y = hx$, where $h$ has row $ {pmatrix} h_{m-1} &amp; \dotsc &amp; h_0 &amp; 0 &amp; \dotsc &amp; 0 &amp; 0 0 &amp; h_{m-1} &amp; \dotsc &amp; h_0 &amp; 0 &amp; \dotsc &amp; 0 \vdot &amp; \vdot &amp; \vdot &amp; \vdot &amp; \vdot &amp; \vdot &amp; \vdot 0 &amp; \dotsc &amp; 0 &amp; h_{m-1} &amp; \dotsc &amp; h_0 &amp; 0 0 &amp; 0 &amp;\dotsc &amp; 0 &amp; h_{m-1} &amp; \dotsc &amp; h_0 {pmatrix}$
what the definit of the correl between two continu function $[h \star x](t) = t h(s)x(t+s) ds$
what anoth name for a system function a filter
what'r the eigenfunct of a continu linear time-invari system $u(t) = e^{st}$, $s \mathbb{c}$
what'r the eigenfunct for a discret linear time-invari system $u(z) = \frac{1}{z^n}$, $n \mathbb{z}$
what a laurent seri a converg seri of the $f(z) = \sum_{- fty}^ fti a_n(z-c)^n$
how the z tran defin $\mathcal{z}\{x[n]\} = \sum_{- fty}^ fti \frac{x[n]}{z^n}$
what the convolut theorem for z tran s $\mathcal{z}\{x * h \} = \mathcal{z}\{x\}\mathcal{z}\{h\}$
what the z tran of $x[n-k]$ $\mathcal{z}\{x[n-k]\} = \frac{1}{z^k} \mathcal{z}\{x[n]\}$
what the z tran of $x[-n]$ $\mathcal{z}\{x[-n]\} = \mathcal{z}\{x[n]\}(\frac{1}{z})$
what the z tran of $x^*[n]$ $\mathcal{z}\{x^*[n]\} = \mathcal{z}^*\{x[n]\}(z^*)$
what the z tran of $\frac{1}{a^n}x[n]$ when $a 0$ $\mathcal{z}\{\frac{1}{a^n}x[n]\} = \mathcal{z}\{x[n]\}(az)$
intuitively, how doe the z tran relat to the fourier tran evalu the z tran on $z = e^{iw}$ (the unit circle) is equival to comput the fourier tran
how the invers z tran defin $\mathcal{z}^{-1}\{x(z)\} = \frac{1}{2\pi i}\oint_c x(z) z^{n-1}dz$ where $c$ is a path within the region of converg and circl the origin
what the z tran equival of $\mathcal{f}\{x\}(\omega + \pi)$ $\mathcal{f}\{x\}(\omega + \pi) = \mathcal{z}\{x\}(-z)$ where $z = e^{i\omega}$
what the z tran equival of $\mathcal{f}^*\{x\}(\omega)$ $\mathcal{f}^*\{x\}(\omega) = \mathcal{z}\{x\}(\frac{1}{z})$ where $z = e^{i\omega}$
what a spectrum in fourier analysi $s(\omega) = \mathcal{f}\{x[n]\}(\omega) ^2$ ie the frequenc energi spectrum
what a fir filter in fourier analysi finit impuls respons filter
what the fejer-riesz theorem if $s(\omega) = \sum_{ k n-1} s_k e^{-ik\omega}$ is real and non-neg it can be written as $s(\omega) = h(\omega) ^2$ where $h(z) = \sum_0^{n-1} \frac{h_n}{z^n}$ is uniqu up to an arbitrari phase and has all it root on or in the unit circl
what a pr-qmf in wavelet theori perfect reconstruct quadratur mirror filter bank
intuitively, what the use of a pr-qmf it'll split a signal into low- and high-pass compon at half the origin sampl rate with exact reconstruct from the compon
how are convolut and correl relat correl with $h[n]$ is equival to convolut with $h[-n]$
what the z tran of $x[n]$ when downsampl let $y[n] = x[2n]$ be the downsampl signal then $y(z) = \frac{1}{2} [x(\sqrt{z}) + x(-\sqrt{z})]$
what the z tran of $x[n]$ when upsampl let $y[2n] = x[n]$, $y[2n+1] = 0$ be the upsampl signal then $y(z) = x(z^2)$
for an iir pr-qmf filter bank, how should the analysi filter $g_0, g_1$ and synthesi filter $h_0, h_1$ be relat qmf requirement: $h_1(z) = h_0(-z)$ pr requirements: $g_0(z) = h_0(\frac{1}{z})$, $g_1(z)=h_1(\frac{1}{z})$ $h_0(z)g_0(z) + h_0(-z)g_0(-z) = 2$
what the qmf properti for infinit filter in the frequenc domain if the frequenc is restrict to the unit circl $h_1(\omega) = -e^{-i\omega}h_0^*(\omega + \pi)$
in wavelet theory, what do $g_0, g_1, h_0, h_1$ usual refer to $g_0$ is the low-pass analysi filter $g_1$ is the high-pass analysi filter $h_0$ is the low-pass synthesi filter $h_1$ is the high-pass synthesi filter
how are wavelet deriv from a mother wavelet $\psi_{\tau \eta} = \mathcal{t}_\tau \mathcal{d}_\eta \psi(t)$
how is the window fourier tran defin as an inner product $\xi_{\tau\omega}(t) = \mathcal{m}_\omega \mathcal{t}_\tau w^*(t)$ where $w$ is the window function then $s_{\tau\omega} = \langl \xi_{\tau \omega} s \rangle$
what $\mathcal{m_\omega}$ in wavelet theori the modul oper $\mathcal{m_\omega} w(t) = e^{i\omega t} w(t)$
what $\mathcal{t}_\tau$ in wavelet theori the translat oper $\mathcal{t}_\tau w(t) = w(t - \tau)$
how is the window invers fourier tran defin as an inner product let $\chi_{\tau \omega} = \frac{1}{\sqrt{2\pi} w ^2} \xi_{\tau \omega}(t)$ then $s(t) = \langl \chi_{\tau \omega} s(\tau, \omega) \rangle$
when do the analyz and synthes function for the window fourier tran coincid when the window function is normal so that $ w = \frac{1}{\sqrt{2\pi}}$
what the reproduc kernel for a fourier tran with a normal window function $k_{\tau\omega;\tau^\prim \omega^\prime} = \langl \xi_{\tau\omega} \xi_{\tau^\prim \omega^\prime} \rangle$
what the intuit interpret of the reproduc kernel of a window fourier tran when appli to $l_2(\mathbb{r}^2)$ it a project onto the rang space of the window fourier tran oper
what doe $\mathcal{w}(t, \tau)$ repres in wavelet theori $\mathcal{w}(t, \tau) = \sum_{- fty}^ fti w(t-m\tau) ^2$ ie the energi of a discret window function $w$ center on $t$ when sampl everi $\tau$
how is the dual window function defin for a discret window fourier tran $\tild w(t) = \frac{1}{t_0}\frac{w^*(t)}{\mathcal{w}(t, \tau_0)}$
what the use of the dual window in the discret window fourier tran if the synthesi function exist, they'll be generat by time translat and frequenc modul of the dual window function
what are suffici condit for a function to be recov from it discret window fourier tran the window $w(t)$ has compact support $[0, t_0]$ the fundament shift and frequenc satisfi $\tau_0 \omega_0 2\pi$ there are positive, finit $a, b$ such that $a \frac{2\pi}{\omega_0 \tau_0} w ^2 b$
how are the frame and dual frame defin for a discret window fourier tran $\xi_{mn}(t) = \mathcal{m}_{n\omega_0} \mathcal{t}_{m\tau_0} w^*(t)$ $\chi_{mn}(t) = \mathcal{m}_{n\omega_0} \mathcal{t}_{m\tau_0} \tild w(t)$
what the reconstruct ula for a discret window fourier tran in term of the frame and dual frame function $ s \rangl = \chi_{mn} \rangle\langl \xi_{mn} s \rangle$
intuitively, what the balian-low theorem if a window discret fourier tran has $\omega_0 \tau_0 = 2\pi$ and a frame exist for it then the window must be `bad behav in either the time or frequenc domain
how'r the time and frequenc width of a window function defin $\sigma_t^2 = \frac{ t (t-\bar t)^2 w^2(t) dt}{ t w^2(t) dt}$, where $\bar t = \frac{ t t w^2(t) dt}{ t w^2(t) dt}$ $\sigma_f^2 = \frac{ t (f-\bar f)^2 w^2(f) df}{ t w^2(f) df}$, where $\bar f = \frac{ t f w^2(f) dt}{ t w^2(f) df}$
how are the time width and frequenc width of a window function relat $\sigma_t \sigma_f \frac{1}{4\pi}$
how is the continu wavelet tran of a function defin $x_{\tau\eta} = \langl \psi_{\tau \eta} x \rangle$
how is the dilat oper defin in the continu wavelet tran $\mathcal{d}_\eta \psi(t) = \frac{1}{\sqrt{ \eta }}\psi\left(\frac{t}{\eta}\right)$
how do the dilat and translat oper commut $\mathcal{d}_\eta \mathcal{t}_\tau = \mathcal{t}_{\eta \tau} \mathcal{d}_\eta$
what the fourier tran of $x(\eta t)$ $\frac{1}{\eta} x \left(\frac{\omega}{\eta} \right)$
what the distinguish properti of constant-q filter bank the ratio of rms bandwidth to central frequenc is the same for all filter in the bank
do wavelet constant-q filter bank yup
what the admiss condit on a mother wavelet that enabl recoveri of a signal from it tran $c_\psi = t \frac{ \psi(\omega) ^2}{ \omega }d\omega$ is finit (note $\psi$ is the fourier tran of the wavelet)
how is the dual to the continu wavelet tran defin $\chi_{\tau \eta} = \frac{1}{c_\psi \eta ^2} \psi_{\tau\eta}$
what the recoveri equat for the continu wavelet tran in term of the dual wavelet function $ x \rangl = \langl \psi_{\tau \omega} x \rangl \chi_{\tau \omega} \rangle$
what parsev relat for the contin wavelet tran if $x, y$ have cwt coeff $x_{\tau \eta}, y_{\tau, \eta}$, then $\langl y x \rangl = \langl y x \rangle$
what the reproduc kernel of the space of cwt coeffici $k_{\tau\eta;\tau^\prim \eta^\prime} = \langl \psi_{\tau\eta} \psi_{\tau^\prim \eta^\prime} \rangle$
what the project oper from $l_2(\mathbb{r}^2)$ onto the rang of the cwt the reproduc kernel $k_{\tau\eta;\tau^\prim \eta^\prime}$
what'r suffici condit for a mother wavelet to be admiss if it got finit energi and zero mean (equiv. no dc fourier component)
how do you discret the continu wavelet tran so as to preserv scale invari use the grid $\{\eta_m = \eta_0^m, \tau_n = n\eta_0^m\tau_0\ m, n \mathbb{z}\}$ is not time-transl invari
how do you discret the continu wavelet tran so as to preserv time translat invari use the grid $\{\eta_m = \eta_0^m, \tau_n = n\tau_0\ m, n \mathbb{z}\}$ is not scale invari
how is the continu wavelet tran discret into the undecim wavelet tran by use the time-translation-preserv grid with $\eta_0 = 2$ and $\tau_0 = 1$
if you discret the cwt on the scale-preserv grid, how do the frame bound (if they exist) respond to chang in $\eta_0$ and $\tau_0$ as $\eta_0 \rightarrow 1$ and $\tau_0 \rightarrow 0$ then $\left \frac{a-b}{a+b}\right \rightarrow 0$
what the a'trous condit an interpol function $f[n]$ satisfi the a'trous condit if $f[2n] = \delta_{0n}$
what the use of a'trous interpol in wavelet theori they ensur that when doubl the scale of a wavelet, even valu will be interpol exact
what the lagrang a'trous filter an a'trous filter which use lagrang polynomi evalu at $0.5$ for the filter coeffici of odd indic
what the advantag of the lagrang a'trous filter the interpol given by the filter of order $2n-1$ is exact at all indic if the target function is a polynomi of order at most $2n-1$
what the haar scale function $\phi(t) = 1$ on $0 t 1$, $0$ otherwis
what a scale function a function satisfi the scale equat $\phi(t) = \sqrt{2} h_0[n] \cdot \phi(2t-n)$ where $h_0[n]$ are known as the low-pass filter coeffici
what the basi for the haar mra subspac $\mathcal{v}_m$ $\phi_{mn}(t) = \mathcal{d}_{2^m} \mathcal{t}_n \phi(t)$ where $\phi(t)$ is the haar scale function
what the haar scale equat $\phi(t) = \phi(2t) + \phi(2t-1)$ which is satisfi by the haar scale function
what $\mathbf{p}_m$ in wavelet theori the ``approximation' project from $l_2(\mathbb{r})$ onto the mra subspac $\mathcal{v}_m$
how $\mathbf{e}_m$ defin in wavelet theori $\mathbf{e}_m x = \mathbf{p}_{m-1}x - \mathbf{p}_m x$ ie it the ``detail' project onto $\mathcal{v}_m^ $
how are the haar wavelet function defin in term of the haar scale function $\psi_{mn}(t) = \frac{1}{\sqrt{2}} (\phi_{m-1, 2n}(t) - \phi_{m-1, 2n+1}(t))$
how are the haar wavelet coeffici defin in term of the haar scale coeffici on the finer scale coeffici for $\psi_{mn}$ is $d_{mn} = \frac{1}{\sqrt{2}}(c_{m-1,2n} - c_{m-1, 2n+1})$ where $c_mn$ are the coeffici of the scale function $\phi_{mn}$
what the wavelet equat $\psi(t) = \sqrt{2} h_1[n] \star \phi(2t-n)$ where $h_1[n]$ are the high-pass filter coeffici and $\phi$ is the scale function
what the haar wavelet equat $\psi(t) = \phi(2t) - \phi(2t-1)$ where $\phi$ is the scale function
how is the mra subspac $\mathcal{v}_{m-1}$ written in term of $\mathcal{v}_{m}$ $\mathcal{v}_{m-1} = \mathcal{v}_m \oplus \mathcal{v}_m^ $
with the mra subspac $\mathcal{v}_m$, is it increas or decreas $m$ that correspond to finer detail decreasing. $\mathcal{v}_{- fty}$ is perfect resolut
how is the mra subspac $\mathcal{v}_{m}$ written in term of detail subspac alon $\mathcal{v}_{m} = \oplus_{k m} \mathcal{v}_k^ $
how are the scale amp; wavelet tran s at a scale written in term of the next finer scale $\psi_{mn}(t) = h_1[k-2n] \star \phi_{m-1, k}(t)$ where $h_1$ is a high-pass filter $\phi_{mn}(t) = h_0[k-2n] \star \phi_{m-1, k}(t)$ where $h_0$ is a low-pass filter
what the intuit version of write the scale amp; wavelet coeffici in term of the coeffici at the next finer scale you analyz the scale coeffici at scale $m-1$ to get the coeffici at the coarser scale $m$ by correl them with a low (scaling) or high (wavelet) pass filter, then take all even number samples.
what a half-band filter a filter which reduc the maximum bandwidth of the input by a factor of 2
what are the shannon mra subspac equival to $\mathcal{v}_m$ is the subspac of function whose band is limit to $\frac{1}{2^m}[-\pi, \pi]$
what are the low-pass filter coeffici for the shannon scale function $h_0[n] = \frac{1}{\sqrt{2}}\text{sinc}\, \frac{n}{2}$
what the shannon mother scale function $\phi = \text{sinc\,} t$
what the shannon mother wavelet function $\psi(t) = \text{sinc}(t-\frac{1}{2})-2\text{sinc}(2t-1)$
intuitively, what meyer wavelet it shannon wavelet with the edg of it frequenc spectrum smooth a littl
how are the scale and wavelet coeffici written in term of the scale and wavelet tran s wavelet coeffs: $d_{mn} = \langl \psi_{mn} x \rangle$ scale coeffs: $c_{mn} = \langl \phi_{mn} x \rangle$
how are the scale and wavelet coeffici at a given scale written in term of the next finer scale $d_{mn} = h_1[k-2n] \star c_{m-1, k}$ where $h_1$ is a high-pass filter $c_{mn} = h_0[k-2n] \star c_{m-1, k}$ where $h_0$ is a low-pass filter
how are the scale coeffici at a certain scale written in term of the coeffici of the next coarser scale $c_{m-1, n} = h_0[2k+n]*c_{mk} + h_1[2k+n]*d_{mk}$ where $h_0$ is a low pass filter and $h_1$ is a high pass filter
what the intuit version of write the scale coeffici in term of the coeffici at the next coarser scale you synthes the coeffici at scale $m-1$ from the coeffici at the coarser scale $m$ by interpol $c_{mn}$ and $d_{mn}$ with zero at odd $n$ then convolv the s with a low-pass ($c$) and high-pass ($d$) filter then ad the two togeth
intuitively, what happen in the analysi stage of the undecim wavelet tran the filter are interpol with zero so at the $m$th scale, onli multipl of $2^m$ are nonzero then the signal is correl with them direct (without downsampling)
intuitively, what happen in the synthesi stage of the undecim wavelet tran upsampl the approxim and detail signal by interpol the coeffici with zeroes, then convolv the s with the filter and sum the two output
what are two other name for the undecim wavelet tran shift-invari wavelet tran redund wavelet tran
how do you write the low-pass filter in term of the scale function of a wavelet tran $h_0[n] = \sqrt{2} \phi(t) \star \phi(2t-n)$
how do you write the high pass filter in term of the scale and wavelet mother function of a wavelet tran $h_1[n] = \sqrt{2} \psi(t) \star \phi(2t-n)$
what can you say about the support of a filter and the support of the scale function of a wavelet tran if the low-pass filter $h_0[n]$ has compact support on $[0, n-1]$, so doe $\phi(t)$ if $\phi(t)$ has compact support on $[0, n-1]$, then $h_0[n]$ has at most $n$ nonzero element
what can you say about the support of a filter and the support of the mother wavelet function of a wavelet tran if the high-pass filter $h_1[n]$ has compact support on $[0, n-1]$, so doe $\psi(t)$ if $\psi(t)$ has compact support on $[0, n-1]$, then $h_1[n]$ has at most $n$ nonzero element
what a low-pass function one with a nonzero averag valu
what three constraint are impos on the low-pass filter coeffici $h_0[n]$ when the scale function is normalized, orthonorm and real $\sum_n h_0[n] = \sqrt{2}$ $\sum_n h_0[2n] = \sum h_0[2n+1] = \frac{1}{\sqrt{2}}$ $h_0[n] \star h_0[n-2k] = \delta_{0k}$
what the partition-of-un properti of a wavelet tran 's scale function $\sum_n \phi(t+n) = 1$
what three constraint are impos on the high-pass filter coeffici $h_1[n]$ when the scale function is normalized, orthonorm and real $\sum_n h_1[n] = 0$ $h_1[k]\star h_1[k+2n] = \delta_{0n}$ $h_1[k]\star h_0[k+2n] = \delta_{0n}$
how is the fourier tran of a scale function written in term of the fourier tran s of the low-pass filter $\phi(\omega) = \prod \left(\frac{1}{\sqrt{2}} h_0\left(\frac{\omega}{2^n}\right)\right)$
how is the orthonorm basi properti of the scale function $\phi_{mn}$ repres in the frequenc domain $ \phi (\omega + 2n\pi) ^2_n = 1$
how is the fourier tran of the wavelet function written in term of the fourier tran of the high-pass filter $\psi(\omega) = \prod \frac{1}{\sqrt{2}} h_1\left(\frac{\omega}{2^n}\right)$
how is the orthonorm basi properti of the wavelet function $\psi_{mn}$ repres in the frequenc domain $ \psi(\omega + 2n\pi) ^2_n = 1$
how is the orthogon between wavelet function $\psi_{mn}$ and scale function $\phi_{mn}$ repres in the frequenc domain $\langl \phi(\omega + 2n\pi), \psi(\omega + 2n\pi) \rangle_n = 0$
what the paraunitari matrix of a pr-qmf in wavelet theori $\mathbf{h}(\omega) = \frac{1}{\sqrt{2}}\left( {matrix} h_0(\omega) &amp; h_0(\omega + \pi) h_1(\omega) &amp; h_1(\omega + \pi) {matrix} \right)$
how can the paraunitari matrix be use to captur the orthonorm of the scale amp; wavelet function $\mathbf{h}^+(\omega) \mathbf{h}(\omega) = \mathbf{i}$
what condit on the the wavelet amp; scale function lead to the qmf condit on infinit filter the orthonorm of the wavelet amp; scale function and the select of $-e^{-i\omega}$ as the scale factor in $h_1(\omega) = p(2\omega) e^{ i \omega}h_0^*(\omega + \pi)$
what an order-$n$ spline a function that piecewis $n$th-degre polynomi and is continu and continu differenti to order $n-1$
what a b-spline a basi spline, a uniqu spline associ with a set of control point into which ani spline can be linear decompos
what the cascad algorithm in wavelet theori a way to construct scale function from filter coefficients. use the scale equat for interpol and the partit of uniti for the base case
what a linear phase filter a filter where the phase respons is linear wrt frequenc correspond to a constant shift in time (the phase delay)
what are biorthogon filter a set of synthesi filter $\{h_0, h_1\}$ deriv from the scale function $\phi(t)$ and analysi filter $\{h_0^\prime, h_1^\prime\}$ deriv from the scale function $\phi^\prime(t)$ where $\phi, \phi^\prime$ are biorthogon
how can the orthogon condit on the scale amp; wavelet function of a biorthogon filter scheme be captur by paraunitari matric $\mathbf{h}^\prim (\omega) \mathbf{h}^+(\omega) = \mathbf{1}$ where $\mathbf{h}^\prime$ is construct from the dual filter $h_0^\prime, h_1^\prime$
how can a discret time signal be interpret as a continu signal in wavelet theori by treat the discret data as sampl and use the sampl theorem to reconstruct the `origin signal the sampl theorem reconstruct most close resembl the invers wavelet tran on the shannon wavelet, but ani scale function with a zero mean time can be used.
how do you write a wavelet analysi step as a matrix multipl $\left( {matrix} h_0 &amp; h_1 &amp; h_2 &amp; h_3 &amp; 0 &amp; 0 &amp; 0 &amp; 0 h_3 &amp; -h_2 &amp; h_1 &amp; -h_0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; h_0 &amp; h_1 &amp; h_2 &amp; h_3 &amp; 0 &amp; 0 0 &amp; 0 &amp; h_3 &amp; -h_2 &amp; h_1 &amp; -h_0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 &amp; 0 &amp; h_0 &amp; h_1 &amp; h_2 &amp; h_3 0 &amp; 0 &amp; 0 &amp; 0 &amp; h_3 &amp; -h_2 &amp; h_1 &amp; -h_0 h_2 &amp; h_3 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; h_0 &amp; h_1 h_1 &amp; -h_0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; h_3 &amp; -h_2 {matrix} \right) \mathbf{x} = \left( {matrix} c_0 d_0 c_1 d_1 c_2 d_2 c_3 d_3 {matrix} \right)$ this is usual follow by a reorder to put all the $c_i$ befor the $d_i$
what a housekeep step to per when carri out a wavelet tran step by direct convolut given a length $n$ signal and a length $m$ filter, the of a convolut is length $n+m-1$ so dure analysis, discard the first $m-1$ point from the output befor downsampl and dure synthesis, discard the last $m-1$ point from the output befor sum
how do you write a wavelet synthesi step as a matrix multipl use the transpos of the analysi matrix.
what are four use function when use python inter eli \ {help(), help(x)} \ {dir(ns)} which list the object in a particular namespac \ {locals()} which list the object in the local namespac \ {globals()} which list the object in the global namespac
how do you carri out divis on integ in python 3 \ {5/2} return the float \ {2.5f} \ {5//2} return the integ \ {2}
how do you includ a modul in python \ {import modulename}
how a list liter denot in python \ {[1, "two", 3l]}
in what way can you get a slice of a list in python \ {list[3:]} to get everi element from the 3rd onward \ {list[-3:]} to get the last 3 element \ {list[2:4]} to get element 2, 3 amp; 4
what the differ between tupl and list in python tupl are immut
how a tupl liter denot in python \ {(1, "two", 3l)}
how do you creat a one-el tupl in python \ {(1,)} the \ {,} is necessary!
in what way can you denot a string in python \ {'string'} \ {"string"} \ {'''string'''} \ {"""string"""}
how do you creat a set in python \ {\{1, "two", 3l\}}
how is a dictionari liter denot in python \ {x = \{1: "one", "two": 2l\}}
what the restrict on dictionari key in python they must be an immutable, hashabl type
how do you creat a file object in python \ {f = open("filename", "w")}
how do you serial and deseri complex object in python use the \ {cpickle} class
how do you write an if-else-if statement in python {verbatim} if condition1: code elif condition2: code else: code {verbatim}
how do you write a for loop in python \ {for x in list:}
how do you declar a function in python \ {def functionname(x, y, z):}
what return by a python function with no return statement \ {none}
which structur handl error in python {verbatim} try: do someth riski except ioerror as error: handl error except emptyfileerror as error: handl other error else: do anoth thing if there no error finally: and alway do this {verbatim}
how do you denot a line comment in python \ { comment}
how do you defin a modul in python each file is implicit it own modul
how do you creat a docstr in python {verbatim} def function(): """docstring""" {verbatim}
how do you declar a class in python \ {class classname:}
how do you inherit from anoth class in python \ {class classname(parentname):}
how'r initi defin in python \ {def \_\_init\_\_(self, other, args):}
how do you defin the string represent of a class in python \ {def \_\_str\_\_(self):}
how are method denot in python they'r function which take (bi convention) a first argument call \ {self}
how do you deliber destroy a variabl in python \ {del x} destroy the \emph{variable}, not the object!
what numer type are avail in python integ float (doubles) complex number boolean
how are boolean implement in python as the number \ {0, 1}, just with differ string represent
how do you write an imaginari number in python \ {3j}
how can you includ the content of a modul in python \ {from modulenam import *} don't do this though
what python name convent lowercas amp; underscor for variables, function and modul pascal case for class
how do you append one list to anoth list use slice in python \ {front[len(x):] = back} list will expand to accomod the new element
how do you prepend to a list use indic in python \ {back[:0] = front} list will expand to accomod the new element
are slice or list method usual prefer in python list method - they communic intent more effect
how do you test membership in python \ {elem in elems}
how do you initi a list to copi of an element in python \ {[initial, elements] * count}
how do you copi nest list in python use \ {copy.deepcopy}
how can you return multipl object from a function in python in the function: \ {return (x, y)} at the call site: \ {a, b = f()} (the \ {a, b} is implicit convert to a tuple)
how can you return a variabl number of argument from a function in python in the function: \ {return (x, y, z)} at the call site: \ {a, *b = f()} (the \ {*} indic \ {b} will captur ani excess returns)
what kind of pattern match appear on the lhs of an assign in python \ {[a, b] = f()}, pattern match against a list \ {(a, b) = f()}, pattern match against a tupl \ {a, b = f()}, pattern match against a tupl
how do you creat a set of set in python make the inner set \ {frozenset}s, which are immut and hashabl
what'r string constant in python thing like \ {string.whitespace} and \ {string.digits} that describ what python think of as whitespac and digit on your system
what the string modulus oper in python the old way of at strings, befor \ {string. at} \ {"one s three" \{"two"\}}
what the best way to use the string modulus oper in python use at spefici of the \ {" (name)"} and pass a dictionari contain the key \ {name}
what the default charact set of python 3 string unicode.
which valu are falsi in python numer zero empti string empti data structur (usually) \ {none}
how can you test if a variabl fall in a numer rang in python \ {if 0 x 10:}
what the inequ oper in python \ {!=} \ { }
how can you get the docstr associ with a python function \ {f.\_\_doc\_\_}
how do you defin a default argument for a function in python \ {def fun(arg=default):}
how do you pass an argument by paramet name in python \ {f(param=value)}
how do you implement variable-numb argument function in python \ {def fun(*x)} will collect excess argument into a tupl \ {def fun(**x)} will collect excess keyword argument into a dictionari
how do you modifi a global variabl from within a function in python bring it into scope with \ {global a} then \ {a = b}
how do you write a lambda express in python \ {lambda x, y: expr}
how do you get the name of a function in python \ {f.\_\_name\_\_}
how do you implement a decor function in python creat a function \ {d} that take anoth function as it first arg and return anoth function write {verbatim} @d def f(): {verbatim} then call \ {f} will instead call the function return by \ {d(f)}
how can you get the modul search path in python \ {sys.path}
how do you denot a privat function in a modul in python give it a name start with an underscor isn't actual private, just \ {from mod import *} will fail to find it
what the builtin namespac in python a proxi for the \ {\_\_builtins\_\_} modul that has thing like \ {len, min} in
what the problem with the builtin namespac in python it unqualified, so it easi to accident overrid some of it member
how do you make a python script execut put a function call in file scope.
how can you tell if a python modul is be call as the main modul of a script \ {if \_\_name\_\_ == \_\_main\_\_':}
what python \ {shelve} modul a way to creat dictionari that resid on disk rather than in memory. allow access into the dictionari without load the entir thing
how do you rais an except in python \ {rais ioerror("message")}
what the general except catch in python \ {except:}
what the null statement in python \ {pass}
what happen if you pass more than one argument to a python except they'll be store in the \ {exceptionvar.args} list
how are assert implement in python if \ {\_\_debug\_\_ = true} then \ {assert expression, exceptionarg} will rais an \ {assertionerror(exceptionarg)} if the express evalu to fals
what'r context manag in python object with \ {\_\_enter\_\_, \_\_exit\_\_} method that'll be call automat on entri to amp; exit from \ {with obj as varname:}
how do you creat an object in python \ {x = classname()}
how do you creat a field in python \ {x.fieldnam = value} it'll be creat if it doesn't exist
what a common problem with class variabl in python if \ {x} has type \ {t} and the valu of \ {x.y} is ask for but \ {x} doesn't have a \ {y} variabl then it'll look for \ {t.y} instead and onli throw an error if it can't find that either
how do you creat a static method in python decor a method with \ {@staticmethod} static method take no \ {self} or \ {class} argument!
how do you creat a class method in python decor a method with \ {@classmethod} the method will be pass the class as it first argument
how do you refer to the base class in python \ {super(class, self)}
what must you do when write a subclass initi in python call \ {super(class, self).\_\_init(x, y)\_\_} as the first statement in the initi
how are constructor defin in python they'r not. initi are though.
what the gil in python the global interpret lock, which ensur onli one python thread can be run at a time.
how do you defin a privat method in a python class prefix it with \ {\_\_}
how are privat method implement by the python compil the method name is mangl by prefix it with \ {\_classname}
how do you defin a properti use decor in python decor the getter with \ {@property} (optionally) decor the setter with \ {@gettername.setter}
how do you defin a destructor in python defin a \ {\_\_del\_\_} method
when are destructor call in python when the refer count for the object reach zero.
what riski about implement python destructor if it call when the program is shut down, member of the global namespac might alreadi have been delet
what prefer to python destructor a \ {cleanup} method possibl call from an \ {\_\_exit\_\_} method for use with \ {with}
what the main problem with memori manag in python circular references.
how do you inherit from multipl class in python \ {class classname(parent1, parent2):}
when multipl parent are defined, how doe python search for inherit variabl depth-first search, go left to right through the list parent of each class
how are packag defin in python they'r implicit defin by directori which contain a (possibl empty) \ {\_\_init\_\_.py} file
how do you do packag initi in python put it in the \ {\_\_init\_\_.py} file in the same directori
what \ {\_\_all\_\_} in python if present in \ {\_\_init\_\_.py}, it list all the name which should be load when \ {from packagenam import *} is call
should you general prefer flat or nest packag structur in python flat.
how do you make modul in a packag privat in python prefix their name with an \ {\_}
how do you get the type of a python object \ {type(x)}
what the base of the inherit hierarchi in python for most `new- ' class (ie all class in python 3) it \ {object}
how do you test whether an object is of a certain class in python \ {isinstance(x, t)}
how do you test whether an object is a subclass of a type in python \ {x issubclass t}
what a special method attribut in python an element of a class that declar as a method, but isn't intend to be use like one instead it call by the python runtim usual has a name like \ {\_\_xxx\_\_}
how do you implement index on a python object implement \ {\_\_getitem\_\_, \_\_setitem\_\_}
how do you overrid the oper for a python object implement \ {\_\_add\_\_} to overrid \ {(+)}, etc
how do you delet part of a list in python \ {del x[1:3]}
what a metaclass in python the class of a class, usual \ {type}
how do you instanti class object via \ {type} in python \ {x = type("classname", (baseclass1, baseclass2), \{attrib1: val1, attrib2: val2\})}
what an attribut in python someth that `attach to an object; and can be access via \ {obj.attributename}
how do you creat your own metaclass in python inherit from \ {type}
what the use in metaclass in python it allow you to design class on the fli
how do you set the metaclass of a class in python \ {class classname(metaclass=metaclassname):}
what'r abstract base class in python class that can be ad to a class inherit tree that ensur it has certain attribut typechecking, basic
how do you creat your own abstract base class in python set the metaclass of the class to \ {abcmeta}
how do you use abstract base class in python creat a class with the desir properti regist it with the baseclass via \ {abstractbaseclassname.register(classname)} then \ {classnam issubclass abstractbaseclassname} will be \ {true}
where are abstract base class most encount in python in the \ {collections} library, with stuff like \ {hashable}, \ {iterable}, etc
how do you denot abstract method in python \ {@abstractmethod} in a class with \ {abcmeta} as it metaclass
what the effect of \ {@abstractmethod} in python ani class whose inherit hierarchi contain an \ {@abstractmethod} which hasn't been overridden can't be instanti
how do you denot an abstract properti in python \ {@abstractproperty} in a class with \ {abcmeta} as it metaclass
what the easiest databas to use with python \ {sqllite3}, becaus it includ in the standard librari
how do you iter over the line of a file in python iter over a file handl
what the generat approach to build a ml classifi creat a joint model $p(y, x)$ condit on $x$ to get $p(i x)$
what the discrimin approach to build a ml classifi fit a model of the $p(i x)$ direct
what the gradient of the likelihood in logist regress $\mb{g}(\mb{w}) = \mb{x}^t(\mb{y} - \mb{\mu})$ where $\mu$ is the mean of the bernouilli distribut use in the regress
what the updat rule in steepest descent $\theta_{k+1} = \theta_k - \eta_k g(\theta_k)$ where $\eta_k$ is the $k$th step size and $g$ is the gradient function
what a global converg algorithm in ml an optim algorithm which is guarante to converg to the local optimum (if it exists) no matter where it start
what line search in ml a heurist for use with gradient descent algorithm which say that onc you'v chosen your direct of descent $d$, pick the step size $\eta$ so as to minim $\phi(\eta) = f(\theta_k + \eta_k d)$ which can be done with an arbitrari 1d minim algorithm
what the typic behaviour of the steepest descent algorithm with naiv line search a zig-zag path that s from the fact that the minimum of $\phi(\eta) = f(\theta_k + \eta_k d)$ is either a stationari point or a point such that the direct of steepest descent is perpendicular to $d$
what the heavi ball method in ml it a way to suppress zig-zag in line search heurist by ad a momentum term $\mu_k (\theta_k - \theta_{k-1})$ to the updat rule that produc $\theta_{k+1}$ where $\mu$ control the strength of the momentum term
what the usual use of the conjug gradient method solv large, spars linear systems. (for dens ones, iter method are no faster than factor and backsubstitution)
what the updat rule in newton algorithm for vector-valu updat in ml $\mb{\theta}_{k+1} = \mb{\theta}_k - \eta_k \mc{h}^{-1}(\mb{\theta}_k)\mb{g}(\mb{\theta}_k)$ where $\mb{g}$ is the gradient and $\mc{h}$ is the hessian
when is newton algorithm applic when the function is strict convex
intuitively, what the levenberg marquardt algorithm a variant of newton algorithm intend for use with non-convex function which interpol between newton algorithm and steepest descent
what the truncat newton algorithm a variant of newton algorithm intend for use with non-convex function the conjug gradient method is use to solv $\mb{h}^{-1}(\theta_k)\mb{g}(\theta_k)$ but if negat curvatur is detected, the cg iter are truncat
what irl stand for in ml iter reweight least squar
simply, how the iter reweight least squar algorithm work at each iteration, the diagon covari matrix $\mb{s}$ is ed for the bernouilli distribut use by the logist regress problem then the work respons vector $\mb{z}$ is found and the next step $\mb{w}^\prime$ is calcul to be the vector which solv the weight least squar problem with weight $\mb{s}$ and target $\mb{z}$
what the use of the iter reweight least squar algorithm to iter solv glms
what the use of quasi-newton method newton algorithm requir comput the hessian, which can be expens to do direct quasi-newton algorithm build up the hessian at each step use in ation from the gradient vector at that point
what'r the most popular quasi-newton algorithm bfgs, name after it authors. low memori bfgs, which is a variant that doesn't store the hessian explicit but calcul $\mb{h}^{-1}\mb{g}$ on demand use a quantiti of memori linear in the dimens of the space
if the data is linear seperable, what the mle of logist regress paramet the linear threshold unit
what the linear threshold unit in ml $\mbb{i}(\mb{w}^t\mb{x} w_0)$ which can be thought of as an infinit steep sigmoid
how do you appli $l_2$ regular to logist regress add a penalti term $\lambda \mb{w}^t\mb{w}$ to the logist regress likelihood then pass the modifi gradient amp; hessian into whichev gradient base optim you were alreadi use
what a maximum entropi classifi in ml anoth name for multinomi logist regress
what the likelihood for a multinomi logist regress model $p(i = c \mb{x}, \mb{w}) = \text{softmax}_{c^\prime}(\mb{w}_c \cdot \mb{x})$
what the kroneck product $\mb{a} \otim \mb{b}$ is a matrix where each entri $a_{ij}$ in $\mb{a}$ is replac with $a_{ij} \mb{b}$
how is the mle of a multinomi logist regress model general found same way as for simpl logist regression: deriv the likelihood, it gradient and it hessian, then feed them into a descent-bas optimizer. (prefer low-memori bfgs though, becaus the hessian in this scale with $c^2$, where $c$ is the number of classes)
what prior is usual use for multinomi logist regress model a product of mvns, one for each class paramet vector $\mb{w}_c$ all with mean of $\mb{0}$ and share varianc $\mb{v}_0$
in ml, how the energi function defin it $e(\theta)$ such that $p(\theta \mc{d}) = \frac{1}{z}\exp(-e(\theta))$ where $z= p(\mc{d})$ is the normal constant
what the gaussian approxim to a margin likelihood $$p(\mc{d}) \left(\sqrt{2\pi}\right)^d\frac{e^{-e(\theta^*)}}{\sqrt{ h(\theta^*) }}$$ where $\theta^*$ is the mode of the posterior
what the gaussian approxim to the posterior of a distribut $$p(\theta \mc{d}) \frac{1}{z}e^{-e(\theta^*)}\exp\left(-\frac{1}{2}(\theta - \theta^*)^t\mb{h}(\theta^*)(\theta - \theta^*)\right)$$ where $\theta^*$ is the mode of the posterior
what'r two altern name for the gaussian approxim laplac approximation, though this is share with someth els in statist saddl point approxim in physic
intuitively, how is the bic deriv take the gaussian approxim to the log margin likelihood, then approxim $ \mb{h}(\theta^*) $ by approxim $\mb{h}(\theta^*) = n\mb{\hat h}$, where $\mb{\hat h}$ is a fix matrix
what the occam factor in ml a term that turn up when deriv the bic, $\ln p(\theta^*) - \frac{1}{2}\ln \mb{h}(\theta^*) $, where $\theta^*$ is the posterior mode which repres the model complex
what'r the option for construct predict distribut for logist regress model construct the predict distribut explicit is requir intract integrals, so the altern are: use the posterior mean as a point estim of the paramet mont carlo approxim probit approxim
what the probit function the cdf of the standard normal: $\phi(a) = t_{- fty}^a \mc{n}(x 0, 1)dx$
what the probit approxim in logist regress if you'v got a gaussian approxim to the posterior of a logist regress model you can approxim it predict distribut by substitut $\phi\left(\sqrt{\frac{\pi}{8}}\mb{w}^t\mb{x}\right)$ for $\text{sigm}(\mb{w}^t\mb{x})$ and then correl it with the gaussian posterior
what a qq plot in ml a plot of the quantil of one distribut against anoth
how residu analysi done for a regress model calcul the residu $r_i = y_i - \mb{\hat w}^t \mb{x_i}$ plot the residu on a qq plot against a $\mc{n}(0, \sigma^2)$ distribut point off the $y = x$ line indic possibl outlier
what'r two name for outlier detect in ml residu analysi case analysi
how are outlier defin in a bayesian perspect outlier are point which have low probabl under the cross-valid posterior predict distribut $p(y_i \mb{x}_i, \mc{d}\backslash\{y_i, \mb{x}_i))$
in outlier analysis, how the cross-valid posterior predict distribut usual assess use sampl method
what rock in ml when use a batch algorithm on a dataset that too larg to fit in memori after a pass over the data, rather than immedi load the start of the data again, start at the end and pass \emph{backwards} through it.
how regret defin in ml $\text{regret}_k = \frac{1}{k}\sum_1^k f(\theta_i, z_i) - \min_{\theta^* \theta} \frac{1}{k}\sum_1^k f(\theta^*, z_i)$
intuitively, what regret in ml the differ between the averag loss incur by updat your estim of the paramet $\theta$ after each new sampl $z$ and the smallest loss that could be achiev by maintain some singl estim throughout the process
what the simplest algorithm for onlin regret minim onlin gradient descent: $\mb{\theta}_{k+1} = \text{proj}_\theta(\mb{\theta}_k - \eta_k \mb{g}_k(\mb{\theta}_k))$
what the main differ between optim wrt regret and optim wrt risk in ml minim regret is with respect to past loss - it bayesian minim risk is with respect to futur loss - it frequentist
what stochast optim optim where some of the variabl in the object function are random
what polyak-ruppert averag the averag of a set of valu can be comput onlin as $\bar \theta_k = \frac{k-1}{k} \bar \theta_{k-1} + \frac{1}{k}\theta_k$
what sgd in ml stochast gradient descent, a version of gradient descent to be use with a stochast object
what a learn rate schedul in sgd the set of step size $\eta_k$
what'r the robbins-monro condit $\sum \eta_k = fty$ $\sum \eta_k^2 fty$
what the use of the robbins-monro condit sgd with a learn rate schedul satisfi the robbins-monro condit is guarante to converg
what are two common learn rate schedul in sgd $\eta_k = \frac{1}{k}$ $\eta_k = \frac{1}{(\tau_0 + k)^\kappa}$, where $\tau_0$ slow earli iter and $\kappa$ control how quick old iter are forgotten
how do you usual optim the paramet for sgd it hard in general but store an initi subset of the incom data and test out various paramet on it choos the paramet which lead to the fastest decreas in the objective, and appli them to the rest of the data
what earli stop in ml monitor per anc on the valid set and termin train when the per anc start to drop
what adagrad in ml adapt gradient, a heurist for sgd that scale the step size per feature: $\eta_{ik} = \frac{1}{\tau_0 + g_{ik} }\cdot \eta_k$ where $g_{ik}$ is a vector of the $i$th compon of all previous step
what an epoch in ml when simul onlin data by sampl without replac from the train set, it a singl pass over all the data after which you repeat from the begin
what a mini-batch in ml when work with onlin algorithms, it can be advantag to process new data in chunks. each chunk of $b$ sampl is a mini-batch.
what a typic mini-batch size in ml $b 100$
what'r the two advantag of sgd over standard gradient descent evalu the precis gradient on a larg dataset is often a wast of time, as the gradient will have to be recomput at the next step anyway. it usual better to have a noisi estim and move through the space quickly, which is what sgd does. sgd less like to get stuck in shallow local minima becaus of the `nois implicit in make decis on onli a subset of the data
what the lms algorithm the least mean squar algorithm, a sgd algorithm for comput the mle for linear regresson
what the updat rule in the lms algorithm $\mb{\theta}_{k+1} = \mb{\theta}_k - \eta_k(\mb{\theta}^t_k \mb{x}_k - y_k)\mb{x}_k$ ie the step is $\mb{x}_k$ weight by the size of the error in the predict
what'r two other name for the lms algorithm the delta rule the widrow-hoff rule
what'r the step in the perceptron algorithm let $y_i \{-1, +1\}$. the perceptron algorithm is descent algorithm with the updat rule $\mb{\theta}_k = \mb{\theta}_{k-1} + \eta_k(\text{sign}(\mb{\theta}_k^t \mb{x}_i) - y_i)\mb{x}_i$
what the intuit explan of the perceptron algorithm it a descent algorithm for solv binari logist regress problem at each step, if the predict base on the current paramet is correct, noth is done. if the predict is wrong and $\mb{x}_i$ is misclassified, a step is taken in the direct of $\mb{x}_i$.
when will the perceptron algorithm work it'll converg exact when the data is linear separ
what the bayesian approach to onlin learn $p(\theta \mc{d}_{1:k}) \propto p(\mc{d}_{k} \theta, \mc{d}_{1:k-1})p(\theta \mc{d}_{1:k-1})$
whi can the bayesian approach to onlin learn end up faster than sgd by model the posterior varianc of each parameter, a differ learn rate is effect associ with each paramet which is a simpl way to model the curvatur of the space
which of generat and discrimin classifi are general easier to fit generative; generat classifi like gda just need count and algebra, while discriminit classifi like logist regress requir solv optim problem
which of generat and discrimin classifi can easili add a new class generative; in discrimin models, all the paramet interact, so the model must be retrain if a new class is ad
which of generat and discrimin classifi general handl miss featur better generat sinc discrimin model assum that $\mb{x}_k$ is alway avail to be condit upon
which of generat and discrimin classifi are general better for deal with unlabel data generat
which of generat and discrimin classifi can general be invert to comput $p(\mb{x} y)$ generative, becaus it deal with $p(y, \mb{x})$ and so treat each paramet symmetr
which of generat and discrimin classifi can general handl input preprocess discrimin where thing like basi function expans are accept while in generat models, tran ation of the input tend to lead to featur correl in complex way
which of generat and discrimin classifi general make stronger assumpt generat becaus generat classifi tend to make strong independ assumpt that are rare valid
what'r three of the most use properti of the exponenti famili of distribut they'r the onli famili with conjug prior subject to certain constraints, they'r the onli distribut with finite-s suffici statist subject to certain constraints, they'r the famili with the least number of assumpt
when is a distribut a member of the exponenti famili when it can be written as $p(\mb{x} \mb{\theta}) = h(\mb{x}) \exp[\eta(\mb{\theta})^t \phi(\mb{x}) - a(\eta(\mb{\theta}))]$
what $\phi(\theta)$ in the definit of a member of the exponenti famili a vector of suffici statist
what $a(\mb{\theta})$ in the definit of a member of the exponenti famili the cumul function, defin as $a(\mb{\theta}) = \ln z(\mb{\theta})$ where $z$ is the partit function
what $h(\mb{x})$ in the definit of a member of the exponenti famili the scale constant
what a natur exponenti famili one where $\phi(\mb{x}) = \mb{x}$
what anoth name for the cumul function the log partit function
how is the partit function defin for a member of the exponenti famili $z(\mb{\theta}) = t_{\mc{x}} h(\mb{x})\exp[\mb{\theta}^t\phi(\mb{x})] d\mb{x}$
when a distribut a member of a curv exponenti famili when it can be written in the of an exponential-famili distribut and $\dim(\mb{\theta}) \dim(\mb{\eta(\theta)})$
what $\eta$ in the definit of a member of an exponenti famili the map from the paramet $\mb{\theta}$ to the canon paramet $\mb{\eta}$
when a distribut in the exponenti famili in canon when $\eta(\mb{\theta}) = \mb{\theta}$
what doe it mean for a represent of a famili of distribut to be minim each distribut in the famili is uniqu identifi by a set of the famili paramet
what doe it mean for a represent of a famili of distribut to be overcomplet some distribut in the famili is encod by more than one set of the famili paramet
what the canon of the bernouilli distribut as a member of the exponenti famili $\text{ber}(x \mu) = (1-\mu)\exp\left[x \ln \left(\frac{\mu}{1-\mu}\right)\right]$
what'r the canon paramet of a famili of exponenti distribut they'r the $\mb{\theta}$ when $\eta = \text{id}$
what'r the canon paramet of the bernouilli distribut $\theta = \ln \left(\frac{\mu}{1-\mu}\right)$
what'r the canon paramet of the multinouilli distribut $\left[\ln \frac{\mu_k}{\mu_k}\right]_k$ for $1 k k-1$ (the $k$th term is exclud becaus the $\mu_k$ must sum to 1)
what the cumul function for the multinouilli distribut $a(\theta) = -\ln \mu_k$
what'r the simplest suffici statist for the univari gaussian distribut $\phi(x) = (x, x^2)$
what'r the canon paramet for the univari gaussian $\mb{\theta} = \left(\frac{\mu}{\sigma^2}, -\frac{1}{2\sigma^2}\right) = \left(\lambda \mu, -\frac{1}{2}\lambda \right)$ (the $-\frac{1}{2}$ can be embed in the suffici statist mind you)
what'r the first and second cumul of a distribut $\mbb{e}[x]$ $\text{var}[x]$
what are the first and second moment of a distribut $\mbb{e}[x]$ $\mbb{e}[x^2]$
what the relationship between cumul and moment two distribut with the same cumul will have the same moments, and vice versa cumul are just easier to work with in some problems, and moment are easier to work with in other
how'r the cumul of a distribut defin the $n$th cumul $\kappa_n$ is the $n$th coeffici in the maclaurin expans of the cumul generat function: $g(t) = \sum_{1}^ fti \kappa_n \frac{t^n}{n!}$
how the cumul generat function defin $g(t) = \ln \mbb{e}[e^{tx}]$
how the moment generat function defin $m(t) = \mbb{e}[e^{tx}]$
how are cumul calcul from the cumul generat function to get the $n$th cumul differenti $g(t)$ $n$ time and evalu it at zero
what the main advantag of cumul over moment $g_{x+y}(t) = g_x(t) + g_y(t)$
intuitively, what the pitman-koopman-darmoi theorem it say that under certain regular conditions, distribut in the exponenti famili are the onli distribut with finit suffici statist
what the main restrict of the pitman-koopman-darmoi theorem it onli appli to distribut with support that independ of the paramet
what the idea in the method of moment draw a sampl estim the popul moment from the sampl use the popul moment to deduc the popul paramet
how the likelihood of a distribut in the exponenti famili written $p(\mc{d} \theta) \propto \exp(n\eta^t \bar s - na(\eta))$ where $\eta$ are the canon paramet and $\bar s$ is the sampl mean of the suffici statist
how the prior of a distribut in the exponenti famili written $p(\theta \nu_0, \tau_0) \propto \exp(\nu_0 \eta^t \bar \tau_0 - \nu_0 a(\eta))$ where $\nu_0$ is the size of the pseudo-data and $\bar \tau_0$ is the mean of the suffici statist on the pseudo-data
given a conjug prior, how the posterior of a distribut in the exponenti famili usual written use the same distribut with size $\nu_0 + n$ suffici statist $\frac{\nu_0 \bar \tau_0 + n \bar s}{\nu_0 + n}$ where $\nu_0$ is the size pseudo-data of the prior, $\bar \tau_0$ is the mean of the suffici statist of the pseudo-data $\bar s$ is the mean of the suffici statist of the data
what the definit of the maxent distribut given $k$ constraint $\mbb{e}_p[f_k(x)] = f_k$ then the maximum entropi distribut of all those satisfi the constraint is $p(x \lambda) \propto \exp\left(-\sum \lambda_k f_k(x)\right)$ where $\lambda_k$ is the invers temperatur wrt the $k$th constraint, chosen to satisfi the constraint and unitar
what a general linear model a general of linear regress whose output distribut is in the exponenti famili (rather than just be normal)
what $\psi$ usual denot in general linear model the invert function that take the mean paramet $\mu$ to the canon paramet $\theta$ $\psi(\mu) = \theta$
in general linear models, how $\psi$ relat to the cumul function $\psi^{-1}(\theta) = a(\theta)$
what $\eta_i$ repres in general linear model $\eta_i = w^tx_i$, some linear function of the input
what the mean function in general linear model the $g^{-1}$ that relat the linear function of the input $\eta$ to the mean paramet $\mu$ $g^{-1}(\eta) = \mu$
what the link function in general linear model the invers of the mean function $g(\mu) = \eta$
what'r the usual restrict on the link function in general linear model $g$ must be invert $g^{-1}$ must rang over all possibl $\mu$
what the canon link function $g(\mu) = \psi(\mu)$ so $\theta = \eta = w^t x$
what $\sigma^2$ usual repres in general linear model the dispers parameter, which is proport to the varianc in the respons usual 1
what the of a general linear model with a scalar respons variabl $p(y_i \theta, \sigma^2) = \exp\left[\frac{1}{\sigma^2}(y_i \theta - a(\theta)) + c(y_i, \sigma^2)\right]$ where $c$ is the normal constant
how do you fit a general linear model same way as a logist model: gradient descent amp; co
what the fisher score method use the expect hessian (ie the fisher in ation matrix) instead of the actual hessian usual in gradient descent method
how is bayesian infer usual carri out use general linear model via mont carlo markov chain method sometim gaussian approxim or variat infer
what probit regress a binary-respons general linear model where the mean function is the probit function $g^{-1}(\eta) = \phi(\eta)$ veri similar to logist regress
what a random util model set the util for respons $y_i = c$ on input $x_i$ to $u_{c,i} = w^tx_i + \delta_{c,i}$ where $\delta_{c,i}$ is a normal random variable. then predict the respons to be the choic with the greatest utility: $y_i = \arg \max_c u_{c, i}$
what the latent variabl interpret of probit regress it a binary-respons random util model with normal random util error
what model by the random part of a random util model factor that might be relev to decis make that we havent (or can't) explicit includ in the model
what ordin probit regress binari probit regress general from the latent variabl perspective: rather than the normally-distribut difference-of-util $z_i$ simpli be judg against a singl decis boundari $z_i = 0$ instead the real line is partitioned, with partit $j$ correspond to the respons $y = j$
what a multivari probit model it a general of the latent variabl perspect on binari probit regression: instead of pick the class with the greatest random util the respons vector has one binari variabl $y_{ic}$ for each class $c$, and $y_{ic} = 1$ if the random util $u_{ic} 0$
what'r two other name for multi-task learn transfer learn learn to learn
how hierarch bay usual use for multi-task learn suppos we have $j$ model $p(x \beta_j)$ then draw the $\beta_j$ from some prior $p(\beta_j \beta_*)$ and train the model by optim over both the $\beta_j$ and $\beta_*$ onc it done, $\beta_*$ can be discard
what a simpl trick use to appli hierarch bay to spam filter creat copi of each featur $x_i$: $x_i$, repres the featur $(x_i, j)$ for user 1 through $j$, repres the featur occur for user $j$ train this model is equival to train a hierarch bay model
what domain adapt in ml a kind of multi-task learn where the task are all the same ex: train a classifi on both email and news stori
what conjoint analysi in ml figur out which featur of a product consum like best reduc to multi-task featur select
what negat transfer in ml when a multi-task approach doe wors than solv each task separ due to correl in the model that don't actual exist in the data
what a glmm general linear mixed-effect model
what'r `fix effect in ml a frequentist term that refer to unknown paramet $\alpha$ that'r share between group
what `random effect in ml a frequentist term refer to unknown paramet $\beta_j$ that vari random between group
what the of a glmm for group $j$ and item $i,j$ $\mbb{e}[y_{ij} x_{ij}, x_j] = g(\phi_1(x_{ij})^t\beta_j + \phi_2(x_j)^t\beta^\prime_j + \phi_3(x_{ij})^t\alpha + \phi_4(x_j)^t\alpha^\prime)$ where $x_{ij}$ are the item featur and $x_j$ are group featur
what a mix model in ml a model with both fix and random effect
what gee in ml general estim equations, a frequentist approach to deduc the popul paramet of a glmm statist ineffici though and not recommend
how do you usual fit a glmm model fulli bay methods: variat bayes, mont carlo markov chain empir bayes: expect maximization, variat expect maxim
what'r letor problem in ml learn to rank problem
what the pointwis approach to letor calcul the `relev of each item to a queri then sort by relev
what the problem with the pointwis approach to letor it penal error low down in the list as much as it doe error at the top it make relev decis without know anyth about other possibl relev document
what the pairwis approach to letor for each pair of item $j, k$ let $y_{jk}$ be the indic that $j$ has a greater relev than $k$ then learn a model for the $y_{jk}$
what ranknet in ml a neural network for pairwis letor that use the model $p(y_{jk} = 1 x_j, x_k) = \text{sigm}(w^t(x_j-x_k))$
what the plackett-luc distribut for a permut $\pi$, $p(\pi s) = \prod_i \frac{s_i}{\sum_{i j} s_j}$ where $s_j$ is the score of the item in the $j$th posit in the output under some score function $s$
how is the plackett-luc distribut deriv by start at the top of the list and condit on item which have alreadi been ranked: so if $\pi = (a, b)$ then $p(\pi) = p(\text{$a$ rank first})p(\text{$b$ rank second} \text{$a$ rank first})$ where the probabl of an option be rank is proport to it score compar to the score of all the item which have yet to be rank
what the listnet model in ml an approach to pairwis letor use the plackett-luc model with a linear score function
what the equival of maxim the likelihood in term of cross-entropi minim the cross-entropi loss between the posterior amp; empir distribut
what mean reciproc rank in ml a loss function for rank system defin as the mean of $1/r(q)$ over all queri $q$, where $r(q)$ is the rank of the first relev document in respons to $q$
what the precis at $k$ in ml rank problem for a permut $\pi$, $\text{p@k}(\pi) = \frac{1}{k}\text{(numb of relev document in the first $k$ positions)}$
what the averag precis in ml rank problem given a permut $\pi$, $\text{ap}(\pi) = \left( \sum_\text{k is relevant} \text{p@k}(\pi) \right) / \text{numb of relev documents}$
what the mean averag precis in ml rank problem it a loss function, defin as the mean $\text{ap}$ over all queri
what the discount cumul gain in ml rank problem a loss function. given the relev $r$ in rank order, $\text{dcg@k(r)} = \sum_{i k} d(i)r_i$ where $d$ is the discount function
what the problem with discount cumul gain, and how is it usual solv it depend on the length of the list return and it solv by use the ideal dcg, which is the maximum dcg taken over all possibl permut
what'r kendal $\tau$ statist a way of measur the correl between two rankings: $\tau(\pi, \pi^*) = \frac{1}{2\sum_{u v} w_{uv}} \cdot \sum_{u v} w_{uv}[\text{2 if $u, v$ are in the same order in both rankings, 0 otherwise}]$ aka the weight pairwis inconsist
how are loss function use in bayesian approach the model is fit use sole the likelihood and the prior and then action (such as set the threshold) can be chosen at test time to minim futur expect loss
how are loss function use in frequentist approach tri and minim empir loss on the train set
what the use of a surrog loss function in ml frequentist tri and minim the expect loss on the train set, but loss function typic aren't diff'abl wrt the model paramet so often a surrog loss function - like the negat log likelihood - is use instead.
what the weight approximate-rank pairwis loss a surrog rank loss function that approxim the precision@k loss: to calcul the $\text{warp}$ on score function $f$, queri $x$ and label $y = c$ calcul the score $f(x,c^\prime)$ for each $c^\prime$ and find the rank $k$ of label $c$ then the $\text{warp}$ is the sum of the first $k$ weight in the nonincreas sequenc $\alpha_i$
what the most common exampl of a glm logist regress
what a right stochast matrix a nonneg matrix whose row all sum to 1
what a condit probabl tabl a tabl of the valu of $p(i \mb{x})$ for all the valu the discret $\mb{x}$ can take on
what the first order markov assumpt $x_{t+1} x_{1:t-1} x_t$
what the intuit interpret of the first order markov assumpt given the present, the futur is independ of the past
what a graphic model in ml a way to repres independ assumpt as a lack of edg between the node that repres differ variabl
what an altern name for graphic model in ml independ diagram
what a direct graphic model in ml a graphic model whose graph is a dag
what'r some altern name for dgms bayesian network belief network causal network
what the order markov properti given a direct graphic model and a topolog ordering, it the assumpt that a node is independ of all it predecessor except it parents, given it parent
what a tan model in ml tree-aug naiv bayes; naiv bay weaken to the assumpt that the correl in it independ variabl can be repres by a tree dgm
what a hidden markov model a model which has two kind of variables: the hidden variabl $z_t$ which a markov chain the observ variabl $x_t$, each of which depend onli on the correspond $z_t$
what state estim in ml infer the state of some hidden variabl given the valu of the observ variabl amp; the model paramet
what'r the two part of the model of a hidden markov model the transit model, describ by $p(z_t z_{t-1})$ the observ model, describ by $p(x_t z_t)$
what a probabilist expert system a probabilist model - usual a graphic model - creat by hand
what knowledg engin in ml the process of convert some domain expertis to a model
what a leak node in ml a variabl in a graphic model which in some way repres thing that the model doesn't explicit emb (like an unknown diseas in a medic diagnosi model)
what a bn2o model in ml a bipartit graphic model with noisi observ variabl which are an or of the hidden variabl
what a direct ggm a direct gaussian graphic model, a dgm where the cpds all have the $p(x_t x_{\text{pa}(t)}) = \mc{n}(x_t \mu_t + w_t^tx_{\text{pa}(t)}, \sigma_t^2)$
what the joint distribut on a ggm if the cpd for variabl $t$ has mean $\mu_t$, varianc $\sigma^2_t$ and weight $w_{ts}$, then the joint is a normal distribut with $\mu = [\mu_i]$ $\sigma = (i - w)^{-1}\text{diag}(\sigma^2_i)(i - w)^{-t}$
what a cpd in ml condit probabl distribut
what anoth name for direct ggms gaussian bay net
what the probabl of the evid in ml given a set of visibl variables, it $p(x_v \theta)$ aka the likelihood of the data
what a common partit of the hidden variabl in ml queri variabl $x_q$, which we'r interest in know the state of nuisanc variabl $x_n$, which we'r not
how are nuisanc variabl dealt with in probabilist infer they'r margin out
what the differ between infer and learn to a bayesian ideolog there isn't one. but there are usual a lot more hidden variabl than there are parameters, so general hidden variabl should be integr out sinc point method would overfit
what it mean for a set of variabl to be exchang in ml exchang their indic doesn't chang the suffici statist
what plate notat in ml if some part of a graphic model is in a box with a number $n$ in the bottom-right then that repres the same part repeat $n$ time
what complet data in ml a model in which all the variabl are fulli observ so no hidden variabl or miss data
what the main advantag of have complet data in a dgm the likelihood decompos accord to graph structur
what it mean for the likelihood of a dgm to decompos accord to the graph structur the likelihood can be written as $p(\mc{d} \theta) = \prod p(\mc{d}_t \theta_t)$ where $\mc{d}_t$ is the data for variabl $t$ and it parent
what the advantag of have a dgm likelihood decompos accord to the graph structur if the prior factor over the variabl too ie $p(\theta) = \prod p(\theta_t)$ then the posterior factor over the graph structur too, so the posterior for each variabl can be calcul independ
what an i-map in ml a graph $g$ is an i-map for a distribut $p$ if $i(g) i(p)$
what $i(g)$ for a graph $g$ denot in ml the set of all condit independ assumpt encod in the graph
what $i(p)$ for a distribut $p$ denot in ml the set of all condit independ assumpt that hold for $p$
what the use of i-map in ml if $g$ is an i-map for $p$ then it `safe to use $g$ to reason about the condit independ properti of $p$
what a minim i-map for a distribut $p$ the graph $g$ which is an i-map for $p$ such that no $g^\prime \subset g$ is also an i-map for $p$
what d-separ in ml an undirect path $p$ in a graphic model is d-separ by a set $e$ iff $p$ contain $s \rightarrow m \rightarrow t$ such that $m e$ or $p$ contain $s \leftarrow m \rightarrow t$ such that $m e$ or $p$ contain $s \rightarrow m \leftarrow t$ such that $m \not e$ and no descend of $m$ is in $e$ either
what it mean for a set $a$ to be d-separ from $b$ by $e$ in ml over the graphic model $g$, everi path from $a$ to $b$ is d-seper by $e$
what the direct global markov properti on graphic model $x_a _g x_b x_e$ if and onli if $a$ is d-separ from $b$ by $e$
what the use of the bay ball algorithm an algorithm for decid whether set of variabl $a, b$ are d-separ by $e$
intuitively, how doe the bay ball algorithm work shade all the variabl in the evid set $e$ place `ball at each node in $a$ let the ball move around, ignor direct in general but obey certain rules. see if ani of the ball end up in $b$
what'r the rule on the ball movement in the bay ball algorithm a ball can't pass through a chain $x \rightarrow y \rightarrow z$ if $y$ is shade a ball can't pass through a fork $x \leftarrow y \rightarrow z$ if $y$ is shade a ball can \emph{only} pass through a collid $x \rightarrow y \leftarrow z$ if $y$ is shade
what berkson `paradox suppos you have two independ coins. condit on their sum coupl them.
what'r two other name for berkson paradox explain away intercaus reason
what the direct local markov properti $t (\text{nd}(t) \backslash \text{pa}(t)) \text{pa}(t)$ where $\text{nd}(t)$ are the non-descend of $t$
how are the direct local, direct global and order markov properti relat they'r equival
what the markov factor properti ani distribut $p$ which is markov wrt a graph $g$ can be factor over the graph structur
what $\text{mb}(t)$ denot in ml graphic model the markov blanket of $t$, the set of node that render $t$ condit independ of everi other node in the graph
what'r the copar of a node in a graph the parent of the node children
what the markov blanket of a node in a dgm compris of the parent of the node the children of the node the copar of the node
what the full condit of a node in a dgm the condit distribut of a node wrt it markov blanket
what'r influenc diagram in ml dgms augment with decis node (rectangles) util node (diamonds)
what anoth name for influenc diagram in ml decis diagram
what an in ation arc in an influenc diagram an edg in the graph from a chanc to node to a decis node repres that the outcom of the rv contribut to the decis
what the valu of perfect in ation of a variabl in ml the differ in util between the expect payoff and the expect payoff if the state of the variabl is avail to contribut to your decis
what a pomdp in ml a partial observ markov decis process ie a hidden markov model augment with decis amp; payoff compon
what a mdp in ml a markov decis process ie a fully-observ markov model augment with decis amp; payoff compon
what'r lvms in ml latent variabl model where the observ variabl are correl becaus they aris from some set of hidden variabl
what doe $p_k(x)$ denot when deal with lvms in ml the $k$th base distribut $p(x z = k) = p_k(x)$ where $z$ is a discret latent variabl
what a mixtur model in ml a latent variabl model with a singl discret scalar latent variabl so call becaus the distribut over the observ variabl is a mixtur of the base distribut
what of lvms are we usual concern with lvms with a one-to-on map between latent and observ variabl sinc if they'r vector-valu variables, this can repres ani of the other kind of lvm
what a mog model in ml mixtur of gaussians, a mixtur model where each base distribut is a gaussian
what a use properti of mog model they can approxim ani distribut on $\mbb{r}^d$
what'r the two main applic of mixtur model black-box densiti model cluster
what a mixtur of multinouilli model a mixtur model where the condit densiti is a product of multinouilli distributions, one for each output scalar
what the respons in ml cluster the respons of cluster $k$ for point $i$ is the posterior probabl that $i$ belong to cluster $k$ $r_{ik} = p(z_i = k x_i)$
what soft and hard cluster in ml soft cluster involv comput the posterior hard cluster just use the map to assign a point to a cluster
what a mixtur of expert model in ml a mixtur model where the mix weight and mixtur densiti can depend on the input allow differ part of the input space to be `judg by differ distribut
what a gate function in ml in a mixtur of expert model, it a function of the input which determin which compon of the input space are judg by which `expert
what a mixtur densiti network in ml a variant of a mixtur of expert model where both the gate function and the expert distribut are implement by neural network these are more flexibl than a moe but slower to train
what a hierarch mixtur of expert model in ml an extens of the mixtur of expert model where each expert is itself a mixtur of expert
what the unidentifi problem in ml in a lvm with a $k$ latent variabl the posterior can be multi-modal, with each peak correspond to a differ label of the variabl (up to $k!$ peak in fact)
how can unidentifi caus a problem for bayesian infer if you draw sampl from a multi-mod distribut and tri to averag them to estim the mean you'll end up with a meaningless point somewher between the mode
if you draw a number of sampl from a posterior distribut which has unidentifi variables, whi is it okay to averag the predict distribut that aris from use these sampl as paramet estim the likelihood multimod becaus it can't distinguish between label and so it also invari as to which mode the paramet estim came from
what the intuit interpret of the em algorithm `e step: infer the valu of the latent variabl `m step: optim the paramet given the infer latent variabl repeat
what the complet data log likelihood in ml $l_c(\theta) = \sum \ln p(x_i, z_i \theta)$ typic this isn't direct computable, as the $z_i$ are hidden
what the auxiliari function in the em algorithm $q(\theta, \theta^{t-1}) = \mbb{e}[l_c(\theta) \mc{d}, \theta^{t-1}]$ where $t$ is the current iter
how the em algorithm defin in term of the auxiliari function e step: construct the function $q(\theta, \theta^{t-1})$ m step: comput $\theta^t = \arg \max_\theta q(\theta, \theta^{t-1})$
what the usual use of the em algorithm estim the mle or map of a latent variabl model
what the e step in the em algorithm for a gaussian mixtur model with $k$ base functions, comput the respons $r_{ik} = \frac{\pi_k p(x_i \theta_k^{t-1})}{\sum \pi_{k^\prime} p(x_i \theta_{k^\prime}^{t-1})}$
what the m step in the em algorithm for gaussian mixtur model with $k$ base functions, $\pi_k = \frac{1}{n} r_k$ $\mu_k = \sum_i \frac{r_{ik}}{r_k} x_i$ $\sigma_k = \sum_i \frac{r_{ik}}{r_k} (x_i - \mu_k) (x_i - \mu_k)^t$ $\theta^t = [(\pi_k, \mu_k, \sigma_k)]_k$ where $r_k = \sum_i r_{ik}$
what a common preprocess step to carri out on data befor appli ml algorithm subtract the mean divid by the standard deviat this can speed converg
how doe the k-mean algorithm relat to em it em for gmms with $\pi_k = \frac{1}{k}$ and $\sigma_k = \sigma^2 i$ fixed, leav onli $\mu_k$ free and the respons estim as $r_{ik} \mbb{i}(k = z_i^*)$, where $z_i^* = \arg \max_k p(z_i = k x_i, \theta)$
what the e step in the k-mean algorithm calcul $z^*_i = \arg \min_k x_i - \mu_k $ and assign point $i$ to cluster $z^*_i$
what the m step in the k-mean algorithm calcul $\mu_k = \frac{1}{n_k} \sum_{i : z_i^* = k} x_i$ ie the mean of the point in cluster $k$
how the reconstruct error usual defin for compress algorithm given input data $[x_i]$, $j = \frac{1}{n}\sum_i x_i - \text{decode}(\text{encode}(x_i)) ^2$ (or anoth loss function than the squar distance)
what the compress interpret of the k-mean algorithm the k-mean algorithm can be thought of as an iter scheme for minim the error of a vector quantit scheme with a $k$ vector codebook where the prototyp vector are $\mu_k$
what a vector quantit scheme a compress scheme for vector data with a codebook consist of $k$ `prototyp vector datapoint are map to the nearest prototyp
what the rate of a compress scheme the number of compress bit need per bit of input
what farthest point cluster in ml a em-for-clust initi scheme where the first point is pick random from the dataset and subsequ point are pick to be as far from those alreadi select as possibl
what the `growth approach to initi em for cluster score each cluster base on the weight assign to it if some cluster score suffici high, split it in two, with the paramet of the two new cluster be random perturb of the old one
what the collaps varianc problem in ml when fit someth like a gmm set the centr of one of the gaussian to be equal to one of the datapoints, then let the varianc go to zero s in an infinit likelihood
what the solut to the collaps varianc problem enforc a prior on the paramet and work with the posterior rather than the likelihood
when appli em to calcul the map estim of a gmm, what prior is usual use dirichlet on the mixtur weight normal invers wishart on the mean and varianc
how doe em usual fail through numer issu with close-to-singular matric
what'r common set for the niw hyperparamet when estim the map of a gmm use em $\kappa_0 = 0$, so the mean are unregular $s_0 = \frac{1}{k^{1/d}}\text{diag}(s_1^2, \dotsc, s_d^2)$, where $s_j^2$ is the pool varianc for dimens $j$ $\nu_0 = d+2$, indic the weakest possibl proper prior
what the of the usual gate function for mixtur of expert model $\pi_{ik} = \mc{s}(v^tx_i)_k$ where $\pi_{ik}$ are the mixtur weight and $v$ are the gate paramet
what a famili margin in ml for a graphic model, it $p(x_{it}, x_{i,\text{pa}(t)} \mc{d}_i, \theta)$ ie the joint posterior on a given node and it parent
what'r the ess in the em algorithm the expect suffici statistics, the output of the e step
how do you calcul the mle of a student distribut use the scale mixtur construct of the student distribut then appli iter method like em
how can student distribut be written as a mixtur of gaussian with a gaussian scale mixture: $$\mc{t}(x \mu, \sigma, \nu) = t \mc{n}\left(x \middl \mu, \frac{1}{z}\sigma\right) \text{ga}\left(z \middl \frac{1}{2}\nu, \frac{1}{2}\nu\right) dz$$
how do you use the em algorithm to find the mle of a student distribut approxim the distribut with a gaussian scale mixtur integr treat the scale paramet as the latent variabl
what a general em algorithm em where the m step involv an iter optim algorithm rather than a closed- updat
what the conjug distribut to the categor distribut the dirichlet distribut
what the digamma function $\psi(x) = \frac{d}{dx}\ln \gamma(x)$
what prior do you use when appli em to probit regress $\mc{n}(0, v_0)$
what jensen inequ in probabl theori for a convex $\varphi$, $\varphi(\mbb{e}[x]) \mbb{e}[\varphi(x)]$
what'r the main step in the theoret deriv of the em algorithm show that the expect complet data log likelihood (ie $q$) is a lower bound on the observ data log likelihood (ie $l$) show that it a tight lower bound the lower bound is monoton increas wrt the iter count $t$ so the observ data log likelihood is monoton increas too
what the general structur of the batch em algorithm if $s_i(\theta)$ is the e step of calcul expect suffici statist for data case $i$ assum paramet $\theta$ and $\theta(\mu)$ is the m step of calcul new paramet from the sum of the ess then batch em involv comput $\mu^\text{new} = \sum s_i(\theta(\mu))$, then updat $\mu = \mu^\text{new}$ until converg
what the general structur of the increment em algorithm if $s_i(\theta)$ is the e step of calcul expect suffici statist for data case $i$ assum paramet $\theta$ and $\theta(\mu)$ is the m step of calcul new paramet from the sum of the ess then increment em involv run through $i$ in a random order, calcul $s^\text{new}_i = s_i(\theta(\mu))$, then $\mu = \mu + s_i^\text{new} - s_i$, then updat $s_i = s_i^\text{new}$ until converg
what the general structur of the stepwis em algorithm if $s_i(\theta)$ is the e step of calcul expect suffici statist for data case $i$ assum paramet $\theta$ and $\theta(\mu)$ is the m step of calcul new paramet from the sum of the ess then stepwis em involv run through $i$ in a random order, and in step $k$ comput $\mu = (1-\eta_k)\mu + \eta_k s_{i}(\theta(\mu))$ until converg where $\eta_k$ is the step size, which must satisfi the robbins-monro condit
when implement an updat scheme $\mu = (1-\eta_k)\mu + \eta_k s$ in increment em, how can you modifi it to make it a spars updat defin $m = \frac{1}{\prod_{j k-1} (1 - \eta_j)}\mu$ then use updat $m = m + \frac{\eta_k}{\prod_{j k} (1 - \eta_j)}s$ in the context of em, scale the by a global constant has no effect
how do batch, increment and stepwis em compar stepwis onli need constant memori stepwis amp; increment are much faster than batch accuraci of stepwis is as good as batch accuraci of increment is wors than both
what anneal em an em variant in which the `temperatur of the posterior is raised, smooth it the global maximum is then found and track while the temperatur fall
what mont carlo em em where in the e step, the expect suffici statist are approxim by averag sampl $z_i^ p(z_i x_i, \theta^t)$
what stochast em similar to mont carlo em, but onli one sampl is drawn
what variat em a variant of em for when it difficult to infer the log likelihood in the e step instead, a variat lower bound on the likelihood is use
what'r two name for the general class of algorithm that includ em bound optim algorithm minorize-maxim algorithm
what over-relax em em where the paramet are updat as a linear interpol of their old valu and their traditional-em-calcul valu
in unsupervis frequentist learning, what the usual way to select how mani paramet your model should use ex: number of base distribut in a mixtur model look for the `knee or `kink in the test set mse curv wrt increas paramet count should go from a steep drop (indic the extra paramet are useful) to a shallow drop (indic the extra paramet might be `unnatural')
what the usual tool for autom `kink find in model select gap statist
what the model use in factor analysi it a lvm with $d$ observ variabl and $l$ latent variables, and $p(x z, \theta) = \mc{n}(wz+\mu, \psi)$ $z \mc{n}(0, i)$ where $\psi$ is a diagon $d d$ matrix
what the factor load matrix in factor analysi the matrix $w$ which relat the latent variabl to the mean of the distribut
how probabilist principl compon analysi relat to fa it factor analysi with an orthonorm $w$ and $\psi = \sigma^2 i$
whi the varianc matrix diagon in factor analysi becaus it forc correl to be explain by the latent variabl rather than be `bake in to the varianc matrix
how can factor analysi be thought of in term of mvns fa is a low rank parameter of an mvn; if you margin out the latent variables, the visibl variabl are distribut as a mvn with covari $ww^t + \psi$ ie a mvn with $o(ld)$ parameters, as oppos to the $o(d^2)$ paramet of a full mvn
what a one-hot encod an encod scheme where the codeword are binari number with one set bit and the other clear
what'r the latent factor in fa if $p(z_i x_i, \theta) \mc{n}(m_i, \sigma_i)$ then $m_i$ are the latent factor
what anoth name for the latent factor in fa latent score
what a biplot a project of high-dimension data onto 2d where data are ed as point the unit vector along each dimens are ed as vector
what the caus of the unidentifi problem in fa the likelihood is invari under ani rotat of the factor load matrix $\hat w = rw$ where $rr^t = i$
how mani orthonorm matrici of size $l l$ are there $\frac{1}{2}l(l-1)$
what'r founder variabl in fa if you solv the unidentifi problem by make the weight matrix lower triangular then the first $l$ of $d$ visibl variabl affect the interpret of the latent factors, so must be chosen care
what spars factor analysi fa, with the unidentifi problem solv by enforc a spars prior on the weight
what varimax in fa a solut to the unidentifi problem which tri to find a rotat matrix $r$ such that $rw$ is spars and henc more easili interpret
what the mixtur of factor analyz model take $k$ fa model $w_k, \mu_k$ and have $x_i$ model by the one indic by $q_i \{1, \dotsc, k\}$
how doe a mixtur of factor analyz compar to a mixtur of gaussian mfa is a low-rank approxim of a gmm, have $o(kld)$ paramet compar to $o(kd^2)$
how principl compon analysi compar to fa it fa with an orthonorm $w$ and $\psi = \sigma^2 i$ with $\sigma^2 \rightarrow 0$
what anoth name for probabilist principl compon analysi sensibl pca
what anoth name for pca in ml the karhunen loev tran
what the synthesi view of classic pca given observ $x$, suppos we want to find an orthonorm basi $\{w_j\} \subset \mbb{r}^d$ and coeffici $\{z_i\} \subset \mbb{r}^l$ such that $j(w, z) = x - wz^t ^2_f$ is minim the solut is $\hat w$, compos from the eigenvector of the $l$ largest eigenvalu from the empir covari matrix $\hat \sigma$ and $\hat z_i = w^tx_i$
what the frobenius norm $ a _f = \sqrt{\sum_{ij} a_{ij}^2} = \sqrt{\text{tr}(a^ta)}$
what the first principl compon in pca of the eigenvector that are drawn from $\hat \sigma$ and are use to build $\hat w$, it the one with the largest eigenvalu
intuitively, what do the principl compon in pca correspond to the direct in which the data has maxim varianc
what preprocess need to be done befor pca is carri out the data need to be center the varianc in each direct need to be normal
how the correl of two random variabl defin $\text{corr}(x, y) = \frac{\text{cov}(x, y)}{\sigma_x \sigma_y}$ ie the pearson correl coeffici
what the analysi view of pca minim the reconstruct error $j(w, z) = x- wz^t ^2$ is equival to find the one-el basi $\{w_1\}$ which maxim the varianc of $\{z_{1j}\}$ then find the basi vector $w_2$ which, when use to the basi $\{w_1, w_2\}$ maxim the varianc of $\{z_{2j}\}$ given that the $\{z_{1j}\}$ are alreadi fix etc
how do the eigenvector of $x^tx$ relat to the svd of $x$ if $x = usv^t$ then $x^tx$ eigenvector are the column of $v$, with eigenvalu $s^2_{ii}$ aka $v = \text{evec}(x^tx)$, $s^2 = \text{eval}(x^tx)$
how do the eigenvector of $xx^t$ relat to the singular valu of $x$ if $x = usv^t$ then $xx^t$ eigenvector are the column of $u$, with eigenvalu $s^2_{ii}$ aka $u = \text{evec}(xx^t)$, $s^2 = \text{eval}(xx^t)$
what the truncat svd of a matrix via the svd, we can write $x = \sum \sigma_i u_i v^t_i$ with the $\sigma_i$ in order of decreas magnitud the truncat svd of rank $l$, $x_l$, is what you get when you throw away all but the first $l$ term of this expans
what the approxim error in a truncat svd approxim for a rank $l$ approxim it $ x-x_l _f \sigma_l$ and it the lowest error possibl out of all rank $l$ approxim to $x$
how pca relat to svd pca with $l$ latent variabl on data $x$ is exact the same as the rank $l$ truncat svd approxim to $x$
how can the sampl varianc be written in matrix notat when the data is center $\hat \sigma = \frac{1}{n}x^tx$
intuitively, what the effect of have a non-zero varianc in ppca the project of the datapoint onto the columnspac of $\hat w$ are shrunk toward the empir mean $\bar x$
what the mle for the nois in ppca if $\lambda_j$ are the eigenvalu of $\hat \sigma$, sort in order of decreas magnitud $\hat \sigma^2 = \frac{1}{d-l} \sum_{l j d} \lambda_j$ ie it the averag varianc of the dimens not use by the project
how a pca model usual fit either with eigenvector method or with em
what the output of em when appli to pca a weight matrix $\hat w$ that span the correct subspace, but which isn't orthogon onc em is finish though, it can be orthogon easili
what an orthogon matrix in term of it column it a matrix whose column a \emph{orthonormal} set
what the intuit interpret of em on pca with $2 1$ weight matrix imagin attach each datapoint in $\mbb{r}^2$ to a stiff rod via a spring e step: hold the rod fix and let the spring slide around to minim the energi m step: hold the point where the spring attach fix and let the rod slide around to minim the energi
what'r the advantag of fit a pca model use em instead of eigenvector method if $n, d \gg l$ then em is much faster than naiv eigenvector method em work onlin em can handl miss data
what a hinton diagram a visual of a matrix as a matrix of squares, with the size of each squar correspond to the magnitud of the entri and white/black denot positive/neg
what ard in ppca automat relev determin when fit a model use em, an ard heurist allow you to start at some maximum number of latent variables, and irrelev weight will be automat prune
what a scree plot in ml a plot of the magnitud of the eigenvalu of a matrix, sort in decreas order
how can the residu error in a pca model be relat to the data use $l$ latent variabl and $d$ dimension data $e(l) = \sum_{l j d} \lambda_j$ where $\lambda_j$ are the eigenvalu of $\hat \sigma$ sort in decreas order
what the fraction of varianc explain in pca $f(l) = \frac{\sum_{j l} \lambda_j}{\sum_{j l_\text{max}} \lambda_j}$ ie the ratio of all the eigenvalu involv in a $l$-dimens model compar to the eigenvalu involv in the best possibl model
whi doesn't pca generat u-shap curv on test set becaus it not a probabilist model, it a compress techniqu give it more latent dimens will alway allow it to model the test set more accurately. it isn't penal for use the extra paramet to store small amount of in ation
whi doesn't k-mean generat u-shap curv on test set becaus it not a probabilist model, it a compress techniqu if you give it more centroids, it'll tile the space more densely, so a test vector is more like to find a nearbi centroid it isn't penal for use the extra paramet to store small amount of in ation
what the profil likelihood in pca model fit if you can have up to $l_\text{max}$ parameters, let $\gamma_k$ be the error with $k$ paramet now construct a change-point model: for $k l$, let $\lambda_k \mc{n}(\mu_1, \sigma^2)$ and for $k l$, let $\lambda_k \mc{n}(\mu_2, \sigma^2)$ then the profil log likelihood at $l$ is defin as the log likelihood of this model
what the use of the profil log likelihood in pca pca isn't a probabilist model, so fit with differ number of paramet can't be compar via margin likelihood instead, the profil likelihood $l(l)$ can be calcul for each threshold $l$, and $l^* = \arg \max_l l(l)$ select as the number of paramet to use
what the categor pca model $p(i z, \theta) = \prod_j \text{cat}(y_j \mc{s}(w^t_jz + w_0))$ $z \mc{n}(0, i)$
what the supervis pca model $z \mc{n}(0, i)$, the latent variabl of dimens $l$ $y z \mc{n}(w^t_i z + \mu_y, \sigma^2_y)$, a scalar target variabl $x z \mc{n}(w_xz + \mu_x, \sigma^2_x i)$, the vector-valu data
what anoth name for supervis pca bayesian factor regress
what the in ation bottleneck in ml in supervis learn with latent variables, you want two things: prediction: for the input $x$ to tell you a lot about the target variabl $y$, compression: for the latent $z$ to hold as littl in ation about $x$ as possibl ie, minim the in ation bottleneck $\mbb{i}(x; z) - \beta\mbb{i}(y; x)$ where $\beta$ is a tradeoff between the two: the larger $\beta$, the more import predict is vs compress
what algorithm is the in ation bottleneck close relat to canon correl analysi
what semi-supervis learn in ml supervis learn where the target variabl isn't alway observ
what the problem discrimin supervis pca fix standard supervis pca pay as much attent to predict the input as it doe to predict the target variabl by weight the term in the log-likelihood, focus can be put on the latter
what the idea in partial least squar in ml it supervis pca, but with the latent variabl partit into two subspaces: $z^x_i$ are use to explain the relat between the input $z^s_i$ are use to explain the relat between the input and the output
what the model use in partial least squar $z^s \mc{n}(0, i)$, $z^x \mc{n}(0, i)$ $y z \mc{n}(w_yz^ + \mu_y, \sigma_y^2i)$ $x z \mc{n}(w_xz^s + b_xz^x + \mu_x, \sigma_x^2i)$
intuitively, what canon correl analysi a symmetric, unsupervis version of partial least squares: each view $x^k$ of the problem get it own latent variabl subspac $z^{x^k}$ and then there a share subspac $z^s$ as well
what the model use in canon correl analysi $z^s \mc{n}(0, i)$ and for each view $x^k$, $z^{x^k} \mc{n}(0, i)$ $x^k z \mc{n}(w_{x^k}z^s + b_{x^k}z^{x^k} + \mu_{x^k}, \sigma_{x^k}^2i)$
what the cocktail parti problem an exampl of blind signal separation: you'r at a cocktail parti and have two ears, each of which get a linear combin of the convers go on. extract each individu conversation.
what the model use in independ compon analysi $x_t$ is the vector of observ at time $t$, $z_{tj} \mc{p}_j$ for $j [1, l]$ are the sourc signal at time $t$, where $\mc{p}_j$ is some non-gaussian distribut $x_t \mc{n}(wz_t + \mu, \psi)$
what the differ between pca and ica in pca, each latent variabl is gaussian in ica, the latent variabl can be taken from ani \emph{non} gaussian distribut
what the prototyp problem that ica solv blind sourc separ ie the cocktail parti problem
what whiten in ml it a tran ation of the data whose output has a covari matrix $i$ it call whiten becaus it take the input vector to a white nois vector
in ica, what'r the generat amp; recognit weight the generat weight matrix is the $w$ which take the latent variabl $z$ to the observ the recognit weight matrix is the $v = w^{-1}$ which take the observ to the latent variabl
what the usual approach to whiten data use pca with as mani latent variabl as observ
what preprocess usual need to be done befor ica can be appli center the data whiten the data
what the object function when fit an ica model minim $\text{nll}(v) = -\sum_j \mbb{e}[\ln v^t_j x]$ and $v$ is constrain to be an orthogon matrix
intuitively, how the fastica algorithm work construct a lagrangian that enforc the orthogon constraint $v^tv = i$ then approxim the hessian of the lagrangian by make some independ assumpt and use the approxim to a newton updat rule project back onto the constraint surfac use $v^* = \frac{1}{ v }v$ after each updat
what a newton updat in one dimens an updat in a root-find algorithm of the $x^\prime = x- \frac{f(x)}{f^\prime(x)}$
what the kurtosi of a distribut $\text{kurt}(x) = \frac{1}{\sigma^4}\mu_4 - 3$ where $\mu_4$ is the 4th central moment
what the $k$th central moment of a distribut $\mu_k = \mbb{e}[(x-\mbb{e}[x])^k]$
what the intuit interpret of the kurtosi of a distribut it describ the `peaki of the distribut in comparison to the normal distribution, which has a kurtosi of 0
what a leptokurt distribut a distribut with $\text{kurt}(x) 0$
what a platykurt distribut a distribut with $\text{kurt}(x) 0$
what anoth name for a leptokurt distribut a super-gaussian distribut
what anoth name for a platykurt distribut a sub-gaussian distribut
how the skew of a distribut defin $\text{skew}(x) = \frac{1}{\sigma^3}\mu_3$
intuitively, how do you interpret the skew of a distribut posit skew indic the right tail is heavier than the left negat skew indic the left tail is heavier than the right
what the pdf of the logist distribut in term of trigonometr function $p(x \mu, s) = \frac{1}{4s} \text{sech}^2\left(\frac{x-\mu}{2s}\right)$
what the advantag of the logist distribut it sparser than the gaussian (like the laplac distribution), and differenti everywher (unlik the laplac distribution)
when fit ica via maximum likelihood, what common use for $g(z) = -\ln p(z)$ $g(z) = \sqrt{z}$, which approxim correspond to a laplace-distribut prior $g(z) = \ln \text{cosh}(z)$, which approxim correspond to a logistically-distribut prior
when fit ica via em, what an altern to assum a specif for the prior $g(z)$ use a mixtur of $k$ gaussian then calcul $\mbb{e}(z x, \theta)$ by sum over all $k^l$ set of the mixtur weight
what the negentropi of a distribut $\text{negentropy}(x) = \mbb{h}(\mc{n}(\mu, \sigma^2))-\mbb{h}(x)$ where $\mu, \sigma^2$ are the mean and varianc of $x$
what the use of negentropi sinc the gaussian is maxim entrop for a given mean and variance, it non-neg and larg if the distribut is far from be gaussian
what the multi-in ation of a distribut $i(x) = \mbb{kl}\left(x \middl \prod_j x_j \right)$ ie it the amount of in ation gain by consid a distribut as a whole vs consid it compon independ
what the infomax principl when do supervis learning, with input $x$, output $y$ and a system we design $\phi$ we should tri and pick $\phi$ to maxim the flow of in ation between $x$ and $y$ repres by the mutual in ation $\mbb{i}(x, y)$
what are some equival to fit an ica model by maxim the likelihood maxim the negentropi of the latent variabl minim the multi-in ation of the latent variabl maxim the mutual in ation between the latent variabl and the observ variabl
whi isn't the mutual in ation between the respons and each input variabl enough to `solv the featur select problem becaus you can have model like $y = x_1 \oplus x_2$ over binari variabl where each input variabl alon doesn't tell you anyth about the respons
what bayesian variabl select let $\gamma_j = 1$ if featur $j$ is relevant, and $\gamma_j = 0$ otherwis comput $p(\gamma \mc{d})$ for all $2^d$ valu of $\gamma$ calcul some summari statist (like the mode) of the posterior to help choos the model
what the median model approach to variabl select if $\gamma_j$ indic whether variabl $j$ is relev then the median model use variabl is $\hat \gamma = \{ j : p(\gamma_j = 1 \mc{d}) 0.5\}$
what'r inclus probabl in featur select the margin $p(\gamma_j = 1 \mc{d})$, where $\gamma_j$ indic the relev of variabl $j$
what the $l_0$ pseudo-norm the number of nonzero element in the vector
what prior usual use for the relev variabl $\gamma_j$ in bayesian featur select $p(\gamma) = \prod \text{ber}(\gamma_j \pi_0)$
how do you write a product of bernouilli in term of the $l_0$ pseudo-norm $\prod \text{ber}(\gamma_j \pi_0) = \pi_0^{ \gamma _0}(1-\pi_0)^{d- \gamma _0}$
how can a bernouilli log-likelihood be written in term of sparsiti $\ln p(\gamma \pi_0) = -\lambda \gamma _0 + c$ where $\lambda = \ln \frac{1-\pi_0}{\pi_0}$
what the spike and slab model a model for featur select problems, where the weight are distribut as $p(w_j \sigma^2, \gamma) = \delta_0(w_j)$ if $\gamma_j = 0$ $p(w_j \sigma^2, \gamma) = \mc{n}(w_j 0, \sigma^2\sigma^2_w)$ if $\gamma_j = 1$ where $\sigma^2_w$ control how big we expect the coeffici of $w_j$ to be and $\sigma^2$ is the global nois level
what the inspir for the spike and slab model if $\gamma = 0$, then featur $j$ is irrelev so on center data the associ weight $w$ would be 0. if $\gamma = 1$, we should make the least number of assumpt possibl about the weights, so use a gaussian (which approxim a uni `slab when $\sigma_w^2$ is larg enough
what is the use of kendal tau correl use in per ing non-linear correl btwn data set vs spearman which is an extens of r^2 to rank kendal is more niche, but can be good in certain circ
in genetics, what is the mc use of the q-q plot to character the extent to which the observ distribut of the test statist follow the expect distribut i.e., expect = null distribut
what the binari model in ml a model for featur select problems. the weight are $\gamma_j w_j$, where $\gamma_j \text{ber}(\pi_0)$ $w_j \mc{n}(0, \sigma^2_w)$
what anoth name for the binari model the bernouilli-gaussian model
from a bayesian perspective, how doe $l_0$ regular aris in ml from appli a bernouilli prior to each inclus variabl $\gamma_j$ which give rise to a $\lambda w _0$ term in the negat log posterior
what'r wrapper method in ml featur select algorithm that work by search through the paramet space and fit the model at each point use some generic method
what a common optim to wrapper method in ml if the wrapper method involv take a single-bit step from $\gamma_\text{old}$ to $\gamma_\text{new}$ then comput the loss function $f(\gamma_\text{new})$ can often be done with a rank-1 updat
what loss function do you get after appli $l_0$ regular to the binari model $f(w) = y -xw _2^2 + \lambda w _0$
what the single-best-replac algorithm for featur select start with $\gamma = 0$ at each step, flip the singl bit that'd most reduc the loss function
what the orthogon least squar algorithm for featur select if the complex penalti (ie $\lambda w _0$) is ignor then singl best replac reduc to ad the featur $\arg \min_{j \not \gamma_t} \min_w y - x_{\gamma_t j} w ^2$ ie just keep ad the featur which most reduc the loss
what the orthogon match pursuit algorithm for featur select it similar to orthogon least squares, but `freez the weight and instead solv $\arg \min_{j \not \gamma_t} \min_\beta y - xw_t - \beta x_{:,j} ^2$ ie pick the featur whose column is closest to the residual.
how doe the orthogon match pursuit algorithm compar to orthogon least squar in term of per anc omp onli requir one least squar calcul per iteration, so is much faster it less accur though
what the match pursuit algorithm for featur select an even faster/mor inaccur variant of orthogon match pursuits: just keep pick the featur whose vector $x_{:,j}$ most strong correl with the current residu $y - xw_t$
what the backward select algorithm for featur select start with a satur model $\gamma = 1$ and delet the worst featur at each step
what anoth name for the orthogon least squar algorithm for featur select greedi forward select
how do the forward select and backward select algorithm for featur select compar forward select is faster becaus you onli need to fit a veri small model initi backward select give relev decis the `context of all the other variabl that depend on it
what the stochast search approach to featur select use a stochast algorithm to generat a set $\mc{s}$ of high-scor model approxim the posterior probabl of ani specif model in the set as $p(\gamma \mc{d}) \frac{e^{-f(\gamma)}}{\sum_{\gamma^\prime} e^{-f(\gamma^\prime)}}$ where $f$ is a loss function that approxim the negat log of the joint $p(w, \mc{d})$
what $l_1$ regular the practic of appli a laplac prior to each weight, $w_j \text{lap}(0, 1/\lambda)$ which give rise to a $\lambda w _1$ term in the log posterior
what the effect of $l_0$ regular in featur select it convert a discret sparsity-promot problem over the inclus variabl $\gamma \mbb{b}^d$ into a continu optim problem over the weight $w \mbb{r}^d$ it lead to a veri `noisi surfac though
what the effect of $l_1$ regular in featur select for larg enough $\lambda$, it can be thought of as a convex approxim to a non-convex $l_0$ regular which encourag the variabl to be spars it not a smooth surfac though!
what lasso stand for in ml least absolut shrinkag and select oper
what the lasso problem for linear regress $\min_w \text{rss}(w)$ such that $ w _1 b$
how $l_1$ regular correspond to lasso a larg regular coeffici $\lambda$ correspond to a small bound $b$ on $ w _1$
what the advantag of use lasso over $l_1$ regular the lasso enforc a bound, but it object is smooth
intuitively, whi doe $l_1$ regular in spars solutions, but $l_2$ doesn't translat both to constrain optim problems: $ w _1 b_1$ and $ w _2 b_2$, with object $f(w)$ optim soln is the lowest level set of $f$ that intersect the constraint surfac $l_1$ surfac has corner - spars solut - which are veri like to be the point of intersection. $l_2$ surfac is smooth; point correspond to spars soln are no more like to be the first intersect than other point
what a subderiv a subderiv of a convex $f\colon \mc{i} \rightarrow \mbb{r}$ at $\theta_0$ is a scalar $g$ such that $f(\theta) - f(\theta_0) g(\theta - \theta_0)$ for all $\theta \mc{i}$
what the intuit for subdifferenti they'r an extens of deriv for non-smooth function
what a subdifferenti it the closur of the set of subderiv of a function at a point
what the subdifferenti of a function at a point where it smooth $ f (\theta_0) = \frac{df(\theta_0)}{d\theta}$
how the subdifferenti of a function at a point denot $ f(\theta_0)$
in term of subdifferentials, when a point a minimum of a function when $0 f(\theta_0)$
what the $\text{soft}(a; \delta)$ function $\text{soft}(a; \delta) = \text{sign}(a)( a -\delta)_+$ where $x_+ = \max (x, 0)$ is the posit part of $x$
if you appli $l_1$ regular to linear regression, what the optim weight $w_j$ $\hat w_j = \text{soft}\left(\frac{c_j}{a_j}; \frac{\lambda}{a_j}\right)$ where $a_j, c_j$ are defin by $\frac{ }{dw_j}\text{rss}(w) = a_j w_j + c_j$
how doe ridg regress compar to lasso regress ridg regress enforc a $l_2$ complex penalti lasso regress enforc a $l_1$ complex penalti
if you take the ol solut coeffici to a linear regress problem and hard threshold them, what are you effect do subset select - rank the featur by weight of their solut coefficient, then select the first $k(\lambda)$
what a disadvantag of lasso regress it s in a bias estim of the weights, sinc even larg weight are shrunk toward zero by the soft threshold it enforc
what a regular path the curv describ by the weight estim $\hat w$ as regular strength $\lambda$ chang
what the shrinkag factor of a lasso solut it the ratio $b/b_\text{max}$ of the bound $b$ for a specif model to the bound $b_\text{max}$ which no longer constrain the paramet
what the lar algorithm `least angl regress and shrinkage', an adapt of lasso regress which make use of the fact that the regular path is piecewis linear to comput the path veri cheapli
what it mean for an algorithm to be model select consist as $n \rightarrow fty$, it recov the true sparsiti pattern $w^*$
what $\lambda_\text{max}$ in regular method the regular strength at which all the weight are clamp to zero
what debias in featur select algorithm like lasso give bias estim which shrink even larg weight toward zero but onc the support of the weight vector has been identifi by a bias algorithm, an unbias algorithm can be use to calcul unbias estimates.
what the problem with use cross-valid to select regular strength becaus algorithm that use regular are usual bias cross-valid will often choos a $\lambda$ which is too small to in model select consist
what a disadvantag of use $l_1$ regular to select variabl it can be veri sensit to the input
what frequentist stabil select take a sensit featur select algorithm (like lasso) and use bootstrap resampl to run it mani time then use the output to estim the posterior inclus probabl of each featur
what the bolasso featur select algorithm bootstrap lasso: stabil select is appli with lasso regress and the approxim posterior probabl are threshold (at say 90 ) to decid which featur to includ
with a linear model and a laplac prior, which posterior estim are spars just the mode the mean and median aren't
what a bayesian altern to map estim of a linear model over a laplac prior map with a spike-and-slab prior
what the coordin descent algorithm a model fit algorithm which cycl through the coordin $j$ evalu $w^*_j = \arg\min_z f(w + ze_j) - f(w)$
what'r e set algorithm in ml a general of coordin descent which adjust mani variabl at the same time
what warm start in model fit give an algorithm the valu of the weight $\hat w(\lambda_k)$ at some regular strength $\lambda_k$ and ask it calcul $\hat w(\lambda_{k+1})$ where $\lambda_{k+1} \lambda_k$
what a continu method in model fit rather than calcul the weight $\hat w(\lambda_*)$ at regular strength $\lambda_*$ continu method calcul a set of solut lead from $\lambda_\text{max}$ down to $\lambda_*$ make use of warm start to do this quick
what anoth name for continu method in ml homotopi method
what the most well known exampl of a homotopi method lar
what the proxim oper for a convex $r$ (usual a regularizer) $\text{prox}_r(y) = \arg\min_z \left(r(z) + \frac{1}{2} z-i ^2_2\right)$ ie it give a point which minim $r$ but is also close to $y$
what the usual use of the proxim oper by use it in the updat rule of an iter optimizer, it ensur that a new iter is close to the old iter
what the proxim oper for the regular $\lambda w _1$ $\text{prox}(w) = \text{soft}(\theta, \lambda)$ where the threshold is appli componentwis
what the proxim oper for the regular $\lambda w _0$ $\text{prox}(w) = \text{hard}(\theta, \sqrt{2\lambda})$ where the threshold is appli componentwis
what the hard threshold oper $\text{hard}(a, \lambda) = a\mbb{i}( a \lambda)$
what the proxim oper for the regular $\mbb{i}_s(w)$ $\text{prox}(w) = \text{proj}_s(\theta)$ where $\text{proj}_s$ is the project onto set $s$
what the definit of a project onto a set it a map into a subset which is idempot
what the most common project oper into the 1-norm ball $\text{proj}(w) = \text{soft}(w, \lambda(w))$ where $\lambda$ is 1 if $ w _1 1$ and otherwis it the solut to $\sum ( w_j - \lambda)_+ = 1$
what a bregman distanc it similar to a metric, but isn't necessarili symmetr doesn't necessarili satisfi the triangl inequ
what the definit of the spectral step size for a function $l$, $\alpha = \frac{\langl \theta^\prim - \theta,g^\prim - g \rangle}{ \theta^\prim - \theta ^2}$ where $g$ and $g^\prime$ are the gradient at $\theta$ and $\theta^\prime$
how doe the spectral stepsiz aris if we approxim the hessian of $l$ with $\alpha i$, then it must be that $\alpha(\theta^\prim - \theta) g^\prime - g$ where $g, g^\prime$ are the gradient of $l$ at $\theta, \theta^\prime$ the spectral step size solv this equat
how doe use the spectral step size in gradient descent compar to standard line search algorithm spectral step size doesn't guarante a monoton decreas in the object but it much faster
what the updat rule in the proxim gradient algorithm for a regular $r$ and loss $l$, $\theta_{k+1} = \text{prox}_{\frac{1}{\alpha_k}r}(\theta_k - \frac{1}{\alpha_k}\nabla l(\theta_k))$ where $\alpha_k$ is the step size
what project gradient descent proxim gradient descent with a regular of the $r(\theta) = \mbb{i}_s(\theta)$ for some set $s$
how doe proxim gradient descent relat to gradient descent proxim gradient descent is equival to gradient descent when the regular is $r(\theta) = 0$
what iter soft threhold it proxim gradient descent where $r(\theta) = \lambda \theta _1$
what the updat rule in nesterov method of proxim gradient descent for a regular $r$ and loss $l$, $\theta_{k+1} = \text{prox}_{\frac{1}{\alpha_k}r}(\phi_k - \frac{1}{\alpha_k}\nabla l(\theta_k))$ where $\phi_k = \theta_{k-1} + \frac{k-1}{k-2}(\theta_k - \theta_{k-1})$ and $\alpha_k$ is the step size
how doe nesterov method relat to proxim gradient descent it the same, except the quadrat approxim to the loss function is center around an interpol of the previous two paramet valu rather than just around $\theta_k$
how do you ensur converg when use the spectral step size requir that the object decreas on averag over some slide window of $m+1$ updat if it doesn't, re-calcul the regular function use the latest paramet valu
how can the laplac distribut be written as a gaussian scale mixtur $\text{lap}(x 0, b) = t \mc{n}(0, z) \text{ga}(z 1, \frac{1}{2b^2})dz$
how is em appli to lasso problem by convert the problem to it laplace-prior then expand the laplac prior as a gaussian scale mixture, with scale weight $\tau_j$ e step: infer the $\tau_j$ m step: estim $w$
what the laplac prior of a lasso problem $\mc{n}(i xw, \sigma^2 i)\prod_j \text{lap}(w_j 0, b)$ where the prior scale $b$ reflect the lasso bound $b$
what the wald distribut anoth name for the invers gaussian distribut
what the definit of a group lasso problem one which in regular has object $f(w) = \text{rss}(w) + \lambda \sum w_g _2$ where each $g$ denot a group of paramet
what'r two altern regular sometim use in group lasso $\lambda_g w_g _ fty$ $\lambda_g \sqrt{d_g} w_g _2$, where $d_g$ is the size of group $g$
whi it import that group lasso use $ w_g _2$ rather than $ w_g _2^2$ the er promot group sparsity; the latter is equival to ridg regress
how lasso general to problem where there are multipl paramet $w_{jc}$ for each variabl $j$ by group lasso, which encourag group sparsiti rather than individu sparsiti
what the prior which give a map estim equival to solv a group lasso problem $p(w \gamma, \sigma^2) \propto \exp\left(-\frac{\gamma}{\sigma}\sum_g w_g _2\right)$ where $\gamma$ scale with $\lambda$ and $\sigma^2$ is the nois in the likelihood
what the use of fuse lasso it for problem where we'd like $w_j$ to be similar to $w_{j+1}$
what the penalti in fuse lasso $\frac{\lambda_1}{\sigma} \sum w_j + \frac{\lambda_2}{\sigma}\sum w_{j+1} - w_j $ where $\lambda_1, \lambda_2$ control the relat import of sparsiti and of similar and $\sigma^2$ is the nois in the likelihood
what the penalti in graph-guid fuse lasso $\frac{\lambda_1}{\sigma} \sum_{ v} w_s + \frac{\lambda_2}{\sigma}\sum_{(s, t) e} w_s - w_t $ where $\lambda_1, \lambda_2$ control the relat import of sparsiti and of similar and $\sigma^2$ is the nois in the likelihood
how is fuse lasso repres as a hierarch model $w \sigma^2, \tau, \omega \mc{n}(0, \sigma^2\omega^{-1}(\tau, \omega))$ $\tau_j^2 b_1 \text{expon}\left(\frac{1}{2b_1^2}\right)$, the sparsiti regular $\omega_j^2 b_2 \text{expon}\left(\frac{1}{2b_2^2}\right)$, the similar regular and $\omega$ is a tridiagon matrix with central entri $\frac{1}{\tau_j^2} + \frac{1}{\omega_{j-1}^2} + \frac{1}{\omega_{j+1}^2} $ and off-diagon entri $-\frac{1}{\omega^2_j}$
what'r some of the problem with lasso model if there are a group of high correl variables, it'll tend to arbitrarili select just one of them to be nonzero if $d n$, lasso will onli be abl to select $n$ variabl befor satur if $n d$ but the variabl are correlated, it'll often do wors than ridg regress
intuitively, what'r elast net model a combin of lasso and ridg approach that amelori some of the problem with lasso model
what the penalti in vanilla elast net model $\lambda_1 w ^2_1 + \lambda_2 w ^2_2$
what the group effect in elast net model if a penalti on $w$ is strict convex (which elast net is) then the estim coeffici of high correl variabl tend to be equal in magnitud
how'r elast net regress problem usual solv elast net reduc to lasso on suitabl modifi data at which point lar can be appli
what the problem with vanilla elast net, and how is it solv it produc poor estim becaus it shrink twice: onc by the $l_1$ penalti and onc by the $l_2$ penalti the solut is to scale the comput $\tild w$ to $\hat w = \sqrt{1 + \lambda_2}\tild w$
intuitively, how can elast net be interpret in term of lasso it lasso but with a covari matrix that shrunk toward $i$ as $\lambda_2$ increas
what'r the problem with use a laplac prior it often not suffici sparse: not enough mass is put at the origin, so it won't suppress nois not enough mass is put at extrem values, so it shrink larg weight
what the penalti use in bridg regress $\lambda\sum w_j ^b$ for some $b 0$
what the main advantag of use a laplac prior over someth like an exponenti power distribut with $b = 0.5$ the laplac prior give a convex optim problem
what properti do differ paramet $b$ generat in bridg regress $b 1$ is sparsity-promot $b 1$ produc convex optim problem
what the definit of an exponenti power distribut $\text{exppower}(x \mu, a, b) \propto \exp\left(-\frac{ x - \mu ^b}{a}\right)$
what kind of prior doe bridg regress correspond to an exponenti power distribut
what a hierarch adapt lasso model a lasso model where each weight get it own tune parameter: $b_j \text{ga}(a, b)$ $w_j b_j \text{lap}(0, b_j)$
what the advantag of the hierarch adapt lasso model over vanilla lasso vanilla lasso need a larg $\lambda$ to suppress irrelev parameters, but this also shrink relev paramet hal allow larg paramet to `stay larg
what hal in ml hierarch adapt lasso
what distribut do you get when you integr out the tune paramet $b_j$ in hal a general $t$ distribut
what the of a general $t$ distribut $\text{gt}(x \mu, a, c, q) \propto \left(1 + \frac{1}{a}\frac{ x - \mu ^q}{c^q}\right)^{-\frac{1+a}{q}}$
what'r the paramet in the general $t$ distribut $\mu$, the mean $a$, which control the degre of freedom $c$, the scale $q$, which control the sparsiti
what the penalti function that aris from hal $(1+a)\sum_j\ln\left(1+\frac{1}{b} w_j \right) + c$ where $c$ is a constant
what'r the effect on the penalti of the two paramet $a, b$ in hal $a$ increas the strength $b$ increas the sparsiti
what the threshold function given rise to by hal look like it flat in the center and asymptot close to $y = x$ and the slope of the transit between the two behaviour increas with $b$
what a possibl altern to hal in term of sparsity-promot prior normal-jeffrey which put a nonin ativ jeffrey prior on the varianc of each weight that give margin $p(w_j) \propto \frac{1}{ w_j }$
how a normal-jeffrey prior compar to hal model normal-jeffrey has no paramet which mean there noth to tune but no abil to adapt the level of sparsiti
what a factori prior a prior of the $p(w) = \prod p_j(w_j)$
what spars bay learn ard appli to linear model
what do $\alpha_j$ and $\beta$ denot in the context of ard $\alpha_j$ is the weight precis $\beta$ is the measur precis
in ard when draw precis $\alpha_j \text{ga}(a, b)$, what set of $a, b$ give the greatest sparsiti the improp prior $a, b = 0$
what'r the effect of the shape paramet $\alpha$ and the rate paramet $\beta$ on the gamma distribut increas $\alpha$ shift the peak down and to the right increas $\beta$ reduc the spread of the distribut
what the effect of a scale paramet on a distribut increas scale stretch the distribut
what the model use in spars bayesian learn $y x,w,\beta = \mc{n}(w \cdot x, \frac{1}{\beta})$ $w \mc{n}(0, a^{-1})$ where $a = \text{diag}(\alpha)$
what sbl in ml spars bayesian learn
what other model doe sbl close resembl a spike and slab model with a normal likelihood
how is sbl regular by appli a gamma prior to the precis $\alpha_j, \beta$ with a shape $ 1$
when is a gamma distribut sparsity-promot when the shape $\alpha 1$
what'r the main step in spars bayesian learn regular the precis with a gamma distribut estim the precis $\alpha_j, \beta$ use the map use the estim precis to calcul a posterior over the paramet $w$
what the main thing that distinguish ard/sbl from map estim have estim the precisions, the effect prior $p(w \hat \alpha)$ is non-factori
in what sens are non-factori prior strict better than factori prior it can be shown that non-factori object alway have fewer local minima than factori object
how doe spars code relat to ica it ica with a possibl non-orthogon factor load matrix $w$ and sparsity-promot prior place on the $z$
how doe the nomenclatur of spars code carri over to the ica alism the factor load matrix $w$ is the dictionari each column in $w$ is an atom if $l d$, then the represent is overcomplet
what spars pca pca with a sparsiti promot prior put on the weight $w$ \emph{not} the same as spars code
what model is common use in spars code when tri to learn the dictionari $\ln p(\mc{d} w) \sum_i \max_{z_i} [ \ln \mc{n}(x_i wz_i, \sigma^2 i) + \ln p(z_i)]$ where $p(z_i)$ is a laplac prior and the column of $w$ are bound as $ w_j ^2_2 1$
what the analysis-synthesi approach to learn a dictionari in spars code analysis: fix the coeffici $z$ and solv a least squar problem to estim $w$ synthesis: fix the basi $w$ and solv a lasso problem to estim $z$ repeat!
what spars matrix factor the intersect of spars code and spars pca: tri and simultaen learn a spars basi $w$ and spars coeffici $z$ such that $x=wz$
what the idea in compress sens suppos $y=rx+\epsilon$ is a noisy, low dimension project of some data $x$, where the sens matrix $r$ is known. if we assum $x = wz$, ie $x$ is spars in some basi $w$, then we can still recov $x$ from $y$ use bayesian infer despit $y$ be much lower dimens than it!
what a kernel it a real-valu bifunct $\kappa(x, x^\prime)$ which is typic symmetr and posit definit though this isn't requir
conceptually, what doe a posit definit kernel function do it measur the similar between two object
what the infinite-dimension analogu of a posit definit matrix a posit definit kernel function
what the se kernel the squar exponenti kernel, anoth name for the gaussian kernel
what the gaussian kernel $\kappa(x, y) = \exp\left(-\frac{1}{2}(x - y)\sigma^{-1}(x - y)\right)$
what the characterist length scale of a dimens in ml given a gaussian kernel $\kappa$ with diagon covari the characterist length scale of dimens $j$ is $\sigma_j$
what an ard kernel a gaussian kernel with diagon covari where some $\sigma_j = fty$ sinc the infinit varianc mean that dimens $j$ is ignor
what the bandwidth of a gaussian kernel if the kernel is isotropic, it $\sigma^2$
what a rbf kernel a radial basi function kernel, a kernel that onli depend on $ x- x^\prime $
what the cosin similar kernel $\kappa(x, y) = \frac{\langl x, y \rangle}{ x _2 y _2}$
what the babylonian method for find squar root $x_0 \sqrt{s}$ $x_{n+1} = \frac{1}{2}\left(x_n + \frac{s}{x_n}\right)$
what the term frequenc of a featur in a document given featur vector $x$ describ the document, it $\text{tf}(x_{j}) = \ln(1+x_{j})$
what the invers document frequenc of a featur given the featur vector $x_i$ repres the documents, it $\text{idf}(j) = \ln \frac{n}{1 + \sum_i \mbb{i}(x_{ij} 0)}$ ie the negat log fraction of the number of document $j$ appear in
what the tf-idf represent of a document the term-frequ inverse-document-frequ representation, $\text{tf-idf}(x_i) = [\text{tf}(x_{ij}) \text{idf}(j)]_j$
what a more robust variant of the cosin similar kernel for document match the tf-idf kernel: $\kappa(x, y) = \text{cosine-sim}(\text{tf-idf}(x), \text{tf-idf}(y))$
what the gram matrix of a kernel given input $\{x_i\}$, it $k = [\kappa(x_i, x_j)]_{ij}$
what a mercer kernel anoth name for a posit definit kernel
what a posit definit kernel one whose gram matrix $k$ is posit definit for ani set of input
what a stationari kernel a translation-invari kernel, ie there exist a $\tild \kappa$ such that $\kappa(x, y) = \tild \kappa(x - y)$
what the polynomi kernel $\kappa(x, y) = (\gamma x \cdot y + r)^m$ where $r 0$
what an exampl of a non-merc kernel the sigmoid kernel $\kappa(x, y) = h (\gamma x^ti + r)$
what a linear kernel a kernel of the $\kappa(x, y) = x \cdot y$
when a linear kernel appropri when the individu featur in the data are alreadi in ativ so the decis boundari is like alreadi represent as a combin of the origin featur
what the usual use of the matern kernel it usual use to defin the covari between point that are a specif distanc from eachoth
what the kernel usual use for string comparison the substr count kernel $\kappa(x, y) = \sum_{ \mc{a}^*} w_s \phi_s(x)\phi_s(y)$ where $\mc{a}$ is the alphabet and $w_s$ is the weight of substr $s$ and $\phi_s(x)$ count the number of time $s$ appear as a substr in $x$
how the substr count kernel usual calcul use suffix tree or array to do it in linear time
what the bag-of-charact string kernel the kernel you get when you take the substr kernel and set $w_s = 1$ for all $s$ of length 1, and $w_s = 0$ otherwis
what the bag of word kernel the kernel you get if you take the substr kernel and set $w_s = 1$ on ani $s$ which is bound by whitespace, and $w_s = 0$ otherwis
what the $k$ spectrum string kernel the kernel you get if you take the substr kernel and set $w_s = 1$ for string of length $k$, and $w_s = 0$ otherwis
how are bag-of-word represent usual construct for imag identifi region of interest character each region by a vector the set of those vector is the represent of the imag
what a pyramid match kernel given a datapoint repres by a featur set, the set is map to a multi-resolut histogram and the histogram are then compar use weight histogram intersect
what doe a pyramid match kernel approxim construct a bipartit map between the element of the two featur set and then sum the pairwis similar between match element
what the most common use of pyramid match kernel imag similar calculation: construct a bag of word represent for each imag then appli the pyramid match kernel to calcul similar
what a probabl product kernel $\kappa(x, y) = t p(z x)^\rho p(z y)^\rho dz$ where $\rho 0$ and $p(z x)$ is often approxim by $p(z \hat \theta(x))$
what a fisher kernel $\kappa(x, y) = s(x)^t\mc{i}^{-1} s(y)$ where $\mc{i}$ is the fisher in ation and $s(x) = \nabla_\theta \ln p(x \theta) _{\hat \theta}$
what the intuit behind the fisher kernel let $g(x)$ be the direct $x$ would like the paramet $\hat \theta$ to move to maxim it own likelihood the fisher kernel evalu whether $g(x), g(y)$ are similar with respect to the geometri encod by the curvatur of the likelihood function
what a kernel machin a glm with an input vector of the $\phi(x) = [\kappa(x, \mu_k)]_k$
what'r the centroid of a kernel machin the $\mu_k$ in the definit of the kernel machin featur vector
what an rbf network a kernel machin built on a rbf kernel
what jitter in data visual if a visual would have a larg number of point on top of eachoth add some uni nois to them so the group can be seen proper
how can a kernel featur vector be use for logist regress just use $y x, \theta \text{ber}(w^t\phi(x))$
how can a kernel featur vector be use in linear regress just use $y x, \theta \mc{n}(w^t\phi(x), \sigma^2)$
what a spars vector machin creat a kernel machin with the datapoint as centroids: $\mu_i = x_i$ then use a sparsity-promot prior to select a subset of the centroid
what a l1vm in ml a $l_1$ regular vector machin ie a spars vector machin use a laplac prior
what a relev vector machin a spars vector machin whose relev centroid are select use ard/sbl
what the kernel trick in ml when build a kernel machin rather than work with the kernel featur vector use the origin featur vector and replac all inner product $\langl x, y \rangle$ with a call to $\kappa(x, y)$ the kernel must be mercer though!
how do you kernel a nearest neighbour classifi by note that $ x - y _2^2 = \langl x, x \rangl + 2\langl x, y \rangl + \langl y, y \rangle$ and appli the kernel trick
what the idea in the k-medoid algorithm it similar to k-mean but rather than averag all the vector in the group to get a centroid one of the vector in the group is chosen to be the centroid itself in particular, we choos the vector which has the smallest sum of distanc to all the other vector in the group, $m_k = \arg \min_i \sum_{j g_k} d(x_i, x_j)$
what nearest medoid classif classif use k-medoid cluster
when kernel ridg regression, what are the dual variabl often use $\alpha = (k^t + \lambda i)^{-1} y$ where $k = xx^t$ in plain ridg regress
what the use of the dual variabl $\alpha$ in kernel ridg regress the primal variabl $w$ can be written as $w = x^t \alpha$ and the predict mean for input $x$ is $w^tx = \sum \alpha_i \kappa(x, x_i)$
what the comput advantag of use the dual variabl rather than the primal variabl when fit ridg regress comput the dual variabl is $o(n^3)$ comput the primal variabl is $o(d^3)$ so in very-high-dimension settings, use the dual variabl can be advantag
what $xx^t$ in term of inner product it the matrix of inner product of the row of $x$
what $x^tx$ in term of inner product it the matrix of inner product of the column of $x$
given the eigenvectors, $u$, and eigenvalues, $\lambda$, of $xx^t$, what are the eigenvector and eigenvalu of $x^tx$ $v = x^t u \lambda^{-\frac{1}{2}}$ and the eigenvalu are the same
how can the scatter matrix be written as a product of matric $\sum x_i x_i^t = x^tx$ where $x$ is construct by stack row
how do you kernel pca pca reduc to find the eigenvector of the covari matrix $x^tx$ which can be further reduc to find the eigenvector of the inner product matrix $xx^t$. replac $xx^t$ with the kernel $k$. then to center the featur vectors, replac the kernel with $\tild k = hkh$, where $h$ is the center matrix
what the definit of the center matrix $h = i - \frac{1}{n}\mb{1}\mb{1}^t$
what the use of the center matrix $h$ if we want the featur vector of a kernel machin with kernel $k$ to be centered, $\tild \phi_i = \phi(x_i) - \bar \phi$ then $\tild k = hkh$ implement this adjust
what one of the main advantag of kernel pca over linear pca linear pca is limit to use $l d$ compon kpca can use up to $n$ compon
what multidimension scale in ml it a way to find a low-dimension embed of a set of object such that the euclidean distanc between point in the embed space approxim the origin dissimilar matrix
what techniqu is multidimension scale close relat to kernel pca
from a bayesian standpoint, what'r the problem with support vector machin the sparsiti is encod in a loss function rather than in the prior kernel are encod use an algorithm trick, rather than be part of the model they don't give probabilist output
at a high level, how'r support vector machin deriv from the empir risk start with a $l_2$-regular empir risk function $j(w, \lambda) = \sum l(y_i, w^tx_i) + \lambda w ^2$ kernel the calcul of $w^tx_i$ adapt the loss function $l$ to enforc sparsiti
what the epsilon-insensit loss function $l_\epsilon(y, \hat y) = 0$ if $ y-\hat y \epsilon$ $l_\epsilon(y, \hat y) = y-\hat y - \epsilon$ otherwis ie ani point in an $\epsilon$-tub around the predict is not penal
what svm stand for in ml support vector machin
how the loss function usual written for svms $j(w) = c \sum l_\epsilon (y_i, w^tx_i) + \frac{1}{2} w ^2$ where $l_\epsilon$ is the epsilon-insensit loss function and $c$ control the regular strength
how a svm loss function $j(w)$ usual minim by ing amp; solv the quadrat program $\min j(w)$ s.t $y_i w^tx_i + \epsilon + \xi_i^+, \quad \xi_i^+ 0$ $y_i w^tx_i - \epsilon - \xi_i^-, \quad \xi_i^- 0$ where the slack variabl $\xi_i^+, \xi_i^-$ have been chosen such that $y_i = \xi_i^+ + \xi_i^-$
what doe the optim solut to an svm model look like $\hat w = \alpha^t x$ where $\alpha$ is nonneg and sparse.
what are the support vector of a svm the $x_i$ with nonzero $\alpha_i$ in the solut ie the point for which the predict error lie outsid the $\epsilon$ tube
what the definit of the hing loss $l_\text{hinge}(y, \eta) = (1 - y\eta)_+$ where $\eta$ is the confid ie a slope (determin by $\eta$) into a plateau
how are svms use for classif take a logist regress classifi and bound it risk of misclassif with $l_\text{hinge}(y, w^tx)$. then the object look like $c\sum l_\text{hinge}(y_i, w^tx_i) + \frac{1}{2} w _2$ where $c$ control the regular strength. this can be solv use a linear svm, get $\hat w = \alpha^t x$ and predict $\hat y(x) = \text{sign}\left(\sum \alpha_i \kappa(x_i, x)\right)$
what the larg margin principl when linear separ data, there are often mani plane that separ the train set the plane to choos as the decis boundari should be the one which is the greatest distanc from it nearest datapoint ie, the one with the largest margin
what a larg margin classifi a classifi whose object encourag correct predict and implement the larg margin principl
what the most common exampl of a larg margin classifi svms
what the constrain problem that a svm classifi tri to solv $\min_{w, \xi} c\sum\xi_i + \frac{1}{2} w ^2$ such that $y_i(w^tx_i) 1 -\xi_i$ $\xi_i 0$
what do the slack variabl in an svm classifi constrain problem correspond to they'r soft margin constraints, with $\xi_i 1$ repres a misclassif
how is the regular strength $c$ usual set in an svm classifi $c = \frac{1}{\nu n}$ where $0 \nu 1$ is the fraction of misclassifi point that'll be toler dure the train phase $\nu$ is set by cross-valid
how can you measur the confid in a svm classifi predict interpret $w^tx$ as the log-odd ratio $\ln \frac{p(y=1 x)}{p(y=0 x)}$ then $p(i = 1 x) = \text{sigm}(a w^tx + b)$ where $a, b$ are set by maximum likelihood estim \emph{on a separ valid set} - use the train set lead to sever overfitting! from a theoret perspect there no justif for this though!
how are svm classifi usual general to multipl class one-versus-the-rest: train a classifi for each class amp; either deal with ambigu or pick the predict with the largest output $w^tx$ one-versus-one: train a classifi for each pair of class and take a plural vote
what'r the problem with the one-versus-the-rest approach to use svms for multi-class classif two classifi might predict a point belong to two differ class and while you can pick the predict with the largest output $w^tx$, there noth that say the output of two differ classifi are compar is also prone to the class imbal problem
what the class imbal problem if you train a classifi on a train set which is almost all one class, it can often degrad per anc
what the problem with the one-versus-on approach to general binari classifi to multi-class problem there the possibl for ambigu when there no plural in the vote
what a common problem with set the regular strength $c$ in svms it interact with the kernel paramet ex: if you'r use a rbf kernel with varianc $\sigma$, then larg varianc should go with light regular (ie small $c$)
how can svms be interpret from a bayesian perspect rigorously, they can't - or at least the hing loss provabl doesn't have a correspond likelihood. it doe have a correspond pseudo-likelihood though, which can be repres as a gaussian scale mixtur which mean it can be fit use em or gibb sampl
are gradient optim or cross valid general quicker gradient optim
of the various kernel-bas classifiers, which should general be prefer rvm if speed matter gp if it doesn't
what the comput complex of fit a svm $o(n^3)$ general $o(n)$ for a linear kernel this doesn't includ cross-valid time
what a smooth kernel one satisfi $ t \kappa(x) dx = 1$, ie unit total mass $ t x\kappa(x) dx = 0$, ie zero mean $ t x^2\kappa(x) dx 0$, ie posit varianc
how do you control the bandwidth of a smooth kernel given a smooth kernel $\kappa$, the bandwidth-adjust version is $\kappa_h(x) = \frac{1}{h} \kappa\left(\frac{x}{h}\right)$
what the most common smooth kernel the gaussian kernel $\kappa(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$
what the epanechnikov kernel a smooth kernel with the $\kappa(x) = \frac{3}{4} (1- x^2)\mbb{i}( x 1)$
what the advantag of the epanechnikov kernel it got compact support
what the disadvantag of the epanechnikov kernel it not smooth at the boundari of it support
what the tri-cub kernel a smooth kernel with $\kappa(x) = \frac{70}{81}(1 - x ^3)^3\mbb{i}( x 1)$
what'r the advantag of the tri-cub kernel it got compact support it everywher differenti
what the boxcar kernel a smooth kernel of the $\kappa(x) = \mbb{i}( x 1)$
what the parzen window densiti estim $\hat p(x) = \frac{1}{n}\sum \kappa_h (x -x_i)$
intuitively, what the parzen window densiti estim do place a distribut (describ by the kernel) on top of each datapoint then take their sum
what a kernel densiti estim anoth name for a parzen window densiti estim
what kde in ml kernel densiti estim
how can k-nearest neighbour be interpret in term of kernel densiti estim to predict the class of a point $x$, place a boxcar kernel $\kappa_h$ on top of it and increas it bandwidth until $k$ datapoint are encountered. then we can estim the class posterior as $p(x y=c, \mc{d}) = \frac{n_c(x)}{k}$ where $n_c(x)$ is the number of point of class $x$ in the kernel which is the standard ula for knn
what kernel regress we can approxim the joint densiti as $p(y, x) = \frac{1}{n}\sum \kappa_h(x - x_i)\kappa_h(y-y_i)$ which can be rearrang to show that $\mbb{e}[i x] = \frac{1}{c} \sum \kappa_h(x - x_i) y_i$ where $c = \sum_i \kappa_h(x - x_i)$
what the nadaraya-watson model anoth name for kernel regress
what the median absolut deviat in ml $\text{mad} = \text{median}( x - \text{median}(x) )$
what the use of the mean absolut deviat in ml it provid a robust approxim to the standard deviat $\hat \sigma 1.5 \text{mad}$
how is local weight regress deriv take the estim of kernel regress and use a general kernel rather than a smooth kernel: $\hat f(x_*) = \sum \kappa(x_*, x_i) y_i$ which effect fit a constant function locally. then general it by fit a linear function locally.
what the object function in local weight regress $\min_\beta \sum \kappa(x_*, x_i) [y_i - [\beta(x_*)]^tx_i]$
what loess in ml local weight scatterplot smooth a kind of local weight regress
what lowess in ml anoth name for loess
in local weight regression, how are the paramet $\beta(x_*)$ comput for a test case by solv a least squar problem
intuitively, what a gaussian process it a distribut over the space of functions: it assum $p(f(x_1), \dotsc, f(x_n))$ is joint gaussian, with $\sigma_{ij} = \kappa(x_i, x_j)$ be some posit definit kernel function the idea is that if $\kappa$ consid $x_i, x_j$ to be similar, the valu of the function at that point is similar too
in the context of regression, what the comput complex of fit a gaussian process $o(n^3)$
what a gp in ml a gaussian process
what krige in ml anoth name for gaussian process regress
how is a gaussian process usual denot $f \text{gp}(m, \kappa)$ where $m(x)$ is the mean function (usual 0) and $\kappa(x_1, x_2)$ is the kernel/covari function
what a se kernel in ml a squared-exponenti kernel, aka a gaussian kernel
what dace in ml design and analysi of comput experi where you model a model with gaussian process regress so you can explor modifi the paramet of the simul without actual rerun the simul
what the invers lemma concern $(m/h)^{-1}$ $(m/h)^{-1} = e^{-1} + e^{-1}f(m/e)^{-1}ge^{-1}$
what the invers lemma concern $(m/h)^{-1}fh^{-1}$ $(m/h)^{-1}fh^{-1} = e^{-1}f(m/e)^{-1}$
what the matrix determin lemma concern $ m/h $ $ m/h h = m/e e $
given a matrix $m = \left( {smallmatrix} a &amp; b c &amp; d {smallmatrix} \right)$, what the schur compliment $s_d$ $s_d = a - bd^{-1}c$
given a matrix $m = \left( {smallmatrix} a &amp; b c &amp; d {smallmatrix} \right)$, what the schur compliment $s_a$ $s_a = d - ca^{-1}b$
what the invers of a matrix $m = \left( {smallmatrix} a &amp; b c &amp; d {smallmatrix} \right)$ $m^{-1} = \left( {matrix} i &amp; -a^{-1} b -d^{-1} c &amp; i {matrix} \right) % \left( {matrix} s_d &amp; &amp; s_a {matrix} \right)^{-1} $
what the joint distribut for a predict gp on noise-fre observ look like $\left( {matrix} f f_* {matrix} \right) = \mc{n}\left(\left( {matrix} \mu \mu_* {matrix} \right), \left( {matrix} k &amp; k_* k^t_* &amp; k_{**} {matrix} \right) \right)$ where $f, x$ are the train point and $f_*, x_*$ are the test point and $k = \kappa(x, x), k = \kappa(x, x_*), k = \kappa(x_*, x_*)$
how is a gp with noise-fre observ use for predict construct the join distribut $p(f_*, f x_*, x)$ use the standard ula for condit gaussian to deduc $p(f_* x_*, x, f)$
what the joint distribut for a predict gp given noisi observ the same as the noise-fre distribution, but with $k$ replac with $k_i = k + \sigma_y^2 i$ where the observ $y$ suffer nois $\epsilon \mc{n}(0, \sigma^2_y)$
how can the posterior predict mean on a test point $x_*$ be written for a gp with noisi observ $\bar f_* = \sum \alpha_i \kappa(x_i, x_*)$ where $\alpha = k^{-1}_i y$
what the factor analysi distanc function $m(l) = \lambda\lambda^t + \text{diag}\left(\frac{1}{l^2}\right)$ where $\lambda$ is a $d k$ matrix, with $k \ll d$
what the use of the factor analysi distanc function when defin a gaussian kernel over a vector space, a covari matrix need to be select the factor analysi distanc function $m(l)$ approxim the covari matrix as a low rank matrix $\lambda\lambda^t$ plus a diagon matrix $\text{diag}\left(\frac{1}{l^2}\right)$ where the column of $\lambda$ correspond to relev directions, and the $\text{diag}\left(\frac{1}{l^2}\right)$ defin the varianc in the various dimens
how do you usual set the paramet for the kernel of a svm cross-valid sinc the svm regular strength is intertwin with the kernel parameters, it usual has to be done on a grid
how are the kernel paramet for a gp usual estim empir bayes, with the pre-nois observ $f$ be margin out done with continu optim method
what are the compon of the margin likelihood when fit a gp with noisi observ $p(i x) = -\frac{1}{2}yk^{-1}_i y - \frac{1}{2}\ln k_i + c$ where the first term describ how well the data fit the model and the second term describ the complex of the model
which covari matric general have veri high determin diagon one
which covari matric general have veri low determin one that are almost all $1$s
when fit a gp, is the margin likelihood convex wrt to the kernel paramet not usually.
what the `length scale in gps the varianc of the kernel: a small varianc impli that similar point have littl to do with eachoth so the function vari veri rapid
how can you approxim the predict posterior for a gp calcul the map for the paramet sampl point $s$ from around the map $p(f \mc{d}) \propto \sum_ w_s p(f \mc{d}, \theta_s)p(\theta_ \mc{d})$
what a central composit design in ml when approxim the posterior of a distribut by sampl point from around the map it the sampl strategi of pick point $ 1$ standard deviat along each dimens
what multipl kernel gp rather than have a singl kernel and optim it paramet multipl kernel gp use $\kappa(x, y) = \sum w_j \kappa_j (x, y)$ and optim the weight $w_j$
comput speaking, what tricki about calcul the predict mean of a gp to comput the predict mean, you have to invert $k_y$ which is often poor conditioned. choleski decomposit is preferred.
what the comput complex of fit a gp the same as invert $k_y$ usual $o(n^3)$ via choleski decomposit
what a semi-parametr gp it a linear model $f(x) = \beta^t \phi(x) + r(x)$ where $r(x) \text{gp}(0, \kappa)$ model the residu
what happen if the $\beta$ in a semi-parametr gaussian process are drawn from a normal distribut the semi-parametr model for $f(x)$ becom a regular old gp
what the model usual use to adapt gps to binari classif $p(i x) = \text{sigm}(yf(x))$ and $f \text{gp}(0, \kappa)$
whi is the mean usual set to 0 in gp model becaus it make the math simpler and the model can easili accomod a non-zero empir mean by rais the varianc of the first few point
how are gps usual adapt to multiclass classif $y x \text{cat}(\mc{s}(f(x)))$ where $f = [f_c]$ is a vector of $c$ latent functions, one for each label where $f_c \text{gp}(0, \kappa_c)$ and the softmax function $\mc{s}$ is appli componentwis
when adapt gp to classif problems, how are the model usual fit by analyt deriv the gradient amp; hessian then appli irl to comput the map
what model is usual use when adapt gps for poisson regress $y \text{poi}(r)$ where $\ln r \text{gp}(0, \kappa)$
how do gps compar to bayesian linear regress bayesian linear regress with covari $\sigma$ is equival to a gp with $\kappa(x, y) = x^t\sigma y$ note this is a degener covari function, sinc it onli has $ d$ nonzero eigenvalu
what the consequ of use a degener covari function in a gp underfit usually, sinc the model isn't flexibl enough captur the data it'll be overconfid in it fit too, sinc it isn't awar of the altern not repres by the covari function
what kernel is usual use when construct gps over geograph data a matern kernel.
what a linear smoother in ml a regress function which is linear in it train inputs: $\hat f(x_*) = \sum w_i(x_*) y_i$
what the differ between a linear smoother and a linear model a linear model is linear in all it input a linear smoother is onli linear wrt the train respons
what'r the most common linear smoother in ml kernel regress local weight regress smooth spline gp regress
what an equival kernel in ml when interpret a gaussian process as a linear smoother, the weight function are found to depend on the invers of $k$. this cloud the issu of whether the weight function has local support an equival kernel is a $\kappa^\prime$ in term of which the weight function can be approxim which clear demonstr whether the weight are local or not
how doe the of a gp classifi compar to a svm in the object they have the same regular term $\ln k $ but differ likelihood - hing in the svm case and a gaussian nois likelihood $p(i f)$ in the gp case
how do spars kernel machin relat to linear model they'r just linear model with a basi of the $\phi(x) = [\kappa(x, x_i)]_i$
what the effect object function for fit a univari smooth spline $j(f) = \text{rss}(f) + \lambda t \left[\frac{d^m f}{dx^m}(x)\right]^2 dx$ where the first term control the fit and the second term control the smooth
what the of a univari smooth spline on input $\{x_i\}$ with a $m$th-deriv objective, it a piecewis polynomi of degre $2m-1$ in the interv $[x_i, x_{i+1}]$ and of degre $m-1$ in the tail
how are univari spline usual fit conceptu easiest way is to use ridg regress but there are linear time method too
what a `knot with respect to spline it the point where two of the polynomi piec in a spline connect
what'r the paramet use for spline regress the knot and scale coeffici
what the method of penal spline in ml it spline regress with a $l_2$ regular on the scale coeffici of each polynomi piec
how are smooth spline relat to gps if a regress gp has the kernel $\kappa_{sp}(x, y) = t_0^1 (x - u)_+(i - u)_+ du$ then the map estim will be a cubic spline this is a pretti unnatur kernel though!
what a thin plate spline in ml it a general of spline to 2d with a regular that minim $ \nabla^2 f ^2$
how can a posit definit kernel be use to construct a rkhs sinc it posit definite, $\kappa(x, y) = \sum \lambda_i \phi_i(x) \phi_i(y)$ and if we suppos $f = \sum f_i \phi_i$ then we can defin $\langl f, g \rangl = \sum \frac{1}{\lambda_i} f_i g_i$ then we get the reproduc properti $\langl \kappa(x, \cdot), \kappa(y, \cdot) \rangl = \kappa(x, y)$
what a rkhs in ml a reproduc kernel hilbert space; ie a hilbert space with a kernel $\kappa$ such that $\langl \kappa(x, \cdot), \kappa(y, \cdot) \rangl = \kappa(x, y)$
what the represent theorem in ml defin an object $j(f) = l(f) + \frac{1}{2} f ^2_h$ where $l$ is some convex loss function (like the rss) and $ f _h$ is the norm generat by a posit definit kernel $\kappa$ then the minim of $j$ can be written as $f(x) = \sum \alpha_i \kappa(x, x_i)$
how doe rkhs regress relat to gps linear regress with a rkhs regular generat by $\kappa$ is ident to map estim with a gp with kernel $\kappa$
what the model use in gp-lvm it base on pca, but we integr out $w$, optim over $z$, and replac the linear kernel $zz^t$ of the latent space with our own kernel $k$. this give $y_i k, \sigma^2 \mc{n}(y_i 0, k + \sigma^2 i)$
what a gp-lvm in ml a gaussian process latent variabl model
how are gp-lvms usual fit gradient base optim
how do gp-lvms compar to kpca in kpca we learn a kernel map from the observ space to the latent space in gp-lvm we learn a kernel map from the latent space to the observ space the latter is usual much more effect
what the main drawback of gps they'r not sparse, so they'll usual take $o(n^3)$ time to fit
what are templat match method in ml one which essenti tri to repres input as a combin of prototyp $\mu_k$
what the most common exampl of templat match algorithm in ml kernel method
what the ard kernel $\kappa(x, y) = \theta_0 \exp( -\frac{1}{2}\sum \theta_i (x_i - y_i)^2)$
what the biggest problem with kernel method you need to know a good kernel first
what the of an abm $f(x) = w_0 + \sum_1^m w_m \phi_m(x; v_m)$ where $v_m$ are the paramet for basi function $\phi_m$
what abm stand for in ml adapt basi function model
what cart stand for in ml classif and regress tree
what a decis tree in ml anoth name for a cart
what the idea behind cart in ml recurs partit the input space and defin a local model in each space
what the axis-parallel split model in ml a cart where each node ask whether the input $j$th compon is greater or less than a specifi threshold
how can a cart model be use for classif by store the distribut over class label in each leaf
how are cart model with axis-parallel split node usual fit a cost function is defin over the space of dataset $\text{cost}(\mc{d})$ then the space is partit recursively: at each step, if there is a node deem by some worth-splitting-funct to be worth splitting, the algorithm search for the dimens $j$ and threshold $t$ such that split the data along $j$ at $t$ minim $\text{cost}(\mc{d}_l) + \text{cost}(\mc{d}_r)$, where $\mc{d}_l, \mc{d}_r$ are the datapoint to the left amp; right of the threhold
when fit cart trees, what are common heurist use in decid whether to split a node will the reduct in cost be suffici larg has the tree exceed maximum depth is the data at the node suffici homogen is there enough data at the node to be worth split
in classif problems, what it mean for a subset of the data to be pure the classif label are all the same.
defin model identifi a model for which it is possibl to identifi the true valu of the under paramet after obtain an infinit number of observ for it as oppos to model for which differ combin of the paramet do not yield differ probabl distribut for the observ variabl
when fit a cart for regression, what the usual cost function the sse
when use cart for regression, what an altern to the $\sum_\mc{d} (y_i - \bar y)^2$ cost fit a linear regress model for each leaf use as input the featur that were chosen on the path from the root and use the residu error as the cost function
when you'r fit a cart for classification, what are some common cost function that are use first, fit a multinouilli model to the data in the leaf, give class probabl $\hat \pi_c$ then common cost function are misclassif rate of the most probabl class label by the model entropi of the model gini index of the model
what the devianc of a multinouilli distribut anoth name for it entropi
what the in ation gain of a test it the chang in entropi between some distribut $y$ and that distribut when condit on a specif outcom of the test, $y t(x) = c$
what the gini index of a multinouilli distribut if the distribut has class probabl $\pi_c$, $\sum_c \pi_c(1-\pi_c)$
what the usual interpret of the gini index of a multinouilli distribut it the expect error rate; $\pi_c$ is the probabl of outcom $c$ and $1 - \pi_c$ is the probabl it'll be misclassifi
what the usual indic of overfit in ml valid set error be much higher than train set error
what the use of prune in ml when fit a cart, it hard to make a good judgement as to whether to split the data at a node use just the next split instead, it better to be liber when grow the tree, then prune it afterward by cut off subtre that give small reduct in the cost
what the usual rule of thumb use when prune cart in ml evalu the error of each root subtre use cross-valid then pick the smallest one within 1 standard deviat of the error of the best
what'r the advantag of cart model they can be convert into logic rule they can handl a wide rang of input they per automat variabl select they'r robust to outlier they scale well
what'r the disadvantag of cart model they'r not veri good predictor compar to other kind of model they'r unstabl - small chang in the input can have larg effect on the structur of a tree
how are cart tree usual adapt to handl miss data surrog split add `miss as a new valu and treat the data as fulli observ
what'r surrog split in cart fit dure training, for each decis variabl look for other variabl with which it is strong correl if the decis variabl is miss at test time, the surrog variable(s) can be use instead
what bag in ml if you'v got an unstabl model/high-vari estim (like a cart tree) then you can reduc the varianc by train $m$ differ model on differ subset of the data and then averag their output
what the usual problem with bag in ml learn model on differ subset of the same data is prone to produc highly-correl predictors, which limit the varianc reduct
what the random forest approach to improv cart train sever tree on differ subset of the data \emph{and} differ subset of the variabl then averag their output this lead to veri accur predictors!
what'r the bayesian altern to bag mcmc sampl over the space of model ensembl bayesian infer over the space of model ensembl
what bart in ml bayesian adapt regress tree it a bayesian altern to bagging, which use bayesian infer over the space of cart to generat the ensembl of cart to averag
what a popular altern to cart model hierarch mixtur of expert
what'r the main advantag of hme over cart hmes make predict by averag over all experts, wherea cart just use the model in the leaf fit a hme involv smooth optimization, vs cart greedi discret approach it much easier to take a bayesian approach to the paramet of a hme than it is a cart
what the of a general addit model $f(x) = \alpha + f_1(x_1) + \dotsb + f_d(x_d)$ where the $f_i$ are model by some scatterplot smoother and $f(x)$ can be map to $p(i x)$ use a link function (like in a glm)
what the two kind of function usual use for the $f_i$ in a general addit model regress spline or more commonly, smooth spline
when construct general addit models, what the convent that ensur the addit constant $\alpha$ is uniqu identifi assum $\sum_i f_j(x_{ij}) = 0$
what the backfit algorithm for general addit model fit $\hat \alpha = \frac{1}{n}\sum_i y_i$ center the responses: $y_i^\prim = y_i - \hat \alpha$ updat each $f_j$ in turn, use as a target vector the residu obtain when $f_j$ is omit when it converges, ensur the output of each $\hat f_j$ is zero by set $\hat f_j = \frac{1}{n}\sum \hat f_j(x_{ij})$
what the suffici condit for the backfit algorithm to find a global optimum of a general addit model $x$ has full column rank.
how can the backfit algorithm be appli to nns with a top-level glm replac the weight least squar step of irl with a backfit algorithm where the respons are weight
what a gam stand for in ml general addit model
what the comput complex of fit a gam use backfit it $o(ndt)$, where $t$ is the number of iter
what mar stand for in ml multivari adapt regress spline
how is a mar model construct $\mc{c} = \{(x_j - t)_+, (t - x_j)_+ : t \{x_{ij}\}_i\}_j$ $\mc{m} = \{1\}$ at each step, pick a $h_m \mc{m}$ and a reflect pair $c_1 = (x_j - t)_+, c_2 = (t- x_j)_+$ from $\mc{c}$ add $h_m c_1, h_m c_2$ to $\mc{m}$ and fit a linear combin of them to the current residual. when the order of interact becom too large, stop ad terms. then prune out terms, start with the one that caus the smallest increas in residu error, until the cv error stop improving.
what a reflect pair in mar $(x_j - t)_+, (t - x_j)_+$ for some $t \{x_{ij}\}_i$
intuitively, what the boost techniqu in ml some basi function $\phi_i$ are generat by a `base learner algorithm and the base learner is repeat appli to weight version of the data where more weight is given to exampl that were misclassifi in earlier round
what a weak learner in ml it anoth name for the base learner in boost
what weak learner is boost most common use with cart model
how do boost decis stump compar with the altern very, veri well. they turn out lower error and better-calibr probabl than random forest and are much better than singl decis tree
what the guarante provid by boost for binari classification, it been proven that ani weak learner can be boost to arbitrarili high per anc on the train set if it doe better than random chanc in the first place
what a decis stump in ml a 2-leaf cart, usual use for boosting.
how doe boost per in term of overfit it veri resist to it often the error on the test set will continu drop some time after it reach zero on the train set
how can boost be view from a theoret perspect it a of gradient descent in the space of function
what the optim problem use in boost $\min_f \sum_i l(y_i, f(x_i))$ where $l$ is a loss function and $f$ is a abm
what l2boost it forward stagewis addit model with a l2 loss where $\beta = 1$ and at each step the weak learner is ask to predict the residu
what gradboost it a generic stagewis boost algorithm that'll work for ani loss function
what adaboost it forward stagewis addit model with an exponenti loss function.
what logitboost it a forward stagewis addit model with a logloss function where optim over $\phi$ is done by use a newton updat scheme
what estim do you get from optim the exponenti loss for a binari classifi $\frac{1}{2}\ln\frac{\pi}{1-\pi}$
what estim do you get from optim the logloss for a binari classifi $\frac{1}{2}\ln\frac{\pi}{1-\pi}$
what'r the two most common smooth upper bound on the 0-1 loss exponenti loss logloss, which is nicer comput
what the logloss function for binari label $\ln(1 + e^{-yf(x)})$
what the exponenti loss function for binari label $\exp(-yf(x))$
what the forward stagewis addit model method a boost technique: $f_0(x) = \arg \min_\gamma \sum_i l(y_i, f(x_i; \gamma))$ then for $m$ iterations, $(\beta_m, \gamma_m) = \arg \min_{\beta, \gamma} \sum_i l(y_i, f_{m-1}(x_i) + \beta\phi(x_i;\gamma))$ $f_m(x) = f_{m-1}(x) + \beta_m\phi(x;\gamma_m)$ where $\phi$ is the function generat by the weak learner
what the main tune paramet in forward stagewis addit model the number of iter $m$
what are common criteria for pick the number of iter $m$ in forward stagewis addit model earli stopping: monitor per anc on a separ train set and termin if it start to decreas alternatively, use model select criteria like aic/bic
what the shrinkag approach to improv forward stagewis addit model use updat of the $f_m(x) = f_{m-1}(x) + \nu\beta_m\phi(x;\gamma_m)$ where $0 \nu 1$ is the shrinkag paramet
what the shrinkag paramet in forward stagewis addit model usual set to $\nu 0.1$
what least squar boost anoth name for l2boost
what a common speed optim to adaboost the weight can be updat in an onlin manner
what'r the problem with exponenti loss it put a lot of weight on misclassifi examples, which make it veri sensit to outlier it not the logarithm of ani pmf for binari variables, so it can't be use to recov a probabl estim
what object is the weak learner pass in adaboost at each step, let $w_i = \exp(-y_i f_m(x_i))$ be the weight and ask the weak learner to solv $\phi = \arg\min_{\phi^\prime} w_i \mbb{i}(i \phi^\prime(x_i))$
how is the solut updat in adaboost given weight $w_i$ and a $\phi$ from the weak learner, let $\text{err} = \frac{1}{\sum w_i} \sum w_i \mbb{i}(y_i \phi(x_i))$ be the weight misclassif rate $\beta^\prim = \frac{1}{2}\ln \frac{1 -\text{err}}{\text{err}}$ $f^\prime(x) = f(x) + \beta^\prim \phi(x)$
what the updat rule in gradient boost given a loss function $l(y, f)$ and approxim $f_m$, calcul $g_i = \left[\frac{ l(y_i, f(x_i))}{ f(x_i)}\right]_{f_m}$ then ask the weak learner to solv $\gamma = \arg\min_\gamma \sum(-g_i -\phi(x_i;\gamma))^2$ and set $f_{m+1} = f_m(x) + \nu \phi(x;\gamma)$ where $\nu$ is the step size, either constant or found by some linesearch alg
what binomialboost in ml it gradient boost with a logloss function
what spars boost in ml it boost with a weak learner that just pick the variabl which best predict the residu
what'r two other name for spars boost match pursuit forward stagewis linear regress
how is spars boost relat to lar as the shrinkag in the updat $\nu \rightarrow 0$, it turn into lar with the number of updat $m$ correspond to the lar regular penalti $\lambda$. if boost is further modifi to allow some variabl delet steps, full lar is recov
what the boost variant of mar spars boost but with smooth spline rather than linear regress when map from the chosen variabl $x_j$ to the residu
whi should shallow tree usual be use when appli boost to cart becaus their varianc is low
what the mart model in ml the multivari adapt regress tree model which is gradient boost with a shallow regress tree as the weak learner with the modif that after a tree is fitted, the paramet in the leav are re-estim to minim the loss
intuitively, what are the two reason boost per s so well it a sort of $l_1$ regularization, but in reverse: rather than start with a larg set of weakly-learnt $\phi_k$ and select a subset, you start with the empti step and at each step add a new $\phi_k$ it can be shown that boost maxim the margin
what l1-adaboost in ml a combin of adaboost and $l_1$ regular which greedili add the best featur $\phi_m$ use adaboost then prine off the irrelev one use $l_1$ regular
how can boost be view from a bayesian perspect the output of a boost algorithm is basic a mixtur of expert model and the process of boost can be view as a one-at-a-tim version of em: in the e step, the posterior respons will reflect how well the exist expert explain a datapoint, and if it a poor fit those datapoint will have the greatest influenc on the next expert and in the m step, we adjust the new expert to best explain the residu between the posterior and the sampl
what the usual consequ of boost inabl to alter the weight assign to a $\phi$ it alreadi ad unnecessarili larg models, sinc the onli option thing the algorithm can do is to re-add the weak learner with a differ weight
conceptually, what a neural network a bunch of logist (or similar) regress stack on top of eachoth with the top layer be either anoth logist (or similar) regress (for classification) or a linear regress model (for regression)
what the al model of a two layer regress neural network $y x \mc{n}(w^tz(x), \sigma^2)$ $z(x) = g(vx)$ where $g$ is the ation function, which is appli componentwis
what a transfer function in neural network anoth name for an ation function
what are the most common ation function in neural network sigmoid tanh rectifi linear unit
what a relu in neural network rectifi linear unit a common transfer function, with the $\max(0, u)$
what'r the hidden layer in neural network all the variabl except the output and the input
what a mutual inhibit arc in neural network an edg between two (or more) output that ensur onli one of them turn on
what a multilay perceptron in ml anoth name for neural network
what a univers approxim in ml a system that can model ani suitabl smooth function to ani desir level of accuraci
what the most common kind of univers approxim neural network
what a mlp in ml a multilay perceptron
what the purpos of the hidden layer in a neural network to learn nonlinear combin of the inputs, ie featur extract
when is featur extract import in ml when the individu input aren't individu veri in ativ (like pixels)
what a convolut neural network one in which each hidden unit take input from a contigu chunk of the previous layer, it `recept field and where the weight are tie across the imag
what the use of tie the weight of differ hidden unit in a neural network togeth it ensur that if a featur is learnt from one part of the input domain, it'll be recognis if it appear in other part ie the network is translat invari
what a featur map in neural network it the output of a set of replic hidden units, each correspond to a differ contigu chunk of the input and with their paramet all tied. each one effect recognis the same feature, but in differ part of the input
when train neural networks, what a standard trick to expand the train set generat distort version of the origin data
what a simpl way of generat distort version of imag generat amp; appli a random flow field
what a subsampl layer in neural network one which comput a a summari function over each small window in the input
what a skip arc in a neural network an arc that goe direct from input to output
what a recurr neural network in ml one with feedback arcs, ing a nonlinear dynam system
what a rnn in ml recurr neural network
what task are rnns particular good at natur languag model
what the main problem with rnns they'r veri hard to train, as the gradient in the weight space common vanish
what a lstm in ml a long short-term memori model a variant of rnns that tri to solv the vanish gradient problem rnns are prone to
what a hopfield network in ml a neural network that permit symmetr connect
what associ memori anoth name for in ml a hopfield network
what the probabilist counterpart of a hopfield network a boltzmann machin
what kind of optim strategi are neural network usual train use onlin first-ord methods, sinc they usual use veri larg train set
what are the pre- and post-synapt valu in the hidden layer of neural network the pre-synapt valu is $a = vx$, befor ani nonlinear is appli the post-synapt valu is $z = g(a)$, after the nonlinear is appli
what are the pre- and post-synapt valu in the output layer of a neural network the pre-synapt valu is $b = wz$, befor ani nonlinear is appli the post-synapt valu is $\hat y = h(b)$, after the nonlinear is appli where, if a glm is be used, $h$ is the canon link function
what a nn in ml neural network
how are offset/bia term usual account for in nns by clamp some variabl in each layer to 1
what the object use by the backpropag algorithm for regress problem $\text{rss}$
what the object use by the backpropag algorithm for classif problem the cross entropi
what the idea in the backpropag algorithm for nns calcul the gradient of the nll by appli the chain rule of calculus to the output then appli a gradient descent routin
what ation function are general prefer when design neural network for hidden nodes, becaus it symmetr around 0 sigmoid for output nodes, becaus it symmetr around 1/2
what doe $\delta^v_j$ usual stand for in the context of nns if $a$ is the pre-synapt output of the layer with weight $v$, it $\delta^v_j = \frac{ j}{ a_j}$ ie the error signal for that layer
given a nn with top layer with weight $w$ and a hidden layer with weight $v$, how doe backpropag calcul the error signal for the hidden layer $\delta^v_j = g^\prime(a_j) w_{:,j}^t \delta^w$ where $\delta^w$ is the error signal in the top layer
what'r the step in the backpropag algorithm for train a nn on input $x$, make a forward pass through the network to calcul the pre amp; post-synapt output then comput the error signal for the top layer $\delta^{(l)}$ and repeat backpropag it to calcul the error signal $\delta^{(l)}$ for all the previous layer have done this for all input $x_n$, the gradient for layer $l$ is $\sum_n \delta^{(l)}_n z_n$ where $z_n$ is the output of the previous layer
under the canon link function, what the error signal for the top layer of a nn $\delta^{(l)} = \hat y - y$
what the identifi problem with nns on a hidden layer with $h$ unit we can flip all the input amp; output and get the same , give $2^h$ equival set of the paramet and we can chang the ident of all the hidden unit and get the same , give $h!$ equival set of the paramet
how do nn train algorithm usual deal with local minima add more data and use a stochast method per multipl restart and pick the model with the best per anc averag the output of multipl model (\emph{don't} averag the paramet though, they'r unidentifiable!)
what weight decay in the context of nns regular a nn model by impos a prior of $\mc{n}(0, \frac{1}{\alpha}i)$ on the weight give a gradient of $\sum \delta_n^{(l)} z_n + \alpha v$
what the main reason to center amp; normal the varianc of the input in ml becaus then spheric prior make sense.
how is overfit usual suppress when train nns earli stop weight decay
how should the layer size for a nn be chosen as larg as is feasible, sinc with suffici strong regular the excess unit will be ignor
whi shouldn't the same regular prior be use for differ layer of a nn becaus suppos we shift or scale the train set then we'd like the nn to learn the same function by suitabl scale it intern weight amp; bias term but this scale is differ for differ layer so a differ regular strength is need for differ layer
what soft weight share in the context of nns it a way to regular paramet such that similar weight will share statist strength achiev by model the weight prior as a mixtur of diagon gaussians. similar weight will have the same mean and variance, and so will draw from the same gaussian in the mixtur
what a cnn in ml convolut neural network
what semi-supervis embed a way to forc neural network to treat similar object similarly: let $f(x_i)$ be an embed of object $x_i$ (ex: $f(x_i)$ is the output of a layer in the nn) let $s_{ij}$ indic whether $x_i$ amp; $x_j$ are similar defin a loss function $l(f(x_i), f(x_j), s_{ij})$ that encourag $f(x_i)$ to be close to $f(x_j)$ if $s_{ij} = 1$ then the loss function for the network as a whole (usual the nll) can be augment with $\lambda l$, with $\lambda$ describ the strength of the supervis
what the standard algorithm for train nns with semi-supervis embed object adapt stochast gradient descent: first pick a random $(x_n, y_n)$ and take a gradient step to optim the nll then pick a pair of similar $x_i, x_j$ and take a gradient step to optim $\lambda l(f(x_i), f(x_j), 1)$ then pick a random $x_k$ to the (veri probably) dissimilar pair $x_i, x_k$ an take a gradient step to optim $\lambda l(f(x_i), f(x_k), 0)$
what an ensembl learn model $f(i x, \pi) = \sum_m w_m f_m(i x)$ where each $f_m$ is a base model and the $w_m$ are paramet
what a committe method in ml anoth name for ensembl learn
what stack in ml train an ensembl model by solv $\hat w = \arg \min_w \sum_i l(y_i, \sum_m w_m \hat f_m^{-i}(x))$ where $\hat f^{-1}$ is the model train without $(x_i, y_i)$
what the use of stack in ml it much more robust against overfit than the naiv approach which doesn't leav one input out
what the error correct output code approach in ml a of ensembl learn where the class label are encod with an ecc then each bit is predict use an individually-train classifi
what bma stand for in ml bay model averaging.
what stochast gradient boost gradient boost but at each step, choos a differ fraction (usual 80 ) of the data to use
what a partial depend plot a way of visual black-box models: pick a subset of variabl $s$ plot $f(z_s) = \frac{1}{n} \sum_i f(x_i^\prime)$ where $x_i^\prime$ is $x_i$ with the variabl in $s$ replac with $z_s$ effect averag out all but the variabl in $s$
how can you determin the relat import of predictor variabl in ensembl of tree calcul the fraction of node in which variabl $j$ is used.
what the joint distribut of a first order markov model $p(x_{1:t}) = p(x_1)\prod p(x_t x_{t-1})$
what the transit function in a markov model the $p(x_t x_{t-1})$
what a markov chain anoth name for a markov model
what a homogen markov chain one whose transit function is independ of time
what'r two other name for a homogen markov chain stationari time-invari
conceptually, what a stochast process it a general of probabl distribut to the space of function
what a finite-st markov chain one where $x_t \{1, \dotsc, k\}$
what the transit matrix of a discrete-st markov chain the $a$ such that $a_{ij} = p(x_t = j x_{t-1} = i)$
what a stochast automaton equival to a stationary, finite-st markov chain
what a left-to-right transit matrix an upper triangular transit matrix that correspond to a markov chain where the state can be number left-to-right and no transit ever goe left
what'r the chapman-kolmogorov equat $a(m+n) = a(m)a(n)$ for a homogen discrete-st markov chain
what $a(m)$ denot in the context of a markov chain the $m$-step transit matrix, $a_{ij}(m) = p(x_{t+m}=i x_t = j)$
what a $k$th order markov model one where the transit consid the last $k$ state
what an $n$-gram model an $n$th order markov chain whose state space is all word
what the mle for an bigram model let $n_j^1$ be the count of the number of time a sentanc start with $j$ let $n_{jk}$ be the number of time $j$ is follow by $k$ then $\hat \pi_j \propto n^1_j$ $\hat a_{jk} \propto n_{jk}$
what $\pi$ repres in the context of markov model some kind of distribut over the state of the model usual the initi one or a stationari one
when is the zero-count problem particular notic with $n$-gram model whenev the number of state $k$ or the order $n$ is particular large, sinc $n$-gram model have $o(k^n)$ state
what delet interpol in the context of $n$-gram model avoid the spars data problem by interpol between the unigram and bigram frequenc in the transit matrix: $a_{ij} = (1-\lambda)f_{ij} + \lambda f_j$
what'r the unigram frequenc in an $n$-gram model $f_k = \frac{n_k}{n}$ where $n_k$ is the count of unigram $k$
what'r the bigram frequenc in a markov model $f_{ij} = \frac{n_{ij}}{n_i}$
what backoff smooth in the context of $n$-gram model if a bigram frequenc $f_{ij}$ is too small we `back off to a more reliabl estim $f_j$
what the bayesian view of backoff smooth draw the $i$th row of the transit matrix $a_i$ from $\text{dir}(\alpha_0 m)$ where $\alpha_0$ is the prior strength and $m$ are the prior means. then the posterior is $a_i \text{dir}(\alpha + n_i)$, where $n_i$ is compos of the transit count $n_{ij}$ this model will back off to $m$ as the regular is increas rather than $f_i$ though
when use a dirichlet distribut for the prior of a bigram model transit matrix, how should the prior strength $\alpha_0$ and mean $m$ be chosen either empir bay or $m_k \propto \{i : n_{ij} 0\} $, ie $j$ prior is proport to the number of bigram in which it occur
how do $n$-gram model usual handl novel word everi novel word in the input is replac with `unk
what the stationari distribut of a markov chain the $\pi$ such that $\pi = \pi a$
what'r the global balanc equat $\pi_i \sum_{j i} a_{ij} = \sum_{j i} \pi_j a_{ji}$ the probabl of $i$ time the net flow out of $i$ is equal to the probabl of each $j$ time the probabl of move into $i$
how can the stationari distribut be comput for posit transit matric find the eigenvector of $a^t v = v$
how can the stationari distribut be comput for \emph{non-negative} transit matric we'v got $k+1$ constraint from $\pi = \pi a$ and $\pi\mb{1} = 1$, and $k$ free variables. so replac the last column of $i - a$ with $\mb{1}$ to get the matrix $m$, and let $r = [0, \dotsc, 0, 1]$ then solv $\pi m = r$.
what an absorb state in a markov model one where onc the model enter the state it can't leav it
what an irreduc markov model one which is strong connect
what the limit distribut of a markov model $\pi_j = \lim_{n \rightarrow fty} a_{ij}(n)$, if it exist
what the period of a state in a markov model $d(i) = \text{gcd} \{t : a_{ii}(t) 0 \}$
when is a state in a markov model aperiod when it period is 1
when is a markov model aperiod when all it state are aperiod
what the theorem concern when a finite-st markov model has a stationari distribut everi irreducible, aperiodic, finite-st markov chain has a uniqu limit distribut which is equal to it stationari distribution.
what a regular markov model one where for some $n$, $a_{ij}(n) 0$ ie everi state is reachabl from everi other in bound time
what the theorem concern when an markov model - finit or not - has a stationari distribut everi irreducible, ergod markov chain has a uniqu limit distribut which is equal to it stationari distribution.
what a recurr state in a markov model a state where the return probabl is 1
what a non-nul recurr state in a markov chain it a recurr state where the return time is finit
what a non-nul markov model one where everi state is non-nul recurr
what an ergod state in a markov model an aperiodic, non-nul recurr state
what an ergod markov model a model where everi state is ergod
what a time revers markov chain one satisfi the detail balanc equat
what'r the detail balanc equat for a markov model $\pi_i a_{ij} = \pi_j a_{ji}$
what a weaker suffici condit for identifi whether a markov model has a stationari distribut than ergod if a markov model is regular amp; satisfi the detail balanc equat with respect to $\pi$, then $\pi$ is a stationari distribut of the model
how can you modifi a markov chain to ensur it stationari distribut is uniqu give each state a small but nonzero probabl of transit to ani other state, $a^\prim = pa + \frac{1-p}{n}11^t$ this make the model aperiod amp; regular
what an effici method for find the lead eigenvector of a larg matrix the power method: repeat power the matrix then normal it or better, the mont carlo power method: construct the markov model associ with the graph and count how mani time each state is visit
what a markov switch model anoth name for a hmm
what a hmm in ml hidden markov model
what the usual observ model for a hmm with discret observ a matrix: $p(x_t = l z_t = k) = b_{kl}$
what the usual observ model for a hmm with continu observ a gaussian: $x_t \mc{n}(\mu_k, \sigma_k)$
what the filter task for hmms comput the belief state $p(z_t x_{1:t})$ for each $t$ repres a stream scenario
what the smooth task for hmms comput $p(z_t x_{1:t})$ repres an offlin scenario
what the fix lag smooth task for hmms comput $p(z_{t-l} x_{1:t})$, where $l 0$ is the lag this adapt the filter task by allow you to model an accuraci vs delay tradeoff
what the predict task for hmms comput $p(z_{t+h} x_{1:t})$ where $h 0$ is the predict horizon
what the viterbi decod task for hmms comput the joint map estim of the hidden state $z_{1:t}$ given $x_{1:t}$
in the context of a hmm, what the belief state at time $t$ $\alpha_t = p(z_t x_{1:t})$
what the forward algorithm for carri out the filter task on hmms in term of probabl comput the one-step-ahead predict density: $p(z_t = j x_{1:t-1}) = \sum_i p(z_t = j z_{t-1} = i)\alpha_{t-1}(i)$ this s the prior for $z_t$. use bay to updat the belief state: $\alpha_t = \frac{1}{z_t}p(x_t z_t = j)p(z_t = j x_{1:t-1})$
what the forward algorithm for the filter task on hmms in matrix notat $\alpha_t = \psi_t \odot (\psi^t\alpha_{t-1})$ where $\psi_t(j) = p(x_t z_t = j)$ is the local evid and $\psi$ is the transit matrix
what the local evid in the context of hmms $\psi_t(j) = p(x_t z_t = j)$
what the hadamard product $u \odot v$, ie elementwis multipl of two vectors/matric
what the forwards-backward algorithm for the smooth task on hmms $\alpha_t(j) = p(z_t = j x_{1:t})$, the filter belief state $\beta_t(j) = p(x_{t+1:t} z_t = j)$, the condit likelihood of futur evid $\gamma_t(j) = p(z_t = j x_{1:t})$, the posterior margin let $\psi_t$ be the local evid at time $t$ and $\psi$ be the transit matrix comput the forward messag $\alpha_t \propto \psi_t \odot (\psi^t\alpha_{t-1})$ comput the backward messag $\beta_t = \psi (\psi_{t+1} \odot \beta_{t+1})$ then $\gamma_t \propto \alpha_t \odot \beta_t$
what a smooth two-slic margin in the context of hmms $\xi_{t, t+1}(i, j) = p(z_t = i, z_{t+1} = j x_{1:t})$
how can smooth two slice margin for a hmm be comput use the forwards-backward algorithm: $\xi_{t, t+1} \propto \psi \odot (\alpha_t(\psi_{t+1} \odot \beta_{t+1})^t)$
what the comput complex of the forwards-backward algorithm $o(k^2t)$ for dens transit matric $o(kt)$ for spars transit matric
what a trelli diagram for hmms a visual of a markov model where each time is a column and each state is a row, with edg weight with the negat log-prob of a transit
how can find the map state of a hmm be interpret in term of trelli diagram it equival to find the shortest path through the diagram
what the mpm of a hmm maxim of posterior marginals, the $\hat z$ such that each $\hat z_t$ maxim $p(z_t x_{1:t})$
what the viterbi algorithm calcul forwards: $\delta_t(j) = \max_z p(z_t = j, z_{1:t-1} x_{1:t})$, the probabl of end up in $j$ at $t$ via the most probabl path $a_t(j) = \arg\max_i \delta_{t-1}(i)\psi_{j,:}\psi_t$, the most like previous state to $j$ then we can carri out traceback: $z^*_t = a_{t+1}(z^*_{t+1})$ where $z^*$ is the map estim of the joint state
what a use trick when implement the viterbi algorithm $\max \ln = \ln \max$ so the arithmet can be done in the log domain
what properti of a hmm doe the forwards-backward algorithm comput the mpm
what the $n$-best list in the context of hmms it the list of the $n$ most probabl path through the hidden state space usual obtain from a modif to the viterbi algorithm
what the advantag of the $n$-best list vs the map for hmms it allow some discriminit method to appli global knowledg that isn't avail to the hmm solver
what the problem with generat $n$-best list for hmms the path are often veri similar, so no new knowledg is gain
what the three-pass method to sampl path from the posterior distribut of a hmm use forwards-backward to comput the two-slic smooth posterior $p(z_{t-1}, z_t x_{1:t})$ normal the posterior get the condit $p(z_t z_{t-1}, x_{1:t})$ make a forward pass to recurs construct the sample: $z^*_t z_t z^*_{t-1}, x_{1:t}$
what the two-pass method to sampl path from the posterior distribut of a hmm appli the forward algorithm to comput the belief state $\alpha_t(j) = p(z_t = j x_{1:t})$ then $p(z_t = i z_{t+1} = j, x_{1:t}) = \frac{\alpha_t(i)}{\alpha_{t+1}(j)} \psi_{ij} \psi_{t+1}(j)$
intuitively, what the baum-welch algorithm it em for hmms
what the em algorithm for hmms the paramet are the initi state distribut $\pi$, the transit matrix $a$ and the observ paramet $b$ or $(\mu, \sigma)$ e step: fix the paramet and use the forwards-backward algorithm to calcul the node margin $p(z_t x_{i, 1:t_i},\theta)$ and the edg margin $p(z_{t-1}, z_t x_{i,1:t_i}, \theta)$. these can then be use to calcul $\mbb{e}[n_k^1]$, the expect number of first node $z_1$ in state $k$, $\mbb{e}[n_j]$, the expect number of node in state $j$, $\mbb{e}[n_{jk}]$, the expect number of transit from state $j$ to $k$. m step: use the expect count to calcul the mles for the parameters.
how are the paramet usual initi when appli em to hmms use fully-label data to get start ignor the markov depend and estim the observ paramet use mixtur model estim method random initi and use restart
what viterbi train in ml approxim the posterior over path through a hmm use the map path then use that to get the em alg for train a hmm start this isn't veri reliabl though as the initi paramet will be poor estim
how are hmms usual use for classif they'r use as the class-condit densiti insid a generat classifi and are train separ (sinc em can't be appli to the whole model), then tune discriminatively.
when select a hmm to use, how is the number of hidden state usual chosen grid search variat bay use an infinit hmm
what structur learn in the context of hmms learn a spars transit matrix
what the minimum entropi prior $p(a) \propto \exp(-\mbb{h}(a))$ where $a$ is a vector
what the usual way to do structur learn with hmms enforc a minimum entropi prior on the row of the transit matrix take the map estim use em
what a common problem with structur learn method for hmms they can sometim prune out all the incom connect to a state
what a semi-markov model a general of a markov model that allow transit to depend on how long the model been in it current state for
what a hsmm in ml hidden semi-markov model ie a semi-markov model where the state space isn't observ direct
what'r two other name for hsmms variabl durat hmm explicit durat hmm
what the main use of hsmms model $p(x_{t:t+l} z_t = k, d_t = l)$ ie the of $l$ correl obser when the model in state $k$ for $l$ timestep this is good for data that show local trend (like piecewis linear models)
how can hsmms be thought of as augment hmms can be thought of as a high-ord hmm: creat `durat counter state $d_t \{0, \dotsc, d\}$ when the model enter a new state $j$, it immedi transit to one of the durat state which it `count down through then it can transit to anoth state $k$, condit on $j$
what a simpl way to introduc non-geometr wait time into a hmm without have to use a hsmm replac each state with $n$ state with a chain of $n$ state which have the same emiss probabl by tune $n$ and the self-loop probabilities, a wide rang of wait time distribut can be achiev
what hhmm stand for in ml hierarch hidden markov model
how do hhmms compar to hmms a hhmm can alway be flatten out to a hmm but hhmms are easier to interpret and fit
what a hierarch hidden markov model in ml a transit between two state $i, j$ can be replac with a whole hmm where the model onli transit back to $j$ when the sub-model reach it termin state.
what an input-output hmm a hmm where the hidden state are condit on some control signal $u_t$
how are continu input usual coupl into a io-hmm $z_t x_t, z_{t-1} \text{cat}(\mc{s}(w_{z_{t-1}}u_t))$ ie a logist regress model whose paramet depend on the previous state
what an auto-regress hmm a hmm where the observ aren't condit independ given the hidden state
how are continu observ usual coupl in auto-regress hmms $x_t x_{t-1}, z_t \mc{n}(w_{z_t}x_{t-1}, \sigma_{z_t})$ ie a linear regress model where the paramet depend on the previous hidden state
what a regieme-switch hmm anoth name for an auto-regress hmm
how can ar-hmm be interpret in term of regular hmms they'r effect two markov chains: one on the hidden variabl to captur long-rang depend and anoth on the observ variabl to captur short-rang depend
what'r buri markov model an extens of ar-hmm which allow the depend structur between observ node to chang depend on the hidden state
what ar-hmm stand for in ml auto-regress hidden markov model
what a dynam bayesian multi-net anoth name for a buri markov model sinc it a mixtur of differ network
what a factori hmm a hmm that use a distribut encod of the hidden state: state $\{1, \dotsc, k\}$ can be encod as $\lg k$ coupl states, each repres a binari digit
what the problem with factori hmms condit on the observations, all the hidden state are coupled. effici approxim infer algorithm still exist though
what a coupl hmm a sequenc of hmms where transit in one chain depend on the state of the neighbour chain with all the hmms share the same set of observ variabl
what the influenc model in ml a kind of coupl hmm that use fewer parameters, but allow all chain to affect all others: the joint condit distribut $z_{ct} z_{t-1}$ is a convex combin of pairwis transit matric
what a dynam bayesian network a dgm which relat variabl to eachoth over time you provid the structur of the first time slice the structur that take one time slice to the next and the structur that take the hidden state to the evid
what dbn stand for in ml dynam bayesian network deep belief network
what the of a state space model $z_t = g(u_t, z_{t-1}, \epsilon_t)$ $y_t = h(z_t, u_t, \delta_t)$ where $u_t$ is the control signal $z_t$ are the hidden state $\epsilon_t, \delta_t$ are the system amp; observ nois $g,h$ are the system amp; observ function
what'r $g, h$ in the definit of a state space model $g$ is the transit model $h$ is the observ model
what'r $\delta_t, \epsilon_t$ in the definit of a state space model $\delta_t$ is the observ nois $\epsilon_t$ is the system nois
what'r $z_t, y_t$ in the definit of a ssm $z_t$ is the hidden state $y_t$ is the observ
what $u_t$ in the definit of a ssm the control signal
what ssm stand for in ml state space model
what a linear dynam system a ssm with linear-gaussian transit amp; observ model $z_t \mc{n}(a_t z_{t-1} + b_t u_t, q_t)$ $y_t \mc{n}(c_t z_t + d_t u_t, r_t)$
what anoth name for a linear-gaussian ssm a linear dynam system
what lg-ssm stand for in ml linear-gaussian state space model
what the advantag of the lg-ssm over more general ssms lg-ssms support exact infer and if the initi belief state $z_1$ is gaussian, so are all subsequ belief state
in the context of lg-ssms, what doe $\mu_{t \tau}$ denot $\mbb{e}[z_t y_{1:\tau}]$
what $\mu_t$ denot in the context of ssms $\mu_t = \mu_{t t}$ ie the posterior belief state of the mean
what the slam problem in ml simultan locat and map ie the problem of a robot travers an unknown world
what the recurs least squar algorithm use a lg-ssm to model a linear regress model by defin the hidden state to be the parameters, which are fix over time and the observ to be the input vector and then appli a kalman filter to solv the system.
what rls stand for in ml recurs least squar algorithm
how doe rls compar to lms lms fall out of rls if you approxim the covari term $\frac{1}{\sigma^2}\sum_{t t-1}$ in the rls updat rule with $\eta_t i$, where $\eta_t$ is the step size so an advantag of rls is it set the step size automat it also converg in a singl pass over the data
what lms stand for in ml least mean squar algorithm
what arma stand for in ml auto-regress move averag
what an auto-regress move averag model in ml a type of ssm where $x_t$ is ed from linear combin of the $p$ previous $x_{t^\prime}$ plus a linear combin of the previous $q$ nois term $w_{t^\prime}$
what'r arma model common use for time seri predict
what a structur time seri model a kind of ssm which assum some generat model for some time-seri data then margin out the hidden variabl to make predict
what the use of the kalman filter it solv the filter problem exact for lg-ssms in a bayesian manner, give the full posterior
what the analogu of the kalman filter for hmms the forward algorithm
what the predict step of the kalman filter $z_t y_{1:t-1}, u_{1:t} \mc{n}(\mu_{t t-1}, \sigma_{t t-1})$ $\mu_{t t-1} = a_t\mu_{t-1} + b_t u_t$ $\sigma_{t t-1} = a_t\sigma_{t-1} a^t_t + q_t$
what the measur step in the kalman filter $z_t y_{1:t}, u_t \mc{n}(\mu_t, \sigma_t)$ $\mu_t = (i - k_t c_t) \mu_{t t-1} + k_t y_t$ $\sigma_t = (i - k_t c_t)\sigma_{t t-1}$ where $y_t$ is the observ $k_t$ is the kalman gain matrix
what the residu $r_t$ in the kalman filter $r_t = y_t - \hat y_t$ where $\hat y_t = c_t \mu_{t t-1} + d_tu_t$
what the kalman gain matrix $k_t$ $k_t = \sigma_{t t-1}c^t_t s^{-1}_t$ where $s_t$ is the covari of the residu given the observations, $s_t = c_t \sigma_{t t-1} c^t_t + r_t$
when there are mani more observ than latent states, what optim can be made to the kalman filter $k_t$ can be precomputed, sinc it doesn't depend on the actual observations.
what the comput complex of the vanilla kalman filter $o( y ^3)$ to comput the gain matrix $o( z ^2)$ to comput the covari so it overal complex is determin by whether $ z $ is much larger than $ y $
what are some more numer stabl altern to the kalman filter the in ation filter, which updat the in ation paramet of the gaussian rather than the moment param the squar root filter, which work with the choleski decomposit of the varianc
what the analogu of the kalman smoother for hmms the forwards-backward algorithm
what the advantag of the kalman smoother over the kalman filter sinc the smooth task is offline, we can condit on all the data and signific reduc our uncertainti
what the rts algorithm anoth name for the kalman smooth algorithm
what the kalman smooth algorithm use the kalman filter to comput $\mu_{t t}$ and $\sigma_{t t}$ then work backwards: $z_t y_{1:t} \mc{n}(\mu_{t t}, \sigma_{t t})$ $\mu_{t t} = \mu_{t t} + j_t(\mu_{t+1 t} - \mu_{t+1 t})$ $\sigma_{t t} = \sigma_{t t} + j_t(\sigma_{t+1 t} - \sigma_{t+1 t})j^t_t$ where $j_t$ is the backward kalman gain matrix
what the backward kalman gain matrix $j_t = \sigma_{t t} a^t_{t+1} \sigma^{-1}_{t+1 t}$
what a neat featur of the backward pass of the kalman smooth algorithm it doesn't need the data!
what the biggest differ between the kalman smooth algorithm and the forwards-backward algorithm for hmms the backward pass need all the data from the forward pass and while the updat can be rewritten to be independ of the forward pass, it has sever disadvantages: it introduc a depend on the data the backward messag is a likelihood rather than a posterior
what two-filt smooth a kalman smooth variant that calcul $z_t y_{1:t}$ in the forward pass $z_t y_{t+1:t}$ in the backward pass and then combin them to calcul $z_t y_{1:t}$
what system identif in ml estim the paramet of a lg-ssm
what are some common constraint enforc when learn lg-ssms fix the system nois $q = i$, sinc it effect can be swallow by $a$ fix the observ nois $r$ to be diagonal, sinc it effect can be swallow by $c$ limit the eigenvalu of $a$ to $ 1$, which stop $z_t$ from blow up (nois stop the system from collapsing)
how is em appli to lg-ssms similar to the baum-welch algorithm for hmms but use kalman smooth rather than forwards-backward in the e step
what the advantag of the subspac method over em for learn lg-ssms em can be veri sensit to the initi paramet
if a lg-ssm is fulli observed, how can you train it by solv a linear regress problem where the datapoint are the pair $(z_{t-1}, z_t)$ and $(z_t, y_t)$
what the intuit behind the subspac method for train lg-ssms assum there was no observ or system noise. then $z_t = az_{t-1}$ and $y_t = cz_t$ so $y_t = ca^{t-1}z_1$ henc all observ must be generat from a $\dim(z_t)$-dimension manifold which can be estim use pca. onc we'v got estim for all the $z_t$, the model can be fit as though it were fulli observ
given a $y = f(x)$, where $x$ is gaussian, what are the two main way to approxim $y$ use a first-ord approxim of $f$, so $y$ is gaussian use $f$ exactly, but project $y$ onto the space of gaussian by moment match
what a nonlinear gaussian dynam system a ssm with possibly-nonlinear transit amp; observ function $g, h$ but with gaussian nois
what the idea behind the extend kalman filter algorithm given a nonlinear gaussian dynam system linear $g, h$ around the previous state estim use a first-ord taylor seri then appli the standard kalman filter equat this effect convert a stationari nonlinear system to a nonstationari linear system
when doe ekf work poor when the prior covari is large, so probabl mass end up a long way from where the function is linear when the function is high nonlinear near the current mean
what ekf stand for in ml extend kalman filter
what the idea in the unscent kalman filter it a modif of the extend kalman filter but rather than tri to linear approxim a function it pass a set of point through the function and tri to fit a gaussian to them afterward
what the idea in assum densiti filter given a ssm, at step $t$ suppos we have an approxim prior $q_{t-1} \mc{q}$ such that $q_{t-1}(\theta_{t-1}) p_{t-1}(\theta_{t-1} y_{1:t-1})$ predict: the predict distribut is $q_{t t-1}(\theta_t) = t p(\theta_t \theta_{t-1})q_{t-1}(\theta_{t-1})d\theta_{t-1}$ update: the posterior is $\hat p_t(\theta_t) \propto p_{t-1}(y_t \theta_t)q_{t t-1}(\theta_t)$ project: $q_t(\theta_t) = \arg\min_q \mbb{kl}(\hat p_t(\theta_t) q(\theta_t))$
how can the project step in assum densiti filter be done with $q$ in the exponenti famili by moment match
what the boyer-kol algorithm it assum densiti filter appli to discret dynam bay net with an approxim of the $q(\theta_t) = \prod_j \text{cat}(\theta_{tj} \pi_{tj})$ where the $\pi_{tj}$ are set by moment matching, $\pi_{tjk} = \hat p(\theta_{tj} = k)$
what a hybrid system a system with both discret amp; continu variabl
what a switch linear dynam system a combin of hmm and lg-ssm where $q_t$ evolv as a discret hmm and $z_t, y_t$ evolv as a lg-ssm, with the condit distribut be addit condit on the state of $q_t$
whi is exact infer intract on sldss becaus the discret variabl induc an exponenti explos in the number of mode the posterior has
what slds stand for in ml switch linear dynam system
what'r some other name for sldss jump markov linear system switch state space model
what multipl hypothesi track in ml an approach to approxim infer for hybrid system where low probabl trajectori in the discret tree are prune off
what the idea in a gaussian sum filter it an approach to model switch ssms. if there are $k$ possibl state for the switch then model the belief state at each step as a mixtur of $k$ gaussian and pass them through the updat by run $k^2$ kalman filter in parallel then project back to $k$ gaussian
what the optim way to approxim a mixtur of gaussian with a singl gaussian the solut that minim the kl diverg is $\mu = \sum \pi_k \mu_k$ $\sigma = \sum \pi_k (\sigma_k + (\mu_k - \mu)(\mu_k - \mu)^t)$
what the data associ problem in ml you'r given a seri of observ $y_{tk}$ and a set of object $z_{tj}$ there may be more or fewer observ than object at ani time slice, but the number of object is fix find a consist match of object to observ
what multi-target track in ml it the data associ problem with a variabl number of object at each time slice
how do you give a raw string liter in python \ {s = r'string'}
what'r panda primari class seri - 1d array timeseri - 1d array index w/ time datafram - 2d labelled, columnar structur panel - 3d label structur
what'r the two kind of input to a cluster algorithm similarity-bas clustering, use a dissimilar matrix $d$ feature-bas clustering, use a design matrix $x$
what a design matrix the $x$ of input variabl which has each datapoint as a row
what are the two kind of output of a cluster algorithm partit clustering, which return a partit hierarch clustering, which return a tree
what the usual comput complex of a cluster algorithm $o(nd)$ for partit cluster $o(n^2\lg n)$ for hierarch cluster
what a dissimilar matrix a nonneg matrix $d$ with a zero diagon with $d_{ij}$ repres the `distanc between $i$ and $j$ doesn't acut have to be a distanc though!
how are similar matric $s$ usual convert to dissimilar matric $d = \max(s) - s$ though ani monoton decreas function will do
what are common dissimilar measur for real-valu featur $l_2$ distanc $l_1$ distanc negat correl coeff
what the usual dissimilar metric for ordin data (like low, med, high) encod them as equally-spac real in $[0, 1]$ eg low is $1/3$, med is $2/3$, high is $3/3$ then appli ani dissimilar function over the real
what the usual dissimilar function for vector of categor variabl (like red, green, blue) ham distance.
what the puriti of a cluster in ml given class-label data, $n_i$ is the number of object in cluster $i$ $p_{ic}$ is the fraction of cluster $i$ which is class $c$, $p_i = \max p_{ic}$ is the puriti of cluster $i$ $\text{purity} = \sum \frac{n_i}{n}p_i$ is the puriti of the cluster
what the problem with puriti as a measur of the per anc of a cluster algorithm it doesn't penal put everi object in it own cluster
what the rand index of a cluster given a refer cluster $u$ and a comput cluster $v$ let $tp$ be the number of pair in the same cluster in $u$ that are also in the same cluster in $v$ let $tn$ be the number of pair in differ cluster in $u$ that are also in the same cluster in $v$ let $n_\text{pair}$ be the total number of pair then $r = \frac{tp + tn}{n_\text{pair}}$
what the adjust rand index assum a distribut of cluster base on the hyper-geometr distribution, $ar = \frac{\text{index} - \text{expect index}}{\text{max index} - \text{expect index}}$
what the normal mutual in ation of two cluster $u, v$ defin $p_u, p_v$ to be the distribut of randomly-select item over cluster in $u, v$ defin $p_{uv}$ to be the distribut of randomly-select item over $u v$ $\text{nmi}(u,v) = \frac{2\mbb{i}(u,v)}{\mbb{h}(u) + \mbb{h}(v)}$
what finit model base cluster fit a finit mixtur model and assign item to the model whose weight is highest
intuitively, whi is fit a mixtur model base on a dirichlet process often faster than fit model for differ $k$ when fit the dp-base model, the appropri valu of $k$ are determin long befor the paramet are estim wherea when fit model for differ $k$, the paramet must be estim for each valu that tri
what the definit of a dirichlet process $\text{dp}(\alpha, h)$ it a distribut over probabl measur $g\colon \theta \rightarrow \mbb{r}^+$, implicit defin by the fact that $(g(t_1), \dotsc, g(t_k)) \text{dir}(\alpha h(t_1), \dotsc, \alpha h(t_k))$ for ani finit partit $(t_1, \dotsc, t_k)$ of the space $\theta$
what'r the paramet of a dirichlet process $\alpha$, the concentr paramet $h$, the base measur
how can a dirichlet process be thought of in term of conjug prior $\text{dir}(\alpha)$ is a conjug prior for the categor distribut and if a sampl is observ of type $k$, the posterior is dirichlet distribut with $k$th paramet $\alpha_k + 1$ a dp general this in that an observ is of type $k$ if it fall in partit $t_k$, and the prior probabl of this happen is given by $\alpha h(t_k)$ so $\alpha$ is effect the prior sampl size
what a \text{gem} distribut let $\pi$ be an infinit sequence, generat by $\beta_k \text{beta}(1, \alpha)$ $\pi_k = \beta_k \prod_1^{k-1}(1 - \beta_l)$ then $\pi \text{gem}(\alpha)$
how do dps relat to the gem distribut $\pi \text{gem}(\alpha)$, $\theta_k h$ if we defin $g(\theta) = \sum \pi_k \delta_{\theta_k}(\theta)$ then $g \text{dp}(\alpha, h)$
what dp stand for in ml dirichlet process
what the consequ of the relationship between dps and the gem distribut sampl $g \text{dp}(\alpha, h)$ are discret with probabl 1: if you keep sampl from it, you'll eventu just keep get repetit of valu you'v seen alreadi
what the behaviour of the gem distribut the weight will alway go to zero eventu although how long that take increas with $\alpha$
what the definit of a chines restaur process $p(z_{n+1} = z z_{1:n}, \alpha) = \frac{\alpha}{\alpha + n}$ if $z$ has not yet appear $p(z_{n+1} = z z_{1:n}, \alpha) = \frac{n_k}{\alpha + n}$ if $z$ has appear $n_k$ time
what the intuit behind a chines restaur process when a person enter the restaurant, she may choos to join tabl $k$ with probabl proport to the number of peopl sit there already, $n_k$ or she might sit at a new tabl with probabl $\alpha$
what the blackwell-macqueen sampl scheme anoth name for the modifi polya urn which correspond to a dp
what the asymptot behaviour of the chines restaur process the number of distinct valu approach $\alpha \ln n$
how is a dp use to build a generat cluster model $g \text{dp}(\alpha, h)$ is a distribut over the possibl cluster paramet $\theta_i g$ are the paramet for generat $i$ $x_i f(\theta_i)$ for some distribut $f$
what the polya urn sampl scheme for a dirichlet process start with an urn with $\alpha$ black balls. if we draw a black ball, pick a new color accord to $h$ and drop it into the urn along with the black ball. if we draw a non-black ball, return it and anoth ball of the same color into the urn.
what the idea in affin cluster each data point must choos anoth data point as it exemplar (possibl choos itself) and each ing tree s a cluster
what the usual object in affin cluster maxim $s(c) = \sum s(i, c_i) + \sum \delta_k(c)$ where $s(i, c_i)$ measur the similar between $i$ and the centroid it chosen $c_i$ $\delta_k(c)$ is a penalti for some $i$ choos $k$ as it centroid when $k$ hasn't chosen itself
what algorithm usual use to fit dpmms a collaps gibb sampler
what algorithm is usual use to fit an affin cluster model loopi belief propag
what the normal cut cost of a cut $a$ $\text{ncut}(a_1, \dotsc, a_k) = \frac{1}{2} \sum \frac{\text{cut}(a_k, \bar a_k)}{\text{vol}(a_k)}$ where $\text{vol}(a_k) = \sum_{i a_k} d_i$ and $d_i$ is the weight degre of $i$
what the weight degre of a node in a graph $d_i = \sum w_{ij}$
what the cost of a cut $a$ of a graph $\text{cut}(a_1, \dotsc, a_k) = \frac{1}{2} w(a_k, \bar a_k)$ where $w(a_j,a_k)$ is the total weight of edg go from $a_j$ to $a_k$
conceptually, what spectral cluster the problem of minim the normal cut cost $\text{ncut}(a_1, \dotsc, a_k)$ of a cluster $a$ can be thought of as an optim problem over binari vector $c_i$ such that $c_{ik} = 1$ if point $i$ belong to cluster $a_k$ this is np-hard. if we replac `binari with `real though, we get an eigenvector problem and solv it give you a spectral cluster of the graph
how do you get a $k$-regular spars graph from a similar matrix use the $k$ most similar object to $i$ as the neighbour of $i$ and use the similar as edg weight
how the graph laplacian defin let $d$ be the diagon matrix of weight degre then $l = d - w$
what the theorem relat the eigenvalu of a graph laplacian to it connect compon the set of eigenvector of $l$ with eigenvalu 0 is span by the indic vector $1_{a_1}, \dotsc, 1_{a_k}$ where the $a_k$ are the $k$ connect compon of the graph
what'r two version of the normal graph laplacian $l_\text{rw} = d^{-1}l$, the random walk version $l_\text{sym} = d^{-\frac{1}{2}}ld^{-\frac{1}{2}}$, the symmetr version
what the connect between the random walk laplacian $l_\text{rw}$ and random walk $p = i - l_\text{rw}$ is the transit matrix of a walk on the graph
how can the normal cut cost be interpret in term of random walk if the graph is connect and bipartite, then for a binari cut $\text{ncut}(a, \bar a)$ is the probabl of the stationari distribut make a step across the boundari of $a$
what the advantag of the normal graph laplacian over the graph laplacian it account for the fact that some node are more high connect than other
conceptually, what the algorithm use in spectral graph cluster construct a graph from the similar matrix comput the laplacian $l$ (or normal laplacian) find $k$ eigenvector with zero eigenvalue, $u_k$ form $u$ use the $u_k$ as column let $y_i$ be the row of $u$ appli k-mean cluster to the $y_i$ then assign $i$ to cluster $k$ iff $y_i$ was assign to cluster $k$
what are the two main approach to hierarch cluster agglom cluster divis cluster
what the problem with standard hierarch cluster techniqu they'r just heuristics, they don't optim ani well-defin object which mean it hard to assess the qualiti of the hierarchi and they'll happili build a hierarchi out of nois
what a dendrogram a binari tree that repres a hierarch cluster
conceptually, how doe agglom cluster work each item start in it own cluster then the two `most similar cluster are repeat merg
what single-link cluster an agglom cluster techniqu which defin cluster similar as the distanc between the two closest member of each group
what kind of tree is produc by single-link cluster a mst
what complet link cluster an agglom cluster algorithm where the distanc between cluster is the distanc between the most distant two point
what the problem with single-link cluster the cluster violat the compact properti
what doe it mean for a cluster to be compact everi point within a cluster is similar to everi other point in that cluster
what the diamet of a cluster $d_g$ it the greatest dissimilar between two member of the cluster
what kind of cluster will the complet link cluster algorithm creat one with low diamet
what furthest neighbour cluster anoth name for complet link cluster
what averag link cluster an agglom cluster algorithm where the distanc between cluster is defin as the averag distanc between pair
what the problem with averag link cluster the cluster produc are sensit to the measur scale
what kind of cluster will averag link cluster produc cluster that are more compact than singl link cluster but further apart than complet link cluster
which of the common agglom cluster heurist is usual use averag link cluster
what the bisect k-mean algorithm a divis cluster algorithm where the split is chosen use k mean with $k =2$
what the divis cluster equival to single-link cluster form the mst of the graph then repeat break the link correspond to the greatest dissimilar
what the dissimilar analysi algorithm it a divis cluster algorithm where cluster are broken by split off the element with the greatest averag dissimilar from all other element and then move across ani other element which is more similar on averag to the member of the new cluster than the old cluster, in order of greatest differ in similar
what the advantag of divis cluster over agglom cluster it can be faster to build $l$-level hierarchies, as the algorithm can termin earli split decis are made in the context of see all the data
given a hierarch clustering, how can the `correct number of cluster be determin hope by observ a `jump in the length of the link of the dendrogram repres the merg of an `unnatur cluster with a `natur cluster
conceptually, what bayesian hierarch cluster it similar to agglom cluster but it use bayesian hypothesi test to decid which cluster to merg with the test base on a dirichlet process model
what'r the variabl use in bayesian hierarch cluster algorithm let $\mc{d}_i$ be the data in the leav of the tree $\mc{t}_i$ suppos we'r decid whether to merg $\mc{t}_i, \mc{t}_j$ then let $\mc{d}_{ij}$ be their merg data and $m_{ij}$ be an indic variabl determin whether to merg them or not.
what the probabl of a merg in the bayesian hierarch cluster algorithm $r_{ij} \propto p(\mc{d}_{ij} m_{ij} = 1)p(m_{ij} = 1)$ where $p(m_{ij} = 1)$ is the prior probabl of a merg
how the likelihood of the data defin in the bayesian hierarch cluster algorithm if the data should be merged, $p(\mc{d}_{ij} m_{ij} = 1) = t \left[ \prod_{\mc{d}_{ij}} p(x_n \theta)\right] p(\theta \lambda)d\theta$ if the data shouldn't be merged, $p(\mc{d}_{ij} m_{ij} = 0) = p(\mc{d}_i t_i)p(\mc{d}_j t_j)$ where the term on the rhs have alreadi been calculated, sinc this is a bottom-up algorithm
at a high level, what are the step in the bayesian hierarch cluster algorithm start with each datapoint in it own tree. then at each step merg the pair of cluster $\mc{d}_i, \mc{d}_j$ with the greatest merg probabl $r_{ij}$ when there onli one cluster remaining, delet ani edg with $r_{ij} 0.5$.
what a dpmm in ml dirichlet process mixtur model
how is the prior probabl of a merg at node $k$, $\pi_k$, on tree $\mc{t}_i, \mc{t}_j$, calcul in bayesian hierarch cluster initi $d_i = \alpha$, $\pi_i = 1$ then in a bottom-up fashion, the node $k$ with children $i, j$ has $d_k = \alpha\gamma(n_k) + d_i d_j$ $\pi_k = \frac{1}{d_k}\alpha\gamma(n_k)$ where $n_k$ is the number of point in $\mc{d}_k$, which is the union of $\mc{d}_j, \mc{d}_k$
what the intuit behind the bayesian hierarch cluster algorithm calcul of the prior probabl of a merge, $p(m_{ij} = 1)$ assum the cluster are generat by a dpmm with concentr $\alpha$. then $\alpha \gamma(n_k)$ repres $\mc{d}_k$ be a cluster drawn from the dpmm $\alpha \gamma(n_k) + d_id_j$ repres all the other partit of $\mc{d}_k$ consist with the current tree
how are the paramet $\lambda$ of the prior $p(\theta)$ and the dpmm paramet $\alpha$ fit in the hierarch bayesian cluster algorithm the gradient $\frac{ }{ \lambda} p(\mc{d}_k \mc{t}_k)$ ( amp; the same wrt $\alpha$) can be back-propag through the tree allow empir bay estim of the hyperparamet to be done
what biclust in ml cluster the featur as well as the sampl
what coclust in ml anoth name for biclust
what a simpl bayesian approach to coclust associ a latent cluster indic $r_i \{1, \dotsc, k_r\}, c_j \{1, \dotsc, k_c\}$ with each datapoint $i$ and featur $j$. assum the data are iid across sampl and featur within each cluster $p(x \theta) = \prod_{i,j} p(x_{ij} \theta_{r_i, c_j})$ and draw the $\theta$ from a dp.
what multi-view cluster a general of biclust the featur $j$ are cluster into views: $c_j \{1, \dotsc, v\}$, with $c \text{dp}(\alpha)$ then the datapoint $i$ in each view $v$ are clustered: $r_{vi} \{1, \dotsc, k(v)\}$, with $r_v \text{dp}(\alpha)$ and then the data is generat use these subclust
what the main advantag of multi-view cluster over biclust it robust to irrelev features, as they'll be farm off in their own featur cluster
what the stick-break process interpret of a gem distribut at the $k$th step, break off $\beta_k$ fraction of the remain stick. $\pi_k$ is the length of the broken off piece.
what multipl hypothesi test make multipl binari decis of the $p(y_i = 1 \mc{d}) \tau$ (note we'r condit on all the data!)
what the fals discoveri rate in multipl hypothesi test let $p_i = p(y_i = 1 \mc{d})$ be the probabl of a discovery. then $\text{fd}(\tau, \mc{d}) = \sum (1-p_i)\mbb{i}(p_i \tau)$ is the number of fals discoveri $\text{n}(\tau, \mc{d}) = \sum \mbb{i}(p_i \tau)$ is the number of discoveri and the fals discoveri rate is $\text{fdr}(\tau, \mc{d}) = \frac{\text{fd}(\tau, \mc{d})}{\text{n}(\tau, \mc{d})}$
what the direct posterior probabl approach to multipl hypothesi test set an accept fals discoveri rate $\alpha$ (usual $\alpha = 0.05$) model the distribut of the discoveri rate $p_i$ model the distribut of the fdr minim $\tau$ subject to $\text{fdr}(\tau, \mc{d}) \alpha$
what the usual import alia for panda \ {import panda as pd}
what the basic tenet of panda data align is intrins
what the index of a panda seri the collect name for the key use to label the data
what panda approach to non-uniqu index valu they'r allow but oper which don't support non-uniqu index will throw an except
what the standard miss data marker in panda nan
how do you creat a seri in panda \ {series(data, index=index)} where the default \ {index} is the first \ { data } natur number
which numpi object do panda seri correspond to a 1d ndarray
what the main differ between a panda seri and a numpi ndarray oper that combin seri elementwis will automat align the data base on the label with miss element be substitut by nan
how do you select a column in a panda datafram \ {df['colname']} \ {df.colname}
what the best way to think of the structur of a panda datafram as a dict of columns. so whole column can be inserted, del'd, etc
how do you select a row by label from a panda datafram \ {df.loc['rowname']}
how do you select a row by index from a panda datafram \ {df.iloc[5]}
what doe panda do when two seri with disjoint label are combin elementwis the union of the label is taken with miss data fill in by na*
what panda default behaviour for combin datafram and seri elementwis the seri is align on the datafram column and the oper is broadcast down the row
what panda default behaviour for combin datafram and timeseri elementwis if the datafram index contain dates, the timeseri will be align with the datafram row and the oper broadcast columnwis
how do you transpos a panda datafram \ {df.t}
how do you get a text summari of the structur of a panda datafram \ {df.info()}
what'r the axe of a panda panel \ {items}, axi 0, which index the datafram \ {major\_axis}, axi 1, which is the index/row of each of the datafram \ {minor\_axis}, axi 2, which is the column of each of the datafram
what panel data record of multipl phenomena (the columns) over multipl subject (the rows) over multipl observations/tim
how do you get one of the datafram item from a panda panel \ {wp['itemname']}
how do you get a row from a panda panel \ {wp.major\_axis['rowname']}
what doe the \ {squeeze} oper do in panda remov length-1 dimens from the shape of the object so \ {[[[x], [y]]} becom \ {[x, y]}
how doe panda support higher-dimension data via panel4d and panelnd
how do you get a small sampl of a panda seri \ {s.head()} \ {s.tail()} default number of item return is 5
what the \ {shape} of a panda object it an array of integ give how mani label there are along each dimens
when you assign to the index/columns/item of a panda object, how is data migrat from the old index/columns/item data under the first old label is move to the first new label data under the second old label is move to the second new label etc
how do you get an array of the data in a panda object \ {df.values}
which librari are common use by panda to acceler calcul \ {numexpr}, for general calcul \ {bottleneck}, for calcul involv array with nan
what the prefer way to conduct elementwis arithmet on a panda datafram \ {df.sub}, \ {df.add}, \ {df.mul}, \ {df.div}
what a common confus when appli elementwis arithmet to panda datafram and panel \ {df.sub(x, axis='columns')} specifi that \ {x} should be align with \ {df} on the column \ {wp.sub(x, axis='columns')} specifi that the oper should be broadcast over the column
what broadcast in panda if you conduct an elementwis oper with say a panda datafram and a scalar the scalar will be `broadcast over everi element of the datafram
how do you specifi the default valu for an elementwis oper in panda \ {df.add(df2, fill\_value=x)} which will be use in place of na*
how do you conduct elementwis numer comparison in panda \ {df.gt(df2)}, \ {df.lt(df2)} \ {df.ge(df2)}, \ {df.le(df2)} \ {df.eq(df2)}, \ {df.ne(df2)}
what are the boolean reduct for collect of bool in panda \ {df.any()} \ {df.all()}
what doe \ {df.bool()} do in panda if \ {df} has a singl element which is a boolean, it return it
how should you compar panda object for equival \ {df.equals(df2)} sinc this'll consid na* in the same locat to be equal
how can you arbitrarili combin the data from two panda datafram with the same column \ {df1.combine(df2, combiner)} where \ {combiner} take two series, one from each datafram
what panda \ {combine\_first} method \ {df1.combine\_first(df2)} replac ani na* in \ {df1} with the data from the same locat in \ {df2}
how do you comput summari statist over a panda datafram \ {df.sum(i)} or similar, where \ {i} specifi the axi to summar over \ {i = 0}, index, is the default \ {i = 1}, column
how do you comput summari statist (like the sum per row) over a panda panel \ {df.sum(i)} or similar, where \ {i} specifi the axi to summar over \ {i = 0}, item \ {i = 1}, major, is the default \ {i = 2}, minor
how can you handl miss data in panda when comput summari statist pass \ {df.sum(skipna=true)} to ignor na* data
what a common mistak when use axis-depend method in panda mix up which function have \ {axis} attribut that set the axi to match on and which have \ {axis} attribut that set the axi to broadcast over
what panda \ {describe()} \ {df.describe()} give the common summari stat of an object
how can you get the index of the minimum valu in a panda object \ {ds.idxmin()}
how do you get the number of each uniqu valu in a panda object \ {s.value\_counts()}
how can you bin the data in a panda seri into $k$ equally-s rang \ {cut(s, k)}
how can you bin the valu of a panda object into $k$ quantil \ {qcut(s, k)}
how infin repres in python \ {sp.inf}
how do you appli a function along an axi of a panda object \ {df.apply(f, axis=i)} \ {f} can return an object of the same dimens as the one it consumed, or a smaller one
how can you appli a function to the scalar of a panda object \ {df.applymap(f)}
how do you appli a lookup tabl to the scalar of a panda object repres the lookup tabl as a seri \ {s} \ {df.applymap(s)}
how can an axi be specifi in panda either with an index or with it name (\ {index}, \ {columns}, \ {items})
at a high level, what doe \ {df.reindex(idx)} do in panda reorder the data to match the new index throw away data for which there no correspond new label fill label for which there no data
how are index pass in panda by refer
what the relev of reindex to per anc in panda oper on align data are much faster than oper that have to implicit realign the data first
how can you reindex a panda object to match anoth panda object axe \ {df1.reindex\_like(df2)}
what the best way to align two object to eachoth in panda \ {df1.align(df2)} and option the align strategi can be specifi
when reindex a panda object, how can you specifi how miss valu should be dealt with \ {df1.reindex(idx, method='ffill')} copi in the previous non-miss valu \ {df1.reindex(idx, method='bfill')} copi in the next non-miss valu
how can you remov label from a panda axi \ {df.drop([label1, label2])}
how can you updat the label of a panda object \ {df.rename(f)} where \ {f} is a total function on \ {df} label \ {df.rename(l)} where \ {l} is a lookup tabl (seri or dict) on some subset of \ {df} label
what do you get when you iter over a panda object series: the valu dataframe: the column label panel: the item label
how can you iter through the row of a panda datafram \ {for (i, r) in df.iterrows()}
how can you iter through the key-valu pair of a panda object \ {for (col, series) in df.iteritems()}
how do you per string tran ation on the valu of a panda seri use the method in \ {s.str}
how can you convert categor data, repres as strings, into indic variabl in panda \ {get\_dummies}
how do you sort a datafram by a specifi column in panda \ {df.sort\_index(by='colname')}
how do you sort a panda seri by valu \ {s.order()} note that \ {s.sort()} doe it in place!
how do you get the $k$ largest element in a panda seri \ {s.nlargest(k)} equival to sort by valu and then call \ {head(k)}, but faster
what a multi-index in panda a hierarch index with label given by tupl \ {('top level label', second level label')}
what panda polici on mutat method none of panda method automat modifi data in place they'll onli do it when you explicit specifi so
how can you get the type of the object store in a panda datafram \ {df.dtypes} will return a seri of the datatyp store in differ column if a column hold more than one datatype, it'll return their most recent common ancestor
what the default integ type size in panda 64 bit, on all plat s
how do you upcast the element in a panda object \ {df.astype('typename')}
what a common gotcha concern select oper over column of integ in panda if the column contain a nan then everyth in the column will be cast to a float
how can you downcast the object in a panda object \ {df.convert\_objects()}
how can you select column from a panda datafram conting on their type \ {df.select\_dtypes(include=[bool])}
what the advantag of special index method over subscript and attribut access to an object in panda the type of the object isn't known in advanc when subscript or use attribut so certain optim can't be made
what'r valid argument to \ {loc()} in panda a singl label \ {'a'} a list/array of label \ {['a', b', c']} a slice object \ {'a':'f'} - note both endpoint are included! a boolean array \ {[true, false, false]}
what'r valid argument to \ {iloc()} in panda an integ \ {5} a list/array of integ \ {[1, 2, 3]} a slice object with int \ {1:7}
what \ {ix()} in panda the most general of indexing: it primarili label-bas access (ie \ {loc}) but will fall back to posit access (ie \ {iloc}) it also support floating-point access
how do \ {loc, iloc, ix} select the axe to use on a multidimension panda object all indic should be supplied: \ {df.loc[row\_idxr, col\_idxr]} but ani of them can be replac with the null slice \ {:} and ani index are left out complet are assum to be \ {:}
what happen if you tri and creat a new column on a panda datafram use attribut access it fail silently, creat a new attribut rather than a new column
what a common gotcha with subscript on a panda datafram while in general subscript oper over column subscript with a slice oper over rows!
what a common mistak with slice with index in panda forget that the rang are \emph{inclusive}
how doe panda deal with out-of-bound slice argument it'll truncat them to the length of the object
what the purpos of \ {at} in panda it for getting/set the scalar at a locat in an object without the overhead that \ {loc} has to do to sort out indexes/slices/etc \ {df.at['rowname', colname']}
what panda \ {isin} method do on datafram \ {df.isin(arr)} return a boolean datafram indic which valu were in the array \ {df.isin(dict)} doe the same, but use the dict to look up an array for each column label
what the differ between subscript with a boolean vector and use the \ {where} method in panda \ {where} preserv the shape of the data, replac valu that fail the condit with some fill valu
what the invers of \ {where} in panda \ { }, which replac each valu satisfi the condit with a fill valu
what panda \ {query} method \ {df.query()} take a string like \ {'(a b) amp; (b c)'} substitut each dummi variabl with the match column, get \ {(df.a df.b) amp; (df.b df.c)} then use it as a boolean vector index \ {df[(df.a df.b) amp; (df.b df.c)]}
in a panda queri expression, how do you refer the index on a datafram \ {df.query('index 2')}
how do you refer to the compon of a multi-index in a panda queri express \ {df.query('ilevel\_0 2')} to get the 0th level of the index, etc
what panda \ {take} method \ {df.take([1, 2, 3])} get the item at indic 1, 2, 3 and sinc it intend for just integ indices, it a lot faster than subscript
how can you find duplic row in a panda datafram \ {df.duplicated(['a', b'])} where the array indic which column to use to check equal by default, the first appear of a row is assum to be the uniqu one
how can you drop duplic row from a panda datafram \ {df.drop\_duplicated(['a', b'])} where the array indic which column to use to check equal by default, the first appear of a row is assum to be the uniqu one
how can you get a copi of a slice from a panda object use the \ {truncate} method
how can you get an arbitrari intersect of row and column in panda \ {df.lookup(rows, columns)}
what a float index in panda an index whose label are float subscript can use a slice of float to select a rang
what chain index in panda \ {df['colname']['rowname']} which caus a copi of the column to be made, then the scalar index from the copi column which mean assign will be to the copi column, not to the dataframe! better to use \ {loc['colname', rowname']}
how do you take the intersect of two index in panda \ {index1 amp; index2} they'r effect multiset
in what way can you creat a multi-index in panda pass a list of tupl use \ {multiindex.from\_tuples} pass two list to be product'd togeth \ {multiindex.from\_product} pass an array of list of the same length straight to the \ {index} argument of \ {series}
what happen in panda if you pass a list of tupl to an \ {index} argument of a constructor it use the tupl as atom valu for the index it doe \emph{not} creat a hierarch index
how can you get the label for a particular level of a panda multiindex \ {index.get\_level\_values(i)} where \ {i} can be an integ index or a name
how do you subscript into a multiindex panda object \ {df['levelzero', levelone']} (preferred) \ {df['levelzero']['levelone']}
what'r the two way to slice a multiindex in panda provid one or more tuples: \ {df[('start0', start1'):end1]} use a slice object: \ {df[(slice('start0', end0'), slice('start1', end1')), :]}
what the univers slice object in panda \ {slice(none)}
what a common mistak with use slice object for multiindex in panda you should specifi \ {:} for ani axi you don't want sliced! otherwis the slice might be misinterpret as be intend for all axe
what a prerequisit for slice an axi in panda the axi must be lexsort
how can you easili creat multipl slice object in panda \ {idx = pd.indexslice} then \ {idx[:, ['start1', end1']]} will creat the correspond slice object
what the easiest way to select data at a particular level of a multiindex \ {df.xs('label', level=2)}
how can you sort an index with respect to a level in panda \ {s.sortlevel(i)}
what the catch about sort and multiindex in panda multi-index don't implicit enforc an order on the index but mani of the method on them requir the data to be sort and other will return a copi rather than a view if it isn't!
what'r the two way to permut the level of a multi-index \ {df.swaplevel()} \ {df.reorder\_levels()}
how can you test whether a valu is null in panda \ {isnull()} \ {notnull()} cover both none/nan/nat/etc
how are na* valu usual fill in panda \ {df.fillna()} with either a default value, a fill method (\ {ffill}, \ {bfill}) or a lookup on the column label
how can you drop the label from a panda axi which correspond to null data \ {df.dropna()}
what an altern to fill miss valu in panda interpol them with \ {df.interpolate()}
what the nan liter in panda \ {np.nan}
how can you replac all instanc of some set of valu in a panda object \ {df.replace()} this has a lot of overloads, inc for regex
what a groupbi object in panda it a proxi object for a group note that the actual group oper is lazy!
how do you group the row of a datafram by their valu in two column \ {df.groupby(['colname1', colname2'])}
what do you get when you iter over a groupbi object \ {(key, group)} pair
how do you appli an arbitrari scalar-valu function to the group of a panda groupbi object \ {grouped.aggregate(f)} collect of function can also be pass
how can you appli an arbitrari size-preserv function to the group of a panda groupbi object \ {grouped.tran (f)} where \ {f} take and return the entir group
how can you select the group in a panda group satisfi some condit \ {grouped.filter(f)}
what dispatch in panda group if a member function is call on the group which doens't correspond to an actual member function it'll instead be dispatch to each individu group to execut
how can you appli an arbitrari function to the group in a panda group \ {df.apply(f)}
what a common gotcha with call \ {apply} on panda group it'll be call twice on the first group, so it can decid whether to take a fast or slow code path so don't pass a function which has side effects!
how doe panda groupbi handl na* key it doesn't - they'll be discard
how can you see where each row appear in a panda group use \ {grouped.cumcount()}
what doe \ {stack()} do in panda convert the lowest level of the column index to a new innermost level of the row multiindex
how do you join two panda object along an axi \ {concat(df1, df2)}
what doe panda \ {concat} do with the axe other than the one be join along depend on the argument it pass most commonly, \ {join='inner'} to take the intersect of the axe label \ {join='outer'} to take the union of the axe label
what a shortcut for concaten the row of two differ panda object \ {df1.append(df2)}
what the default behaviour of panda \ {append} concern the row label the indic must be disjoint unless \ {ignore\_index=true} is pass
how do you carri out full-on sql-esqu join in panda use \ {merge} and it million argument
what the shorthand for concaten two panda datafram by column \ {df1.join(df2)}
how can you conduct sql- join on order data in panda \ {ordered\_merge}
what panda \ {update} method \ {df1.update(df2)} replac the valu in \ {df1} correspond to non-na valu in \ {df2} with those valu
how do you use the pivot function in panda \ {df.pivot(index='cola', columns='colb', values='colc')} will creat a tabl with row given by the valu in \ {cola} column given by the valu in \ {colb} and valu given by \ {colc}
what happen if you omit the \ {values} argument to pivot in panda you'll get a hierarch column index where each column that wasn't specifi by the \ {index} or \ {columns} argument to pivot s the bottom level of the hierarchi
what doe \ {unstack()} do in panda convert the innermost level of the row index to a new innermost level of the column index
what doe \ {melt} do in panda it `unpivot to the row axi all but the column specifi by the \ {id\_vars} argument leav two non-identifi columns: \ {variable} and \ {value}
what doe \ {pivot\_table} do in panda can specifi which column(s) to use with \ {values}, \ {index}, \ {columns} and an aggreg function \ {aggfunc}, default to the mean
how can you construct detail group heurist in a reusabl way in panda creat a \ {grouper}
what \ {crosstab()} do in panda it calcul a crosstab/conting tabl between two object default to a frequenc tabl
how can you encod 1d valu as an enumer type in panda \ {labels, uniqu = factorize(x)} where \ {uniques} is an index of the distinct valu in \ {x} and \ {labels} give the index for each element of \ {x} in uniqu
what an andrew curv plot mani timeseri sampl from each class are plot as curv with each class be color differ
what the use of a hexagon bin plot they'r intend for data which is too dens for a scatterplot to be effect
what a scatter matrix plot given sampl $x_i$ over featur $j$ a scatter matrix plot is a grid of plots, where the $j,k$th plot is a scatterplot of $j$ vs $k$ while the $j,j$th plot is a densiti plot of $j$
what a parallel coordin plot meant for data where the featur have some order to them the featur are repres as equally-spac vertic line and point are repres as piecewis linear curv between them
what a lag plot a way to check whether a seri is random: plot $x_i$ vs $x_{i+l}$ where $l$ is some `lag
what an autocorrel plot a plot of the correl between $x_{i}$ and $x_{i+l}$ as $l$ vari use for check for random
how do you plot each seri in a panda datafram on it own subplot \ {df.plot(subplots=true)}
what trelli plot in panda a way to arrang data in a rectangular grid of plot by valu of specifi attribut
when doe a panda subscript return a view and when doe it return a copi subscript with a list of indic or a collect of boolean will return a copi subscript with a slice or a singl valu will return a view
what a causal mrf anoth name for a markov mesh
what a markov mesh the grid version of a markov chain where causal flow down and to the right
what a mrf in ml a markov random field
what a markov random field a graphic model whose edg are undirect
what a markov network anoth name for a markov random field
what an undirect graphic model anoth name for in ml a markov random field
what ugm stand for in ml undirect graphic model
at a high level, what'r the advantag of ugm over dgms they'r symmetr condit ugm work better than condit dgms
what'r the disadvantag of ugm compar to dgms the paramet are harder to interpret the paramet are less modular paramet estim is more expens
what the global markov properti for ugm node $a$ are independ of node $b$ condit on node $c$ if $c$ separ $a$ from $b$
what the shorthand for the markov blanket of a node $t$ $\text{mb}(t)$
what the closur of a node in a ugm $\text{cl}(t) = \text{mb}(t) \{t\}$
what the undirect local markov properti for ugm the markov blanket of a node is it immedi neighbour
what the pairwis markov properti for ugm two node are independ in a ugm if they don't share an edg
how do the local, global and pairwis markov properti relat on a ugm if the distribut is positive, then they'r equival (if the distribut is zero anywhere, that induc independ that aren't captur in the graph)
what moral a process for convert dgms to ugm copar are connect then direct are drop this lose some ci in ation though!
how can you use ugm to determin whether $a b c$ in a dgm extract the ancestr graph of $a b c$ moral it see if $c$ separ $a, b$
what the ancestr graph of a set of node in a digraph the subgraph on the node which are in the set or ancestor of the node in the set
what a perfect map of a distribut $p$ with respect to graphic model a graph $g$ such that $g$ is an i-map of $p$ and $i(g) = i(p)$ ie $g$ captur exact the ci properti of the distribut
what a ci relationship that can be repres by a dgm but not by a ugm a non-monoton one: $a b$ but $a \not b c$ repres in a dgm as $a \rightarrow c \leftarrow b$
what a ci properti that can be repres by a ugm but not by a dgm a cyclic one
what a monoton ci properti one such that if $a b c$ then $a b c,d$
what a chordal graph a graph in which everi length-4-or-great cycl has a chord, an edg between two non-adjac node
what a decompos graph a graph such that moral (with respect to some order of the variables) add no edg
what the special properti of chordal graph with respect to graphic model they can be perfect model as either dgms or ugm
what a factor of a ugm a potenti function $\psi_c(y_c \theta_c)$ of a cliqu $c$
what the hammersley-clifford theorem a posit distribut $p(y) 0$ satisfi the ci properti of an undirect $g$ iff $p(i \theta) = \frac{1}{z(\theta)}\prod_c \psi_c(y_c \theta_c)$ where $c$ are the maxim cliqu of the graph and $z(\theta)$ is the partit function
what the gibb distribut $p(i \theta) = \frac{1}{z(\theta)} \exp\left(-\sum_c e(y_c \theta_c)\right)$ where $e$ is the energi function
what the maxent distribut strong resembl the gibb distribut
how can a gibb distribut be convert to a ugm defin factor $\psi_c(y_c \theta_c) = \exp(-e(y_c \theta_c))$
what a pairwis mrf a mrf which is written as a product over the edg of the graph $p(i \theta) \propto \prod_{ t}\psi(y_s, y_t)$
what do the potenti in a mrf repres the `compat of the variabl assign they'r not probabilities!
what a maximum entropi mrf one where the log-potenti are linear in the parameters: $\ln \psi_c(y_c \theta_c) = \phi_c(y_c) \cdot \theta_c$ where $\phi_c$ are the featur function
what an associ markov network a discret mrf with potenti that encourag the state of each variabl in a cliqu to match
what the ise model a pairwis mrf such that $\psi_{st}(y_s, y_t) = {pmatrix} e^{w_{st}} &amp; e^{-w_{st}} e^{-w_{st}} &amp; e^{w_{st}} {pmatrix}$ where $y_ \{-1, +1\}$
what'r the two most common case of the ise model the same weight is set for all edges, $w_{st} = j$ and if $j 0$ then we get an associ markov network and if $j 0$ we get a frustrat system
what a frustrat system one in which all the constraint can't be satisfi simultan
what the comput complex of comput the partit function for mrfs in general it np-hard but it polynomi time for associ markov network
in an ise model, what an extern field an energi term which bias the spins, of the $\ln p(y) = \dotsb + b^ty$ where $b$ is the bias term
how doe an ise model relat to a ggm if the ise model has weight $w$ and bias $b$, $\lambda\mu = b$ $\lambda = w$ and the onli differ is that the ggm has continu parameters, while the ise model use bit-vector
what a hopfield network in term of mrfs an ise model on a complet graph with a symmetr weight matrix
what the main use of hopfield network associ memory: train the network on fulli observ bitvectors, correspond to the pattern we want it to memori and then present it with a partial bitvector and ask it to minim the energi over the unknown
what the iter condit mode algorithm it an algorithm for solv ise model where at each step a node is set to it most like state, given it neighbour
what the pott model a general of the ise model to $k$ states, use a matrix of the $\psi_{st}(y_s, y_t) = {pmatrix} e^{w_{st}} &amp; 1 &amp; 1 1 &amp; e^{w_{st}} &amp; 1 1 &amp; 1 &amp; e^{w_{st}} {pmatrix}$
what the behaviour of the pott model as the associ strength $j$ vari $j 1.44$ give small cluster $j 1.44$ give larg cluster $j = 1.44$ is the critic valu which give a mix of small and larg cluster
what the use of the pott model for imag segment as a prior for imag segmentation, sinc it encourag neighbour pixel to have the same discret label
what a chain graph a combin of an undirect and a direct graph
what the factor of a gaussian pairwis mrf it a pairwis mrf with $\ln \psi_{st}(y_s, y_t) = -\frac{1}{2}y_ \lambda_{st} y_t$ $\ln \psi_t(y_t) = -\frac{1}{2}\lambda_{tt} y_t^2 + \eta_t y_t$
what the distribut of a gaussian mrf it a mvn in in ation
what'r structur zero in a gaussian mrf the zero in the total precis matrix that correspond to absent edg in the mrf
what a vector auto-regress process $y_t y_{1:t-1} \mc{n}(\sum^l_1 a_l y_{t-l}, \sigma)$ where $l$ is the order of the process
how can a vector autoregress process be repres as a graphic model use direct edg to repres transit between time slice and if the precis is sparse, use undirect edg for the relationship within a time slice or if the covari is sparse, use bidirect edg for the relationship within a time slice
what a bidirect graph a way to repres spars covari matric where an absenc of an edg repres uncondit independ
how can a bidirect graphic model be convert to a dag each edg $x \leftrightarrow y$ is replac with $x \leftarrow z \rightarrow y$ where $z$ is a hidden `confound variabl
what a direct mix graphic model a graph which has both direct and bidirect edg
how can spars covari matric be repres as graphic model use covari graphs, a kind of bidirect graph
what probabilist relat model a topic that tri to combin first order logic with probabl theori
what a markov logic network first order logic repres as a mrf. use factor of the $\ln \psi_c(x_c) = w_c \phi(x_c)$ where $\phi(x_c)$ evalu the horn claus $c$ on variabl $x$ and $w_c$ are the weight of the model
in a markov logic network, what the intuit interpret of the weight $w_c$ repres the probabl of a world where the claus is satisfi relat to the probabl of a world where it isn't.
what the open univers problem a logic question about the number or exist of some object or relat
what a horn claus in logic an or claus with at most one posit liter
what moment match a method of fit model that consist of find paramet $\theta$ such that $\mbb{e}_\text{emp}[y] = \mbb{e}_{p(i \theta)}[y]$ where the left is the clamp term and the right is the unclamped/contrast term
how are maxent mrfs usual fit gradient descent
what the problem with fit maxent mrfs via gradient descent to comput the gradient at each step, the distribut of the variabl need to be infer from the paramet which is veri slow on mrfs as there no close
what a maxent mrf anoth name for a log-linear mrf
what the problem with train mrfs to do gradient-bas optim properly, you need to be abl to calcul map/ml estim which in general isn't possibl with mrfs
what the pseudo-likelihood $l_{pl}(\theta) = \frac{1}{n}\sum_i \sum_d \ln p(y_{id} y_{i, -d}, \theta)$ where $d$ denot the dimens
what the use of the pseudo-likelihood for mrfs it can be use to approxim the mle when train a mrf
what'r the limit of the pseudo-likelihood in the context of mrfs it hard to appli to model with hidden variabl if a neighbour $j$ of a node $i$ s a perfect predictor of $i$, then the model will learn to reli complet on $j$, even if it mean ignor other use evid
what the stochast maximum likelihood algorithm an algorithm for fit mrfs that use stochast gradient descent and mcmc for generat sampl with the mcmc initi at the start of each gradient step with it state at the end of the last gradient step
what sml stand for in ml stochast maximum likelihood
what featur induct in ml a way to find good featur function for model like maxent mrfs start with a base set of features, then continu creat new featur combin out of old ones, greedili ad the best to the model and optim the new paramet
what the idea behind iter proport fit when fit a general mrf, at optimum it must be that $p_\text{emp}(y_c) = p(y_c \theta)$ and in chord graphs, this mean that $p_\text{emp}(y_c) = \psi_c(y_c \theta_c)$ while this doesn't hold in general, we can imagin tri to enforc this condition. ipf follow
what the iter proport fit algorithm a coordin ascent algorithm for fit discret log-linear mrfs use the updat $\psi^{t+1}_c (y_c) = \frac{p_\text{emp}(y_c)}{p(y_c \psi^t)}\psi^t_c(y_c)$ where $\psi_c$ are vector and $y_c$ is a one-hot encod of the cliqu $c$
what a condit random field a mrf where the cliqu potenti condit on the input featur $p(i w, w) = \frac{1}{z(x,w)}\prod_c \psi_c(y_c x,w)$
what crf stand for in ml condit random field
what a discrimin random field in ml anoth name for condit random field
what the of a maximum entropi markov model $p(i x,w) = \prod_t p(y_t y_{t-1}, x, w)$ where $x = (x_{1:t}, x_g)$ and $x_t$ are the featur specif to node $t$ and $x_g$ are the global featur
how do maximum entropi markov model aris they'r a discrimin version of a hmm creat by `revers the arrow in the observ model of the hmm so that the hidden state are now the output
what memm stand for in ml maximum entropi markov model
what the problem with memm model label bias: local featur $x_t$ at time $t$ do not influenc observ $y_{t-1}$ and all earlier the root of this problem is that memm are local normalized, with each condit probabl distribut sum to 1
what a chain structur crf a crf whose hidden variabl a chain
whi are crfs less use for onlin task than dgms becaus crfs are global normal by the partit function, which depend on all the node vs dgms, which are locally-norm condit probabl distribut
what the definit of the partit function for a mrf $z(\theta) = \sum_i \prod_c \psi_c(y_c \theta_c)$
how are crfs use for handwrit recognit each letter has a potenti associ with it, calcul by a discrimin classifi (like a nn) and neighbour letter have edg potenti given by a languag bigram model
what the of a truncat gaussian potenti $\exp\left(-\frac{1}{2\sigma^2} \min(x^2, \delta_0^2)\right)$ where $\delta_0$ encod the cutoff
what the use of the truncat gaussian potenti when $x = x_1 - x_2$, it a discontinuity-preserv potenti use for thing like imag processing, where you don't want to smooth out edg
what a metric crf infer is veri hard on crfs over continu variabl unless the model is joint gaussian so a common trick is to discret each variabl and if this is done with a pairwis crf, the potenti often a metric
whi is train crfs much slower than train mrfs the partit function in a crf depend on the input $x_i$ so infer has to be done for everi sampl at everi gradient step
whi is train a mrf with hidden variabl general much harder than train a fulli observ mrf when there are hidden variables, the object can be non-convex
what loss calibr infer the bayesian distinct between infer and decis make is onli optim if the exact posterior can be calcul when it can't (ex: mrfs), it can make sens to take the loss function into account when fit the model
how can you bound a term of the $\ln \sum_{i \mc{y}} \exp(y)$ $\max_{i \mc{y}} y \ln \sum_{i \mc{y}} \exp(y) \ln \mc{y} + \max_{i \mc{y}} y$
what the addit of big o notat $f g$ is equival to $ f(x) - g(x) c$ for all $x$ and some $c$
what the loss function use by a ssvm $r_\text{ssvm} = \frac{1}{2} w ^2 + c\sum \left[\max_i \{l(y_i, y) + w^t\phi(x_i, y)\} - w^t\phi(x_i, y_i)\right]$ where $\phi$ is the featur vector $l$ is the loss function $c$ is the regular paramet
what ssvm stand for in ml structual support vector machin
how do ssvms aris from a probabilist standpoint take a log-linear crf and a gibb prior and the posterior expect loss for some loss function $l$ then by upper bound this, you can get the object function for a ssvm
how do svms relat to ssvms svms fall out of ssvms when you use label $\mc{y} = \{-1, 1\}$ a 0-1 loss function featur vector of the $\phi(x, y) = \frac{1}{2}yx$
when use slack variabl to object functions, what are two way to penal a larg loss more than a small loss given constraint $c(w, y)$ and loss $l$, slack rescaling: $c(w, y) 1 - \frac{\xi}{l(y)}$ margin re-rescaling: $c(w, y) l(y) - \xi$
what the frequentist perspect on how ssvms aris frequentist want to minim the regular empir risk. given a log-linear crf, and a $l_2$ regularizer, the object of a ssvm is a convex upper bound on this
what the idea in the cut plane approach to fit svms use the constraint-with-slack interpret of the object function start with an initi guess $w, \xi$ and no constraint on ani sampl then for each sampl in turn $i$, find the `most violat constraint involv predict $\hat y_i$ if the violat exceed the current valu of $\xi_i$ by more than $\epsilon$, add the constraint to the work set for $i$, $\mc{w}_i$ and solv the qp to find new $w, \xi$. keep repeat this until no sampl work set has chang in the last iter
whi doe the cut plane method for fit ssvms work so well becaus onli polynomi mani constraint need to be ad befor converg and as soon as they are, the exponenti number of other constraint are guarante to be satisfi within a toler of $\epsilon$
what the constraint interpret of the object of a ssvm $\min_{w, \xi} \frac{1}{2} w ^2_2 + c \xi _1$ such that for each $i$ and each $y \mc{y}\backslash y_i$ $w \cdot (\phi(x_i, y_i) - \phi(x_i, y)) 1 - \frac{\xi_i}{l(y_i, y)}$
how doe the constraint interpret of ssvms aris we want the predictor $\arg\max_{i \mc{y}}w^t\phi(x, y)$ to choos correct on each test case $(y_i, x_i)$ which mean that for all $i$ and $y \mc{y}\backslash y_i$, we want $w^t\phi(x_i, y_i) w^t\phi(x_i, y)$ add a $l_2$ regular on the weight and some slack variabl (which you then solv and sub out) and you get the object for a ssvm
what the comput complex of the cut plane method for fit ssvms $o(1/\epsilon^2)$
what the tricki part of the cut plane method for fit ssvms find the most violat constraint at each step the method for do this is conting on the loss function
what the structur perceptron algorithm an onlin algorithm for fit ssvms on train sampl $(y, x)$ calcul $\hat y = \arg\max p(y^\prim x)$ then if $\hat y y$, take a step $w_{k+1} = w_k + \phi(y, x) - \phi(\hat y, x)$
what the idea in the stochast subgradi descent algorithm for fit ssvms calcul the subgradi of the ssvm object function and then estim the gradient use just one of the sample-depend term use this estim to take steps, project $w$ back onto the unit ball after each one
what a latent crf a crf where the factor are allow to reli on hidden variabl
what the concave-convex procedur an algorithm for optim a differ $f(w) - g(w)$ between convex function $f, g$ which work by find a linear upper bound $u$ on $-g$ then minim $f(w) + u(w)$
what algorithm is usual use for solv latent svms the concave-convex procedur
in general, what kind of model can the forwards-backward algorithm be appli to ani chain-structur graphic model
what the sum-product algorithm anoth name for belief propag for tree
what'r the phase of the belief propag for tree algorithm pick a root collect evidence: send messag from the leav to the root distribut evidence: send messag from the root to the leav
what the interpret of the messag in the collect evid phase of belief propag for tree $m^-_{s\rightarrow t}(x_t) = p(x_t v^-_{st})$ where $v^-$ is all the evid from the `downstream side of $s-t$, away from the root
what the definit of the belief state at $t$ after the collect evid phase of the belief propag on tree algorithm $\text{bel}^-_t(x_t) = \frac{1}{z_t} \psi_t(x_t) \prod_{c \text{ch}(t)} m^-_{c\rightarrow t}(x_t)$
what the model under the belief propag for tree algorithm a pairwis mrf with factor $\psi_s(x_s)$ at each node indic local evid $\psi_{st}(x_s, x_t)$ at each edg indic the potenti for edg $s-t$
what the definit of the messag in the collect evid phase of belief propag for tree $m^-_{s\rightarrow t}(x_t) = \sum_{x_s} \psi_{st}(x_s, x_t) \text{bel}_s^-(x_s)$
what the interpret of the belief state at $t$ after the collect evid phase of the belief propag algorithm $\text{bel}^-_t(x_t) = p(x_t v_t^-)$ where $v_t^-$ is all the evid from the `downstream side of $t$, away from the root
what the equival of the belief propag algorithm on tree graphic model for hmms the forwards-backward algorithm
when conduct belief propag on trees, how can you calcul the probabl of the evid $p(v) = \prod_t z_t$ where $z_t$ is the normal constant for node $t$ after the collect evid phase
what the interpret of the messag in the distribut evid phase of belief propag for tree $m^+_{s\rightarrow t}(x_t) = p(x_t v^+_{st})$ where $v^+$ is all the evid from the `upstream side of $s-t$, toward the root
what the definit of the belief dure the distribut evid phase of the belief propag algorithm for tree $\text{bel}_s(x_s) \propto \text{bel}^-_s(x_s) \prod_{t \text{pa}(s)} m^+_{t\rightarrow s}(x_s)$
what the definit of the messag dure the distribut evid phase of the belief propag algorithm for tree $m^+_{t \rightarrow s}(x_s) = \sum_{x_t} \psi_{st}(x_s, x_t)\frac{\text{bel}_t(x_t)}{m^-_{s\rightarrow t}(x_t)}$
what the interpret of the belief after the distribut evid phase of belief propag for tree $\text{bel}_s(x_s) = p(x_s v)$ where $v$ is all the evid in the tree
what the differ between the belief updat and the sum-product s of the belief propag on tree algorithm belief updating: the evid distribut messag $m^+_{t \rightarrow s}$ are calcul by divid out $m^-_{s \rightarrow t}$ from $\text{bel}_t$ sum-product: the evid distribut messag $m^+_{t \rightarrow s}$ are calcul without refer to $\text{bel}_t$, use the messag $m^+_{p \rightarrow t}$ direct instead
what a systol array an algorithm architectur where each comput unit receiv messag from neighbour doe some work send messag to neighbour
what the parallel version of belief propag on tree all messag are initi to all 1s each node receiv and send messag onc per step $d$ steps, where $d$ is the diamet of the graph, will in converg
how can the joint and margin distribut for a tree gaussian pairwis mrf be calcul use belief propagation, where the messag and belief are gaussian repres by their precis and precision-mean
what non-parameter belief propag belief propag appli to pairwis mrfs where the potenti aren't gaussian and so the integr that show up when calcul the messag have to be approxim use sampl method
what happen when loopi belief propag is appli to a gaussian mrf which isn't a tree the posterior mean are calcul exact but the precis will usual be too small
what the max product algorithm a variant of bp where the sum are replac with max and so it calcul the local map margin of each node
what the problem with the max product algorithm the same as the problem with the viterbi algorithm ie the map for the margin at a node might not be from the same minima that gave rise to the map at anoth node
what barren node remov in the context of dgms suppos we'r given a partially-observ dgm where node $v$ are observ and we'r ask to infer node $q$ if there a node $b$ such that $\sum_{x_b} p(x_q x_b) = 1$ for all $x_q$, then the node can be excis and this can be done recurs to shrink the graph
what the idea in the variabl elimin algorithm for ugm given locally-norm factor $\psi_a(a)$ and $\psi_b(a, b)$ suppos we want to calcul the margin $p(a)$. this can be written $p(a) = \sum_{a, b} \psi_a(a)\psi_b(a, b)$, take $ a b $ multipl $p(a) = \sum_a \psi_a(a) \sum_b \psi_b(a, b)$, take $ a $ multipl this is variabl elimin
how can variabl elimin be use to calcul condit calcul the margin $p(x_q, x_v), p(x_v)$ then $p(x_q x_v) = \frac{p(x_q, x_v)}{p(x_v)}$
when can variabl elimin be appli whenev you'r calcul an express of the $\sum_x \prod_c \psi_c(x_c)$ where $(+), ( )$ are taken from a commut semi-r
what a commut semi-r a set $k$ with oper $(+), ( )$ which are each associative, commutative, and have ident $0, 1$ and also such that $( )$ distribut over $(+)$
what are some common commut semir $(+, )$, sum-product $(\max, )$, max-product $(\min, +)$, min-sum $( , )$, boolean satisfi
after infer a linear model for the effect of covari on your outcome, how can you use the valu of the data point with those covari "removed" via select the residu of the regress e.g., resid(lm_obj) in r
whenev you assum that error in a regress model follow a gaussian distribution, how should you check this graphic via a q-q plot of your residu versus a normal distribut with mean and varianc deriv from your residu
in variabl elimination, what are fill-in edg when a variabl $x$ is sum over add an edg to the ugm between ani $y, z$ that both share a factor with $x$
what the induc width of a variabl order $( )$ in variabl elimin start with the graph of the variabl $g$ and elimin variabl accord to $( )$, ad the fill-in edg to $g$ that the elimin impli then the induc width $w( )$ is the size of the largest cliqu in the graph minus one.
what the intuit reason for fill-in edg when visual variabl elimin when you sum out a variable, you effect a cliqu over everi variabl it share a factor with the fill in edg repres these cliqu
what the run time of variabl elimin in term of the induc width of the ugm if the variabl each have $k$ states, $o(k^{w( ) + 1})$ where $w( )$ is the induc width of $g$ by $( )$ so in general greedi heurist are use
what the treewidth of a graph in term of variabl elimin $w = \min_{ } \max_{c \mc{c}(g( ))} c - 1$ where $( )$ is an order for variabl elimin $\mc{c}(g( ))$ is the set of cliqu in the graph construct by ad the fill-in edg produc by $( )$ to $g$
what the comput complex of find an optim variabl elimin order np-hard in general sinc it reduc to find the treewidth of a graph
what the treewidth of a $m n$ 2d lattic $o(\min\{m, n\})$
what'r the problem with variabl elimin exponenti depend on treewidth comput a differ margin mean re-elimin all the variabl from scratch
how can bp on tree be use to effici calcul multipl margin store the messag generat each time and re-us them where possibl
what a junction tree a represent of the vertic of a graph $g$ as a tree $\mc{t}$ over set of vertic such that everi vertex appear in at least one set for everi edg $x-y$ in $g$ there a set $s$ contain $x, y$ and the run intersect properti hold
what the run intersect properti of a junction tree ani set of node in the tree contain a given variabl s a connect subgraph
what a tree decomposit anoth name for a junction tree
how do chordal graph relat to junction tree the maxim cliqu of a chordal graph the set of a junction tree
how doe the run intersect properti of a junction tree relat to belief propag bp appli to a tree with the run intersect properti will return the exact valu of $p(x_c v)$ for each node $c$ in the tree
what'r the separ of a junction tree for an edg $s\leftrightarrow t$ between two set of variables, it $s t$
how doe the junction tree algorithm correspond to a kalman smoother jta appli to a chain-structur gaussian graphic model is equival to a kalman smoother
what the upper bound on the treewidth of a graph linear in the number of node
what the comput complex of the junction tree algorithm $o(k^{w+1})$ where $w$ is the treewidth of the graph and $k$ is the number of state per variabl
what the comput complex of exact infer in general on graphic model p-hard
in general, when is exact infer tractabl on graphic model when the graph is a chain a tree or general low-treewidth
what are some problem that can be solv by general of the junction tree algorithm comput the map estim comput the $n$ most probabl configur comput posterior sampl
what the variat free energi given a target distribut $p(x \mc{d})$, it $j(q) = \mbb{kl}(q \tild p)$, the revers kl-diverg where $\tild p(x) = p(x, \mc{d})$ is the unnorm target
what the helmholtz free energi anoth name for the variat free energi
how doe the variat free energi relat to the likelihood of the evid $e^{-j(q)} p(\mc{d})$
how doe the variat free energi relat to the energi $j(q) = - \mbb{h}(q) + \mbb{e}_q[e(x)]$
how doe the variat free energi relat to the negat log likelihood $j(q) = \mbb{e}_q[\text{nll}(x)] + \mbb{kl}(q(x) p(x))$ where $p$ is the prior
what the differ between the forward and revers kl diverg as object given target $p$ and approxim $q$ forward kl: $\mbb{kl}(p q)$ is infinit if $p(x) 0$ but $q(x) = 0$, so it overestim $p$ support revers kl: $\mbb{kl}(q p)$ is infinit if $p(x) = 0$ but $q(x) 0$, so it underestim $p$ support
whi the revers kl prefer as an object over the forward kl becaus the forward kl overestim the support of the target distribut on multimod target distribut it'll have massiv higher varianc than it should while the revers kl will `lock on to a singl mode
how can you interpol between the properti of the forward and revers kl diverg as an object function use the alpha diverg
what the approxim posterior in the mean field approxim $q(x) = \prod_i q_i(x_i)$
what the mean field updat rule $\ln q_j(x_j) = \mbb{e}_{-q_j}[\ln p(x, \mc{d})] + c$ where the expect is taken over all the variabl except $j$ and $c$ is chosen to normal the distribut
what the energi function in variat infer $l(q) = -j(q)$ where $j$ is the variat free energi
how doe the mean field updat equat aris start with the variat free energi $j(q)$ if we ignor all but the $j$ terms, we get $l(q_j) = - \mbb{kl}(q_j f_j) + c$ where $f_j(x) = \exp\left(\mbb{e}_{-q_j}[\ln p(x, \mc{d})]\right)$ this can be minim by set $q_j \propto f_j$ and the updat equat follow
what the structur mean field approach sometim assum that the variabl in the posterior are independ is too strong of an assumpt instead, group variabl into `mega-vari such that each group has some substructur that allow infer to be per ed effici on it and then appli the vanilla mean field approach
at a high level, how is the mean field algorithm run at each step, pick a variabl $j$ and appli the mean field updat rule to replac a $q_j$ with it expectation. averag over just the variabl involv in the markov blanket of $j$
how is the structur mean field approach appli to factori hmms sinc the observ $y_t$ is influenc by $x_{tk}$ for $1 k \lg k$ and sinc for each fix $k$, the $x_{tk}$ a chain pick the mega-vari to be $\{x_{tk}\}_t$ and conduct infer on them use the forwards-backward algorithm then per infer between chain use the mean field approach
what the idea in variat bay appli a mean field approxim to the posterior $p(\theta \mc{d})$ with $\prod_k q_k(\theta_k)$
what a use fact when debug vb algorithm the variat free energi $j(q)$ must alway decreas at each step
what kind of problem is variat bay em suit to model with both latent variabl and paramet
whi doe em distinguish paramet from latent variabl the posterior uncertainti in the paramet is usual much less than in the latent variabl sinc $\theta$ is influenc by all the data cases, while $z_i$ is onli influenc by $x_i$ so take a map estim of $\theta$ is much more reason than a map estim of the $z$
what vb stand for in ml variat bay
what vbem stand for in ml variat bay expect maxim
what the idea in vbem approxim the posterior $p(\theta, z \mc{d})$ with $q(\theta)\prod q(z_i)$ e step: take the posterior mean $\bar \theta$ and calcuat $p(z_i \mc{d}, \bar \theta)$ use standard methods. m step: use the expect suffici statist to updat the hyperparameters.
what the main advantag of vbem over em by margin out the parameters, a lower bound on the margin likelihood can be calcul which can be use for model select
what variat messag pass a general of vbem which can be appli to ani dgm whose condit distribut are in the exponenti famili and whose parent node all have conjug distribut
what local variat approxim where a specif term in the joint distribut is replac with a simpler function so as to make the posterior easier to comput
what kind of problem are variat techniqu best suit for one in which one or more of the hidden node are continu if they'r all discrete, lbp is prefer
what the log-sum-exp function $\text{lse}(\eta_i) = \ln \left(1 + \sum_{m=1}^m e^{\eta_{im}}\right)$
what the usual use of variat bound on the log-sum-exp function approxim the posterior that s from appli a gaussian prior to a multinomi likelihood
what the loopi belief propag algorithm initi all the messag to $m_{s\rightarrow t}(x_t) = 1$ initi all the belief to $\text{bel}_s(x_s) = 1$ repeatedly: updat the messag updat the belief until the belief stop chang signific
what a factor graph an undirect bipartit graph, with round node and squar node round node repres variabl squar node repres factor edg repres which variabl appear in which factor
how are the messag defin for belief propag on factor graph $m_{x \rightarrow f}(x) = \prod_{h \text{nbr}(x) \backslash f} m_{h \rightarrow x}(x)$ $m_{f \rightarrow x}(x) = \sum_i f(x, y) \prod_{i \text{nbr}(f) \backslash x}m_{i \rightarrow f}(y)$
how'r the belief defin in belief propag on factor graph $\text{bel}(x) = \prod_{f \text{nbr}(x)} m_{f \rightarrow x}(x)$
what a comput tree in the context of belief propag it a visual of the genesi of the messag receiv by a node after $k$ iter of belief propag in particular it a $k+1$ height tree where the edg at height $k$ repres the messag pass in round $k$ that eventu contribut to the root state after round $k$
how are comput tree use to model the converg of lbp $k$ round of lbp on a graph are equival to a height $k+1$ comput tree and if the strength of the edg is suffici weak then the root valu will converg as $k$ increas
what damp in the context of belief propag use messag $\tild m^k_{st} = \lambda m_{st} + (1-\lambda)\tild m_{st}^{k-1}$ at iter $k$ valu of $\lambda 0.5$ can drastic improv converg
from the outside, what doubl loop belief propag a variant of bp that is guarante to converg to a local minima of the object that lbp is minim but which is rather slow, complic and give margin gain
what the tree reparameter heurist for lbp rather than updat the messag at all node synchron pick a set of span tree and per an up-down sweep on one of them at a time
what the residu belief propag heurist for lbp rather than updat all the messag synchron updat them one at a time, prioritis the messag which is most differ from it previous valu
which of the schedul heurist for lbp work best residu belief propag
what the cost of comput a messag in vanilla bp $o(k^f)$ where $k$ is the number of state and $f$ is the size of the largest factor
how can bp be sped up when the potenti have the $\psi(x_s, x_t) = \psi(x_ - x_t)$ in this case, messag comput is just convolut over $x_s$ so it can be acceler use the fft
what the multi-scal method for bp if you'r comput bp over a self-similar structur (like a grid) then construct a coars grid, solv it use bp the use the s to initi bp on the fine grid
what a comput cascad in the context of ml when work in a high-dimension state space a filter step can be use to prune away improb states, make the remain problem easier to solv by appli the filter repeatedly, a hierarchi of model can be built that trade off accuraci for speed. this is a comput cascad
what'r the mean paramet of a model in ml $\mu = \mbb{e}[\phi(x)]$ wherer $\phi(x)$ are some suffici statist of the model
what the margin polytop of a graphic model $g$ $\mbb{m}(g)$, the set of mean paramet for the model that'r generat by a valid probabl distribution: $\mbb{m}(g) = \{ \mu : \mu = \mbb{e}_p[\phi] \text{ for some distribut } p\}$ where $\phi$ are some suffici statist for $g$
how can the geometr of the margin polytop be character it a convex hull over the featur set: $\mbb{m}(g) = \text{conv}\{\phi_d(x)\}_d$
how doe the margin polytop relat to the likelihood of the data we can write the variat free energi in term of the mean paramet of the model to get that $\max_{\mu \mbb{m}(g)} \theta^t \mu + \mbb{h}(\mu) = \ln z(\theta)$ and $\theta$ are the canon paramet and $z(\theta) = p(\mc{d})$ is the partit function and where equal is achiev when $\mu = \mbb{e}_p [\phi(x)]$, with $p$ be the true distribut
whi is it hard to optim over the margin polytop becaus while it convex, it got an exponenti number of face
given a graphic model $g$ and a subgraph $f \subset g$, what the inner approxim to the margin polytop of $g$ $\mbb{m}_f(g) = \{\mu : \mu = \mbb{e}[\phi(x)] \text{ for some } \theta \omega(f)\}$ where $\omega(f)$ is the paramet space for the submodel
given a graphic model $g$ and a subgraph $f \subset g$, what the canon paramet space of the submodel if $\omega$ is the paramet space for $g$, then $\omega(f) = \{ \theta \omega : \theta_\alpha = 0 \text{ for $\alpha$ outsid of } \mc{i}(f)\}$ where $\mc{i}(f)$ is the subset of suffici statist associ with the cliqu of $f$
given graphic model $f g$, what doe the inner approxim $\mbb{m}_f(g)$ to the margin polytop of $g$ look like geometr it a non-convex volum nest insid $\mbb{m}(g)$
what'r the local consist constraint for a pairwis mrf $\sum_{x_s} \tau_s(x_s) = 1$, the normal constraint $\sum_{x_t} \tau_{st}(x_s, x_t) = \tau_s(x_s)$, the margin constraint
what the outer approxim to the margin polytop of a graphic model $g$ $\mbb{l}(g) = \{\tau 0 : \tau \text{ the local consist constraint are satisfied} \}$
what doe the outer approxim $\mbb{l}(g)$ of the margin polytop of $g$ look like it a convex polytop with linear number of face that contain $\mbb{m}(g)$
when is the outer approxim to the margin polytop equal to the margin polytop when the graph a tree
what'r the compon of $\tau \mbb{l}(g)$ call pseudo-marginals, sinc they might not correspond to ani valid probabl distribut
what the beth approxim to the entropi of a pairwis mrf $\mbb{h}_\text{bethe}(\mu) = \sum_v h_s(\mu_s) - \sum_ i_{st}(\mu_{st})$ where $i_{st}$ is the mutual in ation of $\mu_s, \mu_t$
how can the joint distribut over a pairwis tree-structur mrf be written in term of it margin $p(x) = \prod_v p_s(x_s) \prod_ \frac{p_{st}(x_s, x_t)}{p_s(x_s)p_t(x_t)}$
when the beth approxim to the entropi exact when the graph is a tree
what the beth free energi $f_\text{bethe}(\tau) = -[\theta^t\tau + \mbb{h}_\text{bethe}(\tau)]$ where $\tau$ is a pseudo-distribut over the model and $\theta$ are it canon paramet
what the beth variat problem $\min_{\tau \mbb{l}(g)} f_\text{bethe}(\tau)$ ie minim the beth free energi over the outer approxim to $g$ margin polytop
what the relev of the beth variat problem to loopi belief propag ani fix point of lbp is a stationari point of the beth variat energi in particular, if $\lambda_{ts}$ is the lagrangian multipli for the margin local consist constraint then at convergence, $m_{t \rightarrow s}(x_s) = \exp(\lambda_{ts}(x_s))$
what the main advantag of lbp over mf the mean field object has mani more local optima than the lbp object which mean that lbp often per s much better
what'r the main advantag of mf over lbp it lower-bound the likelihood of the evidence, which is veri use for model select it extend easili to non-discret distribut
whi is the beth variat problem non-convex becaus although $\mbb{l}(g)$ is a convex set $f_\text{bethe}(\tau)$ is not a convex object
what the cluster variat method cluster togeth node that a tight loop, ing a hypergraph appli general belief propag to the hypergraph
what the variat problem that general belief propag minim it minim the kikuchi free energi over an outer approxim to the valid distribut on the hypergraph
how doe general belief propag compar to belief propag it more complic and has a higher comput complex but give more accur s than lbp
in what scenario would you want to use a linear vs logist regress 1) use linear regress when your outcom is continu 2) use logist regress when your outcom is binari http://stats.stackexchange.com/questions/7701/adjusting-for-confounding-vari aha, that make thing a littl different. the differ between linear and logist regress stem from natur of your outcome. if the outcom is continu and distribut reason normally, ol may apply. if the outcom is binary, logist regress may apply. (i'm not come down definit in either case becaus there can be mani other factor worth take into account when pick a model). both logist and ol model can accommod binary, categorical, ordin and continu covariates. – ashaw feb 28 11 at 14:23
what the convex entropi of a distribut $\mu$ over a graphic model $g$ with respect to a set of submodel $\mc{f}$ $\mbb{h}(\mu, \rho) = \sum_{f \mc{f}} \rho(f)\mbb{h}_f(\mu)$ where $\rho$ are the coeffici of a convex combin over $\mc{f}$
what the object optim by convex belief propag $\max_{\tau \mbb{l}(g; \mc{f})} \tau^t \theta + \mbb{h}(\tau, \rho)$ where $\mc{f}$ is a set of tractabl submodel of $g$ (like trees) $\mbb{l}(g; \mc{f})$ is the intersect of the margin polytop for each $\mc{f}$ $\mbb{h}(\tau, \rho)$ is the convex entropi
what the most common kind of convex belief propag tree reweight belief propag
what the of the entropi in tree reweight belief propag $\mbb{h}(\mu) = \sum_v h_s(\mu_s) - \sum_ \rho_{st}i_{st}(\mu_{st})$ where $i_{st}$ is the mutual in ation between $s$ and $t$ $\rho_{st}$ is the probabl of edg $s-t$ appear in the distribut over tree use by the algorithm
what the idea in tree-reweight belief propag it belief propag but the messag $t \rightarrow s$ is condit by the probabl that the edg $t \rightarrow s$ exist in a tree random select from the set of span tree of the graph
what one of the main advantag of tree-reweight belief propag over vanilla loopi belief propag if trbp converges, it object is a lower bound on the likelihood of the evid
what expect propag a general of assum densiti filter but while assum densiti filter is online, expect propag is offlin
what the clutter problem suppos we have two superimpos gaussians, one at $x$ and one at $0$ infer $x$.
what the invers probabl tran if a distribut has cdf $f$ and $u u(0, 1)$ then $f^{-1}(u) f$
what the box-mul method sampl $z_1, z_2 u(0,1)$ discard pair that don't lie in the unit disc defin $x_i = 2\frac{\sqrt{-\ln r}}{r}z_i$ where $r = z $ then $x \mc{n}(0, i)$
what the use of the box-mul method sampl pair of independent, standard, normal distribut random number use a uni ly-distribut sourc
how is the box-mul method use to sampl from a multivari gaussian $\mc{n}(\mu, \sigma)$ comput the choleski decomposit $\sigma = ll^t$ sampl $x \mc{n}(0, i)$ use box-mul comput $y = lx + \mu$
how do you per reject sampl on a distribut $p(x)$ pick a propos distribut $q(x)$ such that $mq(x) \tild p(x)$, where $m$ is a constant and $\tild p(x)$ is the nonnorm version of $p$ sampl $x q$ and $y u(0,1)$ if $u \frac{\tild p(x)}{mq(x)}$, accept, els reject.
what the probabl of accept in reject sampl $p(\text{accept}) = \frac{1}{m} t \tild p(x) dx$
how is reject sampl often appli to a posterior distribut set the propos distribut to the prior $q(\theta) = p(\theta)$ set $m$ to the probabl of the mle, $m = p(\mc{d} \hat \theta)$ this is onli reason when the prior isn't vagu
what the idea in adapt reject sampl given a log-concav densiti $p(x)$ construct a piecewis linear upper bound on it and iter add linear piec at ani point $x$ where a sampl is reject
what reject sampl particular bad at high dimensions.
what the use of import sampl approxim $\mbb{e}[f]$
what a super effici import sampler one which need to draw fewer sampl to approxim $\mbb{e}[f]$ to a given level of accuraci than a sampler which onli work with $p(x)$ would need
what import sampl for a function $f$ given a propos distribut $q$ to a target $p$ take sampl $x^s$ from $q$. then $\mbb{e}_p[f] \frac{1}{s} \sum_ w_s f(x^s)$ where $w_s$ are the import weight $w_s = \frac{p(x^s)}{q(x^s)}$
how should the propos distribut for import sampl be chosen to minim the varianc of the estim
what the optim propos distribut for import sampl $q^*(x) = \frac{ f(x) p(x)}{ t f(x^\prime) p(x^\prime)dx^\prime}$ though comput this can obvious be tricky.
how can import sampl be appli when onli a non-norm propos $\tild q$ and a non-norm target $\tild p$ can be comput $\frac{z_p}{z_q} \frac{1}{s} \sum \tild w_s$ so $\mbb{e}_p[f] \frac{1}{\sum \tild w_s} \sum \tild w_s f(x^s)$ this is bias but asymptot unbias
what ancestr sampl to sampl from the joint of a dgm with no evid sampl from the root then it children then it grandchildren etc
what logic sampl to sampl from the joint of a dgm with evid (ie with some node clamp to specif values) conduct ancestr sampl but throw the sampl out and start again if a valu is encount which is inconsist with the observ valu
what likelihood weight for sampl dgms it an improv over logic sampl for dgms rather than throw a sampl away when a sampl from an observ node contradict the evid instead just don't sampl observ nodes, and weight the entir sampl as $w(x) = \prod_{t e} p(x_t x_\text{pa}(t))$ where $e$ is the set of observ node
what sampl import resampl use import sampl with a propos $q$ to construct a distribut $p(x) \sum_1^ w_s\delta_{x^s}(x)$ where $w_s$ are the import weights. then sampl with replac from the abov to get an unweight distribut $p(x) \frac{1}{s^\prime} \sum^{s^\prime}_1 \delta_{x^s}(x)$ where $s^\prime \ll s$
how are the weight calcul in sequenti import sampl given a propos $q$ and target $p$, $w_t \propto w_{t-1} \frac{p(z_{1:t} y_{1:t})}{q(z_{1:t} y_{1:t})}$ where various markov assumpt can be appli to $p$ and $q$ to make comput the posterior tractabl
what sequenti import sampl approxim the belief state of an entir trajectori $z_{1:t}$ use a weight set of particl trajectori $\{z^s_t\}_s$: $p(z_{1:t} y_{1:t}) \sum_ \tild w^s_t \delta_{z^s_{t-1}}(z_{1:t})$ where $\tild w_s^t$ is the normal weight of particl $s$ at time $t$
what the main problem with sequenti import sampl the degeneraci problem: the state space is grow by a factor of $ \mc{z} $ at everi step while a constant number of sampl are be use
what the effect sampl size of a set of particl $s_\text{eff} = \frac{s}{1 + \text{var}[w^{*s}_t]}$ where $w^{*s}_t$ is the true weight of particl $s$ at time $t$, ie the weight as calcul without ani markov assumpt appli to $p$
how is the effect sampl size of a set of particl usual approxim $\hat s_\text{eff} = \frac{1}{\sum_ (w_t^s)^2}$
what the resampl step in particl filter whenev the (approximate) effect sampl size of the particl set drop below a threshold, particl with low weight are cull and the popul is rejuven by sampl from $p(z_t y_{1:t}) \sum_ \hat w_s^t \delta_{z^s_t}(z_t)$
what multinomi sampl in particl sampl rejuven a popul by comput $k \text{multi}(s, w_t)$ and make $k_s$ copi of $z^s_t$
what sampl impoverish when resampling, it where a few high-weight sampl get replic to the exclus of all other
what a regular particl filter a particl filter algorithm that use a kernel densiti estim for rejuven which help with the sampl impoverish problem
what propos distribut is most common use in particl filter the prior though when this is unin ative, it can be veri wast
what the name of the vanilla particl filter algorithm sequenti import sampl
what are two common distanc metric for histogram the bhattacharya distanc earth mover distanc
what rao-blackwellis particl filter in some model there are hidden variabl $z_t$ that can be analyt integr out provid we know the valu of $q_t$ so each particl $s$ repres a valu $q^s_{1:t}$ and a distribut $p(z_t y_{1:t}, q_{1:t}^s)$ these are distribut particl and the associ techniqu is rao-blackwellis particl filter
what model is rao-blackwellis particl filter most common appli to switch linear gaussian state space model
what the idea in gibb sampl given a sampl $x^s$, generat $x^{s+1}$ by sampl each featur in turn use the previous sampl featur for the other compon
what the full condit for a featur $p(x_i x_{-i})$
what mcmc stand for in ml mont carlo markov chain
what the idea in mcmc construct a markov chain on the state space $\mc{x}$ whose stationari distribut is the target densiti $p^*(x)$ then draw sampl from the chain to per mont carlo integr with respect to $p^*(x)$
what the main problem with gibb sampl label switching/unidentifiability: differ sampl may be draw their paramet from differ cluster so you can't just take the mont carlo averag of the sampl to comput the posterior mean
what the usual solut to the label switch problem in gibb sampl onli ask question that are invari with respect to label ie rather than `doe point $i$ belong to cluster $k$ ask `do point $i, j$ belong to the same cluster
what a collaps gibb sampler gibb sampl use rao-blackwellis to margin out some of the paramet first
what the rao-blackwel theorem for ani variabl $z, \theta$ and scalar function $f$, $\text{var}_{z,\theta}[f(z, \theta)] \text{var}_z[\mbb{e}_\theta[f(z, \theta) z]]$
what the intuit interpret of the rao-blackwel theorem the varianc of the estim creat by integr out $\theta$ will never be higher than the varianc of a direct mc estim
how do collaps gibb sampler general per compar to vanilla gibb sampler their averag per anc is better but they'll still get stuck in poor local mode from time to time
what the imput posterior algorithm a mcmc version of em, where variabl are split into hidden variabl and paramet
what data augment in ml ad auxiliari variabl to make comput the posterior easier
how doe the mix time of a gibb sampler correspond to the varianc of the condit distribut if the varianc along dimens $i$ is $l$ and the distribut has a $l$-siz support in that dimens then $o((l/l)^2)$ step are need to get independ sampl
what ed gibb sampl sometim there are group of variabl which can all be effici sampl at onc this allow greater step through the state space and acceler mix
what'r the high-level step in the metropoli hast algorithm let $q$ be the propos distribut at each step, given state $x$, sampl $x^\prime q(x)$ then either accept $x^\prime$, in which case the new state is $x^\prime$ or reject it, in which case the new state is $x$
what a random walk metropoli algorithm mh with a symmetr gaussian propos center on the current state
what mh stand for in ml metropoli hast
what an independ sampler mh with a propos that independ of the current state
what the probabl of accept a propos in mh with a symmetr propos distribut $r = \min(1, \frac{p^*(x^\prime)}{p^*(x)})$ where $p^*$ is the target distribut
what a symmetr propos distribut in mh a $q$ such that $q(x^\prime x) = q(x x^\prime)$
what the hast correct it the probabl of accept a move in mh when the propos distribut isn't symmetr $r = \min(1, \alpha)$ $\alpha = \frac{p^*(x^\prime)}{p^*(x)}/\frac{q(x^\prim x)}{q(x x^\prime)}$ this correct for the fact that the propos distribut (rather than the target distribution) might favour certain state
how can mh be appli when the target distributon can onli be calcul in non-norm the same way as it is for the normal becaus the normal constant for the target $p^*$ cancel out in the calcul for the accept probabl
how doe gibb sampl relat to mh it mh with a propos of the $q(x^\prime x) = p(x^\prime_i x_{-i})\mbb{i}(x^\prime_{-i} = x_{-i})$ which has an accept rate of 1
intuitively, what the effect of the way propos are accepted/reject in mh the accept probabl is chosen so that the time spent in state $x$ is proport to $p^*(x)$ becaus it'll alway move to a higher-prob proposal, and to a lower probabl propos with probabl proport to how much wors the propos is than the current state
what propos are consid valid for mh for target $p^*$, it ani $q$ such that $\text{supp}(p^*) _x \text{supp}(q(\cdot x))$
if you'r explor a bimod target use random walk mh, what the effect of alter the varianc of the propos too low variance: onli one of the mode will be explor just right variance: both mode are effici explor too high variance: a `sticki chain with a high reject rate that spend too long in singl state
what a sticki markov chain in the context of mh it when propos have a high reject rate, so the chain spend a long time in each state
what a pilot run in the context of mh short run use to calibr the paramet of the propos
as a rule of thumb, what accept rate should you aim for in mh $25 $ to $40 $
how should the varianc of the propos be set in random-walk mh comput the hessian $h$ at some point on the target then use $\sigma = s^2h^{-1}$ where $s^2$ is some scale paramet chosen to acceler mix
when appli random walk mh to a gaussian posterior, what the optim varianc scale paramet $s$ to choos $s^2 \frac{2.38^2}{d}$, where $d$ is the number of dimens
what a data-driven mcmc a mcmc algorithm where the propos distribut depend on the data, $q(x^\prime x, \mc{d})$
how are propos for data-driven mcmc usual creat by sampl pair $(x, \mc{d})$ from the forward model and train a discrimin classifi to predict $p(x_s d)$ for some subset of the variabl $s$ with a standard data-independ propos use to predict $x_{s^\prime}$
what adapt mcmc mcmc where the propos paramet chang over the life of the chain but the paramet shouldn't depend on the whole life of the chain, or it'd violat the markov property!
at a high level, whi doe mh work mh construct a transit function $p(x^\prime x)$ such that $p^*(x)$ satisfi the detail balanc equat and with a valid propos distribution, the chain is also irreduc and ergod so $p^*(x)$ is it uniqu limit distribut
what trans-dimension mcmc mcmc that can handl with move between model with differ paramet space such as in mixtur model where the number of compon is unknown
what revers jump mcmc a kind of trans-dimension mcmc that correct the problem with comput accept probabl by augment the low-dimension model with extra random variabl
what the problem with vanilla trans-dimension mcmc when comput accept probabilities, you'r compar densiti defin in differ measur space which is meaningless
what burn-in in the context of mcmc all the sampl that are thrown away until the chain has reach it stationari distribut
how the mix time of a mcmc defin given transit function $t$ $\tau_\epsilon = \max_{x_0} \min_t \{t : t^t(\delta_{x_0}(x)) - p^* _1 \epsilon\}$
for a discret mcmc, how can the mix time be bound in term of the eigenvalu of the transit matrix $\tau_\epsilon = o(\frac{1}{\lambda_1 - \lambda_2}\ln \frac{n}{\epsilon})$ where $\lambda_1, \lambda_2$ are the two largest eigenvalu of the transit matrix $n$ is the number of state
what the eigengap of a transit matrix $\gamma = \lambda_1 - \lambda_2$ where $\lambda_1, \lambda_2$ are the two largest eigenvalu of the matrix
for a discret mcmc, how can the mix time be bound in term of conduct $\tau_\epsilon = o(\frac{1}{\phi^2}\ln\frac{n}{\epsilon})$ where $\phi$ is the conduct of the chain $n$ is the number of state
what the conduct of a discret markov chain $\phi = \min_{ \colon p^*(s) 0.5} \frac{1}{p^*(s)} \sum_{x s, y s^\prime} p(i x)$ ie it the least probabl of move from some set to it compliment
what kind of distribut usual lead to mcmcs with high mix time one with well-separ mode
what a trace plot in the context of mcmcs a way to evalu burn-in time: run multipl chain from overdispers start point and plot the trace of some variabl of interest the chain have mix when they'v forgotten where they start from, and their trace converg to the same dsitribut
how is the estim potenti scale reduct of a mcmc defin take $s$ sampl $y$ from $c$ chain calcul the between-sequ varianc $b$ and within-sequ varianc $w$ defin $\hat v = \frac{s-1}{s}w + \frac{1}{s}b$, which is an unbias estim of $\text{var}[y]$ under stationar defin the epsr, $\hat r = \sqrt{\hat v / w}$
how is the estim potenti scale reduct of a mcmc interpret it the degre to which the posterior varianc would drop as $s \rightarrow fty$ if $\hat r 1$ for a feature, the estim is not unreliable.
what the autocorrel function given order sampl $x_s$, $\rho(t) = \frac{\text{cov}[x_s, x_{s+t}]}{\text{var}[x_s]}$
what the problem with autocorrel with respect to mcmcs becaus the sampl produc by mcmc are autocorrelated, their in ation content is reduc relat to independ samples. this can be quantifi by plot the autocorrel $\rho(t)$ of a set of samples.
what thin in the context of mcmcs onli keep everi $k$th sample, to reduc autocorrel this doesn't improv the effici of the sampler, but it doe reduc the number of high correl sampl that have to be store
as a rule of thumb, how mani chains, steps, and burn-in sampl should be use with mcmc three chain and 100,000 step each, half of which is burn-in.
what the idea in auxiliari variabl mcmc replac the distribut $p(x)$ with a distribut with some addit variabl $p(x,z)$ such that $\sum_z p(x, z) = p(x)$ and $p(x, z)$ is easier to sampl from than $p(x)$
what the pdf of a logist distribut in term of exponenti $p(x) = \frac{e^{-x}}{s(1+e^{-x})^2}$
how can auxiliari variabl mcmc be use to sampl from a student distribut use the scale mixtur parameter of the student distribut and add the mixtur coeffici as the auxiliari variabl of the mcmc
what the idea in slice sampl it a kind of auxiliari variabl mcmc, with joint $p(x, u) \propto 1$ if $0 u p(x)$ $p(x, u) = 0$ otherwise. effectively: $u$ is select from the vertic slice of $p$ through $x$. $x^\prime$ is select from the horizont slice of $p$ through $u$
how are the slice calcul in slice sampl by step out: construct a rang $x_l x x_r$ and extend $x_l, x_r$ until they'r no longer in the slice through the distribut
how is slice sampl appli to multivari distribut by use an extra auxiliari variabl for each dimens
what the main advantag of slice sampl over mh the user doesn't have to specifi a propos distribution, just the width of the step out interv
what the main advantag of slice sampl over gibb it doesn't need a specif of the full conditionals, just the unnorm joint
what the swendsen-wang model an auxiliari mcmc sampler for ise model where an auxiliari bond variabl $ \{0, 1\}$ is ad for each edg that indic whether the bond is ` e'
what hamiltonian mcmc mcmc for continu state space where the log-posterior can be calculated. think of the paramet of the model as a particl and use auxiliari variabl to repres the `momentum of the particle.
what the effect of reduc the temperatur of the boltzmann distribut the mass concentr at the minimum-energi (most probable) state
what simul anneal let $f$ be the energi of the system for each temperatur $t$, start at $t_0$, repeatedly: take old state $x$ and sampl a new state $x^\prime$ and calcul $\alpha = \exp\left(\frac{f(x) - f(x^\prime)}{t}\right)$ then accept the new state with probabl $\min(1, \alpha)$ after a larg number of step has been taken, decreas the temperatur
what cool schedul is general use in simul anneal an exponenti one: $t_0 1$ $c 0.8$ $t_k t_k c^k$
what the tradeoff in select the speed of the cool schedul in simul anneal too fast a cool schedul will leav you stuck in a local maximum too slow a cool schedul just wast time
what the idea in anneal import sampl given an easi distribut to sampl from $f_n(x)$ and a distribut that difficult to sampl from $f_0(x)$, construct a sequenc between them $f_j(x) = f_0(x)^{\beta_j}f_n(x)^{1-\beta_j}$ and a markov chain $t_j$ for each that leav $p_j \propto f_j$ invariant. then sampl $z_{n-1} p_n$ and $z_k t_{k+1}(z_{k+1})$ for each $k$ and output $x = z_0$ with weight $w = \prod \frac{f_k(z_k)}{f_{k+1}(z_k)}$
what parallel temper a simul anneal variant where multipl chain are run at differ temperatur and each chain can sampl from anoth chain at a neighbour temperatur this way high temp chain can make larg move through the paramet space, and have this influenc on lower temperatur chain
what the candid method in ml it a way to approxim the margin likelihood. write it as $p(\mc{d} m) = \frac{p(\mc{d} \theta, m) p(\theta m)}{p(\theta \mc{d}, m)}$ for some $\theta$ then $p(\mc{d} \theta, m)$ and $p(\theta m)$ are easi to comput and $p(\theta \mc{d}, m)$ can be approxim with mcmc
what the problem with the candid method in ml $p(\theta \mc{d}, m)$ must be approxim by margin over all the mode in the posterior if this isn't done, the approxim of the margin likelihood will be poor
what the harmon mean estim in ml approxim the margin likelihood use $\frac{1}{p(\mc{d})} \frac{1}{s}\sum \frac{1}{p(\mc{d} \theta^s)}$ where $\theta^ p(\theta \mc{d})$ are sampl from the posterior
what the problem with the harmon mean estim in ml it work veri poor in practic the sampl use to generat it are drawn from the posterior, which can be veri insensit to the prior but the margin likelihood can be veri sensit to the prior
what the anneal import sampl approach to approxim the margin likelihood appli anneal import sampl to the target $p_0$ with easi distribut $p_n$. then $\frac{z_0}{z_n} \frac{1}{s}\sum w_s$ where $w_s$ are the anneal import weights. so if $z_n = p_n(\mc{d})$ can be computed, so can $z_0 = p_0(\mc{d})$
what the prefer method for estim the margin likelihood use mcmc the anneal import sampl approach
when you use a linear model, what assumpt must you test (3) 1) normal distribut error (usu via q-q plot inspection; and mi, no correl error terms) 2) constant error varianc (usu via inspect student residualss vs fit responses) 3) model linear (inspect via residu plots; ideally, the residu plot will show no discern pattern) anoth import assumpt of the linear regress model is that the error term have a constant variance, var(i) = σ2. the standard errors, confid intervals, and hypothesi test associ with the linear model reli upon this assumption. unfortunately, it is often the case that the varianc of the error term are non-constant. for instance, the varianc of the error term may increas with the valu of the response. one can identifi non-const varianc in the errors, or heteroscedasticity, from the presenc of a funnel shape in heterosceda- the residu plot. if the residu plot indic that there are non-linear associ in the data, then a simpl approach is to use non-linear tran ation of the predictors, such as log x, √x, and x2, in the regress model. from http://www.nature.com/nmeth/journal/v8/n11/full/nmeth.1710.html complianc with three assumpt of linear least-squar fit was investigated: normal distribut errors, constant error varianc and linearity. error distribut were deem to be normal by compar the sampl distribut of student residu with quantil of the normal distribut (quantile-quantil plot), for a larg number of fit probe sets. similarly, we examin student residu versus fit respons for a larg number of express models. most probe set had constant error variance. a minor show increas error varianc with increas fit express values. this increas was modest and was deem not to compromis ordinari least-squares-bas coeffici estimation. finally, model linear was check by look at partial residu plots14. the vast major of fit probe set did not reveal clear nonlinearity. from
defin precis (statistics) the invers of the varianc origin term gauss use
what doe \ {pyplot.plot()} return a tupl of object repres the line that have been plot
what'r the argument to \ {pyplot.plot()} \ {pyplot.plot(xs1, ys1, 1, xs2, ys2, 2)} etc where \ { 1, 2} are string describ the for that line
how can you get the figur that pyplot consid current \ {pyplot.gcf()}
what \ {pyplot} a modul of \ {matplotlib} that emul matlab plot capabl ie plot are creat in an extrem state way
how do you get the current axe from pyplot \ {gca()}
how do you select which figur should be e in pyplot \ {figure(i)}
how do you set the current e axe to a sub-plot in pyplot \ {subplot(no\_of\_rows, no\_of\_cols, subplot\_no)} where \ {subplot\_no} index 1 through \ {no\_of\_rows*no\_of\_cols}, top to bottom amp; left to right
what a common shorthand for pyplot \ {subplot()} if the number of rows, column and the subplot index are all less than 10 then \ {subplot(i, j, k)} is the same as \ {subplot(ijk)}
how do you free the memori associ with a plot in pyplot \ {close()}
how do you clear the current figur in pyplot \ {clf()}
how do you clear the current axe in pyplot \ {cla()}
how do you annot a point on a plot use pyplot \ {annotate()}
what a \ {suptitle} in pyplot the titl for the whole figure; \ {title} is for subplot
how do you an imag as a plot in pyplot \ {imshow(img)} where \ {img} is a numpi array
what an artist in matplotlib an object that know how to use a render to paint onto canva
what an \ {axes} in matplotlib the object which set the coordin system and contain most of a figur element
what a \ {figurecanvas} in matplotlib it the area onto which the figur is drawn
what a \ {renderer} in matplotlib it the object which know how to draw onto a \ {figurecanvas}
what are the two kind of artist in matplotlib primat contain
what the main type of object in matplotlib \ {axis}
how can you autoadjust plot in matplotlib to prevent element from intersect \ {tight\_layout()}
how do you place subplot on an arbitrarily-shap grid in pyplot use \ {gridspec}
what are the four main coordin system in matplotlib \ {data} \ {axes}, which goe $(0, 0)$ to $(1, 1)$ \ {figure}, which goe $(0, 0)$ to $(1, 1)$ \ { }, which goe $(0, 0)$ to $(\text{width}, \text{height})$
defin quantil the data valu mark a boundari between consecut subset equal divid data (usual into "q" parts) vs quintile, which refer to a q-quantil where q = 5
defin hyperparamet a paramet in a model about the probabl of anoth paramet
what is the typic ula for the expect valu of a continu variabl x ∫ dx p(x) x
what the fisher-neyman factor theorem $t(x)$ is a suffici statist for a paramet $\theta$ of $p(x \theta)$ if and onli if there are $g, h$ such that $p(x \theta) = h(x) g(t(x), \theta)$
what a minim suffici statist $s$ is minim suffici iff $s$ is suffici if $t$ is sufficient, there a $f$ such that $s = f \circ t$
what a use character of minim suffici statist $s$ is minim suffici for $\theta$ and $p_\theta$ iff $\frac{p_\theta(x)}{p_\theta(y)}$ is independ of $\theta$ iff $s(x) = s(y)$
continu (infinit number of values) or discret (non-neg whole numbers) numer variabl
ordin (ordered) or non-ordin (unordered) categor variabl
variabl that show some relationship with one another; the relationship will have a posit or negat associ depend variabl
a variabl that is not influenc by anoth and influenc the valu of the depend variabl independ variabl
in a pair of variables, it is the variabl suspect of affect the other; even if there is a correlation, it doe not guarante causat explanatori variabl
a studi where data are collect onli by monitor what occurs; onli correl can be concluded; two type of observ studies: *prospective* (identifi individu and collect in ation as event unfold) and *retrospective* (collect data after event have taken place, e.g. research may review past events) observ studi
a studi where research assign treatment to cases; causal can be conclud experiment studi
1. *control* - compar treatment of interest to a control group. 2. *randomize* - random assign subject to treatments. 3. *replicate* - collect a suffici larg sample, or replic the entir study. 4. *block* - block for variabl known or suspect to affect the outcome. principl of experiment design
aka lurk variable, confounder; a variabl correl with both the explanatori and respons variabl confound variabl
when a statist is influenc by extern factor in such a way that it is systemat differ from the popul paramet of interest; type of bias: measurement, selection, volunteer, nonrespons bias
each subject in the popul is equal like to be selected. simpl random sampl (srs)
first divid the popul into homogen strata (subject within each stratum are similar, across strata are different), then random sampl from within each strata. stratifi random sampl
first divid the popul into cluster (subject within each cluster are non-homogenous, but cluster are similar to each other), then random sampl a few clusters, and then random sampl from within each cluster. cluster sampl
experiment unit don't know which group (control or treatment) they'r in blind
both the experiment unit and the research don't know the group assignment. doubl blind
each point in a dataset is defin by two valu (x, y) plot on a pair of axes; relationship between the two variabl can be describ by direct (posit or negative), (linear or non-linear), strength (strong to weak), outlier scatterplot
a statist (such as mean) that repres a singl point on the number line; point estim and sampl statist are synonym point estim
mean (the arithmet average), median (the midpoint), mode (the most frequent observation) measur of central tendenc
measur of center; arithmet average; two kind of mean - sampl (x̄), popul (µ); r cmd: mean(vector) mean
midpoint of the distribution; 50th percentile; r cmd: median(vector) median
most frequent observ mode
standard deviat (variabl around the mean), rang (max-min), interquartil rang (middl 50% of the distribution) measur of spread (dispersion)
rough the averag deviat around the mean, and has the same unit as the data; sampl sd = s, popul sd = σ; r cmd: sd(vector) standard deviat
rough the averag squar deviat from the mean; sampl varianc = s², popul varianc = σ² varianc
dataset that show rough equal trail off in both directions; mean ≈ median. symmetr distribut
dataset where data point trail off to the left; order of centers: mean, median, mode left-skew distribut
dataset where data point trail off to the right; order of centers: mode, median, mean right-skew distribut
graphic of continu data that provid a view of data density; higher bar repres where data are relat more common; bin touch one another; x-axi repres a scale rather than a seri of labels; r cmd: hist(column, breaks=n) histogram
graphic that summar a data set use five statist while also plot unusu observations; r cmd: boxplot(column, data=df) boxplot
graphic where color are use to show higher and lower valu of a variabl intens map
a statist (e.g., median, iqr) that is not heavili affect by skew and extrem outlier robust statist
rescal of the data use a function (e.g., log, squar root, inverse) that can make the distribut of data more symmetric, and henc easier to model tran ation
a tabl for a singl variable; r cmd: table(df$column) frequenc tabl
a tabl that summar data for two categor variables; r cmd: table(df$column_a, df$column_b) conting tabl
graphic of conting tabl in ation (two categor variables); r cmd: barplot(ctable, col=c("color1","color2"), legend = rownames(ctable)) segment bar plot
graphic of conting tabl in ation that is similar to a bar plot for one variabl or a segment bar plot when use two variables; r cmd: mosaicplot(table) mosaic plot
a method of statist infer use data from a scientif studi to deriv a conclus between a null hypothesi (status quo) or altern hypothesis. a is call statist signific if it has been predict as unlik to have occur by chanc alone, accord to a pre-determin threshold probability, the signific level. hypothesi test
(h0) often repres either a skeptic perspect or a claim to be test null hypothesi
(ha) repres an altern claim under consider and is often repres by a rang of possibl paramet valu altern hypothesi
a distribut that has one promin mode (or peak in a histogram) unimod distribut
a distribut that has two promin mode (or peak in a histogram) bimod distribut
a distribut that has no promin mode (or peak in a histogram) uni distribut
a distribut that has more than two promin mode (or peak in a histogram) multimod distribut
probabl (of an outcome) the proport of time the outcom would occur if we observ the random process that give rise to it an infinit number of time
law of larg number as more observ are collected, the proport p̂n of occurr with a particular outcom converg to the probabl p of that outcome.
disjoint (mutual exclusive) event disjoint event are alway depend sinc if one event occurs, the other cannot event that cannot both happen at the same time; p(a and b) = 0; p(a or b) = p(a) + p(b);
independ event the outcom of one event doe not influenc the outcom of another; for example, if a and b are independent, then have in ation on a doe not tell us anyth about b (and vice versa)
non-disjoint event event that can happen at the same time; p(a or b) = p(a) + p(b) - p(a and b)
sampl space collect of all possibl outcom of a trial
probabl distribut a list of the possibl outcom with correspond probabl that satisfi three rules: 1. the outcom list must be disjoint. 2. each probabl must be between 0 and 1. 3. the probabl must total 1.
union of event (a or b) calcul the probabl of union of event use the (general) addit rule: - if a and b are not mutual exclusive, p(a or b) = p(a) + p(b) − p(a and b) - if a and b are mutual exclusive, p(a or b) = p(a) + p(b), sinc for mutual exclus event p(a and b) = 0
intersect of event (a and b) calcul the probabl of intersect of independ event use the multipl rule: - if a and b are independent, p(a and b) = p(a) × p(b) - if a and b are dependent, p(a and b) = p(a b) × p(b)
margin probabl probabl base on a singl variabl without condit on ani other variabl
joint probabl probabl of outcom for two or more variabl or process
condit probabl probabl base on a condition; there are two parts: the outcom of interest and the condition; notation: p(outcom of interest condition), or p(a b), = p(a and b)/p(b); " " mean "given"
tree diagram a tool to organ outcom and probabl around the structur of the data; they are most use when two or more process occur in a sequenc and each process is condit on it predecessors; first branch is call primari branch, other branch are call secondari
z score the z score of an observ is defin as the number of standard deviat it fall abov or below the mean; r cmd: scale(x, mean, sd) a normal score for compar data of differ scales;
for various distribut (normal, left-skewed, right-skewed), state whether the z score of the median is negative, positive, or zero. - normal: zero - left-skewed: posit - right-skewed: negat
binomi distribut r cmd: dbinom(k, size=n, prob=p) the probabl of have exact k success in n independ bernoulli trial with probabl of a success p; conditions: 1. the trial must be independ 2. the number of trials, n, must be fixed. 3. each trial outcom must be classifi as a success or failure. 4. the probabl of success, p, must be the same for each trial.
bernoulli random variabl (aka binomi variable) when an individu trial has onli two possibl outcom
expect valu (mean) of binomi distribut µ = np
standard deviat of binomi distribut σ = √np(1-p)
success-failur rule of binomi distribut a binomi distribut with at least 10 expect success and 10 expect failur close follow a normal distribution. - np = 10 - n(1-p) = 10
what is the relationship between the p-valu of a correl evalu use cor.test and the sampl size use to generat it in general, larger sampl size - more like to be significant, even at a smaller correl valu http://stats.stackexchange.com/questions/17371/example-of-strong-correlation-coefficient-with-a-high-p-valu base on the chi-squar distribution, becaus base on the t-distribut
at a high level, how do you construct a model in pymc instanti \ {node} objects, pass the other distribut a \ {node} depend on as initi parameters.
how can you deal with miss data in pymc pass a numpi ed array of the data instead of a standard array it'll construct predict distribut for the ed valu
what a sensibl first thing to tri to deal with poorly-mix model in pymc if there are highly-correl variabl involved, set the step method to \ {adaptivemetropolis}
at a high level, how do you train a model in pymc have instanti a collect of \ {variables} instanti a \ {mcmc} object use the collect then call \ {mcmc.sample()}
what are the basic model compon class in pymc \ {stochastic} and \ {deterministic}, which are subclass of \ {variable} \ {potential} and \ {variable}, which are subclass of \ {node}
what are the primari attribut of a pymc \ {stochastic} object \ {value} \ {logp}
what the \ {extended\_children} attribut of a pymc \ {stochastic} object it the set of all the variabl which determinist depend on the object \ {value} ie they'r all the variabl that need to have their log-prob recomput when \ {value} chang
what the purpos of the \ {dtype} attribut of a pymc \ {stochastic} object it specifi the variabl valu to fit method
what are the three main way of creat \ {stochastic} object in pymc use the built-in distribut use the \ {@stochastic} decor on an appropri function instanti \ {stochastic} directly, pass everyth it need to the initi
what the \emph{simple} way to use the \ {@stochastic} decor in pymc defin a function \ {f} which has a \ {value=initial\_val} paramet has \ {parent\_name=parentvar} paramet that specifi the variabl parent and which return a log-prob then decor \ {f} with \ {\@stochastic(args)}, pass in ani option that'd usual go to the \ {stochastic} initi
what the advanc way to use the \ {@stochastic} decor in pymc it similar to the common way, but rather than appli it to a \ {f} that return a log probabl appli it to an \ {f} that contain two functions, \ {logp}, which take the same paramet as \ {f} and return the log-prob \ {random}, which take the parent specifi in \ {f} and return a random sampl
how should you updat the \ {value} of a \ {stochastic} object in pymc onli use simpl assignment, \ {x.valu = y} anyth els (like \ {(+=)} or set \ {x.value.attr}) will confus pymc caching!
how is data repres in pymc as \ {stochastic} object with the initi paramet \ {observed=true}
how is a dirichlet variabl repres in pymc as a \ {dirichlet\_like} stochast variabl which give the log-prob of the first \ {k-1} element and a \ {completeddirichlet} determinist variabl which wrap it and give the last element
how can you convert a lambda to a determinist variabl in \ {pymc} use the \ {lambda} determinist variabl
what'r implicit determinist variabl in pymc certain elementari oper (like \ {+, []}, etc) on \ {variables} return a \ {deterministic} variabl repres their valu automat
what are the four main way of creat determinist variabl in pymc use the built-in variabl appli elementari oper to other variabl use the \ {\@deterministic} decor instanti a \ {deterministic} object direct
how is the \ {@deterministic} decor use in pymc defin a function \ {f} which has \ {parent\_name=parentvar} paramet that specifi the variabl parent and return a valu then decor \ {f} with \ {@deterministic(args)}, pass in ani option that'd usual go to the \ {deterministic} initi
what pymc \ {@observed} decor it sugar for \ {@stochastic(observed=true)}
what'r \ {container} in pymc if you pass a collect as the parent of a \ {variable}, you'll silent get a \ {container} back which has a copi of the \ {variable} for each item in the collect and will attempt to act like the contain \ {variables}
how is the \ {@potential} decor use in pymc defin a function \ {f} which has \ {parent\_name=parentvar} paramet that specifi the variabl parent and return a log potenti then decor \ {f} with \ {@potential(args)}, pass in ani option that'd usual go to the \ {potential} initi
how can you visual the structur of a pymc model use the \ {pymc.graph.dag()} function to draw a graphic represent of the model
what cach is done in pymc \ {logp} method are wrap in a \ {lazyfunction} which by default keep a depth-2 cach that match by refer against \ {logp} argument
what are the import class in pymc model fit hierarchi \ {mcmc} and \ {normalapprox} both extend \ {sampler} \ {map}, \ {mcmc} and \ {normalapprox} all extend \ {model}
what can be pass to pymc fit method as \ {input} a collect or nest collect of node a dictionari of node or collect of node a modul contain the node definit
what the advantag of pass a dictionari to a pymc fit method \ {input} the entri will be expos as attribut on the produc model which is use when you want collect of variabl expos as a singl attribute, instead of the normal behaviour of expos everi variabl as it own attribut
what the \ {model.draw\_from\_prior()} method in pymc it set all stochast variabl to new random valu which is effect a sampl from the joint distribut if all data and potenti log-prob return zero
what the \ {model.seed()} method in pymc it set all the stochast variabl whose \ {rseed} attribut is not \ {none} to new randomly-chosen valu
at a high level, what the \ {map} model class in pymc it use continu optim method from scipi to find the map of continuous-variable-on models.
what the \ {normalapprox} model class in pymc an extens of the \ {map} class which use the hessian at the map to comput a normal approxim to the joint, which can then be sampl from
what the \ {model.isample()} method in pymc an inter e version of \ {model.sample()}, which allow the sampl to be interrupt at ani time then later resum with \ {model.icontinue()}
what the \ {method} class in pymc instanti of a \ {method} subclass deal with updat one or more variabl dure mcmc execut
how do you set the step method use for a variabl in pymc \ {model.use\_step\_method(methodclassobj, variable, initializerargs)} where \ {initializerargs} will be use to instanti \ {methodclassobj}
what must be done to implement a metropolis-hast step method in pymc subclass \ {metropolis} overrid \ {propose()}, which should set the valu of the variabl to propos valu overrid \ {reject()}, which should reset the valu of the variabl to what they were befor \ {propose()} was called. for non-symmetr random walks, overrid \ {hastings\_factor}
what tune in pymc dure a mcmc run with a gaussian propos the standard deviat of the propos is automat adjust to keep the accept ratio in the desir rang
what the \ {adaptivemetropolis} step method in pymc it mh but updat the variabl in s use a multivari jump propos with the covari of the propos be tune dure sampl
what'r the standard step method in pymc \ {metropolis} and it discret amp; adapt variant \ {slicer}, which implement slice sampl \ {gibbs}, which (kinda) implement gibb sampl
how are step method chosen by the \ {mcmc} in pymc for each stochast variabl \ {v} and for each method in the \ {stepmethodregistry} \ {sm.competence(v)} is call and the \ {sm} with the highest is use
how can you get a collect of all the sampl accept for a variabl in a pymc \ {mcmc} run \ {m.trace(var)}
how do you get summari statist for a variabl in a \ {mcmc} run in pymc \ {model.varname.summary()}
how can you add a function whose valu should be record dure a mcmc run in pymc add the function (with no args) to the \ {model.\_funs\_to\_tally} dictionary. the talli valu can later be retriev via \ {model.trace['name\_in\_dict']}
how can you save the current state of a \ {mcmc} object in pymc \ {model.save\_state()}, which will store it in the chosen backend
how do you roll back the state of a \ {mcmc} object to a particular previous iter of the chain \ {model.remember(idx)} will restor the variabl valu to the state given at index \ {idx} in the databas
which databas is use by default in pymc \ {ram}, which store everyth in ram
what a relev network in ml given a set of random variables, to generat edg calcul the pairwis mutual in ation and threshold it at a fraction of maximum mi
what a covari graph anoth name for a relev network
what distinguish a relev network from a graphic model relev network repres condit depend graphic model repres condit independ (usual a lot sparser)
what a depend network a way to identifi structur by fit a full, spars cpd $p(x_t x_{-t})$ to each variabl $x_t$
what a common catch when construct a depend network some of the relationship repres in the spars cpds might be strong \emph{anti}correl
what'r the most common use for depend network quick and easi way to visual structur somewhat use for data imput can be use to initi more advanc method
when should direct or undirect tree be prefer for repres tree structur undirect is general better for structur learn direct is general better for paramet learn
are undirect and direct graphic model of tree equival yup
what'r the step in the chow-liu algorithm construct a complet graph on the variabl where the edg weight are the mutual in ation find a maximum span tree over this graph this mst maxim the likelihood of the data over the set of possibl tree topolog
how is the chow-liu algorithm deriv start with the equat for the $p(\mc{d} \theta, t)$ of a tree $t$-structur mrf assum the edg and node distribut are given by the empir distribut throw away the node term becaus they'r indeped of $t$ notic that the edg term are given by the mutual in ation, assum the empir distribut
what the use of a mixtur of tree in structur learn it a half-way hous between tri to repres a model structur as a tree and repres it as a general graph.
intuitively, what markov equival two graphic model are markov equival if they encod the same set of independ assumpt
how can markov equival in dgms be character two dgms are markov equival if they have the same undirect structur and the same set of v-structur
what an essenti graph it a partial direct dag which repres a class of markov-equival graph
in the essenti graph of a class of dgms, what'r the compel edg the direct edges, which, if their direct was changed, would no longer be equival to the dgms in the equival class
what a pdag in ml partially-direct acycl graph
what global prior paramet indpend in structur learn that if $\theta_t$ are the paramet for $t$ cpd, $p(\theta) = \prod_t p(\theta_t)$
what local prior paramet independ for tabular cpds in structur learn that if $\theta_{tc}$ are the paramet for $t$ cpd when it parent are in state $c$, $p(\theta_t) = \prod_c p(\theta_{tc})$
what a consequ of global amp; local prior paramet independ for tabular cpds in structur learn that if $\theta_{tc}$ are the paramet for $t$ cpd when it parent are in state $c$, $\theta_{tc} \text{dir}(\alpha_{tc})$
when defin the margin likelihood for a particular graph structur $g$, what the $\text{score}$ function if $n_{tc}$ is a vector of the count of each state of $t$ given it parent are in state $c$, $\text{score}(n_{t, \text{pa}(t)}) = \prod_c \frac{b(n_{tc} + \alpha_{tc})}{b(\alpha_{tc})}$
what the margin likelihood for a particular graph structur $g$ in structur learn if $n_{tc}$ is a vector of the count of each state of $t$ given it parent are in state $c$, and $n_{t,\text{pa}(t)}$ is a vector of all such $n_{tc}$ $p(\mc{d} g) = \prod_t \text{score}(n_{t,\text{pa}(t)})$
what likelihood equival in structur learn if two graph are markov equivalent, they should have the same margin likelihood
what the definit of the bde prior $\alpha_{tck} = \alpha p_0(x_t = k, x_\text{pa}(t) = c)$ where $\alpha$ is the equival sampl size and $p_0$ is some prior joint probabl distribut
what bde stand for in structur learn bayesian dirichlet likelihood equival prior
what special about the bde prior in structur learn it the onli prior on complet graph which satisfi global prior paramet independ local prior paramet independ likelihood equival
what paramet modular in structur learn if $t$ has the same parent in $g$ and $h$, then $p(\theta_t g_1) = p(\theta_t h)$
how are prior paramet $\alpha_t$ usual deriv from the bde for a graph in structur learn construct the prior for the complet graph then assum paramet modular and so margin out ani irrelev paramet for the graph in question
what the definit of the bdeu prior it the bde prior with a uni iti assumption, so $\alpha_{tck} = \frac{\alpha}{k_t c_t}$ where $\alpha_{tck}$ is the prior probabl of $t$ be in state $k$ when it parent are in state $c$ $k_t$ is the number of state $t$ can be in $c_t$ is the number of state it parent can be in
what doe bdeu stand for in structur learn bayesian dirichlet likelihood-equival uni
at a high level, what the k2 algorithm for structur learn if a total order of the node is known then the distribut over the parent of each node can be evalu independ without risk introduc ani cycl the k2 algorithm is to return the local most probabl set of parent for each node.
what the main problem with do structur learn exact the huge number of dag for even a small number of node
at a high level, how can the map dag be found for $ 16$ node use dynam program
what the comput complex of find the map dag for a set of node np-complet
at a high level, how is the map dag for a set of node usual found in practic use greedi hill climb with step involv reversing/adding/remov an edg
how are summari of the posterior distribut of graph for a set of node other than the map usual calcul in structur learn by sampl dag from the posterior via mcmc and comput estim of the summari use the sampl
how doe the bic usual compar to the actual margin likelihood it usual sever underestim it, lead to too-simpl model be chosen
what the cheeseman-stutz approxim given a graphic model with hidden parameters, estim the margin likelihood of a structur $g$ as $\ln p(\mc{d} g) \ln p(\bar d g) + \ln p(\mc{d} \hat \theta, g) - \ln p(\bar d \hat \theta, g)$ where $\hat \theta$ is the map $\bar d$ is the expect suffici statist of the data (ie with the hidden variabl replac by their expectation)
what cs stand for in ml cheeseman-stutz approxim
how do variat bounds, cs and bic approxim to the margin likelihood compar variat bound are alway better than cs cs is alway better than bic
what the problem with use the margin likelihood to assess model qualiti when there are hidden variabl the margin likelihood doesn't decompos when there miss data
conceptually, how doe the structur em algorithm work under the current structure, estim the hidden variabl then use the estim to assess the margin likelihood of all neighbour structur and move to the most like one
what metric doe structur em usual use to assess the qualiti of a model $\text{score}_\text{bic}(g, \mc{d}) = \ln p(\mc{d} \hat \theta, g) - \frac{\ln n}{2} \text{dim}(g) + \ln p(g) + \ln p(\hat \theta g)$
what a structur signatur in structur learn it a cluster of dens connect node that can be taken to indic the presenc of a latent variabl
what an idiom for structur learn in neurosci ``node that fire togeth should wire together'
what the causal markov assumpt if a dgm has an edg $a \rightarrow b$, then this mean that `$a$ direct caus $b$
what the causal suffici assumpt that all the variabl relev to a causal dag are includ in the model aka there are no confound
what a perfect intervent it a represent of fix a variabl in a causal dag to a specif valu
what the do-calculus notat for a perfect intervent $\text{do}(x_i = x_i)$
what graph surgeri it repres a perfect intervent by cut the edg come into ani node set by the intervent
how can you per infer about the effect of an intervent on a causal dag implement the intervent use graph surgeri appli standard probabilist infer to the mutil graph
what a fat hand intervent an intervent which affect multipl node simultan
what doe it mean for a distribut $p$ to be faith to a graphic model $g$ all the condit independ assumpt of $p$ are captur by $g$
what a stabl distribut anoth name for a faith distribut
given a multivari normal, how can the likelihood of the precis be written in term of the determin and trace $l(\lambda) = \ln \det \lambda - \text{tr}(s\lambda)$ where $s = \frac{1}{n} \sum_i (x_i - \bar x)(x_i - \bar x)^t$ is the empir covari matrix
given a multivari normal, what the gradient of the likelihood of the precis $\nabla l(\lambda) = \sigma - s$ where $s = \frac{1}{n} \sum_i (x_i - \bar x)(x_i - \bar x)^t$ is the empir covari matrix
what a matrix complet of an empir covarainc matrix $s$ in the context of grf structur learn it a covari matrix $\sigma$ which retain as mani entri of $s$ as possibl while preserv a specifi sparsiti pattern on $\sigma^{-1}$
what the object in graphic lasso $j(\lambda) = -\ln \det \lambda + \text{tr}(s\lambda) + \lambda \lambda _1$ where $ \lambda _1 = \sum \lambda_{jk} $ and $\lambda$ control the regular strength
what a nonparanorm distribut a distribut $x$ such that there exist a set of monoton $f_j$, one for each $x_j$, such that $(f_j(x_j))_j$ is joint gaussian
what the relev of nonparanorm distribut to structur learn structur learn on ugm whose distribut is joint nonparanorm can be done by tran ing the distribut to joint gaussian and carri out glasso. this is a consist estim of the graph structure.
whi is learn structur for ugm with discret variabl much harder than learn structur for gaussian ugm comput the partit function for a gaussian ugm is equival to comput the determin comput the partit function for a discret ugm is equival to comput the perman
what the relev of circuit complex to structur learn model with low circuit complex can have high treewidth while also permit fast exact infer
what sampl complex the number of sampl an approxim need to be within some $\epsilon$ of the true valu with high probabl
explanatori variabl independ variabl (predictor)
respons variabl depend variabl (predicted)
linear regress model y = β₀ + β₁x β₀ : intercept β₁ : slope * point estimates: b₀ and b₁, respect
what are the three way to describ the associ between two numer variabl 1. *direction*: posit (x↑, y↑), negat (x↓, y↑) 2. * *: linear or not 3. *strength*: determin by the scatter around the under relationship
correl *linear* associ between two numer variables; note that a relationship that is nonlinear is simpli call an associ
correl coeffici r, also call pearson r, has the follow properties: - magnitud (absolut value) of r = strength of linear associ - sign (posit or negative) indic direct of associ - alway between -1 (perfect negat linear association) and +1 (perfect posit linear association); 0 indic no linear relationship) - unitless, so not affect by chang in center or scale of either variable, such as unit conversions) - correl of x with y = y with x - sensit to outlier
residu (e) differ between observ (y) and predict (ŷ) valu of the respons variabl
least squar line minim the sum of the squar residuals; alway pass through the averag of the respons and explanatori variabl (x mean, y mean); condit necessari for fit such line: 1. linear 2. near normal residu 3. constant variabl
indic variabl binari explanatori variabl (with two levels)
estim for the slope (b₁) r is the correl coefficient, sy is the standard deviat of the respons variable, and sx is the standard deviat of the explanatori variabl
how can the slope be interpret 1. when x is numerical: "for each unit increas in x, we would expect y to be lower/high on averag by b1 units." 2. when x is categorical: "the valu of the respons variabl is predict to be b₁ unit higher/low between the baselin level and the other level of the explanatori variable." - note that whether the respons variabl increas or decreas is determin by the sign of b₁.
estim for the intercept (b₀) b1 is the slope, y ̄ is the averag of the respons variable, and x ̄ is the averag of explanatori variabl
how can the intercept be interpret 1. "when x = 0, we would expect y to equal, on average, b₀" when x is numerical. 2. "the expect averag valu of the respons variabl for the refer level of the ex- planatori variabl is b₀" when x is categorical.
how can you predict the valu of the respons variabl for a given valu of the explanatori variable, x* - onli predict for valu of x* that are in the rang of the observ data. - do not extrapol beyond the rang of the data, unless you are confid that the linear pattern continues.
r² variabl in the respons variabl explain by the the explanatori variabl - for a good model, we would like this number to be as close to 100% as possible. - this valu is calcul as the squar of the correl coefficient.
leverag point a point that lie away from the center of the data in the horizont direct
influenti point a point that influenc (changes) the slope of the regress line; this is usual a leverag point that is away from the trajectori of the rest of the data
what is the null hypothesi (h₀) set to for test the signific of the predictor h₀ : β₁ = 0 - β₁ = 0 mean the regress line is horizontal, henc suggest that there is no relationship between the explanatori and respons variables.
t score for the hypothesi test - df = n − 2 - note that the t score has n - 2 degre of freedom sinc we lose one degre of freedom for each paramet we estimate, and in this case we estim the intercept and the slope.
whi is a hypothesi test for the intercept irrelev it usual out of the rang of the data, and henc it is usual an extrapolation.
confid interv for the slope - df = n − 2 and t*df is the critic score associ with the given confid level at the desir degre of freedom. - note that the standard error of the slope estim seb1 can be found on the regress output
what the trick for do rda when $d n$ let $x = udv^t$ be the thin svd of $x$, so $d$ is $n n$. then we can defin $z = ux$ to be the design matrix in the smaller $n$-dimension space so $\sigma_x = v\sigma_z v^t$ and we can regular $\sigma_z$, then map it back to $\sigma_x$
what the posterior mean given a niw prior it a niw with paramet $\kappa_n = \kappa_0 + n$ $m_n = \frac{\kappa_0}{\kappa_n} m_0 + \frac{n}{\kappa_n} \bar x$
what the posterior scale matrix given a niw prior $\kappa_n = \kappa_0 + n$ $s_n = s_0 + s_{\bar x} + \frac{\kappa_0 n}{\kappa_n}(\bar x - m_0)(\bar x - m_0)^t$
what the of the predict distribut given a niw prior it a student t distribut (the t distribut has wider tails, which account for the fact $\sigma$ is unknown)
when write a student t distribut as a mixtur of gaussians, what distribut are the compon coeffici drawn from an invers wishart distribut
what the of the posterior margin of $\mu$ and $\sigma$ given a niw prior $\sigma \text{iw}$ $\mu \mathcal{t}$
how doe the sampl standard deviat aris from a bayesian perspect if you appli an unin ativ nix (ie niw) prior to a gaussian, the posterior varainc is found to be $s^2 = \frac{n}{n-1}\hat \sigma_{\text{mle}}$
what a simpl approxim to the 95 credibl interv for the mean given an unin ativ nix (ie niw) prior $i_{.95}(\mu \mc{d}) = \bar x 2\frac{s}{\sqrt{n}}$ where $s$ is the sampl standard deviat
how do t test aris from a bayesian perspect an unin ativ nix (niw) prior on a gaussian give rise to a $\mc{t}$-distribut posterior mean
is it covari or precis matric that are drawn from a wishart distribut precis
what a semi-conjug prior one in which each term is individu conjugate, but the whole thing isn't
how can the posterior predict distribut be written given a conjug prior given new data $\mc{d}^\prime$ and posterior hyperparamet $\alpha^\prime$ it the margin likelihood $p(\mc{d}^\prime \alpha^\prime)$
what the distribut of the product of two gaussian $\mc{n}(\lambda_1\mu_1 + \lambda_2\mu_2, \lambda_1 + \lambda_2)$
what a reject option in ml it the abil to refus to make a predict which can be use if mispredict are veri expens
what type ii maximum likelihood anoth name for empir bay
what the second moment of a gaussian $\mbb{e}[x^2] = \mu^2 + \sigma^2$
what are some basic fact about the optim solut set of $\min x _1$ s.t. $b = ax$, where $a$ is a $n m$ matrix it bound it convex it has a solut $x$ with $n$ nonzero entri
how can you convert a $\min x _1$ s.t. $b = ax$ problem to a lp write $x = u - v$ for posit vector $u, v$ write $z = \left[ {smallmatrix} u v {smallmatrix}\right]$ then $\min \sum z_i$ s.t. $b = [a, -a]z$, $z 0$
what danger about $p$-norm with $p 1$ they'r not actual norm - they don't satisfi the triangl inequ
for $q p$, what'r the $x$ with minimum $q$ norm when $ x _p = 1$ the $x$ with onli one non-zero entry, which is 1 so the smaller the $r$, the sparser the $r$ norm
geometrically, whi is the $q$ norm sparser than the $p$ norm when $q p$ imagin the $p$ norm surfac and blow up a $q$ norm `balloon insid it. the first intersect of the two occur on the axe
what the weak $l_p$ norm $ x ^p_{wl_p} = \sup_{\epsilon 0} [n(\epsilon, x)\cdot \epsilon^p]$ where $n(\epsilon, x)$ count the number of element of $ x $ larger than $\epsilon$
what are the necessari properti of $\rho$ in a function $j(x) = \sum \rho(x_i)$ to promot sparsiti $\rho(x)$ is symmetr $\rho(x)$ is non-decreas for $x 0$ $\rho^\prime(x)$ is non-increas for $x 0$
what the danger in the $l_0$ norm it isn't homogen
what doe $p_j$ stand for in the sparsiti literatur the problem $\min x _j$ s.t. $ax = b$
what the uncertainti principl for fourier-conjug function if $f, f$ are a function and it fourier tran , with $f$ scale and the tran defin so $ f _2, f _2 = 1$ then $d_0(f) \cdot d_0(f) \frac{1}{2}$ where $d_0$ is the dispers about zero
what the mutual coher of two base $\psi, \phi$ $\mu([\psi, \phi]) = \max_{i, j} \psi_i \cdot \phi_j $
what the rang of the mutual coher of two orthogon base $\phi, \psi$ $\frac{1}{\sqrt{n}} \mu([\phi, \psi]) 1$
what the first uncertainti principl for orthogon base $\psi, \phi$ given a nontrivi $b = \psi \alpha = \phi \beta$, $ \alpha _0 \beta _0 \frac{1}{\mu([\psi, \phi])^2}$ this hold for the $l_1$ norm too
what a use inequ relat geometr and algebra mean $\sqrt{ab} \frac{a+b}{2}$
what the dispers about zero in fourier analysi $d_0(f) = t x^2 f(x) ^2 dx$
what the uncertainti of redund solut principl for orthogon base $\psi, \phi$ given a nontrivi $b$ and solut $x_1, x_2$ to $[\psi, \phi]x = b$, $\frac{ x_1 _0 + x_2 _0}{2} \frac{1}{\mu([\psi, \phi])^2}$
what the uniqu consequ of the uncertainti of redund solut given orthonorm base $\psi, phi$ and nontrivi $b$ if $[\psi, \phi]x = b$ has fewer than $\frac{1}{\mu(a)}$ nonzero then it is the sparsest one possibl
what the spark of a matrix $a$ the smallest number of column that are linear depend
what the uniqu for the spark of $a$ if $x$ solv $ax = b$ and $ x _0 \frac{1}{2}\text{spark}(a)$ then $x$ is the sparsest solut possibl
what the rang of the spark of a matrix for a $n m$ matrix a, $2 \text{spark}(a) n + 1$ it can be $1$ if one of the column of $a$ is zero, but this is consid an edg case
what the gram matrix of $a$ $a^ta$ ie the matrix of inner product of the column of $a$
what the mutual coher of a matrix $a$ $\mu(a) = \max_{i j} \frac{ \bar a_i \cdot \bar a_j }{ a_i _2 a_j _2}$ ie the maximum normal inner product between two column
what the lower bound on the mutual coher for general $n m$ matric $a$ $\mu(a) \sqrt{\frac{m-n}{n(m-1)}}$
which oper satisfi the lower bound on the mutual coher grassmannian frame
how is the spark of a matrix relat to it mutual coher $\text{spark}(a) 1 + \frac{1}{\mu(a)}$
what the gershgorin disc theorem let $a$ be a $n n$ matrix, let $r_i$ be the sum of the nondiagon entri in row $i$ of $ a $ then everi eigenvalu of $a$ lie in one of the disc $d(a_{ii}, r_i)$
what the intuit interpret of the gershgorin circl theorem if the off-diagon entri of $a$ are small, then the eigenvalu must be close to the eigenvalu of $\text{diag}(a)$, so they must be close to $\{a_{ii}\}_i$
what the uniqu for the mutual coher of $a$ if $x$ solv $ax = b$ and $ x _0 \frac{1}{2}(1 + \frac{1}{\mu(a)})$ then $x$ is the sparsest solut possibl
what the babel function for a matrix $a$ with normal columns, $\mu_1(p) = \max_{ \lambda = p} \max_{j \not \lambda} \sum_{\lambda} \bar a_i \cdot \bar a_j $
what the behaviour of the babel function $\mu_1(1)$ is the mutual coher it non-decreas $\mu_1(p) p \mu(a)$
how can the babel function be calcul effici comput the gram matrix $g$ of the column-norm $a$ sort each row of $ g $ in descend order to get $g_s$. then $\mu_1(p) = \max_i \sum_{j=2}^{p+1} g_s(i, j) $
how doe the babel function lower bound the spark if $p$ is the smallest such that $\mu_1(p) 1$ then $\text{spark}(a) p + 1$
how can the spark of a matrix be upper bound for each $i$, solv $z^{i} = \arg \min_x x _1$ s.t. $ax = 0$ and $x_i =1$ then $\text{spark}(a) \min_i z^i _0$
how well doe the $l_1$-deriv upper bound on the spark work in practic it usual veri tight
what a grassmannian matrix it a $a$ of size $n m$ with normal column such that it gram matrix $g$ satisfi $ g_{ij} = \sqrt{\frac{m-n}{n(m-1)}}$ for all $i j$
what the intuit interpret of a grassmannian matrix all the column are at the same angl to eachother, and the angl is maxim
what are the two strategi for solv $l_0$ optim problem search over possibl support smooth the penalti function
what the differ between match pursuit and orthogon match pursuit in omp, the residu is re-evalu after each select use all the select vector in mp, the residu simpli has the compon along the newli select vector remov
what the idea in ls-omp rather than test each candid against the residu a full least-squar fit with the candid and all current select vector is done
what the weak match pursuit algorithm rather than pick the optim vector in each step the first vector within a factor of $t$ of $ r _2$ is chosen, where $r$ is the residu or the optim vector if there is no such weak choic
what the decay factor of a column normal matrix $a$ wrt a normal vector $v$ $\delta(a, v) = \max_j \bar a_j \cdot v $
what the univers decay factor of a matrix $\delta(a) = f_v \delta(a, v)$ where $\delta(a, v)$ is the decay factor wrt $v$
what the intuit interpret of the decay factor of a matrix it the (arcco of) the maximum angl ani vector can be from a column of the matrix
how can the univers decay of a matrix be use to analyz the progress of match pursuit when solv the system $ax = b$, the $k$th residu is bound by $ r_k ^2_2 (1 - \delta(a))^k b _2^2$
what the univers decay factor of an orthogon matrix $a$ $\delta(a) = \frac{1}{n}$
what the threshold algorithm for $l_0$ optim pick the normal column with the $k$ largest inner product with the target.
what are some common sparsity-promot regular other than the $l_p$-norm $\sum \ln(1+\alpha x_j^2)$ $\sum \frac{x_j^2}{\alpha + x_j}$ $\sum (1 - \exp(-\alpha x_j^2))$
what the idea in the focuss algorithm for $l_0$ optim appli irl with weight that emul the $l_\epsilon$ norm use the $l_2$ norm where $\epsilon$ is veri close to 0
how doe the focuss algorithm approxim the $l_\epsilon$ norm let $x^+_{j} = x_j^{\epsilon-1}$ if $x_j$ is nonzero, and 0 otherwise. then $ x^+ \odot x ^2_2$ is equal to $ x ^\epsilon_\epsilon$
what path do the intermediari solut of the focuss algorithm describ a descent on $\prod x_i $
how should the focuss algorithm be initi with an all-nonzero vector, sinc onc an iter zero a component, it'll remain zerod
what the basi pursuit problem $\min x _1$ s.t. $ax = b$ ie lasso with an exact constraint
in term of the mutual coherence, when is omp guarante to find the optimal-spars solut to $ax = b$ when a solut exist with $ x _0 \frac{1}{2}(1 + \frac{1}{\mu(a)})$
in term of the mutual coherence, when is the threshold algorithm guarante to find the optimal-spars solut to $ax = b$ when a solut exist with $ x _0 \frac{1}{2}(1 + \frac{ x_\text{min} }{ x_\text{max} }\frac{1}{\mu(a)})$ where the min amp; max are taken over the nonzero element of $x$
in term of the mutual coherence, when is solv the basi pursuit problem guarante to find the optimal-spars solut to $ax = b$ when a solut exist with $ x _0 \frac{1}{2}(1 + \frac{1}{\mu(a)})$
which of omp and bp general per s better in practic basi pursuit
what the exact recoveri condit for a matrix $a$ and target support $\mc{s}$ $\text{erc}(a, \mc{s})$: $\max_{i \not \mc{s}} a^+_\mc{s}\bar a_i _1 1$ where $a_\mc{s}$ is $a$ with it column restrict to the set $\mc{s}$ and $a^+$ is the pseudoinvers
what the relev of the exact recoveri condit to bp and omp if $ax = b$ has a solut $x$ over support $\mc{s}$ such that $\text{erc}(a, \mc{s})$ is met then both omp and bp will recov $x$
what are the two general approach to interpol function restrict the set of function that could explain the data pick a prior distribut over the function that could explain the data
what are the two main view on what a gp repres as a distribut over weight in a high-dimension featur space as a distribut over the function space
what the al definit of a gaussian process a collect of random variabl such that ani finit subset have a joint gaussian distribut
what the consist requir of gps if you consid the distribut of $(y_1, y_2) gp$, then margin $y_2$ must lead to $y_1 gp$
how the kernel of a gp relat to it covari $k(x_p, x_q) = \text{cov}(f(x_p), f(x_q))$ so the kernel describ the covari of the output in term of the input
how are gps use for predict by construct the joint distribut on the train amp; test input and then condit on the observ train output
given a featur map $\phi(x)$ and a prior on the weight $w n(0, \sigma_p)$, what the correspond gp kernel $k(x_i, x_j) = \langl \phi(x_i), \phi(x_j) \rangle_{\sigma_p}$
in the gp literature, what do $k$ and $k_*$ signifi $k = k(x,x)$, the kernel matrix of the train input $k_* = k(x, x_*)$, the kernel matrix between the train amp; test input
in the gp literature, what doe $k_*$ signifi the vector of covari between the train input and the test input $x_*$
what the covari of the predict distribut of gp $\sigma = k_{**} - k_*^t[k + \sigma^2 i]^{-1} k_*$
what smse stand for in ml standard mean squar error
what msll stand for in ml mean standard log loss
what the standard loss a loss function rescal by the loss that'd occur under a trivial model
what the standard error in ml the error divid by the standard deviat
how should the msll be interpret trivial predict method will have a msll around 0 better method will have a more negat msll
how can the predict mean of a gp be written in term of the eigenvector of the kernel $\bar f = \sum \frac{\lambda_i}{\lambda_i + \sigma^2_n} (u_i \cdot y) u_i$ where $u_i$ is the $i$th eigenvector of the kernel evalu on the train set
what the effect number of degre of freedom of a gp $\sum \frac{\lambda_i}{\lambda_i + \sigma_n^2}$ where $\lambda_i$ are the eigenvalu of the kernel evalu on the test data, $k$
what the weight function of a linear smoother the $h(x_*)$ such that $\bar f(x_*) = h(x_*) \cdot y$
what the weight function for a gp when interpret as a linear smoother $h(x_*) = (k + \sigma^2_n i)^{-1}k(x_*)$
what a least-squar classifi one in which the target class are interpret as real valu (like $-1, 1$) and least-squar regress is appli to them
what a respons function in ml anoth name for the mean function of a glm
what the function use in calcul the log odd ratio the logit function
what are the two main method for build discrimin classifi from gps normal approxim to the posterior expect propag
how is the mean of normal approxim fit to the posterior of a gp classifi given label $y$ and a latent function $f$ such that $p(y=+1 x) = \sigma(f(x))$ analyt deriv the gradient of the unnorm posterior, $\nabla \phi(f)$ and use newton method to find the map, $\hat f$ this is the mean of the approximation.
how doe the logist function correspond to the logist distribut the logist function is the cdf of the logisit distribut
in a binari classifi with a gaussian latent posterior, what a simpl altern to predict class label by integr over the posterior point predict use the map paramet these are equival becaus the gaussian is symmetr while the respons function (usual $\sigma$) is common antisymmetr
what the use of a latent variabl in a discrimin classifi the latent variabl can be assum to be contin over say $\mbb{r}$ and tran ed to a class probabl use a respons function like $\sigma$
what jitter in the context of numer comput replac a badly-condit matrix $k$ with $k + \epsilon i$ for some small $\epsilon$
what the core approxim in ep for gp discrimin classifi approxim the label likelihood as $p(y_i f_i) \tild z_i \mc{n}(f_i \tild \mu_i, \tild \sigma^2_i)$
what'r the paramet of the approxim to the posterior $p(f x,y)$ in the ep approach to gp classifi if $\tild \lambda, \tild \lambda \tild \mu$ are the paramet for the approxim to the likelihood $p(i f)$ then the paramet for the posterior approxim are $\lambda = k^{-1} + \tild \lambda$ $\lambda \mu = \tild \lambda \tild \mu$
what are the step in an iter of ep for gpcs take the current approxim posterior $q(f x,y)$ leav out one of the approxim likelihood term $q_i(y_i f_i)$ to get the caviti distribut $q_{-i}(f_i)$ combin the caviti distribut with the exact likelihood $p(y_i f_i)$ to get a non-gaussian margin $\hat q(f_i)$ find the paramet such that $q_i(y_i f_i)$ minim the revers kl diverg from $\hat q(f_i)$ replac the old approxim of $q_i(y_i f_i)$ with the new one.
what the in ation score of a model the kl diverg of the model from the empir distribut
how do mcmc, ep and the normal approxim compar for construct gpcs mcmc usual per s the best with ep close behind while the normal approxim often suffer from an inabl to repres skew distribut
what a stationari covari function a translation-invari one
what the gram matrix of a covari function the $k$ such that $k_{ij} = k(x_i, x_j)$ for some input $\{x_i\}$
given a one dimension gaussian with zero mean and kernel $k$, what the expect number of upcross on the unit interv $\mbb{e}[n_u] = \frac{1}{2\pi} \sqrt{-\frac{k^{\prim \prime}(0)}{k(0)}} \exp\left(-\frac{u^2}{2k(0)}\right)$
what it mean for a function $f$ to be continu in mean squar at a point $x_*$ for everi sequenc $x_k$ such that $ x_k - x_* \rightarrow 0$, $\mbb{e}[ f(x_k) - f(x_*) ^2] \rightarrow 0$
what doe it mean for a random field to be continu in mean squar at a point $x_*$ the covari function $k(x, x^\prime)$ is continu in ms at $x = x^\prime = x_*$
what the use of the mean squar limit in the context of stationari process if $k$ is the kernel and deriv are taken use the mean squar limit, then if the $2m$th order partial deriv of $k$ exist at $0$ then the $m$th order partial deriv of $k$ exist everywher in $\mbb{r}^d$
what a stationari statist process one with a stationari kernel
what bochner theorem a complex-valu $k$ is the covari function of a stationari mean-squar continu random process if and onli if it the fourier tran of a posit finit measur
what the wiener-khintchin theorem if a stationari mean-squar continu covari function $k$ has a correspond power spectrum $s$ then $k$ and $s$ are fourier dual
what the power spectrum of a covari function $k$ given a stationari mean-squar continu random process with covari $k$ if the correspond posit finit measur in the fourier domain, $\mu$, has a densiti $s$ then $s$ is the power spectrum of $k$
what the infinit network construct of the squar exponenti kernel the se kernel can be obtain by expand an input $x$ over a dens grid of spheric gaussian
what happen to the matern kernel when $\nu \rightarrow fty$ the se kernel is recov
when is the matern kernel $k$ time ms differenti when $\nu k$
what the general of the matern kernel when $\nu = p+ \frac{1}{2}$ for an integ $p$ it the product of an exponential, and a polynomi of order $p$
for what valu of $\nu$ are matern kernel of interest to ml $\frac{3}{2}$ and $\frac{5}{2}$ below which it becom too rough and abov which it becom indistinguish from a se kernel
what doe ms differenti mean in ml mean squar differentiable; ie the function is differenti when the limit are taken in the mean-squar
what the most general represent of an isotrop kernel a scale mixtur of gaussian
which kernel function are usual use for a gp covari if compact support is need piecewis polynomi one
what kind of problem are polynomi kernel usual suit to high-dimension imag classif
how can a `neural network covari function be construct defin $f(x) = b + \sum v_j h(x \cdot u_j)$ for some transfer function $h$ and hidden weight $u_j$ distribut $b, v_j, u_j$ as independ gaussian then $\mbb{e}[f(x)f(x^\prime)]$ can be interpret as a covari function
when construct a neural network covari function, what transfer function is usual use $h(x) = \text{erf}(x \cdot u_j)$
what warp in the context of gps construct a nonstationari kernel by appli a nonlinear map $u(x)$, then evalu it against a stationari kernel
given two random process $f_1(x), f_2(x)$, what the kernel of $f_1 + f_2$ $k_1 + k_2$
given two random process $f_1(x), f_2(x)$, what the kernel of $f_1f_2$ $k_1(x, x^\prime)k_2(x, x^\prime)$
given random process $f(x)$ and function $a(x)$, what the kernel of $a(x)f(x)$ $a(x)k(x, x^\prime)a(x^\prime)$
given a random process $f$ and a kernel $h$, what the kernel of $ t h(x,z)f(z)dz$ $ t h(x,z)k(z, z^\prime)h(x^\prime, z^\prime)dzdz^\prime$
what an eigenfunct of a kernel a $\phi$ satisfi $ t k(x, x^\prime)\phi(x)d\mu(x) = \lambda \phi(x)$ with respect to a given measur $\mu(x)$
what mercer theorem if $k$ is a posit definit kernel on a finit measur space with normal eigenfunct $\{\phi_i\}_i$ then $\{\lambda_i\}_i$ are absolut summabl and $k(x, x^\prime) = \sum \lambda_i \phi_i(x) \cdot \phi_i(x^\prime)$ almost everywher
what a degener kernel a kernel with onli a finit number of nonzero eigenvalu
how are the eigenfunct of a kernel usual approxim comput by approxim the integr that defin them with a sum, with the point of the sum be sampl accord to the measur and solv the matrix eigenproblem that s
when train a gp with a se kernel, what are the hyperparamet $\sigma_f$, the signal varianc $\sigma_n$, the nois varianc $m$, the precis matrix of the kernel
what are common s for the precis matrix of a se kernel $\frac{1}{l^2}i$ $\text{diag}(l^2)^{-2}$ $\lambda\lambda^t + \text{diag}(l^2)^{-2}$
what doe the characterist length scale repres how far you need to move befor the function becom uncorrel
whi is loocv cheap for gps becaus the expens part of invert the kernel onli need be done onc then the inverse-with-a-row-missed-out need for each loo case can be comput use invers ula
when visual the tensor dot product of two matric $a, b$ along their row axis, how should you orient the compon as three face of a cube: with the output on the front face $a$ on the left $b$ on the top the axi you'r sum over head away from you and the origin for $a, b$ in the top-left corner
in an equat such as y ≈ mx + b, what doe the ≈ refer to "is approxim model as"
whi is the irreduc error for a function in make a predict almost alway greater than zero becaus you havent measur all of the relev variabl or becaus it is unmeasurable, but this is more philisoph imo
what is a parametr model how is a non parametr model differ make an assumpt about the function of the model and therebi reduc the problem to estim that function parameters; make no assumpt about the function of the model
what defin unsupervis statistics, vs supervis statist classic exampl unsupervis refer to when you want to make sens of your measur variabl but lack a respons variabl to predict; cluster analysi
defin semi-supervis learn a situat where you have access to respons variabl measur for onli a subset of the data point so you have some measur which are not associ with respons variables, but you still want to tri to use them to make sens of your data
as the flexibl in a statist model increases, what happen to train mean squar error (mse) test mse monoton decrease; typic it is a u shape curve, as it decreas as the function approx the true model, then onc again increas as the model overfit the data figur 2.9 from "an introduct to statist learning"; http://www-bcf.usc.edu/~gareth/isl/getbook.html
defin bias (statistics) error that is introduc by approxim a complic real world problem by a simpl model so typic model underspecif
defin varianc of a function (statist modeling) the amount that a function would chang if you estim it with a differ train data set higher level of this caus more general error isl
how doe k nearest neighbor classif work (3 steps) 1) for each data point in the test set, it find the k nearest point in the train set base on measur similairti 2) it assign condit proabil to each respons outcom base on the frequenc of each outcom in this set of neighbor 3) it choos the respons outcom with the highest probail as the predict isl
defin residu sum of squar (rss) in a regress model, the sum of the squar deviat of the predict from the actual valu
what phenomenon doe this plot demonstr a represent of the trade-off between flexibl and interpret in use differ statist learn method in general, as the flexibl of a method increases, it interpret decreases. figur from "an introduct to statist learning" by jame et al; http://www-bcf.usc.edu/~gareth/isl/getbook.html
in these contour plot measur rss as a function of two variabl on some data set, what do the red dot repres the least squar estim that minim the rss figur from "an introduct to statist learning"; http://www-bcf.usc.edu/~gareth/isl/getbook.html
in the context of a multipl regression, how do you interpret the mean of the bj regress coeffici where xj repres the jth predictor variabl as the averag effect on y of a one unit increas in xj, hold all other predictor fix isl
when you are ask the follow question, what is a good model to use do all the predictor help to explain y, or is onli a subset of the predictor use multipl regress for example, multipl linear regress isl chapter 3
in a multipl regression, if there are p variabl to consider, then how mani possibl model are there 2 p so 4 (2^2) for 2 variables, and 8 (2^3) for 3 variabl
what are the step for forward model select in a linear regress (3 steps) 1) begin with a null model that contain an intercept but no predictor 2) choos the variabl to add to the regress that s in the lowest rss 3) continu until some stop rule is satisfi in backward selection, start with all variabl in the model, and backward note that this is a greedi approach that might includ variabl earli that later becom redund isl
in linear regression, what happen to r-squar on the test data set when more variabl are ad to the model whi it alway increases; ad anoth variabl to the least squar equat must allow us to better fit the train data (though not necessarili the test data) more accur isl
accd to the jame textbook, what are the two most import assumpt of a linear regress model 1) linear relationship, i.e. the effect of chang one unit in xj is constant, irrespect of the valu of xj 2) addit relationship, i.e. the effect of chang in a predictor x on the respons y is independ of the valu of the other predictor (vs synergi or an interaction) from "an introduct to statist learning" (isl); http://www-bcf.usc.edu/~gareth/isl/getbook.html
defin interact term (regression) describ a situat in which the simultan influenc of two variabl on a third is not addit w: the notion of "interaction" is close relat to that of "moderation" that is common in social and health scienc research: the interact between an explanatori variabl and an environment variabl suggest that the effect of the explanatori variabl has been moder or modifi by the environment variable.
what is like the simplest extens to linear regression, which is seen here polynomi regress still a linear model, so can still be solv use standard multipl linear regress softwar figur from "an introduct to statist learning"; http://www-bcf.usc.edu/~gareth/isl/getbook.html
defin student residu (regression) the of divid a residu by it estim standard error necessari bc point can have differ standard error so compar their deviations, eg when look for outliers, can be peril ow isl: observ whose student residu are greater than 3 in absolut valu are possibl outlier figur from "an introduct to statist learning"; http://www-bcf.usc.edu/~gareth/isl/getbook.html w: typic the standard deviat of residu in a sampl vari great from one data point to anoth even when the error all have the same standard deviation, particular in regress analysis; thus it doe not make sens to compar residu at differ data point without first studentizing. it is a of a student t-statistic, with the estim of error vari between points. this is an import techniqu in the detect of outliers. it is name in honor of william sealey gosset, who wrote under the pseudonym student, and divid by an estim of scale is call studentizing, in analog with standard and normalizing: see studentization.
in a regress setting, what is the name of the problem with the data set on the right whi is this a problem as predictor variables, rate and limit have high collinear ; it can be difficult to separ out the individu effect of high collinear variabl on the respons variabl figur from "an introduct to statist learning"; http://www-bcf.usc.edu/~gareth/isl/getbook.html
defin collinear an exact or approxim linear relationship between two explanatori variabl which is consid a problem beacus it is difficult to separ out the individu effect of high collinear variabl on the respons variabl
what the definit of the error in numer analysi $e_{(i)} = x_{(i)} - x_*$
what the definit of the residu in numer analysi $r_{(i)} = y_{(i)} - y_*$
what the quadrat minim problem associ with solv $ax = b$ for a symmetric, positive-definit $a$ $\min \frac{1}{2}x^tax - bx$ gradient of the quadrat problem give the linear system
in the cg method for solv a linear system $ax = b$, what are two altern interpret of the residu the error tran ed into the same space as the steps, $r_{(i)} = -ae_{(i)}$ the direct of steepest descent in the quadrat problem, $r_{(i)} = -f^\prime(x_{(i)})$
what'r the step in the jacobi method for solv a linear system $ax = b$ split $a = d+e$, where $d = \text{diag}(a)$ then iter $x = bx+z$, where $b = -d^{-1}e$ $z = d^{-1}b$
what the idea in the jacobi method for solv a linear system $ax=b$ hope $b = -d^{-1}e$ will have a smaller spectral radius than $a$, lead to faster converg
in the jacobi method for solv a linear system, whi doe $b$ have a small spectral radius speed converg write $x = x_* + e$ then the error updat as $e^\prim = be$ so the smaller the spectral radius, the faster the error shrink
is the jacobi method guarante to converg nope.
what'r rougher and smoother in the context of numer analysi smoother shrink everi compon of the error at each step rougher onli shrink some compon
how fast doe steepest descent converg in term of $a$ condit number $\frac{ e_i _a}{ e_0 _a} \left(\frac{\kappa-1}{\kappa+1}\right)^i$
when are two vector $x, y$ conjug with respect to $a$ when $x^tay = 0$
what the intuit behind two vector be conjug wrt a positive-definit symmetr $a$ $x, y$ are conjug wrt $a$ if when the space is rescal so the ellips describ by $a$ are circl then $x, y$ are orthogon
what the use properti of the search direct chosen in the cg method the direct $d_i$ at a step $i$ is chosen to be $a$-conjug wrt the direct chosen previous
what the properti of the optim of the error term in cg at step $i$, cg has minim $ e_i _a$ over the subspac $e_0 + \text{span}(\{d_k\}_k)$
given a posit definit matrix $a$, what the definit of the energi norm $ x _a$ $ x _a = \sqrt{\langl a x, x \rangle}$
intuitively, what the consequ of how the search direct is generat in cg $d_i$ is the $i$th residu with the compon of all previous search direct remov
what a krylov subspac the direct sum over $0 i n$ of the subspac generat by $a^ix$
what the definit of the step size in cg $\alpha^\prim = \frac{ r ^2}{ d ^2_a}$
what the definit of the the gram-schmidt constant generat in cg $\beta^\prim = \frac{ r^\prime ^2}{ r ^2}$
what the definit of the search direct in cg $d^\prime = r^\prime + \beta^\prim d$
what the definit of the residu in cg $r^\prime = r - \alpha a d$
what the definit of the locat $x$ in cg $x^\prime = x + \alpha d$
what the converg rate of cg in term of the condit number $\frac{ e_i _a}{ e_0 _a} \left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^i$
what a boltzmann machin a pairwis mrf with both hidden and observ node
in galton height data, parent with height 1 inch abov the mean tend to have children with height differ from the mean by how mani inch whi 0.66 inches; regress to the mean from swirl
in a regression, how do you visual tell how much effect an outlier has by see how much the regress line chang when the outlier is includ vs when it is not this is call "influence" or "leverage" or "hat" can measur this for a regress via view(hatvalues(fit)) figur from "an introduct to statist learning"; http://www-bcf.usc.edu/~gareth/isl/getbook.html
is this regress diagnost indic of a good fit whi yes; residu are uncorrel with the fit valu and have mean zero and approx iid one of the standard plot when you plot an lmfit object in r via plot(lmobj); this was generat from random data
what the smooth prior in the context of ml if you'r tri to approxim a function $f$ it the idea that if $x$ is close to $x^\prime$ then $f(x)$ is close to $f(x^\prime)$
what represent learn where you learn not onli the map from some represent to the output, but the represent itself
what the basic idea in deep learn that good represent are built from other, simpler represent
what the `program view of deep learn network encod a program and deep network can encod program with longer sequenti path
what the consequ of the program view of deep learn when interpret layer represent a layer represent may not alway encod the factor of variat that explain the input it might encod some program state that use in make sens of the input instead
what capac in ml it the number of exampl that the model could alway learn perfect relat to underfit amp; overfit
conceptually, how doe the in ation content of an event relat to the likelihood of an event like event should have low in ation content unlik event should have high in ation content
what the self in ation of an event $x x$ $i(x) = -\log p_x(x)$
what a nat in ml it the equival of a bit, but for base $e$
how the entropi of a distribut defin in term of the self-in ation $h(x) = \mbb{e}_x[i(x)]$
how doe the multinouilli distribut relat to the multinomi distribut multinouilli is a multinomi for a singl case
what the definit of the softplus function $\zeta(x) = \ln (1+\exp(x))$
when measur the in ation content of a random variable, what happen when you measur it in differ base the is rescal
given a scalar $x$, what doe $x^+$ denot in deep learn $x^+ = \max(0, x)$
how can the hessian be defin in term of the jacobian the hessian is the jacobian of the gradient
what the lagrangian equival to minim $f$ subject to $g(x) = 0, h(x) 0$, where $g, h$ are vector of function $l(x, \lambda, \alpha) = f(x) + \lambda \cdot g(x) + \alpha \cdot h(x)$ $\min_x \max_\lambda \max_{\alpha 0} l(x, \lambda, \alpha)$
what doe the lagrangian of an optim problem evalu to at a $x$ where the constraint are all satisfi $\max_\lambda \max_{\alpha 0} l(x, \lambda, \alpha) = f(x)$ becaus the $g$ associ with $\lambda$ are all $g(x) = 0$ and the $h$ associ with $\alpha$ are all $h(x) 0$
what doe the lagrangian $l(x, \lambda, \alpha)$ of an optim problem evalu to at a $x$ where one of the constraint is violat $\max_\lambda \max_{\alpha 0} l(x, \lambda, \alpha) = fty$ becaus one of the $g$ associ with $\lambda$ is nonzero or one of the $h$ associ with $\alpha$ is posit
what are the karush-kuhn-tuck condit $\nabla l(x^*, \lambda^*, \alpha^*) = 0$ constraint are all satisfi at $x^*$ $\alpha \cdot h(x^*) = 0$
what the use of the karush-kuhn-tuck condit they'r first-ord necessari condit for a point to be an optima of a constrain problem
what kkt stand for in optim karush-kuhn-tuck condit
what anomali detect in ml a task identifi input that are not like the other
what e learn a type of learn where the model can request specif addit train exampl
what the intuit interpret of regular it an imposit of a prefer set of function which is whi it often exchang with some prior distribut
what the definit of the vc dimens of a model it the cardin of the largest set of point that the model can shatter
what it mean for a model to shatter a dataset it mean that for everi assign of label to the point in the dataset some set of the model paramet will lead to perfect classif
what optim in ml $\text{gener error} - \text{train error}$
how can underfit be diagnos in term of capac if general error drop when the capac of the model increas then the model is underfit
what the use of the vc dimens it a measur of a model capac
what the theoret predict of how optim respond to chang in capac $\text{optimism} \propto \frac{\text{capacity}}{\text{train set size}}$
what are three typic interpret of a `simpl represent in ml low-dimension spars independ
which are the most common exampl of ml model that exploit the smooth prior kernel method
when do smoothness-prior-bas model work well when the data cover the variat in the target function
what the key idea to nonloc general in ml $o(n)$ sampl can defin $o(2^n)$ region as long as depend are allow between region
what a maxout unit in nns it a nonlinear function $\max_i(w_i x)$
how the softplus relat to the relu the softplus is a smooth version of the relu
what later inhibit in the context of nns excit biolog neuron often inhibit their neighbour
what'r the two usual artifici equival of later inhibit softmax layer and similar `competit function local respons normal
how can neural network be train to mimic a condit distribut let $p(i f(x))$ be a condit distribut of the class label then get the neural network to learn $f$, use the nll as the loss function
what are the three kind of optim method typic use for train nns sgd variant on small minibatch second-ord method on larg minibatch natur gradient method
what the minimum depth need for a neural network to be a univers approxim 2. one hidden layer and one output layer.
which of smooth and rectifi nonlinear general give better imag classif per anc when use in nns rectifi nonlinear
what'r the advantag of relu nonlinear over smooth one in nns smooth nonlinear tend to satur amp; discard in ation, hinder both forward amp; backpropag backpropag with relus is piecewis linear chang in paramet in a linear chang in the network iti until the point it caus some relu to chang piec
what'r the problem with relu nonlinear compar to smooth nonlinear they can't learn via gradient method on exampl where their ation is zero
what an altern to relu nonlinear that doesn't suffer the no-train problem maxout unit each piec of the maxout respons has it own weight vector, so it can alway learn
what are the main type of difficulti with gradient optim of nns local minima saddl point plateaus/vanish gradient cliffs/explod gradient
how do gradient-bas optim for nns usual deal with explod gradient by limit the size of the step that can be taken
whi are the gradient at the output of a deep nn typic veri larg or veri small becaus the jacobian of each layer is $dw$, where $w$ is the weight matrix and $d$ are the deriv of the nonlinear success layer are like to have similar jacobians, so by the final layer some eigenvector will be far larger than other
as a rule of thumb, to avoid overfit how mani exampl are need per paramet in a model ten exampl per paramet
what an energy-bas model in ml a mrf whose overal probabl distribut can be written as $p(x) = \frac{1}{z}\exp(-e(x))$
whi is the choic of domain from which valu are drawn so import when analyz mrfs becaus it can determin whether the partit function exists. example: a mrf with potenti $\exp(b_ix_i)$ is well-defin when $x_i \{0,1\}$, but not when $x_i \mbb{r}$
what harmoni in ml anoth name for the negat energi
what doe `d-separ stand for depend separ
what a harmonium in ml anoth name for an rbm
what are the two categori of autoencod undercomplete, where the bottleneck is enforc through architectur overcomplete, where the bottleneck is enforc through regular
what a contr e autoencod one where $\left \frac{ h}{ x} \right ^2$ is penalis which encourag onli a few hidden unit to take part in encod local chang to $x$
in term of it derivative, what a contr e function one where $ f^\prime(x) 1$ everywher
what are denois autoencod strong relat to contr e autoencod
what a stochast autoencod an autoencod where the variabl are assum to be taken from crds: $h q(h x)$ is the encod distribut $x p(x h)$ is the decod distribut
what greedi unsupervis pre-train for nns given the output of a network train a one-lay autoencod to reproduc the output of the network then stack the autoencod on top of the exist network and repeat
what are the two most popular interpret of the effect of unsupervis pre-train as a of paramet initi as a regular strategi
whi is pre-train call pre-train becaus it expect that onc the pretrain is done and the full network is construct the network will be fine-tun for a specif problem use tradit supervis approach
what concept drift when gradual chang in the input distribut occur over time
what zero shot learn a kind of transfer learn where structur learnt on one domain $x$ is `map onto anoth domain $y$ and use to solv task for which no train data has been provid
what multi-mod learn a transfer learn variant where the represent of relat domain $x, y$ are learnt, as well as the map between those concept
what a featur map in the context of a convolut the output of the convolut
what the of the matrix correspond to a 2d convolut a doubly- circul matrix
when are function $f, g$ equivari when $f \circ g = g \circ f$
what the equivari properti of convolut it equivari wrt translat
what the prior convolut enforc on the function learnt by a layer the function must be local and equivari wrt small translat
what are the three typic stage in a convolut layer convolut detect pool
what the detect stage in a convolut layer the applic of an ation function
what are some typic pool function in nns max averag distance-from-cent weight averag l2 norm
what the prior that a pool layer enforc on the function the layer learn the function should be invari wrt the tran ation the input to the pool layer repres
how can pool be use to induc invari to some general tran ation by pool over separately-parameter convolut ie pool over convolut that respond to differ rotat of an object would induc invari wrt rotat
how is pool usual combin with downsampl if the pool is over a width $k$ window, then often onli $n_\text{previous}/k$ pool node are need
how are pool layer usual use to accomod vari input size by fix the number of pool nodes, irrespect of how larg the previous layer was
what the main advantag of convolut over correl convolut is commut this is much more use in theori work than in practice, where correl is often call convolut
what the stride of a convolut the distanc between the center of the recept field of neighbour neuron
what a same convoluton ad enough zero to each edg of the input to prevent convolut from shrink it
what a valid convolut one that doesn't zero-pad the input, caus the output to shrink
what a full convolut where enough zero are ad to each edg of the input that each pixel will appear in each posit within the kernel
between valid, same and full convolutions, which amount of pad usual per s best some amount of pad between valid and same
when do local connect layer have an advantag over convolut layer when the featur to be learnt is local, but has no reason to occur across the whole input space
what a tile convolut layer one where the kernel use in the convolut cycl as you move through the input
when are tile convolut layer usual use when you need a compromis between straight convolut and a locally-connect layer
what three oper need to be implement for a convolut layer in a nn convolut backprop from output to weight backprop from output to input
whi do convolut layer sometim allow the bias to vari across the imag becaus it allow the model to account for differ in imag statist across the imag for example, detector at the edg receiv less total input so should get larger bias
what a seper kernel a kernel which can be written as the outer product of some lower-dimension kernel
what the tradit approach to manifold learn construct a nearest neighbour graph from the sampl and use each neighbourhood to approxim the tangent plane at that point on the manifold
what the problem with the tradit approach to manifold learn if the manifold has strong curvature, you need a lot of exampl to cover it well and if you don't, you can't reason about part of the manifold that haven't been cover
when do manifold in ml tend to have high curvatur when tran ation that we consid trivial signific chang mani coordin ie translat of an imag
what the relat between autoencod and manifold learn an autoencod effect tri to learn to be veri sensit to chang along the manifold, which minim reconstruct error but insensit to chang orthogon to the manifold, which minim the regular penalti
what the probabilist interpret of the loss function of an autoencod the loss function should repres the negat log-likelihood of the reconstruction, $l(x) = -\ln p(x (\text{dec}\circ \text{enc})(x))$
how are denois autoencod implement by corrupt the input, pass it to an autoencoder, and ask it to reconstruct the uncorrupt input
what the vector field of interest associ with a denois autoencod $(\text{dec} \circ \text{enc})(x) - x$ which approxim $\frac{ \ln q(x)}{ x}$, where $q$ is the data generat distribut
how can denois autoencod be use to approxim the data generat distribut by turn them into a markov chain: corrupt the current step, then encode/decod it to get the next step.
what the practic advantag of use relu unit over tradit nonlinear train is much faster
where are lrn layer most common use follow detector layer like relus
whi do tradit nn nonlinear requir that their input be normal becaus if it isn't, the nonlinear are like to satur
in nns, what the advantag to have the neighbourhood for a pool layer overlap it can help reduc overfit
how are variable-s imag input typic adapt to fixed-s nns by scale along one axi then crop along the other
when enlarg a nn train set by tran ing natur input, what are the usual tran ation extract patch from the imag mirror them horizont find the principl rgb compon across the set and add small random multipl of them to the imag
what the idea behind the rgb-pca imag corrupt techniqu object ident is invari to chang in intens and color
what happen in nn dropout suppress the output of each neuron with probabl $ 0.5$ dure training, ignor it dure both the forward and backward pass then at test time, multipli all neuron output by $0.5$
what the purpos of use nn dropout it prevent neuron from reli on the presenc of other neurons, forc more robust featur to be learnt
what the effect of dropout on train time rough doubl it
what the use in have differ channel in a convolut layer of a cnn differ channel can learn differ function of the input
how should nn layer use relus be initi with posit bias which ensur that that relu neuron will learn from the begin
when work with documents, what doe $y_{il}$ usual denot the ident of the $l$th word in document $i$
when work with document in ml, what doe $v$ denot the size of the vocabulari
when work with document in ml, what doe $n_{iv}$ usual denot the number of time word $v$ appear in document $i$
when work with document in ml, what doe $l_i$ usual denot the length of document $i$
when work with document in ml, what doe $r$ typic denot the number of channels/respons
in a mixtur model for documents, what'r topic the latent cluster-identif variabl $q_i$
what latent semant analysi appli pca to a token-count design matrix
what the model use in exponenti famili pca a discret model of the $y_{il} \text{cat}(\mc{s}(wz_i)), z_i \mc{n}(\mu, \sigma)$ or similar for multinomi or poisson distribut
what the usual use of epca techniqu simpl altern to mixtur model for model document
what epca stand for in ml exponenti famili pca
what the model use in multinomi pca $n_i \pi_i \text{mu}(l_i, b\pi_i)$ $\pi_i \text{dir}(\alpha)$ where $b$ is a left-stochast matrix
what the model use in latent dirichlet alloc $y_{il} q_{il} = k, b_k \text{cat}(b_k)$ $q_{il} \text{cat}(\pi_i)$ $\pi_i \text{dir}(\alpha 1_k)$ $b_k \text{dir}(\gamma 1_v)$ where $q_{il}$ is the topic for word $y_{il}$
what the main advantag of lda over latent semant analysi the latent quantiti $\pi_{ik}$ are nonneg and sum to 1, make them much easier to interpret
what the distribut use in the gap model $n_{iv} z^+_i \text{poi}(b_v \cdot z_i^+)$ $z^+_{ik} \text{ga}(\alpha_k, \beta_k)$
what the use of the gap model to model document whose length is not known
what doe mpca stand for in ml multinomi pca
what prior is often put on the matrix $b$ in lda $\bar b_k \text{dir}(\gamma1_v)$
what the topic interpret of lda everi word $l$ in document $i$ is assign a topic $q_{il} \{1, \dotsc, k\}$ and these topic are drawn from a document-specif distribut $\pi_i$
what the geometr interpret of lda document are repres by a convex combin of topic so lda project the $v$-simplex of document count vector onto the $k$-simplex of topic
how can lda be use to infer the mean of a word by fit an lda model then find the topic $k$ which maxim $p(q_{il} = k y_i, \theta)$ the topic distribut $\pi_i$ that has been chosen for document $i$ help disambigu $q_{il}$
what label lda vanilla lda has a problem with unidentifi which label lda fix by forc the topic to correspond to ground truth tag
what the perplex of two distribut $\text{perplexity}(p,q) = 2^{\mbb{h}(p,q)}$
what the cross entropi between two stochast process $h(p,q) = \lim_{n\rightarrow fty} -\frac{1}{n}\sum_{y_{1:n}} p(y_{1:n})\ln q(y_{1:n})$
what the perplex of $p$ against the empir distribut $\text{perplexity}(p_\text{emp}, q) = \exp\left( - \frac{1}{n} \sum \ln q(y_i y_{1:i-1}) \right)$
what the perplex of a uni ly random distribut $u$ against the empir distribut $\text{perplexity}(p_\text{emp}, u) = v$ where $v$ is the number of token
what the intuit interpret of the perplex it the probabilistically-weight averag `branch factor of the model predict distribut if some symbol are (correctly) predict to be more like than others, the perplex drop
what the rang of the perplex vs the empir distribut $h(p_\text{emp}) \text{perplexity}(p_\text{emp}, q) v$
which variabl are usual collaps in a gibb sampler for lda everyth but the topic $q_{il}$
how is the per anc of languag model usual evalu perplex vs the empir distribut
how is a latent dirichlet analysi model usual fit collaps gibb sampl variat em
how is the number of topic $k$ in a lda model usual determin by model select techniques; anneal import sampl cross valid variat lower bound on the evid
what the correl topic model lda but with $\pi_i z_i = \mc{s}(z_i)$ $z_i \mc{n}(\mu, \sigma)$
what the differ between the correl topic model and categor pca ctm: $y_{il} \text{cat}(b\mc{s}(z_i))$ catpca: $y_{il} \text{cat}(\mc{s}(wz_i))$
what the advantag of the correl topic model over lda lda has troubl with topic which are strong correl like the `busi and `financ topic
how can you visual a correl topic model by sparsifi the precis matrix $\sigma^{-1}$ and interpret it as a gaussian graphic model
what the model use in a dynam topic model the same as a lda but with the weight at time $t$ be $b^t_k b^{t-1}_k \mc{n}(b^t_{k-1}, \sigma^2 1_v)$ $y^t_{il} q^t_{il} = k, b^t_k \text{cat}(\mc{s}(b^t_k))$
what the use of a dynam topic model it allow the distribut of word in each topic to evolv over time
what the structur use in lda-hmm for each document, creat a hmm model with state $z_{il} \{1, \dotsc, c\}$ to repres syntax and a lda model to repres semant if $z_{il} = 0$ then generat word $y_{il}$ use the lda els generat it from the hmm
what the advantag of the lda-hmm over lda it help prevent the lda from be pollut by stop word and can help distinguish when a word is be use syntact or semant
what the model use in supervis lda lda with the class label generat by $c_i \bar{q}_i \text{ber}(\text{sigm}(w^t\bar{q}_i))$ where $\bar{q}_{ik} = \frac{1}{l_i} \sum q_{ilk}$ is the empir topic distribut for document $i$
what the problem that supervis lda aim to solv predict class label $c_i$ for document via their topic distribut as in sentiment analysi
what the model use in discrimin lda same as lda except the topic prior is label-dependent: $q_{il} c_i = c, \pi_i, a_c \text{cat}(a_c \pi)$
what the differ between discrimin and supervis lda they solv the same problem but one is generat while the other is discrimin
what a condit topic random field similar to lda but with the topic variabl $q_{il}$ drawn from a condit mrf
which model doe a condit topic random field extend a correl topic model
what the model use in dirichlet multinomi regress lda similar to lda but with $\pi_i \text{dir}(\text{exp}(wx_i))$
what the use of dirichlet multinomi regress lda it allow the topic distribut for a document to depend on some tag input $x_i$
what the model use in discrimin categor pca $y_{il} z_i \text{cat}(\mc{s}(wz_i))$ $z_i x_i \mc{n}(vx_i, \sigma)$ ie a nn with one hidden layer and categor output
what the model use in the stochat model $r_{ij} q_i = a, q_j = b \text{ber}(\eta_{ab})$ $q_i \text{cat}(\pi)$
in lvms for graph-structur data, what do $q_i$ denot the latent variabl (like a ) associ with node $i$
in lvms for graph-structur data, what do $r_{ij}$ denot whether edg $ij$ exist
what the differ between the cluster induc by a stochast model and tradit communiti cluster in a sbm are ed of node that connect to similar nodes, rather than from node which connect to eachoth
what the use of stochast model they'r generat model of -structur graph
what the intuit purpos of a mixed-membership stochast model it a sbm with the membership variabl $q_i$ replac by $\pi_i s_k$ allow differ compon of a node behaviour to be explain by differ s
what the model use in a mixed-membership stochast model $r_{ij} q_{ij}=a, q_{ji}=b \text{ber}(\eta_{ab})$ $q_{ij} \text{cat}(\pi_i)$ $\pi_i \text{dir}(\alpha)$
what the model use in the relat topic model it similar to supervis lda but with $r_{ij} \bar{q}_i, \bar{q}_j \text{ber}(\text{sigm}(w^t(\bar{q}_i \otim \bar{q}_j) + w_0))$ where $\bar{q}_i$ is the empir topic distribut of document $i$
what the empir topic distribut of a document $\bar{q}_{ik} = \frac{1}{l_i}\sum q_{ilk}$
what the use of the relat topic model if each node in a graph has a document associ with it the rtm creat edg between document that share topic
what the advantag of the relat topic model over fit an lda then appli logist regress to the empir topic distribut rtm learn a latent space that encourag to be predict of the graph structur
what the model use in the infinit relat model give each item $i$ of type $t$ a latent variabl $q^t_i \{1, \dotsc, k_t\}$. then item $i_1$ through $i_m$ a $m$-ari relat iff $r_{i_1:i_m} q^{t_1}_{i_1}, \dotsc, q^{t_m}_{i_m} \text{ber}(\eta_{i_1:i_m})$ $q^{t}_{i} \text{dp}(\alpha_t, h_t)$
intuitively, what an onotolog an organis of knowledg
how an infinit relat model compar to a stochast model it general a sbm from dyadic relat to $m$-adic relat
how can an infinit relat model be use to learn an ontolog $r \colon t^\text{concepts} t^\text{concepts} t^\text{binari predicates} \rightarrow \{0,1\}$ describ when a predic appli to a pair of concepts, and can be generat by an irm
how doe the infinit relat model relat to biclust if the featur are binary, then $r \colon t^\text{items} t^\text{features} t^\text{clusters} \rightarrow \{0, 1\}$ can be generat by an irm
what an irm in ml infinit relat model
what the collabor filter problem given a spars matrix $r$ where $r_{ij}$ is the rate user $i$ gave to movi $j$ predict the blank entri of this matrix
what the model use in probabilist matrix factor approxim $r = uv$ as $r_{ij} u_i, v_j \mc{n}(u_i \cdot \bar{v}_j, \sigma^2)$ $u_i \mc{n}(\mu_u, \sigma_u)$ $\bar{v}_j \mc{n}(\mu_v, \sigma_v)$
what pmf in ml probabl mass function probabilist matrix factor
what was the most success techniqu use in the netflix rate challeng model rate as a gaussian variabl and appli probabilist matrix factor
how doe probabilist matrix factor relat to svd pmf is equival to svd if the data is complet
what an impress log in ml it side-in ation about the set of option ed to a user
what a restrict boltzmann machin an rbm with it node arrang into a hidden and a visibl layer with no intralay connect
what the product-of-expert interpret of an rbm the potenti function on each edg enforc an expert constraint and take their product creat `sharp distribut which predict data that satisfi the constraint
what a localist encod in ml one where a singl hidden variabl is use to generat the respons vector
what a distribut encod in ml one where mani hidden variabl are use to generat the respons vector
what the advantag of rbms over two-lay dgms the hidden variabl are condit independ given the visibl variabl so the posterior of their state factor
what the problem with rbms compar to two-lay dgms rbms are much harder to train
what are the most common kind of rbm almost all have binari hidden variabl and one of binari visibl variabl categor visibl variabl gaussian visibl variabl
what the energi function for a binary-vari rbm $e(v, h) = -\langl v, wh \rangle$
what the posterior distribut for the hidden variabl of a binari rbm a product of bernouilli with mean $\mbb{e}[h v] = \text{sigm}(w^tv)$
in an binary-vari rbm, what are the generat and recognit weight $w$ are the generat weight $w^t$ are the recognit weight
what the generat distribut for the visibl variabl of a binari rbm given the hidden variabl a product of bernouilli with mean $\mbb{e}[v h] = \text{sigm}(wh)$
intuitively, when doe a hidden node $h_k$ in a binari rbm ate when the input $v$ look like $\bar{w}_k$
what happen if you give an rbm gaussian visibl unit \emph{and} hidden unit you get a factor analysi model.
what the gradient of the log likelihood for rbms $\nabla_w l = \mbb{e}_{p_\text{emp}}[vh^t] - \mbb{e}_p[vh^t]$
what'r the clamp and unclamp phase in the gradient of an rbm $\mbb{e}_{p_\text{emp}}[vh^t]$ is the clamp phase, sinc $v$ is held to the empir distribut $\mbb{e}_p[vh^t]$ is the unclamp phase, sinc $v$ is free
how is gibb sampl conduct for an rbm initi the visibl node to a data vector then repeat altern between sampl $p(h v)$ and $p(v h)$
what the contrast diverg algorithm it sgd for rbms but with the unclamp phase of the gradient approxim use $k$ gibb sampl steps, for some small $k$
what persist cd a variant of contrast diverg that resembl sml for maxent model and improv the per anc of cd gibb sampl
what cd in ml contrast diverg
what the replic softmax model it a categor rbm for a bag-of-word document model
what the problem with deep direct network infer is intract becaus the hidden node are correl due to explain away
what the problem with deep boltzmann machin the partit function mean train them is hard
what a deep belief network a layer network which is direct in the bottom layer but undirect in the top layer
how should the partit of undirected/direct layer be interpret in a deebn undirect layer an associ memori direct layer generat the output
what a complementari prior a complimentari prior for a likelihood is one such that the posterior is fulli factor posterior
what the connect between rbms and deebn if you have a deebn with layer $v, h_1, h_2$ with weight $w_1, w_1^t$ then $h_1, v$ are distribut as a rbm
what the connect between complimentari prior and deebn if you have a deebn with layer $v, h_1, h_2$ with weight $w_1, w_1^t$ then $h_1, v$ are distribut as a rbm, which impli $p(h_1 v)$ is fulli factor so the undirect network between $h_1, h_2$ s a complimentari prior for the direct network $v, h_1$
what the greedi layerwis algorithm for deebn given a visibl layer $v_1$, fit a one-hidden-lay rbm $(v_1, h_1)$ with weight $w_1$ approxim $v_2 = \mbb{e}[h_1 v_1, w_1]$ convert $(v_1, h_1)$ to a direct network and repeat the process with $v_2$
what the up-down procedur have construct a deebn use greedi layerwis training, fine-tun it by do an upward sampl pass do a cd updat to the top-level rbm paramet do a downward ancestr sampl pass do a gradient-desc updat to the direct network paramet
what the problem with the up-down procedur it veri slow
what backfit in ml anoth name for the up-down procedur
what the main advantag of probabilist model compar to dnns probabilist model are (usually) abl to do top-down inference, which is use when tri to interpret the signal
what generat pre-train initi a dnn by train it as an auto-encod
what the use effect of generat pre-train it like a data-induc regular and help the backpropag fine-tun stage find minima with good general properti
what semant hash train a deep auto-encod with a binari code layer then interpret the code layer as a hash for the input so semant similar item will have similar hashes, and a hash-tabl can be precomput for fast use at search time
what one of the problem with pool layer they throw away a lot of in ation, make top-down pass difficult
what the signal-to-symbol divid the problem of convert a signal to a symbol represent that can be manipul and composit
defin boost (ml) an ensembl ml method that build the next classifi on the residu of the er therebi (in theory) gradual increas accuraci by highlight more and more subtl interact for reduc bias in supervis machin learn http://stats.stackexchange.com/questions/19224/combining-machine-learning-model
what lemmat in nlp the tran ation of a word to some base or canon
what the differ between lemmat and stem in nlp a lemmat usual has some context about how a word was used, allow it to discrimin between sens
what a sens in nlp a way in which a word can be interpret
what chunk in nlp break text up into phrase ex: recognis `the wall street journal as a singl noun phrase
what shallow pars in nlp anoth name for chunk
what semant role label in nlp identifi the predic of a sentanc and their argument ex: given `mari sold the book to john', identifi `to sell as the predicate, `mari as the seller, `the book as the goods, and `john as the buyer.
what named-ent recognit the nlp problem of extract identifi correspond to people, organizations, etc from text
what an unfold recurs autoencod the input (like a sentence) is structur as a binari tree the encod for each node is creat from the encod of it children and the object is to reconstruct the content of the tree use onli it encod root
what the shortcut for run a cell and jump to the next cell in ipynb shift-ent
what the shortcut for run a cell and keep the cursor where it is in ipynb ctrl-enter
what the shortcut for run a cell and creat a new cell below in ipynb alt-ent
how do you interrupt a comput in ipynb \ {i} twice in command mode
how do you switch from edit to command mode in ipynb esc ctrl-m
how do you select the previous amp; next cell in ipynb \ {k, j} in command mode
how do you move a cell up or down in ipynb \ {ctrl-k, ctrl-j} in command mode
how do you creat a cell abov or below in ipynb \ {a,b} in command mode
how do you merg two cell in ipynb \ {ctrl-m} on the top cell while in command mode
how do you split a cell in ipynb \ {ctrl-shift--} while in edit mode
how do you copi a cell in ipynb \ {c} while in command mode
how do you past a cell abov or below the current cell in ipynb \ {shift-v, v}
how do you get a tooltip in ipynb \ {shift-tab}
how do you indent or unind in ipynb \ {cmd-], cmd-[}
how do you e output scroll in ipynb \ {shift-o}
how do you restart the kernel in ipynb \ {0} twice in command mode
what are the two way to initi beautifulsoup \ {beautifulsoup(html\_str)} \ {beautifulsoup(file\_handle)}
what are the four type of object most common found in bs pars tree tag navigablestr beautifulsoup comment
given a tag object in bs, how do you get the kind of tag it is \ {tag.name}
given a tag object in bs, how do you get the valu of one of it attribut \ {tag['attrname']}
how do you modifi the attribut of a tag in bs by treat the tag object as a dictionari
how doe bs repres multi-valu attribut as list of string
how do you get the text content of a tag in bs \ {tag.string}
how do you chang the text content of a bs tag \ {tag.string.replace\_with(new\_str)}
how do you get a python string from a bs navigablestr \ {unicode(str)}
whi should you alway appli \ {unicode} to the text content of a bs tag to a python string befor export it to anoth part of the program becaus otherwis the \ {navigablestring} will carri a refer to the entir pars tree out with it
how the \ {beautifulsoup} object act in bs like a tag, but without a name or attribut
how can you access the first child of a given name in bs \ {t.tag\_name}
how do you get a list of all the children of a tag in bs \ {tag.contents}
how do you get an enumer over the children of a tag in bs \ {tag.children}
how do you enumer all the descend of a tag in bs \ {tag.descendents}
what \ {tag.string} give you in bs if \ {tag} has a singl child which is a navig string, it give you that string if \ {tag} has a singl child which has a \ {.string}, it give you that string els it give you \ {none}
how do \ {\_\_repr\_\_} and \ {\_\_str\_\_} compar in python \ {\_\_repr\_\_} is meant to unambigu describ an object \ {\_\_str\_\_} is meant to be readabl
when is \ {\_\_repr\_\_} use to print an object in python \ { r} atter \ {repr(obj)} a contain \ {\_\_str\_\_} use the contain object \ {\_\_repr\_\_}.
when should \ {\_\_repr\_\_} and \ {\_\_str\_\_} be implement alway implement \ {\_\_repr\_\_} implement \ {\_\_str\_\_} readabl would be use
what are the two way to enumer over all the text descend from a tag in bs \ {tag.strings} \ {tag.stripped\_strings}, which remov whitespac
how can you get the parent of a tag in bs \ {tag.parent}
how can you get the ancestor of a tag in bs \ {tag.parents}
how can you move to the next or previous sibl of a tag in bs \ {tag.next\_sibling}, \ {tag.previous\_sibling}
how can you iter over a tag next or previous sibl \ {tag.next\_siblings} \ {tag.previous\_siblings}
how can you move to the in-ord successor of a tag in bs \ {tag.next\_element}
how can you iter over a tag in-ord successor in bs \ {tag.next\_elements}
how do you get \ {find\_all} to match against an exact string in bs \ {t.find\_all('str')}
how do you get \ {find\_all} to match against a regex in bs \ {find\_all(re.compile(pattern))}
how do you get \ {find\_all} to match against one of sever string in bs \ {find\_all([str1, str2, str3])}
how do you get \ {find\_all} to match against everyth in bs \ {find\_all(true)}
how do you get \ {find\_all} to match against arbitrari criteria in bs \ {find\_all(f)} where \ {f} is a function that take a tag and return a boolean
how do you find all tag with a specif attribut in bs \ {find\_all(attr\_name=val)}
how do you find all tag with a specif set of attribut in bs \ {find\_all(attrs=\{attr\_name\_1: val\_1, attr\_name\_2: val\_2, attr\_name\_3: val\_3\})}
how do you find all tag with a specif class in bs \ {find\_all(class\_=class\_name)} (sinc \ {class} is a reserv token in python)
how can you search for text match a filter in bs \ {find\_all(text=filter\_name)}
how can you find the first $k$ element match a criteria in bs \ {find\_all(*args, limit=k)}
how can you search over onli the direct children of a tag in bs \ {t.find\_all(*args, recursive=false)}
what a shortcut for call \ {t.find\_all(*args)} in bs \ {t(*args)}
what are the other method relat to \ {find\_all} in bs variant for find parent previous amp; next sibl in order predecessor amp; successor
how can you find just the first tag match a filter in bs \ {find(*args)}
how do you search the descend of a bs tag use a css selector \ {t.select("css\_selector\_string")}
how can you get beautifulsoup to pars onli some part of a document use the \ {soupstrainer} class
what an \ {item} in scrapi a contain for scrape in ation which work like a dict but with explicitly-declar entri
how do you creat a new scrapi \ {item} class deriv from \ {scrapy.item} creat some \ {scrapy.field()} member with the name of the field you want
how do you creat a scrapi \ {spider} class deriv from \ {scrapy.spider} and defin \ {name} \ {start\_urls} \ {parse()}
what the job of the \ {parse()} method in scrapi to consum a \ {response} object and return an enumer of \ {item} contain scrape in ation \ {request} contain new page to scrape
how do you run a scrapi project from the termin in the scrapi project directory, \ {scrapi crawl spider\_name}
what are the two object scrapi use to repres interact with a websit \ {request}, which repres a websit to be crawl \ {response}, which repres the content of a crawl websit
what the standard approach to extract part of scrape webpag in scrapi use the \ {selector} object obtain via \ {response.selector}
what are the basic method of a scrapi \ {selector} \ {xpath()} \ {css()} \ {re()} \ {extract()}
what do the \ {selector.css(), selector.xpath()} method return in scrapi a \ {selectorlist}, which has mani of the method of \ {selector} but broadcast them over it content
what doe \ {selector.extract()} do in scrapi return a unicod string of the materi repres by the \ {selector}
what the best way to experi with selector in scrapi \ {scrapi shell "url"} the \ {response} object will be in the \ {response} variabl
what a use shortcut for select part of a scrapi \ {response} use css amp; xpath \ {response.css()} call \ {response.selector.css()} \ {response.xpath()} call \ {response.selector.xpath()}
what are the two way that item produc by a scrapi spider are output use a feed export that store them direct as json or similar use a pipelin to refin them first
what the definit of a pipelin in scrapi a class with a \ {process\_item(item, spider)} method that either return anoth \ {item} for further process or rais a \ {dropitem} except
what the use of a \ {scrapy.cfg} file it sit in the project root directori and point to the python modul which defin the project set
how do you creat an empti scrapi project \ {scrapi startproject projectname}
how can you instanti a spider from a templat in scrapi use the \ {scrapi genspider} command
what command line tool are use in test how a scrapi project will treat a specif webpag \ {scrapi fetch} will print a url content as scrapi see it \ {scrapi view} will show a url in the browser as scrapi see it \ {scrapi parse} will print the thing pars from a url
how can you run a scrapi spider without creat a project \ {scrapi runspider}
how can you defin metadata for a \ {field} in a scrapi \ {item} creat the field use \ {scrapy.field(metadata\_name=metadata\_val)}
how do you copi the metadata of a scrapi \ {field} into a new \ {field} use the field copi constructor: \ {scrapy.field(item.fields['oldfield'])}
how can you get a \ {field} object from a scrapi \ {item} rather than just it valu \ {item.fields['fieldname']}
how can you pass argument to a scrapi spider run from the command line give the initi keyword arg \ {scrapi crawl spidernam -a param\_name=param\_val}
how do you limit the domain that a scrapi spider can crawl defin the \ {allowed\_domains} member
how can you creat a request for a specif spider to crawl a url in scrapi \ {spider.make\_requests\_from\_url(url)}
how can you log a messag from a scrapi spider \ {spider.log(message)}
what are some of the standard spider avail in scrapi \ {crawlspider} \ {xmlfeedspider} \ {csvfeedspider} \ {sitemapspider}
how doe scrapi \ {crawlspider} work it has a set of \ {rule}s. each \ {rule} take a \ {linkextractor} that return a list of \ {request} and a \ {callback} that will be call on the correspond \ {response}
how is the default \ {itemloader} use in scrapi creat an \ {itemloader} with a given \ {item} and \ {response} make extract call of the \ {loader.add\_xpath('fieldname', xpath)} (or similar for css) get the item by call \ {loader.load\_item()}
what happen if a scrapi default \ {itemloader} load the same field twice the of the second \ {add\_xpath} (or similar) call is append to the first
how doe a scrapi \ {itemloader} work intern call of the \ {loader.add\_css('fieldname', val)} call the input processor for \ {fieldname} and store the intern call of the \ {loader.load\_item()} call the output processor for \ {fieldname} and combin the s into a valu to store in the \ {item}
how is \ {itemloader} subclass in scrapi defin a \ {fieldname\_in} attribut contain the input processor for \ {field\_name} defin a \ {fieldname\_out} attribut contain the output processor for \ {field\_name} defin \ {default\_input\_processor}, \ {default\_output\_processor} for field that don't have processor defin
how can you defin an \ {itemloader} at the same time you defin an \ {item} in scrapi \ {scrapy.field(default=val1, input\_processor=val2, output\_processor=val3)}
what an item loader context in scrapi if a processor take a \ {loader\_context} argument then the item loader will automat pass a dict that the processor can use to communic with eachoth
how can you manual analyz a respons from within a scrapi script \ {scrapy.shell.inspect\_response(response)}
how do you defin which pipelin compon a scrapi project use in the set file, defin \ {item\_pipelin = \{'component\_1': int\_1, component\_2': int\_2\}} where \ {int\_1, int\_2} are (typically) in $[0, 1000]$ and the flow is low-to-high
how do you use the default feed export in scrapi defin the set \ {feed\_format='json'} or similar \ {feed\_uri='string\_with\_ atting\_params'}
what'r the at paramet that can be use with scrapi \ {feed\_uri} set \ { (time)s} will be replac by the time the feed is creat \ { (name)s} will be replac by the spider name \ { (attr\_name)s} will be replac by \ {spider.attr\_name} valu
how do scrapi feed export and item export compar feed export are the easi way to store scrape data item export allow advanc custom
what the definit of a \ {linkextractor} in scrapi an object with a \ {extract\_links()} method that take a respons and return a list of \ {link}
how do you use the log in scrapi start the servic with \ {scrapy.log.start()} log with \ {scrapy.log.msg("message", level=log.warning)} or altern with a spider onboard \ {spider.log()} method
what are contract in scrapi a way to valid the behaviour of a spider \ {parse()} method against a specif url that defin entir in the method docstr
what an easi way to valid that the calcul of some $f$ and $f^\prime$ is correct in python use \ {scipy.optimize.check\_grad}
what a fundament problem with debug ml algorithm ml algorithm are design to deal with noisi data so often they can return use s even while buggi
when implement ml algorithms, what high-level divis should be maintain clear separ the distribut the model the optim algorithm
when implement a ml model, what a good way to test a condit probabl distribut check that $\frac{p(i \theta)}{p(x \theta)} = \frac{p(y, \theta)}{p(x, \theta)}$
when implement ml algorithms, when should probabl be use and when should log probabl be use alway use log probabilities; use \ {lse} for addit
what a lipschitz continu function a $f$ such that there exist a $k$ such that $d(f(x), f(y)) k\cdot d(x,y)$ for all $x, y$
what the sampl complex for learn a lipschitz continu function in $\mbb{r}^d$ $\omega(1/\varepsilon^d)$ where $\varepsilon$ is the excess risk
what the sampl complex for learn a linear function in $\mbb{r}^d$ with a lipschitz continu loss $\omega(d/\varepsilon^2)$
how do the penalti in a deep contr e network improv per anc they approxim a loss term $\frac{ y}{ x}$ encourag the network to be insensit to small chang in the input which increas the minim magnitud of adversari distort
what are the penalti use in a deep contr e network $\lambda_i \left \frac{ h_i}{ h_{i-1}}\right _2$ for each layer $i$
what the group theori intuit for what greedi layerwis learn doe each neuron (approximately) learn to identifi some `figur in the input space. so for images, the first layer learn figur in $\mbb{r}^2$; the second layer learn figures-of-figures, etc. and sinc a neuron stop learn as soon as it find a stabl figure, it learn figur with small orbit and larg stabil
when are default argument for a function evalu in python at definit time! *not* when the function is call
when are the valu of variabl use in a python closur look up when the inner function is call
what the lipschitz constant of a function the $k$ such that $d(f(x), f(y)) k \cdot d(x, y)$ if it exist
how is pca written in term of matric write $\sigma = u\lambda u^t$ then $w_\text{pca} = \lambda^{-\frac{1}{2}}u^t$
how is zca written in term of matric write $\sigma = u\lambda u^t$ then $w_\text{zca} = u \lambda^{-\frac{1}{2}}u^t$
intuitively, what doe zca do it whiten the data while keep it as close as possibl to the origin data
what doe zca stand for in ml zero-phas compon analysi
what the advantag of zca over pca for imag analysi on natur images, pca compon tend to be global zca compon tend to be local
what an extrem learn machin a nn with one veri larg hidden layer where the input weight are random initi and then fix and the output neuron are linear in the hidden layer
what the algorithm probabl of an object $x$ given model $\{s_i\}_i$ which use a tm $m$ to describ $x$ as $s_i(x)$, $p_m(x) = \sum 2^{- s_i(x) }$
what the use of algorithm probabl it a neat theoret construct that allow the definit of a `univers prior
what a common mistak when use perplex to evalu a languag model report improv in term of percentag over a baselin sinc a constant improv over a vari baselin lead to a vari improv in entropi
what the word error rate in nlp $\text{wer} = \frac{d_l(\hat y, y)}{n}$ $d_l(\hat y, y)$ is the word-token levehnstein distanc of a predict string from the actual string $n$ is the number of word in the actual string
what the problem with the word-error rate as an evalu metric it over-emphas unin ativ word and error which preserv sens
what a toeplitz matrix one with constant diagon
what a cach languag model it cach the last few hundr word and s a model from them then interpol it with a static model train on a much larger corpus this help deal with `bursti
what are the seven main class of languag model n-gram cach class-bas structur decis tree maxent neural network
what the use of vocabulari shortlist in nlp it a better-p ing altern to vocab truncat for minim the size of a nn output layer
what'r vocabulari shortlist in nlp to keep complex down, an advanc model can be appli to predict the probabl of word in a shortlist of most common word while an $n$-gram model is use for the rest
what a nnlm in ml neural network languag model
what output factor in nlp group word and use one set of output to predict the group and anoth set to predict which word from the group to output
when use output factorization, how are word most common assign to group by unigram frequenc bin such that each group has rough equal probabl of occur
what a dynam nn one which is train as it process the test data
what the typic size of the vocabulari of an english corpus in nlp hundr of thousand of word
which of vocabulari reduction, shortlist and factor output tend to work best factor output is best shortlist are better than vocab reduct
when train a neural network use both in-domain and out-of-domain data, which should be fed through first the out of domain data, follow by the in-domain stuff
what in-domain and out-of-domain train data in-domain data is specif to the problem out-of-domain data concern other, relat problem
how can you sort a larg nlp train set into chunks, go from furthest-out-of-domain to most-in-domain chunk it random train a simpl $n$-gram model on each chunk and evalu it perplex on the test set sort the chunk in order of decreas perplex
what increment learn train a nn to identifi simpl concept first, then progress more complex one
what a hash-bas languag model one where rather than keep the entir tensor map $n$-gram to outputs, the input are hash to an output this keep memori down
whi are collis okay in hash-bas maxent model becaus dure training, of the input that are map to a singl output, the most common input will domin the way the output is train
what the advantag to ad a maxent model to a rnn lm the maxent model can learn simpl pattern and so save the rnn a lot of hidden variabl
what the high-level algorithm in the paragraph-vector framework learn a vector for each paragraph in the corpus such that the paragraph vector concaten with a the vector for a few consecut word can effect predict the next word
what the point in learn a vector for each paragraph in the paragraph-vector framework the paragraph vector provid context that let the predictor disambigu what word should come next
in the paragraph vector framework, what a good number of word to use when predict the next word 5 to 12
what the differ between memoiz and dp memoiz is top-down and usual vulner to stack overflow dp is bottom-up but take a lot more thought to implement
what a rnn encoder-decod a pair of rnns, the encod take a sourc sequenc (like an english phrase) to a vector represent and the decod take the vector to a target sequenc (like a french phrase)
if a nn put out a vector, when can the vector be safe interpret probabilist when the nn has been train to emul a probabl distribut (like use it to predict the next word in a sentence)
what the differ between \ {@staticmethod} and \ {@classmethod} in python a method decor with \ {@classmethod} is pass the class as it first paramet a method decor with \ {@staticmethod} isn't pass anyth as it first paramet
what the main use of \ {@classmethod} in python \ {@classmethod} can call other method on the class so they work well with inherit
how should you open a file in python \ {with open(filename) as f:}
what \ {\_\_slots\_\_} in python it can be assign a collect of string that will be use as attribut name for the class assign to it suppress the generat of the usual \ {\_\_dict\_\_} that hold the attribut of most python object
what the function differ between old- and new- class in python some magic name have been chang old- use a dfs (kinda) for method resolut in a base class hierarchy. new use a bfs (kinda)
what \ {\_\_mro\_\_} in python a magic name for view the method resolut order for a class which list the order in which the base class hierarchi will be search
what a new- class definit vs an old- one in python new : \ {class classname(object):} old : \ {class classname:}
what the most common use of \ {\_\_slots\_\_} in python \ {\_\_slots\_\_ = ()} effect declar the class is stateless
what \ {eval()} in python a function that interpret a string as a code express and return the
what \ {raw\_input()} in python \ {raw\_input(str)} will print \ {str} to the console, then wait for a new line to be entered, which it return
what \ {exec()} in python a statement which interpret a string as code and execut it local and global variabl can be suppli for use in the execut
how can you index into a python dictionari which use tupl as key \ {dictname[first, second]}, where the actual key is \ {(first, second)}
what \ {\_\_new\_\_} in python a magic name which is call when an object is created, and is respons for return an instanc of the object
how can you set the valu of a programmatically-chosen attribut in python \ {setattr(obj, attrname, value)}
what are the sibl of the \ {setattr()} keyword in python \ {getattr()} \ {delattr()} \ {hasattr()}
how do you creat a local variabl with a programmatically-gener name in python \ {locals()[varname] = value}
when nest lambda in python, how can you make debug them easier give the lambda names: \ {lamb.\_\_name\_\_ = lambdaname}
what \ {\_\_class\_\_} in python the magic name on an object of a new- class that use by \ {type} to get the object class
what the magic name behind \ {bool} in python \ {\_\_nonzero\_\_()}
what'r the magic name for the comparison oper in python preferenti \ {\_\_lt\_\_}, \ {\_\_ge\_\_}, \ {\_\_eq\_\_}, etc but will default to \ {\_\_cmp\_\_} if those aren't avail
what the magic name for an object hash code in python \ {\_\_hash\_\_()}
how can you get the uniqu ident of an object in python \ {id(obj)}
how can you store state in a python function such that it can be access call-to-cal \ {def f(): f.storenam = val}
what the best way to implement a singleton in python store the singleton state in an attribut on a class or function
what the magic method behind the \ {isinstance} keyword in python the class method \ {\_\_instancecheck\_\_}
what a chainmap in python it a dictionari which offer a composit view of a set of back dictionari
what \ {\_\_dict\_\_} in python the magic attribut which hold all the attribut of an object
what the differ between a python function start with one underscor and one start with two one underscor indic the function should be treat as privat two underscor will have it name mangl by the classnam
how is the bridg pattern usual implement by pass an instanc from the implementor hierarchi to the constructor of a class from the abstract hierarchi
how do you make an object iter in python implement the \ {\_\_iter\_\_()} method and have it return an iter object
what doe \ {iter()} do in python iter protocol: \ {iter(o)} will call \ {o.\_\_iter\_\_()} if it exists, els sequenc protocol: \ {iter(o)} will call \ {o.\_\_getitem\_\_(i)} with increas integ \ {i}, els iteration: \ {iter(o, sentinel)} will call \ {o.next()} until it return someth equal to \ {sentinel}
how are iter object implement in python \ {o.\_\_iter\_\_()} should return \ {o} \ {o.next()} should return the next item, or rais \ {stopiteration}
how should \ {\_\_getitem\_\_()} be implement in python \ {\_\_getitem\_\_(key)} should rais \ {typeerror} if the type of \ {key} is wrong for sequenc types, rais \ {indexerror} if the key is outsid the valid rang for map types, rais \ {keyerror} if the key is miss else, return the request item
what the differ between \ {dict.iteritems()} and \ {dict.items()} in python 2.7 \ {items()} return a list of tupl \ {iteritems()} return a generat of tupl
how can you get a view onto a dictionari in python 2.7 \ {dict.viewitems()}
what an easi way to implement all the comparison oper for a class in python implement ($=, $) appli \ {@functools.total\_ordering} to the class
how can you preserv the docstr and name of the wrap function when write your own decor in python decor the wrapper with \ {@functools.wraps(wrapped)}
how do you write a parameter decor in python write a function which take some argument and return anoth function that'll take the function-to-be-wrap and return the wrap function
given getter, setter and delet method in python, what are the two way of creat a properti decor them with \ {@property, @propname.setter, @propname.deleter} use \ {propnam = property(getter, setter, deleter)}
what an eleg way to use class decor to do field valid in python creat an \ {ensure} class that can hold a collect of valid rule defin field in the class as \ {fieldnam = ensure(validation\_rule)} have a class decor \ {@do\_ensure} which take each \ {ensure} field and replac it with a properti that test the associ rule
when can python class decor be use instead of subclass when the `inherit method and data aren't modifi by the subclass
what'r bound and unbound method in python a bound method is a method referenc through an object instance, so it \ {self} paramet has been `fill in an unbound method is a method referenc through the class instance, so it \ {self} paramet is still free
what the flyweight pattern a way to reduc memori use by deduplication; multipl object have pointer to the same state
when should a \ {\_\_attrname} be use rather than a \ {\_attrname} in python when you don't want subclass to overrid the attribut
how can you alter what happen when attribut on a python object are got amp; set by overrid \ {\_\_getattr\_\_} and \ {\_\_setattr\_\_}.
what the most conveni way to get a list of the attribut of a python object \ {dir(obj)} although the list might not be rigor or complet
what the best way to get the next item from a python iter \ {next(iterator[, default])} where \ {default} is the valu to return if the iter is exhaust
what the differ between an iter and generat in python an iter is anyth with method \ {\_\_iter\_\_()} and \ {next()} a generat is the of a call to a function contain a \ {yield} expression. the is an iterator.
what send syntax in python if you have a generat \ {g} which has yield a valu then \ {g.send(x)} will caus the \ {yield} express insid the generat to return with valu \ {x}
how can you creat a python generat when you'r onli concern about \ {send}' valu into them, not about get yield valu use the express \ {sent\_valu = (yield)}
what the easiest way to implement coroutin in python use generat
what the chain of respons pattern each object in the chain take an option successor and after it done with an event, pass the event to it successor
how can you make an object callabl in python implement \ {\_\_call\_\_()}
how can you test if an object can be call in python \ {callable(obj)}
what'r the easiest way to creat anonym object in python \ {type('', (), \{\})} \ {object()}
what a default dictionari in python it a dictionari which you suppli a \ {defaultfactory} to and everi time a miss key is requested, that key is ad to the dictionari with a valu specifi by the \ {defaultfactory}
what'r the compon of the mediat pattern \ {mediator} defin the interfac for communic between \ {colleague}
what the use of the mediat pattern it decoupl communic object from eachoth
how do you implement coroutin use generat in python creat a generat \ {g} call \ {next(g)} to advanc it to it first \ {yield} statement \ {g} is now readi to be use as a coroutine; send item into the coroutin use \ {g.send(obj)}
what the idea in the memento pattern the \ {caretaker} request an opaqu \ {memento} from the \ {originator} which store the \ {originator} state when the \ {caretaker} want to revert the \ {originator} to it previous state, it hand back the \ {memento}
how can you test whether a string is a valid python name \ {s.isidentifier()}
what the idea in the state pattern a \ {context} object deleg request to it to it \ {state} properti depend on what \ {concretestate} is inhabit the \ {state} property, differ behaviour will be obtain
what the use of the state pattern it replac condit logic in everi state-depend method with condit logic onli in the state-chang properti
what the easiest way to make anoth process to do some work in python creat a \ {process = multiprocessing.process(f, args)} then \ {process.start()}
how can you ensur that process creat by \ {multiprocessing} in python will be shut down when the parent process is by set \ {process.daemon = true} on ani process you creat
how do you use a \ {joinablequeue} as a work queue in python popul the joinabl queue with job pass it to some other processes, which will take job from it then call \ {queue.task\_done()} in the origin process, call \ {queue.join()} to wait for the queue to be empti
how do you usual retriev s from process you creat in python by pass a \ {multiprocessing.queue} as an argument when you creat the process and have the other process store it s there
how do you use futur to do multiprocess concurr in python within a \ {with processpoolexecutor() as executor:} creat futur use \ {futur = executor.submit(f, args)} and store them in some collection, \ {futures.add(future)} then outsid the , consum the futur as they complet use \ {for futur in as\_completed(futures):} and extract s use \ {future. ()}
where do error thrown dure the execut of a python futur emerg if ani occured, they'll be return by \ {future.exception()}
what penal problem do aic and bic correspond to solv $\min_\theta l(\theta) + \lambda \theta _0$ where $\lambda$ depend in a complex manner on the criterion
when is solv $\min y-wx _2^2$ s.t. $ x _0 k$ tractable, and how is it solv when $w$ is orthonorm and it solv by hard-threshold $w^ty$
what are three common alize of $l_1$ optim $\min y - wx _2^2 + \lambda x _1$ $\min y - wx _2^2$ s.t. $ x _1 \alpha$ $\min x _1$ s.t. $ y - wx _2^2 \epsilon$
what tikhonov regular anoth name for $l_2$ regular
what the idea in structur sparsiti it an extens of group sparsiti where the group are allow to intersect
what the idea in hierarch sparsiti it an extens of the idea of group sparsiti that, given a tree of variables, encourag root subtre to zero out
how is the idea of a spars solut usual adapt to matrix-valu unknown by ask for low-rank matrix solut (equiv to $l_0$ norm) which can be approxim by low-trac solut (equiv to $l_1$ norm)
what'r two other name for the trace of a matrix nuclear norm schatten norm
without use sparsiti assumptions, approxim how mani sampl $n$ are need to achiev a denois error of $\epsilon$ $n \frac{p}{m} \cdot \frac{\sigma^2}{\epsilon^2}$ where $p$ is the number of atom $m$ is the number of signal dimens $\sigma$ is the standard deviat of the nois $\epsilon$ is the $l_2$ residu
when use sparsiti assumpt and a dictionari satisfi the rip, approxim how mani sampl are need to achiev a denois error of $\epsilon$ $n \frac{k \ln p}{m} \cdot \frac{\sigma^2}{\epsilon^2}$ where $k$ is the number of nonzero in the truth $p$ is the number of atom $m$ is the number of signal dimens $\sigma$ is the standard deviat of the nois $\epsilon$ is the $l_2$ residu
what the restrict isometri properti $w$ satisfi the rip for signal with $k$-nonzero-el represent iff all $k k$ submatric of $w^tw$ have all their eigenvalu close to 1
what the relev of the rip to spars code if a dictionari satisfi the rip for $k$-spars signals, then the lasso algorithm behav well and recov the true support for $k$-spars signal
what rip stand for in spars code restrict isometri properti
what the main problem with the rip there no known way of test whether a matrix satisfi it in polytim
given a spars denois problem where the dictionari satisfi the rip, what valu of $\lambda$ are you general recommend to use by the theori $\lambda \sigma \sqrt{m \ln p}$ where $\sigma$ is the standard deviat of the nois $m$ is the number of signal dimens $p$ is the number of atom
geometrically, what the simplest way of deal with sign-flip unidentifi in dictionari learn by project the signal and the atom onto the posit half-spher ie by consid them as point in the project space $\mbb{p}^{m-1}$
what is a good frequentist method to compar correl coeffici whi is it necessari fisher z-tran ation; the varianc of r grow smaller as ρ get closer to 1, and this provid a variance-stabil tran ation so that they can be correct compar from wikipedia
what the \ {ord} function in python it get the integ correspond to an ascii char
in seaborn, how do you temporarili select a plot \ {with axes\_ ('whitegrid'):}
in seaborn, how do you perman set the plot \ {sns.set\_ ('white')}
what are the main s in seaborn \ {darkgrid} (default) \ {whitegrid} (for large, y data elements) \ {ticks} (for when valu are less important) \ {dark, white} (for when valu aren't important)
what the usual way to control the posit of a plot axe border in seaborn \ {sns.despine}
how can you overrid part of a seaborn by pass a dictionari into the \ {rc} paramet of the \ {set\_ } or \ {axis\_ } method
what the divis of aesthet custom in seaborn \ {set\_ } handl style \ {set\_context} handl scale
how do you rescal compon temporarili in seaborn \ {with plotting\_context():}
how do you reset to the default paramet in seaborn \ {sns.set()}
how can you set the scale of compon perman in seaborn \ {set\_context()}
how do you set the color palett temporarili in seaborn \ {with color\_palette():}
how can you set the color palett perman in seaborn \ {set\_palette()}
how can you examin a color palett in seaborn \ {palplot(list\_of\_rgb\_tuples)}
what doe \ {color\_palette()} return in seaborn a list of rgb tupl
how can you draw a fix number of qualit color in seaborn \ {color\_palette('husl', 8)} for 8 color
what are the two modif to colorbrew palett that can be appli in seaborn \ {bugn\_r} is the revers version \ {bugn\_d} is the dark version
what are the two way to creat a sequenti color palett from a singl color in seaborn \ {light\_palette('blue')} \ {dark\_palette('blue')}
how do you make a custom diverg palett in seaborn \ {diverging\_palette(hue\_angle\_1, hue\_angle\_2)}
what a \ {jointplot} in seaborn a hexbin plot with histogram on the top and right edg
what a \ {rugplot} in seaborn a plot of 1d data, with each datapoint repres as a short stick
what seaborn \ {distplot} a way to combin kde plots, rug plot and histogram
how can you adapt a violin plot in seaborn to show how point chang over sever time interv pass \ {join\_rm=true} will use overlay a line plot to show how point move between each violin
what the basic way to use \ {lmplot} in seaborn \ {lmplot('x\_colname', y\_colname', dataframe)}
what the basic tool for visual linear relationship in seaborn \ {lmplot()}
when plot a discret axi use seaborn \ {lmplot}, what should you do use the \ {x\_jitter} or \ {y\_jitter} argument to spread the point out
what the easiest way to plot multipl dataset in differ color on a seaborn \ {lmplot} pass \ {hue='colname'} to \ {lmplot}. each uniqu valu in \ {colname} will be assign a differ color
what the easiest way to plot multipl dataset in differ subplot on a seaborn \ {lmplot} pass \ {col='colname'} or \ {row='rowname'} to \ {lmplot} each uniqu valu in \ {colname} will be assign it own subplot
how do you disabl the linear regress in seaborn \ {lmplot} pass the \ {fit\_reg=false} argument
what the version of \ {lmplot} that can be pass a subplot to be drawn on in seaborn \ {regplot}
what the version of seaborn \ {lmplot} that can be pass array data rather than a datafram \ {regplot}
what seaborn variant of \ {lmplot} for 3d data \ {interactplot}
what the easiest way to plot correl in seaborn \ {corrplot()}
how do you disabl the signific test in ation in seaborn \ {corrplot} pass the argument \ {sig\_stars=false}
what the simplest way to use seaborn \ {factorplot} \ {factorplot('x\_col', y\_col', hue\_col', dataframe)} where \ {x\_col} and \ {hue\_col} are categor and \ {y\_col} is real
what are the three argument to \ {kind} in seaborn \ {factorplot} \ {point}, which plot a point-and-errorbar estim of each categori and connect hue categori into a line \ {bar}, which plot bar for each hue categori side-by-sid \ {box}, which plot box for each hue categori side-by-sid
how can you use \ {factorplot} to show the count in each categori in seaborn by omit the \ {y} variabl
what'r the version of seaborn \ {factorplot} that can be plot onto a subplot \ {pointplot} and \ {barplot}
what'r the version of seaborn \ {factorplot} that'll accept array data \ {pointplot} and \ {barplot}
how do you easili creat a heatmap in seaborn \ {heatmap(dataframe)}
how do you easili plot a clustermap in seaborn \ {clustermap(dataframe)}
how do you use matplotlib \ {subplot2grid} \ {ax = subplot2grid((n\_rows, n\_cols), (top, left), colspan=width, rowspan=height)}
how do you use \ {gridspec} in matplotlib to creat arbitrari axe \ {gs = gridspec(n\_rows, n\_cols)} \ {ax = plt.subplot(gs[row\_range, col\_range])}
how do you fine-tun the layout of axe creat with a matplotlib \ {gridspec} \ {gs.update()}
how do you creat a \ {gridspec} with non-squar cell in matplotlib \ {gridspec(n\_rows, n\_cols, width\_ratios=arr\_1, height\_ratios=arr\_2)}
what the best way to plot time seri in seaborn \ {tsplot(ys)}
which class implement seaborn support for trelli plot \ {facetgrid}
what the simplest way to use seaborn \ {facetgrid} \ {fg = facetgrid(dataframe, row='row\_name', col='col\_name', hue='hue\_name')} \ {fg.map(f, values\_name\_1', values\_name\_2', ...)} where \ {f} take some data and keyword arg and draw a plot on the e axe
how do you set the size of the figur when use seaborn \ {facetplot} pass the argument \ {size=height, aspect=ratio}
how can you wrap the column variabl down row when use seaborn \ {facetgrid} pass the argument \ {col\_wrap=width}
how do you alter the order of item when use seaborn \ {facetgrid} pass the argument \ {col\_order, hue\_order, row\_order}
how do you set option on everi subplot generat by a seaborn \ {facetgrid} \ {fg.set(xticks=list)}, and similar
how do you get a list of the axe generat by a seaborn \ {facetgrid} \ {fg.axes.flat}
which seaborn class support scatter-matrix-esqu function \ {pairgrid}
what the differ between seaborn \ {facetgrid} and \ {pairgrid} \ {facetgrid} show differ facet of the relationship between a fix set of variabl \ {pairgrid} show a singl facet of of the relationship between mani differ set of variabl
what the simplest way to use \ {pairgrid} in seaborn \ {pg = pairgrid(dataframe)} \ {pg.map(f)} where \ {f} take array \ {x, y} and keyword arg and draw onto the current e axe
how do you make differ kind of plot on the diagon of a seaborn \ {pairgrid} \ {pg.map\_diag(f)} \ {pg.map\_offdiag(g)}
how can you plot multipl dataset in differ color when use a seaborn \ {pairgrid} creat the \ {pairgrid} with a \ {hue='hue\_name'} argument
how do you limit which variabl are use in a seaborn \ {pairgrid} pass the \ {vars=list\_of\_colnames} when you creat the \ {pairgrid}
how do you creat differ plot in the upper and lower triangl of a seaborn \ {pairgrid} \ {pg.map\_upper} \ {pg.map\_lower}
how do you specifi differ set of variabl for the x and y axe of a seaborn \ {pairgrid} pass the argument \ {x\_vars=list\_a, y\_vars=list\_b} when creat the \ {pairgrid}
which method implement scatter plot matric in seaborn \ {pairplot(dataframe)} though this is much less customiz than \ {pairgrid}
which seaborn class general \ {jointplot} \ {jointgrid}
how is seaborn \ {jointgrid} use most simpli \ {jg = jointgrid('x\_name', y\_name', dataframe)} \ {jg.map(bivar\_f, univar\_f, summarize\_f)} where \ {bivar\_f} deal with the joint data \ {univar\_f} deal with the margin on each axi \ {summarize\_f} return a summari statist string
what the more flexibl way of use seaborn \ {jointgrid} \ {jg = jointgrid('x\_name', y\_name', dataframe)} then \ {jg.plot\_marginals(f)} \ {jg.plot\_joint(g)} \ {jg.annotate(h)}
how do you scale the height of the margin plot when use seaborn \ {jointgrid} pass the \ {ratio=height\_ratio} argument when creat the \ {jointgrid} note the central plot is alway square!
how do you alter the space between the joint and margin plot when use seaborn \ {jointgrid} pass the \ {space=dist\_float} argument when creat the \ {jointgrid}
what the problem with normal mutual in ation for compar cluster the approxim $p(x) \frac{n_x}{n}$ is onli valid as $n \rightarrow fty$ for finit $n$, this lead to the nmi be inflat
what relat normal mutual in ation it nmi minus the expect nmi of the refer partit against a complet random partit
what xavier normal initi scheme initi layer $l$ with $\mc{n}\left(0, \frac{1}{n_l}\right)$ where $n_l$ is either $n_l = k^2c$, with $k$ the kernel size and $c$ the number of input channel $n_l = k^2d$, with $k$ the kernel size and $d$ the number of output channel
what the purpos of xavier normal initi scheme it attempt to keep the varianc of the signal and gradient constant across layer of differ size
what the varianc of a uni distribut on $[a,b]$ $\frac{1}{12}(b-a)^2$
what are the constraint that xavier normal initi tri to fulfil the feedforward constraint, $\text{var}(w^i) = \frac{1}{n_i}$ the backprop constraint, $\text{var}(w^i) = \frac{1}{n_{i+1}}$
what the problem with use relu nonlinear in a denois autoencod if they'r use in the output layer, then when a zero is mistaken reconstructed, the gradient generat by the error will be unabl to flow back
how can a relu layer be modifi to ensur that it mean ation is zero multipli half the unit by $-1$
how should the sparsiti penalti general be chosen when train relu network so that about 50 -80 of the neuron are zero'd on ani particular input
(distribut ) x is the the math sat score of one student chosen uni ly at random plus the verbal sat score of a second student chosen uni ly at random. close to normal - two independ distribut ad
(distribut ) y is the averag of the two score in part a. close to normal, averag of two normal distribut
(distribut ) z is the the math sat score of one student chosen uni ly at random plus the verbal sat score of the same student. close to normal - addit of two normal distribut variables.
(distribut ) w is the averag of the two score in part c. (math sat score of student chosen uni ly at random, plus verbal sat score of the same student). close to normal - addit of two normal distribut variables, then scale by a factor of 2.
(distribut ) 200 dice are rolled. x is the sum of the number on top of the first 100 dice plus the sum of the number on the bottom of the second 100 dice. close to normal - all 200 independ number have equal probabl
(distribut ) 200 dice are rolled. y is the averag of the number on top of the first 100 dice plus the averag of the number on the bottom of the second 100 dice. close to normal - all independ number have equal probabilities, then divid by 200.
(distribut ) 100 dice are rolled. z is the sum of the number on top of the dice plus the sum of the number on the bottom of the dice. normal distribut with averag 7 and normal distribut 0.
(distribut ) 100 dice are rolled. w is the averag of the number on top of the dice plus the averag of the number on the bottom of the dice. normal distribut - valu is alway 7.
solv for a median exampl ex. x0, x1,x2, x3. median is n/2 = 4/2 = x2. (round up one)
stat quartil exampl x0, x1, x2, x3, x4, x5, x6, x7, x8. q1 = (x1+x2)/2 q2 = x4 q3 = (x6+x7)/2
prob/c definit of 1st quartil of a smallest number such that at least 25% of the element of a are = x.
prob/c definit of 3rd quartil of a smallest number that at least 75% of the element of a are = x
prob/c definit of nth quartil of a smallest number x such that at least n% of the element of a are = x.
arithmet mean averag of x. 1/n * summat from i=1 to n of xi. also written as x with a line on top of it.
varianc measur "spread" - how spread out your data is. var of xi, ... , xn is 1/(n-1) * summ_i=1 to n[(xi - mean)^2]
standard deviat standard deviat = sqrt(variance)
excel ula for variance, standard deviat var.s, stdev.
inclus exclus rule a u b = a + b - a∩b p(a u b) = p(a) + p(b) - p(a u b)
de morgan law 
multipli and standard deviat and varianc ex. if we multipli a set of number by 5: standard deviat is multipli by 5. varianc is multipli by 5^2.
get at least 1 doubl in 3 roll d(d1 v d2 v d3 = 1-p(notd1 ^ notd2 ^ notd3) = 1- (5/6)(5/6)(5/6) = 1-125/216.
bay rule 
law of total probabl ex. probabl of test positive. s=use steroids, p = test posit p(t) = p(s)p(t s) + p(nots)p(t nots)
bay rule final (expanded) p(a b) = p(b a)p(a) / p(b a)p(a) + p(nota)p(b nota)
complement rule p(nota b) = 1 - p(a b) p(c t) =
random variabl a real number you get from a random experiment. ex. flip 10 fair coins. x = # of heads.
n choos k (n k) =n!/( k!(n-k)! ) note: you'll use this for get someth like 7 3s in 10 dice rolls. pr(x=7) is 10choose3 * (1/6)^10
binomi distribut *p^k (1-p) is the probabl for a singl sequence. remem that ani yes/no (two ) experi is a binomi distribution!
expect valu e[x] e[x] = summat of all possibl valu for x time their probabilities. apr(x=a) the expect valu often equal the average.
if x is bin(n,p) distributed, e[x] equal e[x] = np.
if x is bin(n,p) distributed, var(x) = var(x) = np(1-p)
10 coin flips. expect valu and varianc for either heads/tail e[x] = 5 (10*.5) var = 10 * 0.5(1-0.5) = 2.5
p(e or f) (conditional) = p(e) + p(f) - p(e and f)
addit rule (conditional): p(e or f) = p(e) + p(f) - p(e and f)
special addit rule, when e and f are mutual exclusive: p(e or f) = p(e) + p(e)
subtract rule (conditional): p(e) = 1-p(note)
multipl rule: p(e and f) = p(e f)p(f)
special multipl rule, when e and f are independent: p(e and f) = p(e)p(f)
bay theorem identifi events: a: patient has the disease. b: patient test posit p(a b) = p(a)p(b a)/ [p(a)p(b a) + p(nota)p(b a)]
make sure not to confus a probabl distribut tabl with a normal distribut tabl probabl tabl - show the prob that the random variabl x has the valu x. y 2 3 4 5 6 ... 12 pr(y=y) 1/36 2/36 3/36 4/36 ... 1/36 distribut tabl just show all the outcomes.
mean for random variabl μ is use for popul mean, while x with bar on top is use for data mean. μ = summat of xp(x) = e[x] = expect valu of x.
varianc of a random variabl x: it the expect squar distanc from the popul mean. σ^2 = summat of (x-μ)^2 p(x)
find standard deviat for 2 coin tosses: 0 heads, p(x) = .25, (0-1)^2 * .25 = .25 1 head p(x) = 0.5, (1-1)^2 * .25 = 0 2 head p(x) = .25, (2-1)^2 * .25 = .25 .25+.25 = .5 = σ^2
ad expect valu rule: e[ax + b] = ae[x] + b σ^2 (ax + b) = a^2 o^2 (x) e[x1 + x2] = e[x1] + e[x2]
mean for binomi distribut μ = np
varianc for binomail distribut σ^2 = np(1-p)
z-valu ula z = (x-μ) /σ rememb that σ is standard deviat
example: probabl of y=7 with 9 coin flip bin(9,0.5) = (9 choos 7 ) * 0.5^9 = (9*8)/2 * 0.5^9
when deal with iq values, add 0.5 to each iq valu we do this becaus iq valu have to be whole numbers, and we have to compens when an iq is less than a certain number.
iq exampl less than iq 120 iff x = 119, so use 119.5 as the x-valu
iq exampl between between 90 and 140 inclusive. - 89 values: 89.5, 140.5
iq exampl exact valu iq exact 100. use x-valu 99.5, 100.5. 0.5133-.4867 = 0.0266. *signific to know you have a low chanc of have an exact averag iq with our data from class.
varianc of an n-side die var(z) = (n^2 - 1)/12
bernouli system: p and 1-p are the onli s
p-hat = x/n p-hat -number of success x in the sampl divid by the sampl size n.
big p-hat = x/n big p-hat - the random variable. littl p-hat is the valu for a particular sample.
mean of p-hat equal the expect valu of p-hat. e[p-hat]
standard deviat of p-hat: σ(p-hat) = sqrt(p(1-p)/n) or sqrt(p(p-1))/ sqrt(n)
what is the standard deviat of p-hat sampl error measure. increas the sampl size by a factor of n decreas σ(p-hat) by a factor of sqrt(n)
iid mean independ and ident distribut - no subset of the variabl has ani impact on another.
s.d of x1 is call σ
varianc of a set xn = 1/n * var(x1)
s.d. of a set xn σ(xn) = σ(x1)/sqrt(n) = 1/sqrt(n) * sqrt(var(x1))
confid interv (μ - moe, μ + moe) μ = e[x]
margin of error (moe) = moe = z-valu * σ
varianc if sampl size is larg σ^2 = p(1-p) ~=~ p-hat(1-p-hat)
power of a test, condtion probabl of a true posit 1-condit probabl of a fals negat = 1-pr(test is negat someth is wrong)
group that get the placebo is the control group the test/experiment group get the real product.
what a parametr relu nonlinear $f(x) = x$ if $x 0$ $f(x) = ax$ if $x 0$ where $a$ is channel-specif
what relu-xavi initi initi layer $l$ with varianc $\frac{2}{(1+a^2)n_l}$ and zero bias where $n_l$ is either $n_l = k^2c$, with $k$ the kernel size and $c$ the number of input channel $n_l = k^2d$, with $k$ the kernel size and $d$ the number of output channel and $a$ is the prelu coefficient, if it be use
what the differ between xavier and relu-xavi initi relu-xavi take the nonlinear of the ation into account when set the varianc
which layer is dropout usual use in all but the last fully-connect layer
what a standard valu for the weight decay in deep nns 5e-4
what the purpos of name variabl in theano to help debugging; it not strict necessari
how can you print the graph behind a theano variabl \ {pp(x)} for brief output \ {debugprint(x)} for detail output
suppos you defin a theano express \ {z = x + y}. how do you compil it \ {function([x, y], z)}
suppos you defin a theano express \ {z = x + y}. how do you execut it use \ {eval} \ {z.eval(\{x: 16.3, y: 12.1\})}
how doe \ {eval} compar to \ {function} in theano \ {eval} isn't as flexibl but it doe cach the compil code and can be cleaner
what are the main group of type avail in theano \ {scalar} \ {vector} \ {matrix} \ {row} \ {col} \ {tensor3} \ {tensor4}
what are the prefix that can be use with theano type \ {b}: byte \ {w, i, l}: 16/32/64 bit integ \ {f, d}: floats/doubl \ {c}: complex
how do you creat a theano express that will evalu multipl output \ {f = function([a, b], [a+b, a-b])} then \ {f(1, 3)} will return \ {[4, 2]}
how can you creat a theano express with a default argument \ {f = function([a, param(b, default=val\_1)], a+b)} then \ {f} can be call as \ {f(5)} or \ {f(5, b=2)}
how can you set the paramet name when creat a theano express with an argument that has a default valu \ {f = function([a, param(b, default=val\_1, name='bb')], a+b)} then \ {f} can be call as \ {f(5, bb=2)}
how do you usual creat a share variabl in theano \ {shared(initial\_val)} it'll iter over the various \ {sharedvariable} subclass until it find one whose constructor will accept \ {initial\_val}
how can you get and set the valu of a share variabl in theano \ {sv.get\_value()} \ {sv.set\_value(new\_val)}
how can you creat a theano function that updat a share variabl \ {function(args, f, updates=updater)} where \ {updater} is either a dict which map share variabl to express repres their new valu an alist of \ {(shared\_var, new\_value\_expr)} pair
what the use of share variabl in theano they allow differ function to share state
what the use of the \ {givens} paramet of \ {function} in theano it allow arbitrari chunk of a ula to be replac with other chunk (as long as the new chunk is the same type and shape as the old one)
what danger about the \ {givens} paramet to \ {function} in theano the express that appear in \ {givens} should never be co-dependent, as the order of substitut is not defin
how are random variabl creat in theano by creat a \ {rs = randomstream} then \ {rs.uni (shape)} or similar
how can you caus multipl call to a theano function to use the same random number by pass the \ {no\_default\_updates=true} argument to \ {function}
what import to rememb about the way random number are use in theano a random variabl is drawn from at most onc dure the evalu of an express and if the variabl appear more than once, the valu will be reused!
what the use of a theano \ {randomstream} \ {state\_updates} attribut it keep a list of the variabl generat by the stream which is use for transfer random state between theano function
what an op node in theano a node repres a computation, which take some variabl and return other
what an appli node in theano a node in the comput graph which take an op and appli it to some variabl
what are the three kind of node in a theano graph variabl node op node appli node
where is deriv in ation held in a theano graph in the op node
whi will scalar frequent turn up as \ {inplacedimshuffle} or similar in theano graph becaus the scalar has been broadcast to anoth shape in order to be compat with some express
how can you creat a .png of a theano comput graph \ {pydotprint(x)}
how do you calcul the gradient of an express in theano \ {t.grad(y, x)} will get the gradient of \ {y} in term of {x}
what \ {t} usual an import alia for when work with theano \ {tensor}
how do you calcul the jacobian of a function in theano use \ {gradient.jacobian()}
what are \ {rop, lop} in theano \ {rop} is the r operator, $\frac{ f(x)}{ x}v$ \ {lop} is the l operator, $v\frac{ f(x)}{ x}$
what'r the advantag of the r and l oper over comput the hessian direct if you'r abl to use them, r and l are usual much faster
what a mode in theano a config set that describ how function should be compil
how can you set the mode in theano temporarili by pass the \ {mode} argument to \ {function}
how do you enabl debug mode in theano pass \ {mode='debugmode'} to \ {function}
what the differ between \ {pickle} and \ {cpickle} in python \ {cpickle} is much faster
when use \ {cpickle}, how can you ensur object are store effici by pass \ {protocol=cpickle.highest\_protocol} to \ {dump} when you write the file can make as much as an order of magnitud differ
when write python pickl that'll be transfer across oper systems, what should you alway do open the pickl file in binari mode, \ {'b'}
how do you custom which part of an object are save and load by python \ {pickle} by implement \ {\_\_getstate\_\_()}, \ {\_\_setstate\_\_()}
what'r the differ between \ {ifelse} and \ {switch} in theano \ {ifelse} is lazi and take a boolean \ {switch} is eager, take a tensor and appli elementwis
how do you implement loop in theano use \ {scan}
when use spars matrices, when should \ {csr} or \ {csc} be prefer use \ {csr} if there are more row than column
what do \ {csr} and \ {csc} stand for in scipi compress spars row and compress spars column ats, respect
when run theano code on the gpu, how can you suppress copi back to the host at the end of the comput wrap the in \ {gpu\_from\_host(x)} which will be optim away if \ {x} is alreadi on the gpu
which theano oper will be much faster on the gpu than the cpu matrix mult convolut elementwis oper
which theano oper will be no faster on the gpu than on the cpu index dimens shuffl reshap
which theano oper will be a littl slower on the gpu than on the cpu summat over row or column
whi should you usual prefer \ {matrix, vector, scalar} constructor over \ {dmatrix, dvector, dscalar} in theano the er will inherit their type from the \ {floatx} config set and so make code easier to adapt to gpu
how can you ensur that an often-access theano variabl is kept on the gpu by make it a \ {shared} variable, which are kept on the gpu by default
what the effect of pass \ {borrow=true} to various method in theano it request a refer be use rather than a copi though there are circumst - such as on the gpu - when it'll be ignored.
what doe it mean when you pass \ {borrow=true} to one of theano \ {in} or \ {out} variabl that theano is allow to overwrit than input/output
what the effect of pass \ {return\_internal\_type=true} to various theano method unlik \ {borrow=true}, it will \emph{always} return a refer to the variabl but the return type might chang depend on circumstance!
what a common problem with shape comput in theano theano can frequent carri out optim that end up elid check that dimens are compat this can be avoid by test code in \ {debugmode} or pass certain optim flag
what the best mode to set theano compil in for iter develop \ {fast\_compile}
how can you print a debug statement from insid a theano function \ {printing.print('message')(variable)} return a wrap variabl which can be use as an ordinari variabl might disabl various optim however!
how can you step through a compil theano function by use \ {monitormode}, which allow you to set callback that will be pass the input and output of everi node
what config set should alway be use with theano \ {monitormode} \ {allow\_gc=false}, or the output to function might be collect befor the monitor get to it
how do you profil theano code by set \ {config.profile=true}
in the modifi bnf use in the python docs, what doe \ {"a"..."z"} stand for a choic of the charact `a through `z
in the modifi bnf use in the python docs, what doe \ { text } denot a comment
what the job of the lexic analyz in python to break a program into tokens, which will be fed to the parser
what are physic and logic line in python a physic line is some charact termin by an end-of-lin sequenc a logic line is one or more physic line connect by line-join rule
how do you usual declar the charact encod in python \ { -*- coding: utf-8 -*-} though this is emac standard and there are other way to do it, depend on the text editor
what are the two line-join rule in python if the line end with a backslash it'll be join to the next one if the line end within a parenthesized/bracketed/brac expression, it'll be join to the next one
how doe python lexic analyz deal with tab it replac them left-to-right with the minimum number of space need for the total number of charact to be a multipl of 8 (so if you'v got space-tab, the tab will be replac with 7 spaces) (but for your saniti you shouldn't mix space and tabs)
what doe python do when an invalid escap sequenc appear in a string liter it leav the sequenc where it is
how doe cpython deal with cyclic link garbag it'll notic it eventually, but it isn't guarante to collect it prompt this is whi you shouldn't reli on final for free resourc
what are the two built-in mutabl sequenc in python list byte array
what doe \ {b'string'} denot in python 2.7 the same as \ {'string'}; the \ {b} is onli signific in python 3, where it denot a bytestring. it includ in 2.7 for compat
in python, how do you get the modul a function is in \ {f.\_\_module\_\_}
in python, how do you get the default argument for a function object \ {f.\_\_defaults\_\_}
in python, how do you get the compil code for a function \ {f.\_\_code\_\_}
in python, how do you access the free variabl of a function object \ {f.\_\_closure\_\_}
what are the \ {func\_name} etc attribut on function object in python 2.7 they'r old version of \ {\_\_name\_\_} etc, which replac them in python 3 and have been backport to python 2.7
how do you get the function object under a method in python \ {m.\_\_func\_\_}
how do you get the \ {self} object of a bound method object in python \ {m.\_\_self\_\_}
what differ about the implement of unbound method in python 3 compar to python 2 in python 3, they'r just functions. unbound method don't exist.
in python, what a common problem with programmat access the member of a modul when the modul \ {m} pass out of scope, the \ {m.\_\_dict\_\_} will be clear even if the dict itself still has live refer
what a traceback in python when an except handler is entered, call \ {sys.exc\_traceback} will return an item that allow you to navig the stack that generat the except
when will \ {\_\_getattr\_\_} be call in python onli when the request attribut can't be found through the normal mechan
what \ {\_\_getattribute\_\_} in python an altern to \ {\_\_getattr\_\_} for new- class which will \emph{always} be call
what are the method of the descriptor protocol in python \ {o.\_\_get\_\_(self, obj, type=none)} \ {o.\_\_set\_\_(self, obj, value)} \ {o.\_\_delete\_\_(self, obj)}
what the usual use of descriptor in python to creat reusabl properti
how are descriptor use in python by creat a class \ {descriptor} which implement some part of the descriptor protocol then instanti it as a field \ {desc = desc()} at the class level of some object \ {o} when \ {o.desc} is called, the descriptor protocol will go into action
how can you use descriptor with unhash object in python have the descriptor initi take and store a label equal to it name in the owner class ie \ {x = desc('x')} then store ani state associ with the owner under the label, ie \ {state['label'] = val}
when use descriptor in python, how can you access other method on the descriptor by use the fact that when a descriptor attribut is access from the class level, the first argument to \ {\_\_get\_\_} is \ {none} in which case you can redirect to \ {self}
how do metaclass intercept object construct in python new- class are usual instanti by call \ {type.\_\_new\_\_()} if a class defin \ {\_\_metaclass\_\_}, \ {\_\_metaclass\_\_.\_\_new\_\_()} will be call instead
how can you intercept \ {issubclass} call when use an abc in python by defin a \ {\_\_subclasshook\_\_} method in the abc
when deriv from \ {dict} in python, how can you defin what should happen when a key is miss implement \ {\_\_missing\_\_}
what are the magic method behind the \ {in}/\ {not in} keyword in python \ {\_\_contains\_\_} or if it isn't defined, it'll tri to iter over the collect and see if ani element are equal
how doe scope work in python the scope of a variabl defin in a function extend to ani contain the scope of a variabl defin in a class is limit to the class
what are free variabl in python a variabl is free in a if it use there but not defin there
what a 's environ in python the set of nearest enclos scope for each variabl
if a name is defin at the modul level in python, what doe it becom automat a global variabl
what the differ between a name and a ing in python name can be reused; in differ scope they may be bound to differ valu
where is a name allow to be use in a python code anywher in the (name is differ to ing! the name can be use befor it bound, it'll just throw an error)
what the effect of \ {global x} in python it caus ani use of \ {x} within the same , or from where the statement can be seen, to resolv to the modul namespac the builtin namespac the namespac of the modul \ {\_\_builtin\_\_}
how do you modifi the builtin namespac in python import \ {\_\_builtin\_\_} and modifi it attribut do \emph{not} mess with \ {\_\_builtins\_\_}
what a generat express in python \ {(x for x in range(10))} and it creat a generat
what a set comprehens in python \ {\{x for x in range(10)\}} which return a set
what a string convers in python \ {`[x for x in range(10)]`} which will evalu the express between the backtick and (attempt) to return a string that can then be pass to \ {eval} or \ {exec} as a valid python express onli work for builtin type and builtin contain of builtin type
what the usual use of string convers in python it'll eat string contain `funni charact and return a string with them replac with escap sequenc that are safe to print
what the differ between a python generat and a coroutin coroutin can dictat where execut should continu after it yields; generat alway yield to the caller
what the \ {throw} method of a python generat do \ {throw(type)} will rais an except of the specifi \ {type} at the point where the generat was paused, and return the next valu yield by the generat
what the \ {close()} method of a python generat do it'll rais a \ {generatorexit} except at the point where the generat was paus if the generat then rais a \ {stopiteration} or \ {generatorexit} in response, it'll be swallow and \ {close} will return to the caller.
what the precend of the power oper \ {(**)} in python higher than unari oper on it left; lower than that of unari oper on it right which mean \ {2**-1} return \ {0.5}
what \ {//} in python 3 the floor divis operator, which will round down a float to an integ
how can you get the divisor and remaind of two number simultan in python \ {divmod(a,b)}
how are left and right shift defin in python as divis and multipl by power of 2
how are chain comparison evalu in python \ {a b == c = d} is equival to \ {a b and b == c and c = d} except that \ {a, b, c, d} are onli evalu onc
in python, what order are the argument to an assign evalu in right hand side then left hand side
how do you run a python modul as a script from the commandlin \ {python -m modulename}
how do you run pdb programmat in python \ {pdb.run('statement')}
how do you invok pdb as a script from the commandlin \ {python -m pdb scriptname.py}
how do you programmat set a point to load the debugg when use pdb pdb.set_trace()
how can you go into post-mortem debug for the most recent traceback use pdb \ {pdb.pm()}
how can you specifi the environ that a \ {pdb.run()} call should execut in \ {pdb.run(statement, globals, locals)}
use pdb, how can you debug a function without have to write it as a string statement \ {pdb.runcall(f, args)}
how can you go into post-mortem debug for ani traceback in pdb pdb.post_mortem(traceback)
how do you get the most recent traceback in python sys.traceback
how do you repeat the last command in pdb enter a blank line
how can you enter multipl command in a singl line in pdb seper them with ;; note this isn't smart - it'll break at ;; in the middl of quot strings!
how can you execut a python statement which share a name with a pdb command when work in pdb !command
how do you get help on a command in pdb h command
how can you print a stacktrac when work in pdb w for `where
how can you move up or down the stack in pdb u, d
how can you set a break at a certain line in the current file when work in pdb b 17
how can you set a break at a certain line in anoth file when work in pdb b other_file.py:17
how do you list the current breakpoint when work in pdb b
how can you set a new condit breakpoint when work in pdb commandlin b func, condit
how can you set a break for a specif function when work in pdb commandlin b func
how can you set a one-shot breakpoint when work in pdb commandlin tbreak 17 stand for `temporari break
how do you perman remov breakpoint when work from pdb commandlin use cl for `clear
how do you enabl or disabl a breakpoint when work from pdb commandlin enabl 3, disabl 3 to enable/dis breakpoint 3
what the ignor number of a breakpoint in pdb the number of time the breakpoint will be pass befor ate ie it'll ate when the ignor number is zero
how do you set the ignor number of a breakpoint when work from pdb commandlin ignor 3 7 will set breakpoint 3 ignor number to 7
how do you set a condit on an alreadi exist breakpoint when work in pdb commandlin condit 3 cond will set the conditon on breakpoint 3 to cond
how do you remov a condit from an alreadi exist breakpoint when work from pdb commandlin condit 3 will remov the condit from breakpoint 3
how do you set up some command to run everi time a specif breakpoint is hit when work from pdb commandlin command 3 print some_vari end will print some_vari everi time breakpoint 3 is hit
how do you move forward in pdb and stop at the first possibl occas s for `step
how do you move to the next line in the current function when work in pdb n for `next
when work in pdb, how can you run until you get to a strict greater linenumber, or until you exit the frame unt for `until
when work in pdb, how can you run until the current function return r for `return
when work in pdb, how can you run until the next breakpoint is reach c for `continu
when work in pdb, how can you jump forward or backward to a specifi line number jump 17 but this is onli possibl when you'r at the bottom of the stack
when work in pdb, how can you print the line around the current line l for `list
when work in pdb, how can you list the argument for the current function a for `arg
when work in pdb, what are the two command to print someth p , to print pp , to pretty-print
how do you creat an alia in pdb alia name command where command can contain %1, %2 to repres posit argument and %* repres all argument
when work in pdb, how do you remov an alia unalia
when use pdb to debug a script, how can you restart the program run
when use pdb to debug a script, how can you restart the program with new argument run arg
when work in pdb, how do you quit the debugg q for `quit
how can you put multipl statement on one line in python s1; s2
in a cprofil report, what doe 3/1 mean in the ncall column that the function recursed; it made three total call and one primat call
at a high level, how do you use the pstat modul profil some code and store the s in some file s creat p = pstats.stats( s') call various the method of p , then p.print_stats()
how can you get fine-tun control over the profil when use cprofil creat pr = cprofile.profile() call pr.enable() to start collect data call pr.disable() to stop collect data then call pr.print_stats() to view the s
how can you suppli local and global variabl to a profil command when use cprofil cprofile.runctx(command, globals, locals)
how can you get a list of all caller of the function profil in a cprofil stat object use stats.print_callers()
how can you get a list of all the function call by some function profil in a cprofil stat object use stats.print_callees()
what the differ between determinist and statist profil determinist profil monitor everyth statist profil sampl the instruct pointer
what the easiest way to programmat time small chunk of code in python timeit.timeit(command, number=num_of_repeats)
when use python timeit module, how can you initi the environ befor the time code is run pass a command via the setup paramet
what are the main section of a numpy- docstr one-lin summari extend summari of function paramet return rais note on implement exampl
in a numpy- docstring, how should paramet be at param_nam : type description, detail if necessari
in the descript of a numpy- docstring, how should programmat name be denot use backticks: `x`
what a repositori in git a collect of commit
what a commit in git a snapshot of the work tree
what the index in git where chang are regist befor do a commit also known as the stage area
what a work tree in git a directori which has a repository, along with all the contain subfold and file
how is the parent of a commit defin in git it the state of head when the commit is made
what a branch in git a name for a commit (which commit it name might change) also call a refer
what a tag in git a name for a commit which doesn't chang
what **master** in git the branch on which mainlin develop is done. typic default
what head in git it a name which defin what current check out
what head when a branch is check out in git it a symbol refer to the branch indic the branch should be updat after the next commit oper
what a detach head in git it what happen when you check out a specif commit rather than a branch the head then refer to that commit onli
what an i-nod in unix an index node; a datastructur use to repres an object in a filesystem contain most of the metadata about the associ object (like size and modif date)
what a blob in git how git repres a file content identifi by it hash
how can you get the hash of a file use git git hash-object filenam
when pass a hash to git, what a use time-sav idiom onli give the first 5-6 digit of the hash that all git will need to find the object most of the time
how can you get the type of a git object git cat-fil -t obj
how can you list the object in a tree in git git ls-tree object
how do tree relat to commit in git each commit hold a singl tree
how do tree relat to blob in git tree hold blob
when are blob creat in git when a novel file is ad to the index
what are the step that happen in the background when a git commit is made the current index is use to make a tree the tree is made into a commit the branch is updat to refer to the new commit
how doe garbag collect work in git ani commit not current refer to, and which isn't the child of a reachabl commit will be destroy
how can you refer to the parent of a commit in git name^
how can you refer to the second parent of a commit with multipl parent in git name^2
how can you refer to the 7th ancestor of a commit in git name~7
how can you refer to a file in a specif commit in git commitname:filepath
how can you refer to all commit reachabl from name1 up to name2 in git name1..name2 the rang is open on the right
how can you refer to all commit reachabl from the current head up to some commit name ..name
how can you get all the commit uniqu to two branch name1, name2 in git name1...name2
how can you refer to all commit in the past two week in git -since="2 week ago"
how can you refer to all commit match a regexp of the commit messag -grep=pattern
which way do the pointer run in a git commit histori from children to parent only!
what a merg commit in git a commit with more than one parent
how do you merg branch a, b in git so that a point to the merg commit git checkout a git merg b a will now point to the merg commit
what the base of two branch a, b in git the most recent commit they have in common
when should you rebas and when should you merg in git rebas when it a local branch with no other branch off it merg in all other case
if you'v got two git branch b, c with common parent a , how can you rebas so that a- b- c git checkout c git rebas b
what are the option when rebas inter eli in git \ {pick}, appli the commit to it (rewritten) parent \ {squash}, fold the commit into it parent \ {edit}, return to the shell with the work tree set to reflect progess up to amp; includ the commit \ {(drop)}, throw away the commit perman
how do you rebas inter eli in git rebas -i branchnam then when you drop out to edit, resum later with rebas --continu
when should you use reset in git almost never. use the stash amp; branch instead.
what the reflog in git a chronolog list of all the commit made in the past 30 day
what doe stash do in git creat a commit from the current work tree and add it to the stash
how do you appli the 5th oldest stash commit in git git stash appli stash@{5}
how can you list all the commit in the stash in git git stash list
what a good habit to get into at the end of each day with git stash your current work tree
what the train object of a skip-gram model to find word represent that are use for predict the surround word in a sentenc or document
what the usual problem with use a tradit logist regress for nlp problem the number of possibl word is huge, make the $o(m)$ calcul of it valu or gradient veri expens
what the hierarch softmax structur possibl output as leav of a tree and defin vector $w_i$ for each node or leaf $i$ in the tree then given an input $x$, the probabl of an output $y$ at the end of a path $l$ is $p(i x) = \prod_{i l} \text{sigm}(w_i \cdot x)$ where $v_x$ is the encod of $x$
what the advantag of the hierarch softmax calcul it valu or gradient need onli $o(\lg m)$ oper
how is the tree for hierarch softmax usual construct as a huffman tree
what negat sampl in nlp tri to maxim the train object $\text{sigm}(w_i \cdot x_i) \cdot \prod_{j=1}^k \text{sigm}(-w^\prime_j \cdot x_i)$ where $w$ is the correct output $w^\prime_j p$ are sampl from the nois distribut $p$
when implement negat sampl in nlp, what the usual nois distribut to use the unigram distribut with the frequenc taken to the power of $\frac{3}{4}$
what an easi way to deal with too-frequ word when train a languag model discard word $w_i$ with probabl $p(w_i) = 1 - \sqrt{t/f(w_i)}$ where $f(w_i)$ is the frequenc of word $i$ $t$ is a threshold of $ 10^{-5}$
how mani negat sampl are usual use when train languag model $2$ to $20$, with fewer need the larger the dataset is
what other techniqu doe negat sampl approxim in nlp nois contrast estimation, which use probabl to refin the approxim of the nois distribut
what a simpl way to teach a nlp model phrase score word pair as $\text{score}(w_i, w_j) = \frac{n_{ij} - \delta}{n_i n_j}$ where $\delta$ is a discount coeffici to suppress infrequ word and replac ani pair abov the threshold with a uniqu token then train the model on the revis dataset
what intern covari shift in neural network the way in which the input distribut of a layer chang dure train due to the weight of the previous layer shift
what a batch normal layer in nns it normal each input $x_i$ to the mean 0 and varianc 1 $\hat x_i$ with respect to the minibatch then output $\gamma \hat x_i + \beta$ where $\beta, \gamma$ are learnabl paramet
how are batch normal layer use dure infer the minibatch normal is replac with $\hat x = (x - e[x])/\sigma_x$ where the mean and varianc are estim from the popul
what step usual go hand-in-hand with ad batch normal layer to an exist network increas the learn rate acceler learn rate decay remov dropout reduc weight regular remov lrns reduc input jitter
what the johnson-lindenstrauss lemma given a set of $m$ point in $\mbb{r}^n$, if $n 8\ln m/\epsilon^2$ then there a linear embed of the point into $\mbb{r}^n$ which $\epsilon$-preserv the distanc between them
how can the covari be interpret in term of the multipl of the expect the covari is the amount by which the multiplicit of the expect fail
what a good distanc function for compar histogram the earth-mov distanc
what satur arithmet in opencv the way in which a float intens is taken to a (for example) 8-bit valu by round it then clamp to $0$-$255$
how do you appli a lookup tabl to an array in opencv lut(arr, lut)
how do you calcul an arbitrari linear tran ation of the color of an imag in opencv tran (src, tran _matrix)
how do you usual deal with imag boundari problem in opencv by use copymakebord to construct a mirrored/wrapped/etc border around an imag
what the idea in the bilater filter iter replac each pixel with a weight mean of it local but weight pixel with `similar intensities/color more than one with dissimilar intensities/color
what the use of the bilater filter edge-preserv nois reduct
what a box filter a rectangular filter of constant valu
what a pyramid in imag process a techniqu where the imag is repeat smooth then scale down
what dilat in imag process replac each pixel in a greyscal imag with the local maximum
what eros in imag process replac each pixel in a greyscal imag with the local minimum
what open in imag process an applic of erode-dilate, in that order so name becaus it'll break apart region which are thin connect
what close in imag process an applic of dilate-erode, in that order so name becaus it'll `close small holes.
what the morpholog gradient in imag process the dilat imag minus the erod imag
what the top hat oper in imag process the imag minus the open imag
what the black hat oper in imag process the close imag minus the sourc imag
what the use of the scharr kernel it the sobel kernel adapt to have rotat symmetri which make it much better for optic flow
what mat in imag process like ing, but real-valu for use with hair/fluff/transluc imag
what the mean-shift algorithm a way to find maxima of a densiti function the posit of each point is iter replac with a weight averag of the posit of the point in it neighbourhood until it converg
how is the meanshift algorithm usual appli to imag both color and posit over the color/spac neighbourhood are averag and the next iter will use the posit use by the last one as the center at the end, the averag color is map back to the origin pixel
how should geometr tran ation be appli to imag by calcul an invers tran ation from the destin pixel to sourc space first then interpol in the sourc space for non-exist locat this suppress certain kind of artifact that the naiv forward tran ation is prone to
how do you implement arbitrari geometr tran ation in opencv use remap
what the geometr constraint on a project tran ation it must preserv straight line
what are homogen coordin coordin of point in project space $(kx, ky, k) (x, y, 1)$, $k 0$
in homogen coordinates, what'r the convent point at infin $(x, y, 0)$
in project space, what special about the point at infin nothing.
how affin geometri defin in term of project geometri by select a distinguish line as the line at infin
geometrically, which project are affin the one that preserv the distinguish line
what the equat for a circl in homogen coordin a circl center on $(a, b, 1)$ is the set of point $(x, y, w)$ that satisfi $(x - aw)^2 + (y - bw)^2 = r^2w^2$
what are the circular point in 2d euclidean homogen coordin they'r the point $(1, i, 0)$ which everi circl pass through
how euclidean geometri defin in term of 2d project geometri by select a line at infin and two circular point on it
what the absolut conic it the equat $x^2 + y^2 + z^2 = 0, t = 0$ that ani sphere in 3d project geometri satisfi
how is 3d euclidean geometri defin in term of project geometri by select a plane at infin and an absolut conic
conventionally, how doe central project map $\mbb{p}^3$ to $\mbb{p}^2$ $(x, y, z, t) \rightarrow (x, y, z)$
what the of the camera matrix it a $3 4$ rank 3 matrix take $\mbb{p}^3$ to $\mbb{p}^2$
when are differ imag relat by a plane project tran ation when they share the same camera center when the space point are coplanar
what the iac in camera geometri imag of the absolut conic; the curv induc on the imag plane by the absolut conic
in term of camera geometry, when is a camera said to be calibr when the locat of the iac in an imag is known
without calibr in ation, what ambigu occur in the reconstruct of space point from point correspond in two imag the space point are onli uniqu up to a project tran ation sinc for a camera matrix $p$ and point $x$, $px = (ph^{-1})(hx)$ for ani project tran ation $h$
what a project reconstruct of a scene a reconstruct of the space point which is uniqu up to a project ambigu
in general, how mani point are need to creat a project reconstruct of a scene from point correspond between two imag seven
at a high level, what the fundament matrix of two camera the uniqu $3 3$ matrix $f$ of rank 2 such that for all pair of imag point $x, x^\prime$, $x^{\prime t} f x = 0$
at a high level, what the trifoc tensor of three camera it the $3 3 3$ tensor $\mc{t}^{jk}_i$ such that for point $x$ in one camera and line $l^\prime, l^{\prime\prime}$ in the other two $\mc{t}_i^{jk} x^i l^\prime_j l^{\prime\prime}_k = 0$
are vector covari or contravari contravariant.
in einstein notation, do upper indic indic the compon of covari or contravari vector contravari vector
in einstein notation, do lower indic indic the compon of covari or contravari vector covari vector
what'r the advantag of three-view reconstruct over two-view three view allow a mixtur of line and point to be use three view reconstruct are more stabl onli six correspond are need
given a point correspond in a three-view reconstruct problem, how do you find the line that can be use with the trifoc tensor ani line that pass through one of the point will do
what the intern constraint on the fundament matrix in camera geometri $\det f = 0$
when is it permiss to use an affin camera rather than a project camera when the distanc between the front and back of the scene is neglig compar to the distanc to the scene
what the transfer problem in camera geometri given the posit of a set of point in some imag find out where other point in one imag map to in the other
how are 2d line repres in homogen coordin $ax + by + c = 0$ becom $(a, b, c)$
in homogen coordinates, when doe a point $x$ lie on a line $l$ when $x \cdot l = 0$
in homogen coordinates, how can you find the intersect between two 2d line $x = l l^\prime$
in 2d homogen coordinates, how do you find the line through two point $l = x x^\prime$
what are the ideal point of a project space the point with final coordin $0$
what the canon line at infin in 2d homogen coordin $l_ fti = (0, 0, 1)$
what the dualiti principl for 2d project geometri ani theorem of 2d project geometri still hold when the role of line and point are exchang
what the equat of a conic in homogen coordin $\langl x, x \rangle_c = 0 $
how mani point defin a conic five (the matrix $c$ has six parameters, minus one dof for scale)
in homogen coordinates, what the tangent line to a conic $c$ at $x$ $l = x^tc^t$
algebraically, what the dual conic to $c$ it the adjoint $c^*$ for which everi tangent line $l$ to $c$ satisfi $l^t c^* l = 0$
what anoth name for the dual conic the conic envelop
in term of it matrix, when is a conic degener when $c$ is not of full rank
what'r three other name for project tran ation project homographi collin
algebraically, when is a map a homographi when there a matrix $h$ that implement it over homogen coordin
in term of $\mbb{r}^3$ rays, how can a project of $\mbb{p}^2$ be interpret as a linear tran ation of $\mbb{r}^3$
what a perspect a project that take one euclidean coordin system to anoth
under a project with point tran ation $x^\prime = hx$, how are line tran ed $l^\prime = l h^{-1}$
under a project with point tran ation $x^\prime = hx$, how are conic tran ed $c^\prime = (h^{-1})^t c h^{-1}$
what a good exampl to show that vector tran contravari suppos your tran ation is meter to centimeters, divid the scale by 100 all the vector compon are multipli by 100
when ing a basi out of vectors, are they usual stack horizont or vertic horizontally: $b = [b_1, b_2, \cdots, b_n]$
how is $pl(n)$ defin in term of $gl(n)$ it $gl(n)$ with matric relat by a scalar multipl identifi
in term of tran ation matrices, how is the affin group defin in term of the project linear group it consist of the member of the project linear group whose last row is $(0, 0, \dotsc, 0, 1)$
how is the euclidean group defin in term of the affin group it consist of the member of the affin group for which the top-left submatrix is orthogon
how is the orient euclidean group defin in term of the euclidean group it consist of the member of the euclidean group whose top-left submatrix has determin 1
how can a planar euclidean tran ation be written in term of homogen coordin $x^\prime = \left[ {matrix} r &amp; t 0 &amp; 1 {matrix} \right]$ where $r$ is an orthonorm matrix and $t$ is a translat
which group character the isometri the euclidean group
what are the usual invari of the euclidean group length angl area
what a similar tran ation in camera geometri an isometri compos with an isotrop scale
what anoth name for similar tran ation in camera geometri equi- tran ation becaus it preserv
what are the usual invari of similar tran ation in camera geometri angl length ratio area ratio
what the phrase `metric structur mean in camera geometri that the structur is defin up to a similar tran ation
how can a 2d affin tran ation be written in term of homogen coordin $x^\prime = \left[ {matrix} a &amp; t 0 &amp; 1 {matrix} \right]$ where $a$ is nonsingular and $t$ is a translat
what a good way of interpret the action of a linear tran in 2d use the svd to decompos it as $a = r_\theta d r_{\phi}$ so it a rotation, a scale along the axes, then a second rotat
what are the invari of an affin tran parallel line length ratio of parallel line area ratio
what the concern how mani function independ invari a geometr tran ation has there at least as mani function independ invari as the number of dof of the configur of point minus the number of dof of the tran ation
what'r the three main invari of a project tran ation the cross ratio the order of contact straight line
what the cross-ratio of four collinear point $(a, b; c, d) = \frac{ac \cdot bd}{bc \cdot ad}$ where $xy$ denot the sign distanc from $x$ to $y$ along the line
what are concurr line line pass through the same point
what'r two popular realis of the topolog of $\mbb{p}^2$ a sphere with opposit point identifi a disk with opposit point on the boundari identifi
when is the line at infin preserv by a project tran ation exact when it an affin
given an imag distort by a projectivity, how can you remov the project part, leav onli an affin distort identifi the line at infin and find the tran ation that take it to $(0, 0, 1)$
what are the two usual way to find the line at infin in an imag find the intersect of two pair of parallel line use the length ratio of two tripl of regularly-spac collinear point
given a 2d imag distort by an affinity, geometrically, how can you remov the affin part, leav onli a similar identifi the circular point and find the tran ation that map them to $(1, i, 0)$
when are the circular point of an imag fix under a project exact when the tran ation is a similar
what do $i, j$ usual denot in camera geometri the circular point $i = (1, i, 0)$ $j = (1, -i, 0)$
what the conic dual to the canon circular point in a euclidean coordin system $c_ fty^* = \left( {matrix} 1 &amp; 0 &amp; 0 0 &amp; 1 &amp; 0 0 &amp; 0 &amp; 0 {matrix} \right)$
when is the dual conic fix under a project tran ation exact when the tran ation is a similar
how doe the line at infin $l_ fty$ relat to the dual conic of the circular points, $c_ fty^*$ $l_ fty$ generat $c_ fty^*$ nullspac $c_ fty^* l_ fti = 0$
how do you calcul the angl between two line $l, m$ in 2d that are subject to a perspect tran $ \theta = \frac{\langl l, m \rangle_{c_ fty^*}}{ l _{c_ fty^*} m _{c_ fty^*}}$
whi the black hat tran so name it return an imag contain object which are smaller than the structur element and darker than their surround
whi the white hat tran so name it return an imag contain object which are smaller than the structur element and brighter than their surround
what the white hat tran anoth name for the top hat tran
how doe the dual conic tran under a point tran ation $x^\prime = hx$ $c^{*\prime}_ fti = hc^*_ fti h^t$
when decompos a 2d project into three matrices, what the similar matrix $h_s$ $h_s = \left( {matrix} sr &amp; \mb{t} \mb{0}^t &amp; 1 {matrix} \right)$
when decompos a 2d project into three matrices, what the affin matrix $h_a$ $h_a = \left( {matrix} k &amp; \mb{0} \mb{0}^t &amp; 1 {matrix} \right)$
when decompos a 2d project into three matrices, what the project matrix $h_p$ $h_p = \left( {matrix} i &amp; \mb{0} \mb{v}^t &amp; v {matrix} \right)$
have decompos a 2d project into matric $h_s, h_a, h_p$, how do you reconstruct the full matrix $h = \left( {matrix} srk + \mb{tv}^t &amp; \mb{t} \mb{v}^t &amp; v {matrix} \right)$
given the matrix of the dual conic, how do you recov the project distort that generat it eigendecompos $c_ fty^*$. $\lambda^{1/2}u$ is the matrix of the project
when are two projectively-distort 2d line orthogon when $l^t c^*_ fti m = 0$ where $c^*_ fty$ is the dual conic of the project
geometrically, what the polar of a conic $c$ and point $x$ the polar line $l$ intersect the conic at two point the tangent to the conic at these point intersect at $x$
algebraically, how do you find the polar of a conic $c$ and point $x$ $l = x^tc$ in homogen coordin
what a correl in project geometri it an invert map between line and point repres in homogen coordin by $l = x^ta^t$
algebraically, what are conjug point in project geometri given a conic $c$, point $x, y$ are conjug when $\langl x, c y \rangl = 0$
geometrically, when are point $x, y$ conjug with respect to conic $c$ when $x$ is on the polar of $y$ or equivalently, when $y$ is on the polar of $x$
how are conic classifi accord to the number of intersect they have with $l_ fty$ ellips have no real intersect with $l_ fty$ parabola have one real intersect with $l_ fty$ hyperbola have two real intersect with $l_ fty$
how are the fix point and line of a 2d project character the fix point ($x^\prime = hx$) and fix line ($l^\prime = l^\prime h^t$) a triangl note they may be complex
what $\pi_ fty$ denot in project geometri the plane at infin in $\mbb{p}^3$
what structur are point dual to in $\mbb{p}^3$ plane
what structur are line dual to in $\mbb{p}^3$ line
when doe a point $x$ lie on a plane $\pi$ in $\mbb{p}^3$ when $\pi \cdot x = 0$
how can you test when a point $x$ lie on a plane defin by $x_1, x_2, x_3$ in $\mbb{p}^3$ let $m = [x, x_1, x_2, x_3]$ when $x$ is on the plane, $\text{det} m = 0$ (in homogen coordinates)
under the point tran ation $x^\prime = hx$, how doe a plane $\pi$ of $\mbb{p}^3$ tran $\pi^\prim = \pi h^{-t}$
how is a line in $\mbb{p}^3$ defin in term of the span of two point $a, b$ form $w = [a, b]$ then the span of $w$ is the line connect $a, b$
how is a line in $\mbb{p}^3$ character in term of the nullspac of two plane $p, q$ form the matrix $w^* = \left[ {matrix} p q {matrix} \right]$ then the nullspac of $w$ is the line of intersect
what the plucker matrix of two point $a, b$ in $\mbb{p}^3$ $l = ab^t - ba^t$
what the of a plucker matrix in $\mbb{p}^3$ it a skew-symmetr rank 2 matrix
what the use of the plucker matrix of two point $a, b$ in $\mbb{p}^3$ it character the line through $a, b$.
given plucker matrix $l$ of a line and a plane $\pi$, what doe $l^t \pi^t$ describ the point of intersect between the line and the plane
what the dual plucker matrix of two plane $p, q$ in $\mbb{p}^3$ $l^* = p^tq - q^tp$
what the use of the dual plucker matrix of two plane $p, q$ in $\mbb{p}^3$ it character the line of intersect between $p,q$.
given the dual plucker matrix $l^*$ of a line, what doe $x^t l^*$ describ the plane through the line and the point $x$
what are the plucker coordin of a line in $\mbb{p}^3$ if the plucker matrix of the line is $l$, then the plucker coordin are $\mc{l} = [l_{12}, l_{13}, l_{14}, l_{23}, l_{42}, l_{34}]$ where $l_{42}$ is use instead of $l_{24}$ to suppress various negat in common ula
when doe a 6-vector $\mc{l}$ give the plucker coordin for a line in $\mbb{p}^3$ when $( \mc{l} \mc{l} ) = 0$
in term of plucker coordinates, when are two line in $\mbb{p}^3$ coplanar when $( \mc{l} \mc{\hat l}) = 0$
how is the inner product on plucker coordiant defin $(\mc{l} \mc{\hat l}) = {matrix} l_{12}\hat l_{34} + l_{13}\hat l_{42} + l_{14}\hat l_{23} \hat l_{12} l_{34} + \hat l_{13} l_{42} + \hat l_{14} l_{23} {matrix}$ (follow from $(\mc{l} \mc{\hat l}) = \det[a, b, \hat a, \hat b]$)
given a plucker matrix $l$, how do you get the dual plucker matrix $l^*$ exchang the element at coordin $12 \leftrightarrow 34$ $13 \leftrightarrow 42$ $14 \leftrightarrow 23$ ie replac coordin $i, j$ with coordin not-$i$, not-$j$
what the geometr interpret of the plucker coordin of a line the plucker coordin for a line repres a point in $\mbb{p}^5$ on a 4-dimens surfac call the klein quadric
how mani point defin a quadric nine
what a quadric a co-dimens 1 surfac defin by the root of a quadrat
what the polar of a point $x$ and a quadric $q$ in $\mbb{p}^3$ it the plane $\pi = x^tq$ such that the point of contact between $\pi$ and $q$ are the same as point on $q$ whose tangent pass through $x$
given a plane $\pi$ and quadric $q$ in $\mbb{p}^3$, how do you character the point of intersect pick a basi $m$ that take point in the plane $x$ to point in the space $x = mx$ then the $x$ on the intersect are those satisfi $(mx)^t q (mx) = 0$
what the dual of a quadric $q$ a quadric which is an equat on planes, defin by the adjoint $q^*$
how are quadric classifi in term of their matrix by the sign of the eigenvalu
how are conic curv in $\mbb{p}^2$ usual parameter $x = a\left( {matrix} 1 \theta \theta^2 {matrix} \right)$ for some non-singular matrix $a$
how are twist cubic curv in $\mbb{p}^3$ usual parameter $x = a\left( {matrix} 1 \theta \theta^2 \theta^3 {matrix} \right)$ for some non-singular matrix $a$
given two nondegener twist cubics, what kind of tran ation will take one to the other a project
how do twist cubic and plane interact they'll intersect at three point
what the idea in the screw decomposit ani euclidean tran ation can be decompos into a rotat around the `screw axi and a translat along that axi
what the canon posit of the plane at infin in $\mbb{p}^3$ $\pi_ fti = (0, 0, 0, 1)$
what do the circular point $i, j$ in $\mbb{p}^2$ correspond to in $\mbb{p}^3$ the point at which the absolut conic $\omega_ fty$ intersect ani given circl
what the geometr of the absolut conic it a conic of pure imaginari point lie in the plane at infin
how do you measur the angl between two line in $\mbb{p}^3$ if they'r distort by a project let $d_1, d_2$ be the intersect of the line with $\pi_ fty$ then $ \theta = \frac{\langl d_1, d_2 \rangle_{\omega_ fty}}{ d_1 _{\omega_ fty} d_2 _{\omega_ fty}}$ where $\omega_ fty$ is the absolut conic of the project
geometrically, what the absolut dual quadric $q_ fty^*$ in $\mbb{p}^3$ it the quadric ed from the set of plane tangent to $\omega_ fty$ ie it the rim quadric of $\omega_ fty$
when is a plane $\pi$ in the envelop defin by the absolut dual quadric when $\pi q^*_ fti \pi^t = 0$
how can the absolut conic be view as a limit consid the ellipsoid defin by $\text{diag}(1,1,1,k)$ as $k \rightarrow fty$, the absolut conic is recov
what the canon of the absolut dual quadric $q^*_ fti = \text{diag}(1,1,1,0)$
what the main advantag of the absolut dual quadric over the absolut conic the absolut dual quadric is defin by a singl equation, wherea the absolut conic requir two
which tran ation fix the absolut dual quadric exact the similar
what the nullspac of $q^*_ fty$ the plane at infin $\pi_ fty$
how mani degre of freedom doe the absolut dual conic have eight equival to the eight dof need to specifi metric properti in a project coordin frame
how can you calcul the angl between two plane that have been distort by a project $ \theta = \frac{\langl \pi_1, \pi_2 \rangle_{q^*_ fty}}{ \pi_1 _{q^*_ fty} \pi_2 _{q^*_ fty}}$ where $q^*_ fty$ is the dual absolut quadric of the project
how mani point correspond are need to fulli constrain a project between two 2d imag four
what the basi of the direct linear tran ation algorithm given correspond $x_i^\prim \leftrightarrow x_i$ we want to find a $h$ such that $x^\prime_i hx_i$ which can be done by solv $x_i^\prim hx_i = 0$
what the purpos of the direct linear tran ation algorithm to comput a homographi between two imag use four exact point correspond
in the direct linear tran ation algorithm, what the of the solut a one-dimension subspac
when comput 2d homographies, what the algebra distanc between two vector $x, x^\prime$ if the constraint impos on the homographi element $h$ by $x, x^\prime$ are describ by $a\bar h = 0$ then $ah = \epsilon$ give the algebra cost
what'r the advantag of the algebra distanc as a cost for comput 2d homographi it cheap to comput
what'r the problem with the algebra distanc as a cost for calcul 2d homographi it not geometr or statist meaning
in camera geometry, what do $x, \hat x, \bar x$ usual repres $x$ repres the measur valu $\hat x$ repres the estim valu $\bar x$ repres the true valu
what the symmetr transfer error when comput 2d homographi $d(x, h^{-1}x^\prime)^2 + d(hx, x^\prime)^2$ where $d$ is the euclidean distance, $h$ is the homographi $x, x^\prime$ are correspond point
what the reproject error when comput 2d homographi $d(x, \hat x)^2 + d(x^\prime, \hat x^\prime)^2$ where $\hat x, \hat x^\prime$ are such that $\hat x^\prime = \hat h\hat x$ is exact for some homographi $\hat h$
what the geometr intuit behind the reproject error optim it is effect tri to minim the distanc of a varieti in $\mbb{r}^4$ from the point $(x_i, x_i^\prime)$
what a varieti it the set of root of a multivari polynomi
what anoth name for the reproject error the geometr error
what the sampson error in comput 2d homographi $d_s(x, x^\prime) = j^{-1} \epsilon ^2$ where $\epsilon = \mc{c}_h(x)$ is the distanc of $h$ varieti from the point $x = (x, x^\prime)$ $j = \frac{ \mc{c}_h}{ x}$ is the gradient there
how doe the sampson error aris by linear the geometr error
when is the sampson error ident to the geometr error when the constraint violat cost $\mc{c}_h(x)$ is linear
defin cartesian product a mathemat oper which return a product set from multipl set w: that is, for set a and b, the cartesian product a × b is the set of all order pair (a, b) where a ∈ a and b ∈ b
what the basi of the reproject error in calcul a 2d homographi it follow from an isotrop gaussian error in the measur
which error doe the direct linear tran ation algorithm minim for 2d homographi the algebra error
which of algebra and geometr error are invari to coordin tran ation geometr
what normal should be done befor appli the dlt algorithm for 2d homographi translat so the mean of the correspond in each imag are zero scale isotrop so the averag distanc from the origin is $\sqrt{2}$
what the dlt in the context of 2d homographi the direct linear tran ation algorithm
whi is normal the data so import for the dlt becaus otherwis in homogen coordinates, the averag point is like to be $(100, 100, 1)$ which lead to a veri larg condit number
when comput a 2d homography, what the usual advantag to over-parameter the tran ation it can lead to a much simpler cost function surfac
what the main advantag of the sampson error over the reproject error sampson error requir onli the paramet of the homographi be optim reproject error requir the paramet of the homographi plus the paramet of all the ideal point be optim
how are iter method for find homographi typic initi by use the dlt to solv over a minim subset of correspond (subset is usual select so as not to contain outliers)
what are the two most popular iter scheme for solv 2d homographi problem levenberg-marquardt newton
what the gold standard error when calcul a 2d homographi anoth name for the reproject error
when use ransac for calcul 2d homographies, what the distanc threshold usual use the error/dist is distribut as $\chi^2_m$, so threshold that at $5 $ or whatev where $m$ is the codimens of the object in question (1 for lines, 2 for points, etc)
when use ransac for calcul 2d homographies, what'r two common termin criteria a consensus set of at least $80 $ of the point an iter threshold, pick so at least one iter will be all inlier
what the adapt ransac algorithm it ransac with the termin threshold adapt set by use the fact that if a consensus set of $(1-\epsilon)$ is found, then at most $\epsilon$ of the point can be outlier
what the advantag of the adapt ransac algorithm over regular ransac it mean that the fraction of outlier doesn't have to be guess beforehand
what the polar ident in linear algebra $ u \cdot v = \frac{1}{2}( u-v ^2 - u ^2 - v ^2)$
what the cauchy-schwarz inequ in linear algebra $ u \cdot v u v $
how the vector space of orient area ed associ each planar area of size $a$ with a vector perpendicular to it that has length $a$ the orient of the area correspond to which side the vector point out of
what are the fundament properti of the wedg product $v \wedg v = 0$ antisymmetr linear in the first
in $\mbb{g}^3$, how can you write $u \wedg v$ in term of basi vector $u \wedg v = (u_1v_2 - u_2v_1)(e_1 \wedg e_2) + (u_1v_3 - u_3v_1)(e_1 \wedg e_3) + (u_2v_3 - u_3v_2)(e_2 \wedg e_3)$ follow from the basic properti of outer product
what the geometr of $u \wedg v$ an orient area correspond to the parallelogram with side $u, v$ note that orient area do \emph{not} have shape, onli size and orient
in geometr algebra, what the more common name for an orient area a bivector
how the vector space $\mbb{g}^3$ ed from multivector of the $m = v + b +t$ where $v$ is a vector $b$ is a bivector $t$ is a trivector
algebraically, what a bivector the of the outer product of two vector
what $\mbb{g}^3$ denot the 3d geometr algebra
what the fundament ident of geometr algebra it relat the geometr product to the inner amp; outer product $uv = u \cdot v + u \wedg v$
what doe the geometr product $uv$ reduc to when $u$ is parallel to $v$ $uv = u \cdot v$
what doe the geometr product $uv$ reduc to when $u$ is orthogon to $v$ $uv = u \wedg v$
how do you defin the inner product in term of the geometr product if $a$ is a $j$-vector and $b$ is a $k$-vector $a\cdot b = \langl ab \rangle_{k-j}$
how do you defin the outer product in term of the geometr product if $a$ is a $k$-vector and $b$ is a $j$-vector $a \wedg b = \langl ab \rangle_{j+k}$
what $uv$ denot in geometr algebra the geometr product of $u,v$
what the unit pseudoscalar for a plane in $\mbb{g}^3$ if the plane has orthonorm basi $e_1, e_2$, it $i = e_1 \wedg e_2 = e_1e_2$
what the use of the unit pseudoscalar of a plane in geometr algebra it character a plane
what the squar of the unit pseudoscalar $i^2 = -1$
what the unit pseudoscalar for a volum in $\mbb{g}^3$ $i = e_1e_2e_3$
what the squar of the unit pseudoscalar for a volum in $\mbb{g}^3$ $i^2 = -1$
how do bivector correspond to angl in $\mbb{g}^3$ $i\theta$ correspond to an angl of $\theta$ in the plane with unit pseudoscalar $i$
how are exponenti defin in $\mbb{g}^3$ $e^{i\theta} = \theta + i \theta$ where $i$ is the unit pseudoscalar for a plane
what the polar of $uv$ geometr algebra $uv = re^{i\theta}$ where $r = u v $ $i\theta$ is the angl between $u, v$
what the cartesian of a general complex number $uv$ in geometr algebra $uv = a + bi$ where $a = r \theta$ $b = r \theta$ $r = u v $ $i\theta$ is the angl between $u, v$
what'r general complex number in geometr algebra a scalar plus a pseudoscalar
what the geometr of a general complex number in $\mbb{g}^2$ $r e^{i\theta}$ is an orient arc of radius $r$ and angl $\theta$ in the plane character by $i$
how is conjug defin for general complex number if $uv = a + bi$ then $vu = a - bi$
doe $e^{i_1\theta_1}e^{i_2\theta_2} = e^{i_1\theta_1 + i_2\theta_2}$ for general complex number not unless $i_1 = i_2$
what anoth name for a general complex number in $\mbb{g}^2$ a complex number
what'r two other name for a general complex number in $\mbb{g}^3$ quaternion spinor
what the convent quaternion basi $i_1 = e_3 e_2$ $i_2 = e_1 e_3$ $i_3 = e_2 e_1$
what are the basic ident for the quaternion basi $i_1^2 = i_2^2 = i_3^2 = -1$ $i_1i_2 = i_3$ $i_2i_3 = i_1$ $i_3i_1 = i_2$
when are general complex number close under the geometr product when they'r complex number (ie in $\mbb{g}^2$) or quaternion (ie in $\mbb{g}^3$)
in $\mbb{g}^3$, which multivector are general complex number the one that can be written as a scalar plus a trivector
whi are general complex number refer to as complex number in $\mbb{g}^2$ becaus they have the same properti as regular complex number
given quaternion $z_1, z_2$, what $\overline{z_1z_2}$ $\bar z_2 \bar z_1$
how are rotat repres by quaternion let rotat by $\theta$ in the plane $i$ be repres by $r = e^{i\theta/2}$ so $r_{i\theta}(u) = \bar r u r$
for a rotat in plane $i$ by $\theta$ in geometr algebra, what doe a vector $u$ in the plane get taken to $v = u e^{i\theta}$
are rotat close under composit onli in 2 and 3 dimens
generally, what'r the pseudoscalar of $\mbb{g}^n$ the multipl of $e_1e_2\dotsb e_n$
what a $k$-blade in geometr algebra given a $k$-dim subspac of $\mbb{r}^n$ with basi $\{b_1, \dotsc, b_k\}$, the blade for the subspac in $\mbb{g}^n$ is the subspac generat by $b = b_1b_2\dotsb b_k$
what two thing do blade in $\mbb{g}^n$ correspond to a $k$-blade $b = b_1 b_2 \dotsb b_k$ correspond to a $k$-dim subspac of $\mbb{r}^n$ generat by $\{b_1, \dotsc, b_k\}$ the set in that subspac of size $ b $
how is the norm of a blade $b = b_1 b_2 \dotsb b_k$ defin $ b = b_1 b_2 \dotsb b_k $
how do the possibl blade for a subspac relat they'r all scalar multipl of eachoth
how is the invers of a blade defin if $b = b_1 b_2 \dotsb b_k$ then $b^{-1} = \frac{1}{ b ^2}b_k \dotsb b_2 b_1$
how are vector invers defin with respect to the geometr product $u^{-1} = \frac{1}{ u ^2} u$
what the sign of the permut that revers $k$ element $(-1)^{k(k-1)/2}$
what doe $\langl a \rangle_j$ denot in geometr algebra the $j$-vector part of $a$
what the grade of a blade the minimum number of vector it ed from
is the inner product of the geometr algebra commut onli when the grade of the argument match
what anoth name often use for the inner product in geometr algebra the left contract sinc there are actual sever way to construct an inner product on $\mbb{g}^n$
when is the inner product of the geometr algebra commut when the grade of the argument are the same
how is the orient of a basi of $\mbb{g}^n$ defin by the sign of the unit pseudoscalar of the basis, $i = e_1 e_2 \dotsb e_n$
algebraically, what the dual of a multivector $a^* = a/i$ where $i$ is the pseudoscalar
when work with multivectors, what doe $a/b$ usual denot $a/b = ab^{-1}$
geometrically, what doe the dual $a^*$ of a multivector $a$ repres the subspac orthogon to the one repres by $a$
in the geometr algebra, what $a^{**}$ $a^{**} = (-1)^{n(n-1)/2}a$
how doe the dual relat to the inner amp; outer product in geometr algebra $(a \cdot b)^* = a \wedg b^*$ $(a \wedg b)^* = a \cdot b^*$
how doe the exterior algebra relat to the geometr algebra the geometr algebra is a simplif of the exterior algebra
what anoth name for the exterior algebra the grassmann algebra
how is the cross product defin in the geometr algebra $u v = (u \wedg v)^*$
what doe the dual in the geometr algebra correspond to in the exterior algebra the hodg dual
what are the two main way to construct a blade in geometr algebra as a geometr product of orthogon vector as an outer product of linear independ vector
what an outermorph a linear oper which preserv the outer product $(\wedge)$
what the outermorph extens theorem everi nonzero linear tran ation $f \colon \mbb{r}^n \rightarrow \mbb{r}^m$ can be extend to a uniqu outermorph $f \colon \mbb{g}^n \rightarrow \mbb{g}^m$ by set $f(a_1 \wedg \dotsb \wedg a_k) = f(a_1) \wedg \dotsb \wedg f(a_k)$
what the defin properti of the adjoint $u \cdot av = a^*u \cdot v$
how is the determin defin in the geometr algebra $\det f$ is such that $f(i) = (\det f) i$ where $i$ is the pseudoscalar and $f$ has been extend to an outermorph
what the algebra definit of a symmetr oper $f^* = f$
what the algebra definit of an orthogon oper $f^* = f^{-1}$
what the algebra definit of a skew oper $f^* = -f$
what the intuit version of the cartan-dieudonn theorem in geometr algebra everi $n$-dimension orthogon tran ation can be repres as $r n$ reflect in hyperplan
what the algebra version of the cartan-dieudonn theorem in geometr algebra everi orthogon $f$ can be written $f(v) = (-1)^r v v v^{-1}$ where $v$ is a geometr product of $r$ vectors, one perpendicular to each hyperplan a reflect occur in
what the skew tran ation represent theorem in geometr algebra everi skew $f$ can be written as $f(v) = v \cdot \omega$ for some uniqu bivector $\omega$
what the theorem concern the invari subspac of real linear tran ation everi real linear tran ation has a 1- or 2-dimension subspac
what the polar decomposit of a linear oper $f = op$ where $o$ is orthogon $p$ is posit semidefinit
when can the polar decomposit be appli alway
when is a linear oper posit definit when $f(v) \cdot v 0$ for all $v$
how'd you take the deriv of a geometr product use the usual product rule
what a suffici condit for exchang the order of differenti when the second deriv are all continu
what doe $ _i f(x)$ denot in geometr calculus $\frac{ f}{ x_i}$ evalu at $x$
what the differenti of $f$ in geometr calculus it the linear tran ation $f^\prime_x(h) = h_i ( _i f)(x)$
exact when is a function differenti in geometr calculus when $f(x+h) = f(x) + f^\prime_x(h) + r(h)$ and $\lim_{h \rightarrow 0} r(h)/ h = 0$
what the usual suffici condit for a function to be differenti at $x$ in geometr calculus that all the partial deriv exist in the neighbourhood of $x$ and are continu at $x$
what the usual ula for the jacobian determin of a differenti in geometr caculus $\det f_x^\prime$ is such that $f^\prime_x(i) = (\det f^\prime_x) i$ use the outermorph extens of $f$
in geometr calculus, what are the use of the differenti and the gradient the differenti give tangent the gradient give rate of chang
what the chain rule for differenti $(g \circ f)^\prime_x = g^\prime_{f(x)} f^\prime_x$
when it exists, what the differenti of an invers function $(f^{-1})^\prime_{f(x)} = (f_x^\prime)^{-1}$
when doe the differenti of an invers function exist when $f$ is continu differenti $f^\prime_x$ is invert
how is the direct deriv defin in term of the differenti $ _h f(x) = f^\prime_x(h)$
what the invers function theorem in geometr calculus if $f\colon \mbb{r}^n \rightarrow \mbb{r}^n$ is continu differenti around $x$ and $f^\prime_x$ exist and is invert then locally, $f^{-1}$ exist and is differenti
what the implicit function theorem of geometr calculus if $f \colon \mbb{r}^{m n} \rightarrow \mbb{r}^n$ is continu differenti at $(a, b)$ and the jacobian wrt $b$ compon is invert then there a continu differenti $y(x)$ such that near $a$, $f(x, y(x)) = f(a, b)$
what a continu differenti function one whose deriv are continu
what a common use of the implicit function theorem if $f(a, b) = c$ satisfi the theorem, it can be use to calcul how much $b$ has to chang to keep $c$ constant when $a$ chang
what are the two usual constraint on manifold paramet function they must be differenti (so direct deriv are equival to differentials) the differenti must be biject (so independ vector map to independ vectors)
if $x$ parameter a manifold $m$, what the tangent space to $m$ at $p = x(q)$ it the $t_p$ generat by $x^\prime_q(w)$, where $w$ rang over the same space as $q$
when doe a map between manifold extend to a map between tangent space when the manifold are of equal dimens the map is biject the map is differenti
what the gradient of a field $f$ $\nabla f(x) = e_i _i f(x)$
what a field in geometr calculus a function $f$ that map a manifold to $\mbb{g}^n$
what doe $\dot \nabla f \dot g$ denot in geometr calculus $\dot \nabla f \dot g = e_i f _i g$ so the dot denot which part to take the deriv of, while keep the $e_i$ in the same place
what the product rule for the gradient in geometr calculus $\nabla (fg) = \dot \nabla f \dot g + \dot \nabla \dot f g$
how is the direct deriv defin in term of the gradient $ _h f(x) = h \cdot \nabla f (x)$
how do the diverg and curl aris from the gradient in geometr calculus use the fundament ident for the geometr product $\nabla f = e_i _i f = e_i \cdot _i f + e_i \wedg _i f = \nabla \cdot f + \nabla \wedg f$
how doe the diverg affect the grade of it argument it lower it; $r$-vector go to $r-1$ vector
how doe the curl affect the grade of it argument it rais it; $r$-vector go to $r+1$ vector
what are the basic relat between gradient, diverg and curl in geometr calculus curl of a curl is zero diverg of a diverg is zero curl of a gradient of a scalar field is zero
what the differ between a blade and a multivector a blade is a simpl product of $k$ vector a multivector is a sum of blade
what a common altern to ransac least median of squar ie optim the median of the squar distanc
how doe least median squar compar to ransac it doesn't need ani distanc threshold set but it'll fail if more than 50 of the point are outlier
what'r the three use for local featur detector for semant in ation (ex: find road in satellit images) for anchor point for imag represent
what kind of region do corner detector look for region with (in some sense) high curvatur
what the second-mo matrix for a 2d function $f$ $s = w \star \left( {matrix} f_x^2 &amp; f_x f_i f_x f_i &amp; f_y^2 {matrix} \right)$ where $w$ is a window function
how should the second-mo matrix of an imag be interpret it eigendecomposit captur the gradient within the window the eigenvector with the larger eigenvalu is maxim align with the gradient and the eigenvalu gap indic the anistropi of the gradient
what the idea in the harri corner detector use the second-mo matrix to captur the gaussian-window gradient at each point averag the s with anoth gaussian look for point where both eigenvalu of the matrix are large, indic larg gradient in both direct
what tran ation is the harri corner detector robust against translat covari rotat covari robust against light chang
what repeat in featur extract that the same featur can be extract under mani differ tran ation
what the idea in the susan featur detector consid a small disk around each point partit the pixel of the disk accord to whether they'r similar to the point or not look for point with a small `similar ratio
how doe the susan detector compar to the harri detector it tend to pick up a greater number of edg it tend to be sensit to nois
what the idea in the harris-laplac featur detector induc scale invari by calcul the harri respons at mani differ scale then at each point, pick the scale that maxim the log of the harri respons
what the idea in the harris-affin featur detector for each point calcul an affin tran that'll take the second-mo matrix at the point to a spheric one normal the local imag with the tran amp; appli harris-laplac to extract normal locat amp; scale if the new second-mo matrix isn't spherical, repeat.
how the laplacian oper relat to the hessian the laplacian is the trace of the hessian
what'r the step in the hessian featur detector comput the second-ord gaussian smooth deriv use them to calcul the determin of the hessian find the maxima of the hessian
what do hessian featur detector pick up ridg and blob whose scale match that of the gaussian kernel use to comput the second deriv
what the idea in the salient region featur detector estim the entropi at each point over a rang of scale find the local maxima $h$ of the entropi for each local maxima, calcul $y$, the rate of chang of the entropi with respect to scale find the maxima of $hy$
what doe the salient region featur detector look for region with high entropi that are also self-dissimilar (ie where the amount of entropi vari signific as you move around)
how do blob and corner compar for featur detect corner are easier to local blob give more scale in ation
what the idea in intensity-bas region detector start with the intens extrema of the imag trace out ray in mani direct on each ray, look for a point where the intens shift most rapid link these point into an irregular region replac the irregular boundari with an ellips
what the euler-lagrang equat if $q$ is a function of time and $l$ is a function of time, posit and veloc then the correspond euler-lagrang equat for $q$ is $l_x(t, q, q^\prime) = \frac{d}{dt} l_v(t, q, q^\prime)$
defin degre of freedom the number of independ data point that go into an estim minus the number of paramet use as intermedi step in the estim
given a norm on vectors, how is a norm on oper usual induc $ t = \max_x tx / x $
what'r the two fundament properti of oper norm $ tx t x $ $ st s t $
given an induc oper norm, what $ t^{-1} $ equival to $ t^{-1} = \frac{1}{\min_x tx / x }$
when is a linear oper bound exact when it continu
how is the $\ell_2$ norm of an oper defin in term of eigenvalu $ t _2 = \sqrt{\rho_{t^+t}}$ where $\rho$ is the spectral radius of $t^+ t$
what doe $e_\omega(t)$ denot in wavelet analysi $e_\omega(t) = \frac{1}{\sqrt{2\pi}} e^{iwt}$
in term of norms, when is a project orthogon exact when $ px x $ for everi $x$
what the closur of a set of vector it the set of all infinit sum of those vector with coeffici in $\ell_2(\mbb{z})$
what the defin properti of a reproduc kernel $x(t) = \langl k(t, t^\prime), x(t^\prime) \rangle$
in term of the reproduc kernel, when is an orthonorm basi complet when $k(t, t) = \sum \phi_n(t) ^2$
how doe the dirac delta relat to the reproduc kernel it act like a general reproduc kernel for space not well-behav enough to have actual reproduc kernel (like $l_2(\mbb{r})$, whose function are allow to be unbound on zero-measur sets)
how is the dirac delta written in term of the evalu function $\delta(t - t^\prime) = \langl t t^\prime \rangle$ where $t$ on the rhs denot the evalu function $e_t$
what convent for the fourier analysi amp; synthesi function is use in these card $x(t) = t x(\omega) e_\omega(t) d\omega$ $x(\omega) = t x(t) e_\omega^*(t) dt$
if $x(\omega)$ first $n$ deriv all tend to zero, what follow that $x(t)$ is domin by $\frac{1}{t^n}$
if $x(\omega)$ is domin by $\frac{1}{\omega^n}$, what follow that $x(t)$ first $n$ deriv exist and $\frac{d^n}{dt^n}x(t) = t (i\omega)^n x(\omega) e_\omega(t) d\omega$
what the intuit version of the relat between $x(\omega)$ rate of decay and $x(t)$ differenti the faster $x(\omega)$ decay the smoother $x(t)$ is
if $x(t)$ is domin by $\frac{1}{t^n}$, what follow $\frac{d^n}{d\omega^n} x(\omega) = t (-it)^n x(t) e_\omega(t)dt$
how do the rank and nulliti of $a$ and $a^+$ relat if $a$ is $m n$ then $\text{rank}(a) = \text{rank}(a^+) = r$ $\text{null}(a) = n - r$ $\text{null}(a^+) = m - r$
intuitively, what the use of the right pseudoinvers it give the solut of minimum norm of an underconstrain system
intuitively, what the use of the left pseudoinvers it give the solut of minimum error of a system of overconstrain equat
what the left pseudoinvers of a linear oper $a$ $a^\dagger_l = (a^+a)^{-1}a^+$ (such that $a^\dagger_l a = \mathbf{1}$)
what the use of a tight frame it mimic an orthogon basi even though it vector might be linear depend
in term of it bounds, when is a frame an orthogon basi when it exact
in term of it bounds, when is a frame exact when the bound are both 1
when doe an oper $t^+_\phi t_\phi$ correspond to a frame when there are constant $a, b$ such that $ t^+_\phi t_\phi b$ $ (t^+_\phi t_\phi)^{-1} a^{-1}$
what the reconstruct properti of a frame and it dual the $\{c_n\}$ of minimum norm that minim the error in $x(t) = \sum c_n \phi_n$ are $c_n = \langl \xi_n, x \rangle$ and vice versa for the dual
when can a function be uniqu reconstruct from the valu $\langl \phi_n, x \rangle$ for a set of vector $\{ \phi_n \}$ exact when $\phi_n$ is a frame
given an infinit frame, how can the invers frame oper be approxim use the first $n$ term of the neumann expansion, $s^{-1} = \frac{1}{c}\sum_{k=0}^ fti \left(1 - \frac{1}{c}s\right)^k$ where $s$ is the frame oper $c = \frac{1}{2}(a+b)$ is the averag of the frame bound
what the rate of converg of the iter algorithm for approxim the dual frame oper the error decay as $\left(\frac{b-a}{b+a}\right)^n$ where $a, b$ are the frame bound
how doe the laplac tran relat to the fourier tran evalu the bilater laplac tran over the imaginari axi give the fourier tran
what the laplac tran $f(s) = t_0^ fti e^{-st} f(t)$ where $s$ is the complex frequenc
what the bilater laplac tran it the laplac tran with limit $(- fty, fty)$
what the intuit interpret of the laplac tran it decompos a function into a superposit of moment
what anoth name for up-sampl in signal process interpol
what anoth name for down-sampl in signal process decim
what the quadratur mirror filter properti for discret even-length filter in the time domain $h_1[n] = (-1)^n h_0[n-1-n]$
for discret time, even length filters, what the perfect reconstruct requir in the frequenc domain $x(z)$ component: $g_0(z)h_0(z) + g_1(z)h_1(z) = 2$ $x(-z)$ component: $g_0(-z)h_0(z) + g_1(-z)h_1(z) = 0$
in the frequenc domain, what the of downsampl then upsampl $x(z)$ $y(z) = \frac{1}{2}(x(z) + x(-z))$
what the quadratur mirror filter properti for infinit filter in the time domain $h_1[n] = (-1)^n h_0[1 - n]$
what the quadratur mirror filter properti for infinit filter in the frequenc domain $h_1(z) = -\frac{1}{z} h_0\left(-\frac{1}{z}\right)$
what the quadratur mirror filter properti for discret even-length filter in the frequenc domain $h_1(z) = \left(-\frac{1}{z}\right)^{n-1} h_0\left(-\frac{1}{z}\right)$
how are infinit filter usual chosen to suppress the $x(-z)$ compon in the reconstruct by pick the analysi amp; synthesi filter to be time-revers version of eachoth $g_0(z) = h_0\left(\frac{1}{z}\right)$ $g_1(z) = h_1\left(\frac{1}{z}\right)$
what the practic version of the qmf properti a qmf partner $h_1$ is obtain by revers the coeffici of $h_0$ and then flip the sign of everi odd element
what the intuit use of the qmf properti qmf pair have opposit frequenc respons so togeth with smooth and a vanishes-at-zero-freq constraint on one filter, lead to low and high pass filter
whi are even-length filter usual the onli one consid becaus under the pr and qmf properties, onli even-length finit filter can suppress the $x(-z)$ compon of the reconstruct
if the pr amp; qmf properti are enforced, what the frequency-domain equat that a low-pass reconstruct filter must satisfi $ h_0(\omega) ^2 + h_0(\omega+ \pi) ^2 = 2$
what are the relat enforc by the pr and qmf properti on the synthesi and reconstruct filter qmf relat the high-pass filter and the low-pass filter pr relat the synthesi and analysi filter
what the relat on the dirac delta that suggest the fourier invers theorem $ t e_{\omega}^*(t) e_{\omega}(t^\prime) d \omega = \delta(t - t^\prime)$
what are the frequenc spectra of the haar high and low pass reconstruct filter low pass is $ (\omega/2)$ high pass is $ (\omega/2)$
what the basi properti for mra subspac a set of mra subspac $\mc{v}_n$ has the frame properti if there is some $\phi(t) \mc{v}_0$ such that all shift and suitabl scale version of it a basi for $\mc{v}_n$
what the advantag of biorthogon wavelet the onli orthogon fir wavelet with linear phase respons is the haar wavelet all other with those properti are biorthogon
what properti should a wavelet scheme have when use it to construct a discret approxim of a continu signal first moment (the mean) of the scale function should be zero
if the low-pass filter in a biorthogon scheme has support $[n_1, n_2]$, what the support of the associ high-pass function $[1-n_1, 1-n_2]$
if the low-pass filter in a biorthogon scheme has support $[n_1, n_2]$, what the support of the associ scale function $[n_1, n_2]$
if the low-pass filter $h_0, h_0^\prime$ in a biorthogon scheme have support $[n_1, n_2]$, $[n_1^\prime, n_2^\prime]$, what the support of the associ $\psi$ $\left[\frac{n_1 - (1- n_2^\prime)}{2}, \frac{n_2 - (1 - n_1^\prime)}{2} \right]$
what'r the lazi filter in wavelet analysi the biorthogon filter such that the low-pass one take everi even sampl the high-pass one take everi odd sampl don't actual lead to wavelet themselves; use for lift scheme
if $f$ is scalar-valued, when doe $f(x) = c$ local defin a manifold when it continu differenti and $\nabla f(x_0) 0$
assum it exists, how doe the level set $f(x_0) = c$ relat to the gradient the manifold $m$ of the level set at $x_0$ is orthogon to $\nabla f(x_0)$
how is the gradient of a vector field defin in geometr calculus $\nabla f = e_i e_j _i f_j$
which element of $\mbb{g}^n$ have invers the one such that $u^2 0$
how is the vector diverg defin in geometr calculus $\nabla \cdot f = _i f_i$ (sinc $e_i \cdot e_j = \delta_{ij}$)
how is the vector curl defin in geometr calculus $\nabla \wedg f = e_i \wedg e_j _i f_j$
in geometr calculus, what an irrot vector field one such that $\nabla \wedg f = 0$
given a curvilinear coordin system $\{u_i\}$, what are the base $\{\mb{x}^k\}, \{\mb{x}_j\}$ $\mb{x}^k = \frac{ u_k}{ x} = \nabla u_k$ $\mb{x}_j = \frac{ x}{ u_j}$ differ from $x_j$, which are scalars!
when discuss curvilinear coordinates, what do $x_j$ denot the coordin in an orthonorm basi a map from the curvilinear coordin to the coordin in the orthonorm basi
what are reciproc base in geometr calculus a pair of base $\{u^i\}, \{v_j\}$ such that $u^i \cdot v_j = \delta_{ij}$
given a curvilinear coordin system, what $\nabla$ written in term of the basi $\mb{x}^k$ $\nabla = \mb{x}^k\frac{ }{ u_k}$
when discuss curvilinear coordin in geometr calculus, what doe $\{ \mb{\hat x}_i\}$ denot the normal version of $\{ \mb{x}_i \}$
given a curvilinear coordin system in $\mbb{g}^3$, what the geometr interpret of the basi vector $\mb{x^k}$ if you fix $u_1$ in $\mb{x}(u_1, u_2, u_3)$, it defin a coordin surfac $\mb{x}^1$ is the vector orthogon to this surfac same for the other two
given a curvilinear coordin system in $\mbb{g}^3$, what the geometr interpret of the basi vector $\mb{x_i}$ if you altern fix $u_2$ and $u_3$ in $\mb{x}(u_1, u_2, u_3)$, two coordin surfac are defin $\mb{x}_1$ is the vector tangent to the coordin curv of intersect same for the other two
if $\mb{x}(u, v)$ parameter a surfac $s$, how is the vector deriv on $s$ defin $\mb{ } = \mb{x}^u _u + \mb{x}^v _v$ where $\{\mb{x}^u, \mb{x}^v\}$ are the base of the tangent space induc by the parameter $ _u = \frac{ }{ u}$ no summat is impli by $\mb{x}^u _u$
how can the vector deriv be defin in a coordinate-independ way it the project of the gradient oper onto the tangent space of the surfac $\mb{ } = p_t(\nabla)$
what an open set in a metric space it a set such that if a point is in the set, so is the neighbourhood of that point
what a close set in a metric space a set which is close under the limit oper
what a necessari condit on the gradient for $f$ to have a local extremum at $x_0$ $\nabla f(x_0) = 0$
in geometr calculus, how is the hessian defin $h_f = \left[ _{ij}(x) \right]$
in term of the hessian, what a suffici condit for $x_0$ to be a local minimum of $f$ $\nabla f(x_0) = 0$ $h_f(x_0)$ is posit definit
how should the eigendecomposit of the hessian be interpret geometr $\lambda_i$ indic the curvatur of the surfac along $u_i$
given a constrain problem with object $f$ and constraint $g$, how is the lagrang multipli $\lambda$ defin if $f(x_0)$ is an extremum of $f$ under the constraint $g(x) = c$, then $\nabla f(x_0) = \lambda \nabla g(x_0)$ for some $\lambda$, the lagrang multipli
geometrically, how should the lagrang multipli $\lambda$ be interpret given object $f(x)$ and constraint $g(x) = c_0$ if $m(c)$ is the valu of the solut then $m^\prime(c_0) = \lambda$
what the realiti condit of the fourier tran if $f(x)$ is real, then $f^*(\omega) = f(-\omega)$
when discuss the wavelet packet tran , what doe $c^{(2j)}_{mn}$ denot the coeffici at level $m$ which are a low-pass filter of $c^{(j)}_{m-1,n}$
when discuss the wavelet packet tran , what doe $c^{(2j+1)}_{mn}$ denot the coeffici at level $m$ which are a low-pass filter of $d^{(j)}_{m-1,n}$
when discuss the wavelet packet tran , what doe $d^{(2j)}_{mn}$ denot the coeffici at level $m$ which are a high-pass filter of $c^{(j)}_{m-1,n}$
when discuss the wavelet packet tran , what doe $d^{(2j+1)}_{mn}$ denot the coeffici at level $m$ which are a high-pass filter of $d^{(j)}_{m-1,n}$
when discuss the packet tran , what doe $p^{(j)}_{mn}$ denot the basi function correspond to the approxim coeffici $c^{(j)}_{mn} = \langl p^{(j)}_{mn} x \rangle$
when discuss the packet tran , what doe $q^{(j)}_{mn}$ denot the basi function correspond to the detail coeffici $d^{(j)}_{mn} = \langl q^{(j)}_{mn} x \rangle$
in the context of the wavelet tran , how is $p^{(k)}_{mn}$ defin in term of $p^{(k)}$ $p^{(k)}_{mn} = \mc{d}_{2^m} \mc{t}_{n} p^{(k)}$
in the context of the wavelet tran , how are the packet basi function $p^{(k)}$ defin as a recurs $p^{(2j)}(2\omega) = \frac{1}{\sqrt{2}} h_0 (\omega) p^{(j)}(\omega)$ $p^{(2j+1)}(2\omega) = \frac{1}{\sqrt{2}} h_1(\omega) p^{(j)}(\omega)$
in the context of the wavelet tran , how are the packet basi function $q^{(k)}$ defin as a recurs $q^{(2j)}(2\omega) = \frac{1}{\sqrt{2}} h_0 (\omega) q^{(j)}(\omega)$ $q^{(2j+1)}(2\omega) = \frac{1}{\sqrt{2}} h_1(\omega) q^{(j)}(\omega)$
in the context of the wavelet tran , how are the packet basi function $p^{(k)}$ defin in close by use the binari represent of $k$ and take the product of the scale function the low-pass filter for scale $j$ if the $j$th bit is 0 the high-pass filter for scale $j$ if the $j$th bit is 1
what metric doe the best basi algorithm use to judg the qualiti of a basi the entropi of it coeffici
how the surfac integr defin in geometr calculus for a surfac $s$ parameter by $x$, $ t_s f ds = t_a (f \circ \mb{x}) \mb{x}_u \wedg \mb{x}_v da$
how the flux integr defin in geometr calculus for a surfac $s$ parameter by $x$, $ t_s f d\mb{s} = t_a (f \circ \mb{x}) (\mb{x}_u \wedg \mb{x}_v) da$
in the definit of the flux integral, how the infinitesim $d\mb{s}$ defin $d\mb{s} = (\mb{x}_u \wedg \mb{x}_v) da$
what the fundament theorem of geometr calculus if $m$ is a orient and bound $m$-dim manifold with boundari $ m$ then $ t_m d^m \mb{x} f = \oint_{ m} d^{m-1} \mb{x} f$
what doe $d^m \mb{x}$ denot in the fundament theorem of geometr calculus $d^m\mb{x} = \mb{i}_m(\mb{x}) d^mx$ where $d^mx = dx_1\dotsb dx_m$ $\mb{i}_m(\mb{x})$ is the unit pseudoscalar of the tangent space to the manifold at $\mb{x}$
in the fundament theorem of geometr calculus, whi doe the infinitesim appear on the left becaus the multivector repres by the infinitesim doesn't commut
in the fundament theorem of geometr calculus, how must orient be defin the pseudoscalar orient $m$ and the pseudoscalar orient $ m$ must be chosen such that $\mb{i}_m = \mb{i}_{m-1} \mb{\bar n}$ where $\mb{\bar n}$ is the unit outward boundari normal to $m$
what an analyt field in geometr calculus $f$ is analyt on a manifold $m$ if $\mb{ } f(\mb{x}) 0$
what intuit special about analyt field in geometr calculus their valu is wholli determin by their valu on the manifold boundari
what an implicit definit of the darboux bivector given a curv with tangent $\mb{t}$, the darboux bivector is the $\mb{\omega}$ such that $\mb{\dot t} = \mb{t} \cdot \mb{\omega}$
what doe $\mb{\dot x}$ denot in the context of curv in differenti geometri the deriv with respect to the arc length parameter
what the frenet basi in differenti geometri given a curv $\mb{x}$ in $\mbb{r}^n$, it the basi ed from $\mb{t}$, the tangent $\mb{n} \propto \mb{\dot t}$, the unit normal $\mb{b} = \mb{t} \mb{n}$, the binorm
how is the darboux bivector defin in term of the frenet basi $\mb{\omega} = \frac{1}{2}(\mb{t} \wedg \mb{\dot t} + \mb{n} \wedg \mb{\dot n} + \mb{b} \wedg \mb{\dot b})$
what the curvatur of a curv in 3d differenti geometri it the $\kappa$ such that $\mb{\dot t} = \kappa \mb{n}$ in the frenet basi
what the torsion of a curv in 3d differenti geometri it the $\tau$ such that $\mb{\dot b} = -\tau \mb{n}$ in the frenet basi
what the geometr interpret of the darboux bivector in $\mbb{r}^3$ it character the angular veloc of the frenet basi the plane associ with $\mb{\omega}$ is the plane of rotat (the oscul plane) and $\mb{\omega}$ is the rotat speed
how is the metric of a surfac defin $g = [\mb{x}_i \cdot \mb{x}_j]$ where $\{\mb{x}_i\}$ is a basi for the tangent space of the surfac
how is the length of a curv in a surfac defin in term of the metric if $u(t)$ parameter $c$ in the surface, then $\ell(c) = t \sqrt{g_{ij} u_i^\prim u_j^\prime} dt$ where $g_{ij}$ are the element of the metric $u^\prime_i$ is the deriv of surfac coordin $i$ wrt $t$
how can the invers of the metric of a surfac $g^{-1}$ be defin $g^{-1} = [\mb{x}^{i} \cdot \mb{x}^j]$ where $\{\mb{x}^i\}$ is the reciproc basi to the one use to defin $g$
how are entri of the metric denot in differenti geometri $g_{ij}$ denot the element of $g$
how are element of the invers metric denot in differenti geometri $g^{ij}$ denot the element of $g^{-1}$
how is the differenti on a surfac defin at a point $p$ on the surface, the differenti of a field $f$ is $f^\prime_p(h) = (h \cdot )f(p)$ where $ $ is the vector deriv
how can the differenti of a surfac be interpret in term of tangent space if the field $f$ map $s$ to anoth surfac $\bar s$ then $f^\prime_p$ map the basi $\{\mb{x}_i\}$ of $t_p$ to the basi $\{(f\circ \mb{x})_i\}$ of $t_{f(p)}$
how is the direct deriv on a surfac defin the direct deriv of a field $f$ at a point $p$ in the direct $h$ is $ _h f(p) = (h \cdot )f(p)$ where $ $ is the vector deriv
what the weingarten equat given a surfac with tangent basi $\{\mb{x}_i\}$, shape oper $s$ and metric $g$ defin $[\sigma_{ij}] = \mb{x}_{ij} \cdot \mb{\hat n}$ then $\sigma = gs$
how are curv in $\mbb{r}^3$ character in differenti geometri curv are defin up to congruenc by their curvatur and torsion
how are surfac in $\mbb{r}^3$ character in differenti geometri surfac are defin up to congruenc by their metric and shape oper
what the darboux basi of a curv in a surfac $\{\mb{t}, \mb{\hat n} \mb{t}, \mb{\hat n} \}$ where $\mb{t}$ is the tangent to the curv $\mb{\hat n}$ is the unit normal to the surfac
how can the unit acceler of a curv in a surfac be decompos in the darboux basi $\mb{\dot t} = \kappa_n \mb{\hat n} + \kappa_g (\mb{\hat n} \mb{t})$ where $\kappa_n$ is the normal curvatur $\kappa_g$ is the geodes curvatur
how can the normal curvatur of a curv in a surfac be written in term of the shape oper $\kappa_n = s(\mb{t}) \cdot \mb{t}$
what the intuit interpret of the normal curvatur it the acceler need to `keep the curv in the surfac
what the intuit interpret of the geodes curvatur it the acceler of the curv within the surfac
geometrically, what are the eigenvector of the shape oper they'r the direct of maximum and minimum normal curvatur
what are the principl vector of a surfac they'r the field of direct of maxim and minim normal curvatur
in term of curvature, when is a curv in a surfac a geodes when $\kappa_g = 0$
how is the gaussian curvatur of a surfac defin $k = \det s$ where $s$ is the shape oper
intuitively, what the gauss map $\eta$ of a surfac it the map that take $\mb{p}$ to the point on the unit sphere correspond to $\mb{\hat n}_\mb{p}$
how can the gaussian curvatur be interpret in term of the gauss map let $a_\mb{x}$ be the area of a small disc on the surfac let $a_\eta$ be the area of the imag of that disc under the gauss map then as the radius of the disc shrink to 0, we get $a_\eta = k a_\mb{x}$
how doe the gaussian curvatur correspond to the metric in $\mbb{r}^3$, the metric defin the gaussian curvatur in higher dimensions, thing are more complic
what the differ between extrins and intrins curvatur intrins curvatur can be detect from within the surfac (ex: sphere) extrins curvatur cannot (ex: cylinder)
what anoth name for the metric in differenti geometri the first fundament
what the second fundament in geometri $(u, v) \mapsto u \cdot s(v)$ where $s$ is the shape oper of the surfac
how is the shape oper defin in a coordinate-independ way $s(h) = - _h i / i$ where $i$ is the unit pseudoscalar of the tangent space
what a linear program an optim problem where the constraint and object are all linear
what the convent for denot the object and constraint in \emph{convex optimization} $f_0$ is the object $f_i 0, \, i [1, m]$ are the inequ constraint $h_j = 0, \, j [1, p]$ are the equal constraint
what a convex optim problem a problem where the object and constraint are all convex
what the averag time complex of the simplex method $o(n^2m)$
what doe $\text{aff}(x)$ denot the affin hull of the set $x$, $\{\sum \theta_k x_k \sum \theta_k = 1 \}$
what the affin dimens of a set the dimens of the affin hull of a set
what $\text{relint}(c)$ denot in convex optim it the set of point in $c$ that lie in the interior of $\text{aff}(c)$
what the relat boundari of a set in convex optim $\text{cl} (c) \backslash \text{relint}(c)$
what $\text{cl}(c)$ denot in convex optim the closur of the set $c$
what $\text{conv}(x)$ denot in convex optim the convex hull of the set $x$, $\{\sum \theta_k x_k \sum \theta_k = 1, \theta_k 0 \}$
what the smallest convex set contain a set $c$ the convex hull of $c$
what anoth name for a nonneg homogen set a cone
when is a set $x$ a cone when for all $x x, \theta 0$, $\theta x$ is also in $x$
what the conic hull of a set $\{\sum \theta_k x_k \theta_k 0 \}$
what the smallest convex cone contain a set the conic hull of the set
what are halfspac the solut set to $a \cdot x b$ for some $a 0, b$
what tran ation take the unit sphere to a general ellipsoid the affin tran s
what a norm cone given a norm $ \cdot $, it $\{(x, t) \, \, x t\}$
geometrically, what a polyhedron it an intersect of halfspac and hyperplan
what a hyperplan the solut set of $a \cdot x = b$ for some $a 0, b$
what a polytop in some text it a bound polyhedron in other a polyhedron is a bound polytop
in \emph{convex optimization}, what doe $x eq y$ denot for vector $x, y$ componentwis $( )$
what a simplex it the convex hull of an affinely-independ set $\{v_i\}$
when is a set of point affin independ when $(x_i - x_0)$ are all linear independ
what the affin dimens of $k$ affinely-independ point $k-1$
what the descript of the $\ell_ fty$ unit ball in term of inequ it the intersect of the $2n$ constraint $ e_i \cdot x 1$ for each unit vector $e_i$
what the descript of the $\ell_ fty$ unit ball as a convex hull it the convex hull of the $2^n$ point with coordin $ 1$
what doe $s^n_+$ denot in convex optim the symmetr posit semidefinit matric on $\mbb{r}^n$
what doe $s^n_{++}$ denot in convex optim the symmetr posit definit matric on $\mbb{r}^n$
what doe $r_{++}$ denot convex optim the posit real
what doe $\mbb{r}_{+}$ denot convex optim the nonneg real
what common oper preserv convex on set intersect partial sum project
what the partial sum of $s_1, s_2 \mbb{r}^n \mbb{r}^m$ $\{(x, y_1 + y_2) (x, y_1) s_1, (x, y_2) s_2 \}$ where $x \mbb{r}^n$, $y_i \mbb{r}^m$
what anoth name for a linear fraction function a project
what the perspect function $p(z, t) = z/t$ where $z \mbb{r}^n$ and $t \mbb{r}_{++}$
what the pinhol camera correspond to the perspect function the sheet of paper is in the $x_1, x_2$ plane the origin is the apertur the imag plane is $x_3 = -1$
how are project relat to the perspect function they follow from compos the perspect function with an affin function, $p \circ g$
what a proper cone a cone $k$ which is convex close solid point
what doe it mean for a set $x$ to be solid in convex optim it has a nonempti interior
in convex optimization, what it mean for a cone to be point it a cone that contain no line in it entireti
how doe a proper cone $k$ defin a partial order $x eq_k y$ iff $x - y k$
how doe a proper cone $k$ defin a strict partial order $x _k y$ iff $x - y \text{int}(k)$
which cone produc the componentwis partial order on $\mbb{r}^n$ $k = \mbb{r}_+^n$
what an orthant the general of a quadrant to higher dimens
what are the minim element of a partial order $( eq)$ $x$ is minim iff it the onli element such that $x eq x$
given a proper cone $k$, what doe $x+k$ correspond to when interpret in term of $( eq_k)$ it the set of all $y$ such that $y eq_k x$
given a proper cone $k$, arithmet when is $x$ the minimum element of the order it induc when $x x + k$, where $x$ is the set of all element
in term of sets, arithmet when is $x$ a minim element with respect to a proper cone $k$ when $(x - k) s = \{x\}$
what the separ hyperplan theorem if $c, d$ are disjoint convex sets, then there an affin function $f$ such that $f(c)$ is nonneg and $f(d)$ is nonposit
doe the exist of a separ hyperplan impli that two convex set are disjoint no. stronger condit need to be appli to the set for this to hold
what an altern theorem in convex optim a theorem which establish that exact one of a set of possibl optim problem is solubl
what the theorem of altern for strict linear inequ exact one of $ax b$ and $a^t \lambda = 0, \quad \lambda^t b 0$ $\lambda eq 0, \quad \lambda 0$ is solubl
what $\text{bd}(c)$ denot in convex optim the boundari of $c$, $\text{bd}(c) = \text{cl}(c) \backslash \text{int}(c)$
what a support hyperplan of a set $x$ at a point $x_0$ it a hyperplan $a \cdot (x - x_0) = 0$ such that $a \cdot (x - x_0) 0$ for all $x x$
what the support hyperplan theorem for ani convex $c$ and $x_0 \text{bd}(c)$ there exist a support hyperplan to $c$ at $x_0$
what the partial convers to the support hyperplan theorem if $c$ is closed, solid and has a support hyperplan at everi point on it boundari then it convex
what the dual of a cone in convex optim $k^* = \{i y \cdot k 0 \}$
geometrically, how is the dual cone defin $k^*$ is the set of inward normal to halfspac which contain $k$
how do dual cone relat to subspac if $k$ is a subspace, then $k^*$ is it orthogon complement
what the dual cone to $s^n_+$ it self-dual
what the dual cone to $\mbb{r}^n_+$ it self-dual
what the dual of a norm cone on $\mbb{r}^n$ it the norm cone defin by the dual norm
what the dual norm in the context of convex optim $ u _* = \sup\{u \cdot x\, \, x 1\}$
what are the two guarante properti of the dual cone it close it convex
how doe the subset relat interact with cone dualiti $k_1 k_2 \impli k_2^* k_1^*$
if the cone $k$ is solid, what can we say about $k^*$ $k^*$ is point
when is $k^*$ guarante to be solid when $\text{cl}(k)$ is point
how can the cone $k^{**}$ be character it the closur of the convex hull of $k$
when will $k^{**} = k$ for a cone $k$ when $k$ is a proper cone
given a proper cone $k$, what the dual partial order it induc the partial order induc by $k^*$
in term of the dual cone, when will $x eq_k y$ exact when $k^* \cdot (x - y) 0$
in term of it dual, when will $x _k y$ for some proper cone $k$ exact when $\lambda \cdot (x - y) 0$ for all $\lambda \text{cl}(k^*), \, \lambda 0$
what anoth name for general inequ partial order
what the theorem of altern for linear strict general inequ exact one of $ax _k b$ and $a^t \lambda = 0, \quad \lambda^t b 0$ $\lambda eq_{k^*} 0, \quad \lambda 0$ is solubl
how can the minimum element of a cone-induc partial order be defin in term of the dual cone $x$ is the minimum element of $s$ wrt $( eq_k)$ iff $\lambda \cdot (z - x)$ is a strict support hyperplan of $s$ for all $\lambda _{k^*} 0$
what a strict support hyperplan of a set $s$ at a point $x_0$ it a support hyperplan of $s$ which intersect $s$ at onli $x_0$, if at all
in term of the dual cone, what a \emph{sufficient} condit for $x$ to be minim over $s$ wrt a cone-induc partial order if for some $\lambda _{k^*} 0$ the hyperplan $\lambda \cdot (z - x)$ is a strict support hyperplan of $s$ then $x$ is minim
in term of the dual cone, what a \emph{neccessary} condit for $x$ to be minim over a \emph{convex} set $s$ wrt a cone-induc partial order for ani minim element $x$ there exist a nonzero $\lambda eq_{k^*} 0$ such that $\lambda \cdot (z - x)$ is a strict support hyperplan of $s$ note the `nonzero and $( eq)$!
which function are both concav and convex exact the affin one
geometrically, what a convex function it a function $f$ with a convex domain and such that everi chord lie on or abov $f$
what a chord of a function a line segment between $(x, f(x))$ and $(y, f(y))$ for some $x, y$
what lemma is often use to test if an $n$-dimension function is convex $f$ is convex iff it restrict to ani straight line is convex
what the extended-valu extens of a convex function $\tild f(x) = f(x)$ if $x \text{dom}(f)$ $\tild f(x) = fty$ otherwis
what the usual use of the extended-valu extens of a function it simplifi a lot of s by obviat the need to specifi the domain
what the convent regard the notat for the extend valu extens of $f$ in \emph{convex optimization} it denot by $\tild f$ where necessari but when there no harm from the ambiguity, $f$ will be use
how the indic function for a set $c$ defin in \emph{convex optimization} it the $i_c$ which is zero on $c$ and $ fty$ elsewher
what the extended-valu extens of a concav function $\tild f(x) = f(x)$ if $x \text{dom}(f)$ $\tild f(x) = - fty$ otherwis
how can convex function be character by their first deriv convex function are exact those for which the domain is convex the first-ord taylor approxim global lower-bound it
intuitively, what so special about the character of convex function in term of their first deriv it say that local in ation (the first deriv at a point) can be use to make a global assert (that the first-ord taylor approxim there lower-bound the function everywhere)
how can convex function be character in term of their second deriv convex function are exact those for which the domain is convex the hessian is positive-semidefinit everywher
what the $\alpha$-sublevel set of a function $f$ the $c_\alpha$ of point $x$ such that $f(x) \alpha$
how do convex function relat to their sublevel set the sublevel set of a convex function are all convex the convers doe \emph{not} hold
what the hypograph of a function the set of point below a function surfac
what doe $\text{epi}(f)$ denot in convex optim the epigraph of the function
what holder inequ for all measur $f, g$ on a space $s$, $ fg _1 f _p g _q$ where $\frac{1}{p} + \frac{1}{q} = 1$ and $p, q [1, fty)$ and $ f _p = \left( t f ^p d\mu \right)^\frac{1}{p}$
what the arithmetic-geometr weight mean inequ $a^\theta b^{1-\theta} \theta a + (1-\theta)b$ where $a, b 0$ $\theta [0, 1]$
what common oper preserv convex on function conic combin pointwis maximum/supremum project
what are the condit on $g, h$ for $f = h \circ g$ to be convex $f$ is convex if $h$ is convex and $\tild h$ is nondecreas and $g$ is convex $\tild h$ is nonincreas and $g$ is concav
what are the condit on $g, h$ for $f = h \circ g$ to be concav $f$ is convex if $h$ is concav and $\tild h$ is nondecreas and $g$ is concav $\tild h$ is nonincreas and $g$ is convex
in the rule for the convex of composit functions, whi is the constraint on $\tild h$ import it guarante the domain of the composit will be convex
how do the rule for the convex of composit function extend to composit $h \circ g$ where $g$ is vector-valu they'r the same except the condit on the inner function $g$ are appli component-wis to $g_k$
when is $g(x) = f_{i c} f(x, y)$ a convex function when $c$ is convex $f$ is convex $g(x) - fty$ somewher
what the conjug function in convex analysi $f^*(y) = \sup_{x \text{dom}(f)} (y \cdot x - f(x))$ and it domain is wherev it is finit
when is the convex conjug itself convex always.
what fenchel inequ $f(x) + f^*(y) x \cdot y$ where $f^*$ is the convex conjug
when is $f^{**} = f$ for the convex conjug when $f$ is convex and close
what a close function a function which take close set to close set
in term of it epigraph, when is a real-valu $f$ a close function when it epigraph is close
what the legendr tran it the convex conjug of a differenti function
if $f$ is differentiable, how can the convex conjug be found to find $f^*(y)$ first find the $x^\prime$ such that $y = \nabla f(x^\prime)$ then $f^*(y) = x^\prime \cdot y - f(x^\prime)$
what the convex conjug of $g(x) = af(x) + b$ $g^*(y) = af^*(y/a) - b$
what the convex conjug of $g(x) = f(ax + b)$ for a square, nonsingular $a$ $g^*(y) = f^*(a^{-t} y) - a b \cdot y$
what the convex conjug of $g(u, v) = f_1(u) + f_2(v)$ if $f_1, f_2$ are convex $g^*(w, z) = f_1^*(w) + f_2^*(z)$
in term of sets, when is a function quasiconvex when it domain and all sublevel set are convex
what a quasilinear function one which is both quasiconcav and quasiconvex
what jensen inequ for a quasiconvex function $f$ $f(\theta x + (1-\theta)y) \max\{f(x), f(y)\}$ for ani $\theta [0, 1]$
when doe jensen inequ for quasiconvex function hold exact when the function in question is quasiconvex
what jensen inequ for a quasiconcav function $f$ $f(\theta x + (1-\theta)y) \min\{f(x), f(y)\}$ for ani $\theta [0, 1]$
what lemma is often use to test if an $n$-dimension function is quasiconvex $f$ is convex iff it restrict to ani straight line is quasiconvex
how can continu quasiconvex function on $\mbb{r}$ be character a continu $f$ is quasiconvex if and onli if it is monoton or there is a point such that it nonincreas to the left and nondecreas to the right
what the first order condit for quasiconvex function $f$ is quasiconvex iff it has a convex domain and $f(y) f(x) \impli \nabla f(x) (y-x) 0$ for all $x, y \text{dom}(f)$
what the second order necessari condit for a function on $\mbb{r}$ to be quasiconvex $f^\prime = 0 \impli f^{\prime\prime}(x) 0$
what the geometr interpret of the second-ord necessari condit for quasiconvex function wherev $\nabla f(x) = 0$, the hessian should be positive-semidefinit wherev $\nabla f(x) 0$, the hessian should be positive-semidefinit on the subspac orthogon to $\nabla f(x)$
what the geometr interpret of the second-ord suffici condit for quasiconvex function wherev $\nabla f(x) = 0$, the hessian should be positive-definit wherev $\nabla f(x) 0$, the hessian should be positive-definit on the subspac orthogon to $\nabla f(x)$
what a log-concav function it a function $f 0$ such that $\log f$ is concav
how can log-concav function be character without use logarithm a log-concav function is exact one such that $f(x)^\theta f(y)^{1-\theta} f(\theta x + (1-\theta)y)$ for all $x, y \text{dom}(f)$ and $\theta [0, 1]$
what are the second-ord condit that character a log-concav function $f$ is log-concav if and onli if $f \cdot \nabla^2 f eq \nabla f \otim \nabla f$
what doe $(\otimes)$ denot in vector algebra the outer product
how can the outer product $x \otim y$ be interpret $(x \otim y)w = x \langl y, w \rangle$
how do log-concav and log-convex interact with addit log-convex is preserv under addit log-concav is not though
how do log-convex and log-concav interact with multipl they'r preserv under multipl and posit scale
when is log-concav preserv by integr if $f \colon \mbb{r}^n \mbb{r}^m \rightarrow \mbb{r}$ is log-concav then $g(x) = t f(x, y) dy$ is log-concav
what are some common oper that preserv log-concav posit scale multipl integr convolution/correl
when is a function nondecreas with respect to a cone $k$ exact when $x eq_k y \impli f(x) f(y)$
when is a function increas with respect to a cone $k$ exact when $x eq_k y,\, x y \impli f(x) f(y)$
what a matrix monoton function a $f \colon s^n \rightarrow \mbb{r}$ which is monoton wrt $s^n_+$
what the first-ord condit in term of the dual cone for a function to be $k$-nondecreas a convex-domain'd $f$ is $k$-nondecreas iff $\nabla f(x) \text{cl}(k^*)$ note the $k^*$!
what the first-ord condit in term of the dual cone for a function to be $k$-increas a convex-domain'd $f$ is $k$-increas iff $\nabla f(x) k^*$
what a $k$-convex function a $f$ such that $f(\theta + (1-\theta)y) eq_k \theta f(x) + (1-\theta)f(y)$
in term of the dual cone, what a $k$-convex function a $f$ such that for everi $w k^*$, the real-valu $w \cdot f(x)$ is convex
what the first-ord condit that character a $k$-convex function $f$ is $k$-convex if and onli if it domain is convex and $f(y) - f(x) eq_k \nabla f(x) (y-x)$
what the definit of the optim valu of an optim problem it the infinium/supremum of the feasibl valu
what the convent for denot the optim valu in \emph{convex optimization} $p^*$, and it can take on $ fty$
what $x_\text{opt}$ denot in \emph{convex optimization} the set of all optim point
what doe it mean for the optim valu to be achiev there a point in the feasibl set at which the object match the optim valu
what a local optim point of an optim problem a point which is optim when the problem restrict to a $r$-radius disk around it
what an e constraint in an optim problem the inequ $f_i(x) 0$ is e at $x_0$ if $f_i(x_0) = 0$
what a feasibl problem in optim an optim problem with $f_0(x) = x$
when are two optim problem equival when a solut to one is easili tran ed to a solut of the other, and vice versa
how can the $f_0$ of an optim problem be tran ed in such a way that the optim point are preserv an increas $\psi_0$ appli to $f_0$
how can the $f_i$ of an optim problem be tran ed in such a way that the optim point are preserv a $\psi_i$ such that $\psi_i(u) 0 \iff u 0$ appli to $f_i$
how can the $h_j$ of an optim problem be tran ed in such a way that the optim point are preserv a $\psi_j$ such that $\psi_j(u) = 0 \iff u = 0$ appli to $h_j$
what the idea with slack variabl in optim problem $f_i(x) 0$ iff there a $s_i 0$ such that $f_i(x) + s_i = 0$
how are equal constraint usual elimin from an optim problem by parameter them and phrase the optim in term of the paramet
what an implicit constraint in an optim problem a constraint encod by the object domain
what are two other name for oracl optim model black-box model subroutin model
what an oracl optim model an optim problem where the object or some of the constraint are provid by an oracl
in standard , what a convex optim problem one with a convex object function convex inequ constraint function affin equal constraint function
in standard , what a quasiconvex optim problem one with a quasiconvex object function convex inequ constraint function affin equal constraint function
what a $\epsilon$-suboptim set in optim theori the set of point $x$ for whom $f_0(x)$ is $\epsilon$-clos to $p^*$
what an abstract convex optim problem in the literature, it a problem with a convex object and a convex feasibl set possibl despit it constraint be non-convex or non-affin
are local optim point of quasiconvex problem also global optim nope.
are local optim point of convex problem also global optim yup.
what the first-ord exact condit for an optim point of a convex problem $x$ is optim if and onli if it in the feasibl set and $ (y - x) \cdot \nabla f_0(x) 0$ for all $y$ in the feasibl set
what the complimentar condit vector $x, y$ satisfi the complimentar condit if their sparsiti pattern has a null intersect
what the epigraph of a convex optim problem minim $t$ subject to $f_0(x) - t 0$
when can a quasiconvex inequ constraint be replac with a convex one always, sinc there alway a convex function which has the sam $0$-sublevel set
what a first-ord suffici condit for a point to be optim in a quasiconvex optim problem a $x$ in the feasibl set is optim if $\nabla f_0(x) \cdot (y - x) 0$ for all $y$ in the feasibl set
what the convex-feas approach to solv quasiconvex problem find a convex $\phi_t$ such that $f_0(x) t$ iff $\phi_t(x) 0$ then if the quasiconvex problem augment with $\phi_t(x) = 0$ is feasible, it must be that $p^* t$ now appli binari search.
what a standard linear program one whose onli inequ constraint are $x eq 0$
what an inequality- linear program one without ani equal constraint
what a general linear program one with an affin objective, equal constraint and inequ constraint
what the chebyshev center of a polyhedron the center of the largest ball that can fit in the polyhedron
what a linear-fract program an optim problem with a project object affin inequ constraint function affin equal constraint function
what a general linear-fract program an optim problem with a pointwise-maximum of project object affin inequ constraint function affin equal constraint function
what a quadrat program an optim problem with quadrat object affin inequ constraint function affin equal constraint function
what a quadrat constrain quadrat program an optim problem with quadrat object quadrat inequ constraint function affin equal constraint function
what a qcqp in optim quadrat constrain quadrat program
what a second-ord cone constraint in optim a constraint of the $ ax+b _2 c \cdot x + d$ so name becaus it equival to requir that $(ax+b, c\cdot x + d)$ lie in the second-ord cone of $\mbb{r}^{n+1}$
what the second-ord cone it the norm cone given by the $\ell_2$ norm
what are three other name for the second-ord cone quadrat cone lorentz cone ice-cream cone
what a second-ord cone program in optim an optim problem with an affin object second-ord cone constraint for inequ constraint affin equal constraint function
what a socp in optim second-ord cone problem
what the idea in robust linear program allow the gradient of the linear inequ constraint to vari within an ellipsoid requir that the solut satisfi all possibl set of the gradient reduc to a socp
what a monomi $f(x) = c \prod x_i^{a_i}$
what a posynomi $f(x) = \sum c_k \prod x_i^{a_ik}$ where $c_k 0$
what a geometr program an optim program with a posynomi object posynomi inequ constraint ($f_i(x) 1$) monomi equal constraint ($h_i(x) = 0$)
what a geometr program in convex it a geometr program with the object rewritten as a $\text{lse}$ function the inequ constraint function rewritten as $\text{lse}$ function the equal constraint function rewritten as affin function
what a conic optim problem an optimzi problem with an affin object affin general $k$-inequ constraint affin equal constraint
what anoth name for a conic problem a cone program
what a semidefinit program a cone program over cone $s^k_+$
what a standard semidefinit program an optim problem over $s^n$ with object $\text{tr}(cx)$ inequ constraint $x eq 0$ equal constraint $\text{tr}(a_i x) = b_i$
what a lmi in optim theori linear matrix inequ
what a vector optim problem an optim problem with a vector-valu objective, minim over some partial order
what a convex vector optim problem it an optim problem with a $k$-convex object function convex inequ constraint affin equal constraint
in term of sets, when is a point $x^\star$ in a vector optim problem optim when it feasibl and $\mc{o} f_0(x^\star) + k$
when is a point $x$ in a vector optim problem pareto optim when it feasibl and $(f_0(x) - k) \mc{o} = \{ f_0(x) \}$
what doe $\mc{o}$ denot in optim theori the set of object valu obtain by the feasibl set
what doe $\mc{p}$ usual denot in optim theori the set of pareto-optim valu
how doe the set of pareto optim valu relat to the set of obtain valu in general $\mc{p} \mc{o} \text{bd}(\mc{o})$
what the program use in scalar in optim theori for some $\lambda k^*$, solv the linear program with object $\lambda \cdot f_0(x)$ the same constraint
what the use of scalar if $x$ is an optim point of the scalar program, then it is a pareto-optim point of the origin program
what the name of the $\lambda$ use in a scalar program the weight vector
can all pareto optim point be found via scalar nope
what the geometr interpret of scalar solv the scalar problem find a feasibl point on the boundari of $\mc{o}$ which has a support hyperplan of gradient $\lambda$
if a vector optim problem is convex, what can be said about the scalar problem it also convex
what the convers theorem regard pareto optim point and scalar theorem if the program is convex then for everi pareto optim point $x$, there a weight vector $\lambda \text{cl}(k^*)$ for which $x$ is a solut to the scalar problem correspond to $\lambda$
what the gap between the theorem regard scalar and pareto optim point on a convex problem, the scalar program deriv from $\lambda _{k^*} 0$ will alway find a pareto optim point but for some pareto optim point the weight vector that recov them may onli satisfi $\lambda eq_{k^*} 0$
given a convex problem, in what two way can \emph{all} the pareto-optim point be found by solv the scalar problem for $\lambda eq_{k^*} 0$ and test which s are pareto optim by find the limit of the pareto optim point correspond to $\lambda _{k^*} 0$
what a multicriterion optim problem a vector optim problem involv the cone $\mbb{r}^q_+$
when doe one point $x$ domin anoth $y$ in multicriterion optim when $f_{0i}(x) f_{0i}(y)$ for all $i$ and $f_{0j}(x) f_{0j}(y)$ for some $j$
what a strong tradeoff in multicriterion optim there a strong tradeoff at a pareto optim point if a larg increas in one object must be accept to achiev a small decreas in anoth
what a weak tradeoff in multicriterion optim there a weak tradeoff at a pareto optim point if a small increas in one object will enabl a larg decreas in anoth
intuitively, what a pareto optim point one for which there is no strict better altern
what an optim tradeoff surfac in multicriterion analysi it the surfac over creat by the set of pareto optim point be fed into $f = (f_{01}, f_{02}, \dotsc, f_{0q})$
when scalar a multicriterion problem, how can the weight vector be interpret $\lambda_i/\lambda_j$ is the relat weight of solut compon $i$ and solut compon $j$ ie decreas $f_{0i}$ by $1$ requir increas $f_{0j}$ by $\lambda_i/\lambda_j$
what the definit of the lagrangian for a standard- optim problem, $l \colon \mc{d} \mbb{r}^m \mbb{r}^p \rightarrow \mbb{r}$ $l(x, \lambda, \nu) = f_0(x) + \lambda \cdot f(x) + \nu \cdot h(x)$
what the dual function of an optim problem $g \colon \mbb{r}^m \mbb{r}^p \rightarrow \mbb{r}$ $g(\lambda, \nu) = f_x l(x, \lambda, \nu)$ where $l$ is the lagrangian
how doe the dual function of an optim problem bound the optim valu for ani $\lambda eq 0$ and ani $\nu$, $g(\lambda, \nu) p^\star$
what the dual feasibl region of an optim problem the $(\lambda, \nu)$ such that $\lambda eq 0$ and $g(\lambda, \nu) - fty$
how can the lagrangian be interpret as a linear approxim an optim problem can have it constraint fold into the object by use indic function ie $f_i(x) 0$ becom $i_{ 0}(f_i(x))$ which is zero on the nonposit and infinit elsewher the lagrangian term $\lambda_i f_i(x)$ is a linear approxim of this
how can the dual function of a standard- lp be written in term of the conjug $g(\lambda, \nu) = (-b \cdot \lambda - d \cdot \nu) - f_0^*(-a^t \lambda - c^t \nu)$
how can the domain of the dual function of a standard- lp be written in term of the conjug $\text{dom}(g) = \{(\lambda, \nu) -a^t \lambda - c^t \nu \text{dom}(f_0^*)\}$
what the lagrang dual problem associ with an optim problem maxim $g(\lambda, \nu)$ subject to $\lambda eq 0$ where $g$ is the dual function
what are the optim lagrang multipli for a problem the $(\lambda^\star, \nu^\star)$ which are optim for the dual problem
is the lagrang dual function $g(\lambda, \nu)$ concave, convex or neither concav sinc it the infinium of a famili of affin function on $(\lambda, \nu)$
what the dual problem to a standard lp maxim $-b \cdot \nu$ subject to $a^t \nu + c eq 0$
what the dual problem to a lp in inequ maxim $-b \cdot \lambda$ subject to $a^t \lambda + c = 0$ $\lambda eq 0$
what doe $d^\star$ denot in optim theori an optim valu of a dual problem
what weak dualiti in optim theori that $d^\star p^\star$
what the optim dualiti gap in optim theori $p^\star - d^\star$
what strong dualiti in optim theori $p^\star = d^\star$
what a constraint qualif in optim theori a condit on an optim problem that ensur strong dualiti hold
what slater theorem if the primal problem is convex and slater condit holds, then strong dualiti hold
what slater condit there exist a strict feasibl point in $\text{relint}(\mc{d})$
what doe $\mc{d}$ denot in optim theori the domain of the object of an optim problem
how can slater condit be refin if some of the inequ are linear for the linear constraints, it is onli necessari that $f_i(x) 0$ (rather than $f_i(x) 0$)
what a strict feasibl point one which satisfi the equal constraint and strict satisfi the inequ constraint
when doe strong dualiti fail for lps exact when both the primal and dual are infeas
what the trust region problem an optim problem where an approxim is optim over a restrict `trust region where the approxim is assum to be valid
what the set $\mc{g}$ when discuss the geometr interpret of weak dualiti the set of valu taken on by the object and constraint function $\mc{g} = \{(f(x), h(x), f_0(x)) x \mc{g}\}$
in term of the set $\mc{g}$ of valu obtain by the constraint and objective, how the primal optim valu defin $p^\star = f \{ t (u, v, t) \mc{g}, u eq 0, v = 0 \}$
in term of the set $\mc{g}$ of valu obtain by the constraint and objective, how the dual function defin $g(\lambda, \nu) = f \{ (\lambda, \nu, 1) \cdot (u, v, t) (u, v, t) \mc{g}\}$ ie $g$ is an infinium of support hyperplan to $\mc{g}$
what the set $\mc{a}$ use when discuss the geometr interpret of weak dualiti $\mc{a} = \mc{g} + \mbb{r}^m_+ \{0\}^p \mbb{r}_+$ ie the set of all valu obtain by the constraint and objective, or strict wors than them
in term of the set $\mc{a}$ of valu obtain by the constraint and object or wors than them, how the primal optim valu defin $p^\star = f \{ t (0, 0, t) \mc{a} \}$
in term of the set $\mc{a}$ of valu obtain by the constraint and object or wors than them, how the dual function defin $g(\lambda, \nu) = f \{ (\lambda, \nu ,1) \cdot (u, v, t) (u, v, t) \mc{a}\}$
geometrically, what the purpos of slater condit it ensur that the support hyperplan of $\mc{a}$ correspond to the optim $\lambda^\star$ isn't parallel to the object axi $t$
what the max-min inequ for ani $f$ and ani $w, z$, $\sup_z f_w f(w, z) f_w \sup_z f(w, z)$
how can weak dualiti be describ in term of the max-min inequ $p^* = f_x \sup_{\lambda eq 0} l(x, \lambda, \nu)$ $d^* = \sup_{\lambda eq 0} f_x l(x, \lambda, \nu)$ by the max-min inequality, $d^* p^*$
what the strong max-min properti $f$ has the strong max-min properti over $w, z$ if $\sup_z f_w f(w, z) = f_w \sup_z f(w, z)$
what anoth name for the strong max-min properti the saddle-point properti
what a saddle-point in optim theori $w_0, z_0$ is a saddl point of $f(w, z)$ over $w, z$ if $f(w_0, z_0)$ is a maximum of $f(w, z_0)$ $f(w_0, z_0)$ is a minimum of $f(w_0, z)$
how do saddl point relat to the strong max-min properti the strong max-min properti hold if and onli if a saddle-point exist
what the effect domain of a function the part of the domain where it finit
what a proxim point to a function $f$ at a point $v$ the valu of $\text{prox}_f(v)$
in the scale proxim operator, what the effect of $\lambda$ the larger the $\lambda$, the less weight is place on find a nearbi point
what a euclidean project onto a convex set $c$ $\pi_c(v) = \arg \min_{x c} x - v _2$ ie take each point to the nearest point in $c$
what doe the scale proxim oper approxim when $f$ is smooth and $\lambda$ is small $\text{prox}_{\lambda f} (v) v - \lambda \nabla f(v)$
what are the fix point of the proxim oper exact the minim of $f$
what the seper sum properti of the proxim oper if $f(x, y) = \phi(x) + \psi(y)$ $\text{prox}_f(v,w) = (\text{prox}_\phi(v), \text{prox}_\psi(w))$
what the proxim oper of $f(x) = \alpha \psi(x) + b$ for $\alpha 0$ $\text{prox}_{f}(v) = \text{prox}_{\alpha \psi}(v)$
what the proxim oper of $f(x) = \psi(qx)$, where $q$ is orthogon $\text{prox}_f(v) = q^{-1} \text{prox}_\psi(qv)$
what the proxim oper of $f(x) = \psi(\alpha x + b)$ where $\alpha 0$ let $l(x) = \alpha x + b$. then $\text{prox}_f(v) = (l^{-1} \circ \text{prox}_{\alpha^2 \psi} \circ l)(v)$
what the proxim oper of $f(x) = \psi(x) + a\cdot x + b$ $\text{prox}_f(v) = \text{prox}_\psi (v - a)$
what the proxim oper of $f(x) = \psi(x) + \frac{\rho}{2} x - a ^2_2$ $\text{prox}_f(v) = \text{prox}_{ \frac{1}{1+\rho} \psi} ( \frac{1}{1+\rho} v + \frac{ \rho }{1+\rho} a)$
what a contract map in a metric space a function on a metric space such that $d(f(x), f(y)) kd(x,y)$ for some constant $k$
what a non-expans map a contract map with a constant $k 1$
in hilbert space, what a firm nonexpans map a function on a hilbert space such that $ f(x+\delta) - f(x) ^2 \delta \cdot (f(x+\delta) - f(x))$
what the krasnoselskii-mann theorem $\alpha$-averag oper are guarante to converg to a fix point if one exist
what an $\alpha$-averag oper an oper of the $(1-\alpha)i + \alpha n$ for some non-expans oper $n$
how do $\alpha$-averag oper relat to firm nonexpans oper firm nonexpans oper are exact the $\frac{1}{2}$-averag oper
what nice about the class of $\alpha$-averag oper it close under composition.
what class of oper doe the proxim oper lie in the firm nonexpans one
what the proxim averag of a set of closed, proper convex function let $\frac{1}{m} \sum \text{prox}_{f_i} = \text{prox}_g$ then $g$ is the proxim averag of $\{f_i\}$
what the moreau decomposit $v = \text{prox}_f (v) + \text{prox}_{f^*}(v)$ where $f^*$ is the convex conjug
what the fenchel-moreau theorem $f = f^{**}$ if and onli if $f$ is convex and lower semi-continu
geometrically, what the convex biconjug $f^{**}$ the close convex hull of $f$
what the convex hull of a function the function that describ the convex hull of it epigraph
what a lower semi-continu function one whose sublevel set are all close
what'r two other name for the sublevel set of a function lower levelset trench
what the use of the moreau decomposit it let you find the proxim oper of a function in term of the proxim oper of it convex conjug
what the polar cone of a cone $k$ $k^\circ = -k^*$ where $k^*$ is the dual cone
what the infim convolut of two function $f, g$ $(f g)(v) = f_x (f(x) + g(v - x))$
what the moreau envelop of a function $f$ $m_f(v) = f(x) \left( \frac{1}{2} x _2^2\right)$
what anoth name for the moreau envelop moreau-yosida regular
what the use of the moreau envelop of a function $f$ irrespect of $f$, $m_f$ is continu differenti has a domain of $\mbb{r}^n$ has the same minim as $f$
what the dualiti properti of infini convolut $(f g)^* = f^* + g^*$ where $f^*$ is the convex conjug
what the conjug interpret of the moreau envelop $m_f(x) = (f^*(x) + \frac{1}{2} x ^2_2)^*$ so it regular the conjug then take the conjug again
what a strong convex function a function $f$ such that for some $m 0$, $m \delta ^2 \langl \nabla f(x) - \nabla f(x + \delta), \delta \rangle$
in general, when doe a closed, convex function $f$ have a smooth conjug when $f$ is strong convex
for convex functions, what the analyt relationship between the proxim oper and the moreau envelop $\text{prox}_f = \nabla m_{f^*}$
for convex functions, what the intuit relationship between the proxim oper and the moreau envelop the proxim oper give the point that minim the moreau envelop
what the resolv of an oper for oper $f$ and paramet $\lambda$, $(i + \lambda f)^{-1}$
how doe the proxim oper relat to the resolv $\text{prox}_{\lambda f} = (i + \lambda f)^{-1}$ where $ f$ is the subgradi
is the resolv a map or a function a function - it single-valu (despit be compos of oper on relations)
how can the convex conjug be found for smooth, convex function if $\nabla f = (\nabla g)^{-1}$ $f(x) + g(y) = x\cdot y$ then $f^* = g$
how can the proxim oper be interpret in term of gradient descent algorithm the proxim oper of the taylor approxim to $f$ at first order is a gradient descent step at second order is a levenberg-marquardt step
how can the proxim oper be interpret in term of trust region problem the proxim oper give the solut to the trust region problem for $f$ for some trust radius $\rho$
what the proxim oper of a quadrat function if $f = \frac{1}{2}x^t a x + b^t x + c$ for a positive-definit $a$, then $\text{prox}_f(v) = (i + a)^{-1}(v - b)$
when can the hessian be written as a sum of a rank-1 matrix and a diagon when $f$ has the $f(x) = \gamma(\psi_i(x_i)) + \phi_i(x_i)$ where the $\psi, \phi, \gamma$ are all real-valu
how fast can a system describ by a diagon matrix plus a rank $k$ matrix be solv $o(nk^2)$
what the convex conjug of the indic function for a convex set the support function for the same set
what the proxim oper of the $\ell_1$ norm the soft threshold function
what the convex conjug of a norm the indic for the unit ball of that norm
what the proxim oper for a norm $\text{prox}_\ell(v) = v - \pi_\mc{b}(v)$ where $\mc{b}$ is the unit ball
how can the dual norm be interpret geometr it the support function for the unit norm ball
what moreau ident $x = \text{prox}_f(x) + \text{prox}_{f^*}(x)$
how well do most naiv bay classifi work whi in general, nb classifi work surpris well; they work well when the error due to incorrect assum indeped among the featur balanc rough even in the posit and negat direct http://www.johndcook.com/blog/2015/03/26/clinical-trials-and-machine-learning/
what catastrop forget when a ml system is train on one task then another, it tend to forget about the first task
what the acronym succ stand for in scientif write simpl unexpect concret credibl emot stori
in term of scientif writing, what worth rememb about the ladder of abstract the top (big ideas) and bottom (data) are easi to communic it the one in the middl that are difficult
how is the credibl of a scientif work ed by establish a chain of credibl link data and previous work through to the conclus
what should the emot aspect of scientif write be it should be an appeal to the reader drive for \emph{understanding} rather than \emph{in ation}
how are stori modular a stori is creat by thread togeth multipl smaller stori
what the acronym ocar stand for in scientif write it give a structur for stori open challeng action resolut
what the acronym abdc stand for in scientif write it give a structur for stori action background develop climax end
what the ld acronym stand for in scientif write it a structur for stori lead develop
what the ldr acronym stand for in scientif write it a structur for stori lead develop resolut
how do the ocar/abdce/ldr/ld structur compar they'r suit for differ amount of expect patienc from the audienc ocar is the slowest to start; ld the fastest
what the hourglass analog of scientif write the open should be wide, deal with high-level issu the bodi should be exact, deal with technic issu the close should be as wide as the opening, deal with the same level of issu
what an easi test for whether you'r write a literatur review or an introduct literatur review have phrase of the `[smith 2003] found x`. it talk about what someon did. introduct have phrase of the `x occur [smith 2003]`. it talk about how someth works.
what the at of a good challeng in scientif write `to learn x, we did y'
what the acronym imrad stand for in scientif write introduct method result and discuss
what part of a imrad is cover by the `action compon of ocar method result and (most of) discuss
how should the method section of a paper be structur lead/develop sinc most reader won't care about the detail
what are the two principl to be kept in mind when structur the s and discuss section structur them howev best tell the stori distinguish \emph{what you found} from \emph{what you think}
what are the three kind of in ation found in a paper data inference, which are object deduct interpretation, which are subject deduct
whi it import to give s befor discuss them becaus then the stori be led by the charact as oppos to the charact have plot enforc on them
how should statist play into scientif write they should \emph{support} the story. they definit shouldn't \emph{be} the stori
how should the discuss section of a paper be structur as a mini-story, follow ocar or ldr
how is the resolut of a paper usual written as a single-paragraph walk back through ocar that place the ar of the paper in the origin context given by the oc
what should you be care about when propos new question in the resolut that you frame them as a \emph{new} knowledg gap rather than your failur to close an old one
should new idea be present in the resolut \emph{no}. new idea should appear earlier in the paper, and onli be referenc in the resolution.
what the ts-d model of paragraph in scientif write topic sentenc follow by develop
what are the two s of paragraph point first (ldr) point last (ocar)
what are the key step in write a good paragraph identifi who the stori is about identifi your point identifi where you should make your point (first or last)
from a structur perspective, what'r the compon of a simpl sentanc the topic, which come first (o) the middl bit (ca) the stress, which come last (r)
what the 2-3-1 rule in write the last part of a sentanc carri the greatest emphasi the first part carri the second greatest emphasi the middl bit carri the least emphasi
what the principl of subject-verb connect in scientif write a sentanc should open with an actor (the subject) and follow with an action (the verb) then move on to detail
what structur should long sentenc have ld
what the critic aspect of develop flow in write everi compon should share a topic with what came prior, as well as a topic with what come next. this is what separ stori from list
when are sentences-as-short-list accept in scientif write when they'r on the same topic it onli when you shift topic that you need to be concern about flow
give an exampl of the e voic jane call john jane, the actor, is the subject
give an exampl of the passiv voic john was call by jane john, the acted-on, is the subject
should the e or passiv voic be prefer activ
what'r the usual reason the passiv voic might be use it can improv flow by shift the perspect of a phrase. it can hide actor which aren't import
what are fuzzi verb verb that describ abstract relationship `occur', `affect', etc
give an exampl of a nomin verb. `we conduct an investig rather than `we investig
give an exampl of a nomin adjective. `the characterist of the condit is rather than `the condit is character by
what is nomin convert an adject or verb to a noun
structurally, what the best way to deal with term that \emph{might} be consid jargon by the audienc use the term in posit 3 of a 2-3-1 sentenc `program cell death, or apoptosis, is...
where should term unfamiliar to the audienc appear in a sentenc at the end, in the 1 posit of a 2-3-1 sentenc
give an exampl of a preposit phrase and the correspond compund noun. `rate of reaction vs `reaction rate
what a preposit in write of/in/on/etc
what the prefer altern to a preposit phrase a compound noun
what are the two circumst when a preposit phrase is better than a compound noun when the compound noun is long and unweildi (aka a noun cluster) when the phrase would put the noun you want in the stress posit
what are the five primari target when condens a piec of write redund in ation obvious in ation modifi metadiscours verbos
what should be your high-level strategi for condens write prune larg limb shake out dead leav
what an exampl of metadiscours `we found that
when should a piec of write be condens at the last draft - it easier to cut than it is to add
what the usual argument for keep filler word when write they can improv flow
what the acronym scfl stand for in scientif write it an approach to edit structur clariti flow languag
what the intent of the clariti step in scfl when edit ensur the idea in the write are clear and complet
when write science, where should you deal with weak in your work in the open (for scope limitations) in the method (for practic limitations) in the discuss (for analyt limitations) \emph{definitely} not in the resolut
where should weak be includ in the discuss section in the middl definit not at the start or end
what kind of tone should you use when discuss weak in your research concis and unapologet
what the messag box approach to write for the public keep the audienc in mind, explain the problem explain whi the problem matter explain the solut explain the benefit
what the pump lemma ani suffici long word $w$ in a regular languag $l$ can be broken into three part $w = xyz$ such that for all $n$, $w_n = xy^nz$ is in $l$ too
defin markov equival two direct acycl graph that entail the same condit independ i.e. have the same d-separ
what are the main element of the hdf5 data model dataset group attribut
what'r group in hdf5 hierarch contain
what are attribut in hdf5 metadata tag that can be attach to other object
what the biggest differ between numpi and hdf5pi slice hdf5pi slice return a copi rather than a view
what the command line tool to list the content of a hdf5 file \ {h5ls}
what the command line tool to dump out the content of a hdf5 file to the prompt \ {h5dump}
how can you creat a .hdf5 file in python onli if no other file of the same name exist \ {h5py.file(name, "w-")}
when open a file in python, what the \ {a} mode append mode
when open a file in python, what'r the \ {r+, w+, a+} mode they open the file for updat (reading/writing) but inherit the \ {r, w, a} rule on stream placement/fil creation/fil truncat
what the differ between the \ {r+, w+} file mode in python \ {w+} will creat a new file if it doesn't exist \ {w+} will overwrit the current file if it alreadi exist
what the differ between the \ {w, a} file mode in python write in \ {a} mode will alway appear at the end of the file
how should a h5pi file be open with a context manag
what the hdf5 \ {core} driver it store the file entir in memori
what the hdf5 \ {family} driver it store the file as chunks, with each chunk be limit to a certain size
what'r hdf5 driver they'r a low layer in the hdf5 librari that provid a uni interfac to differ kind of storag
what the \ {mpio} driver in hdf5 it support concurr access to a hdf5 file
what the user in hdf5 when open a file, the librari check for the hdf5 header at 512b, 1024b, 2048b, etc into the file. this mean the first 512/1024/2048b can be use for arbitrari user data
how can you modifi the user of a hdf5 file from python by specifi user _size when creat the hdf5 file then open the file as a normal python file
how do you usual insert a numpi array into a h5pi file h5py_file['key'] = numpy_array
how can you read an entir h5pi dataset into an array arr = dset[...]
how can you alloc a zero-initi h5pi dataset dset = h5py_file.create_dataset('key', shape, dtype)
how do you forc h5pi to write it buffer data to disk h5py_file.flush()
how can you fill an already-exist numpi array with data from a h5pi file dset.read_direct(arr)
when use create_dataset in h5py, how can you initi the dataset to a specif valu specifi \ {fillvalue}
how doe h5pi enforc dataset shape it onli care about the total size of the array; it happi to read a \ {(2, 3)} array into a \ {(3, 2)} dataset
is numpi row- or column-major row-major
what row-major storag when the rightmost index vari the fastest as you scan the array
what happen intern in h5pi when a slice from a dataset is assign from an empti np array of the right shape is allocated, which hdf5 read the appropri part of the dataset into
what step size are allow when slice a h5pi array posit one onli
what doe arr[()] return in numpi for 1d and higher arrays, it return the whole array for 0d arrays, it return the sole element as a scalar
what doe arr[...] return in numpi the whole array
how is boolean index implement intern in hdf5 by convert the array to list of coordin correspond to \ {true} valu
per ance-wise, which of boolean index and coordin list are faster in h5pi coordin lists, as h5pi will translat them into contigu subselect rather than just hunt for specif coordin as it doe with boolean index
what are the limit of slice with a coordin list in h5pi onli one axi at a time can use list slice the list must be strict increas
how doe numpi test whether one array can be broadcast to anoth shape by compar the shape of the two array right-to-left
what the easiest way to creat slice object in numpi sp.s_[idxs]
how can you specifi where data should be read from and into when use h5pi read_direct by specifi source_sel and dest_sel with numpi slice object
what the advantag of the write_direct method in h5pi there isn't any; it no faster than regular slice assign it exist onli for histor reason
how can you specifi the endian in a numpi datatyp " f4" is a little-endian 4-byte float " f4" is a big-endian 4-byte float
what maxshap in h5pi it a keyword that can be specifi to create_dataset that describ the maximum size the array can be reshap to
what happen if you don't specifi maxshap when creat an array in h5pi the array will not be resiz at all
how can you creat a h5pi array that can be reshap to an arbitrari size specifi one or more axi as none to maxshap
what the rank of a numpi array the number of axe
how can you add or remov axe from a h5pi dataset you can't; the rank is fix
what happen to exist data when a h5pi dataset is resiz if ani axi shrinks, the data in the miss region is discard if ani axi expands, the new locat are fill with the dataset fillvalu
what chunk storag in h5pi if a chunk shape is specified, when write data to disk h5pi will split the data into chunk of the given shape, flatten them, and write each flatten chunk as a contigu
how do you specifi a chunk shape in h5pi use the chunks=(64, 64) keyword of create_dataset
what auto-chunk in h5pi if you specifi chunks=tru when creat a dataset then h5pi will tri to keep the chunk rough cubic while obey certain size limit
rough what size should chunk be in h5pi more than 10kib (to limit b-tree overhead) less than 1mib (so that the chunk can be cached)
how doe h5pi track chunk dataset use a b-tree
what a kib a kibibyte, $2^{10}$ byte
what are filter in h5pi modul that a `pipelin between memori and disk when work with chunk dataset
what the deflat filter in h5pi anoth name for the gzip compress filter
how can you specifi gzip compress when creat a h5pi dataset by pass compression="gzip" to create_dataset
how doe h5pi lzf compress compar to gzip lzf is python-on lzf is much faster lzf has a lower compress ratio
what h5pi shuffl filter it rearrang data to exploit the inequ entropi distribut in most datatyp ex: with floats, most of the entropi is in the lower byte so shuffl pack all the first byte togeth (which can be effici compress as they'r almost all the same), then all the second bytes, and so on
what h5pi fletcher32 filter it checksum the data
what the root group in hdf5 the group name / which correspond to the file object
how do you creat a group in h5pi use the create_group method assign to a key contain / s
in general, how do h5pi group behav like dictionari
how can you get the file associ with a h5pi group object group.fil
how can you navig up the group hierarchi in h5pi group.par
what are link in h5pi group don't hold other object directly; they hold link to those object
how doe garbag collect in h5pi work refer count hard links; onc no hard link to an object remain, it destroy
how are hard link usual creat in h5pi by assign to a group key; group['key'] = obj or similar
how do you destroy link in h5pi del group['key']
when a h5pi object has more than one hard link to it, what doe obj.nam return one name; which one is indetermin
what the valu in repack in hdf5 hdf5 doesn't track free space creat by delet across file open/clos cycl so you can end up with a `hole of unus space which can be remov by use the h5repack tool
what a soft link in h5pi it a link that hold a name/path rather than a refer to an object
how do you creat a soft link in h5pi group['key'] = h5py.softlink('path')
what the intuit differ between hard and soft link hard link refer to an object soft link refer to a place
what happen if you tri to access a h5pi group member which is a broken softlink you'll get a \ {keyerror}
what happen if you tri to iter over a h5pi group which contain a broken softlink the broken softlink will evalu to none
what an extern link in h5pi a softlink to an object in anoth hdf5 file
are softlink valid on creation in h5pi nope
what are the two option keyword to group.get in h5pi getclass=tru return metadata about the object rather than the object itself getlink=tru return the link to the object rather than the object itself
what'r the require_group, require_dataset method in h5pi they'r like the create_group, create_dataset method but if a group/dataset alreadi exist with the same name and match the requirements, it'll be return instead
how can you replac the hard-link at a key in a h5pi group by first delet the exist hard link (h5pi won't silent overwrit the hard-link, to prevent accident data loss)
what order will element in a h5pi group be enumer in `nativ order, which is `whatev order is the fastest to get item out of the b-tree can look like alphabet ordering, but isn't
how can you use posix- path ( .. , etc) with h5pi by normal them first use posixpath.normpath
how can you test whether the descend of a h5pi group contain a certain path path in group
what the problem with x in d.keys() in python it unnecessarili construct a list of key in memori use x in d instead
how can you iter over the name of descend of a group in h5pi use the visitor pattern; group.visit(f) will call f on everi descend
how doe h5pi visit method respond to multipl link to the same object each object will be visit onli onc
how can you iter over the name and object themselv descend from a h5pi group group.visititems(f) will pass each name, item in turn to f
when use h5pi visit method, how can you interrupt the iter earli by return someth other than none which the visit call will then return itself
how do you deep-copi a h5pi group group.copy(origin_path, dest_path)
how can you test whether a h5pi group is part of a current open file bool(group)
how do you attach attribut to a h5pi object obj.attrs['key'] = valu
what type can be use in a h5pi attribut one with correspond hdf5 type - strings, numerics, etc
what the maximum size of an hdf5 attribut 64kib most of the time
how can you get a refer to a h5pi object obj.ref
given a h5pi reference, how do you get the associ object file[ref]
what the advantag of h5pi refer over softlink refer are independ the object name
what the kind of a numpi array arr.dtype.kind , which is a charact code indic whether the data boolean, integer, object, etc
what a region refer in h5pi it a refer to a specif slice of a dataset
how can you creat a region refer in h5pi arr.regionref[slice]
how can you get the dataset associ with a h5pi region refer file[region_ref] to get the slice that the refer was origin creat for, file[region_ref][region_ref]
what a name datatyp in h5pi it a numpi dtype store in a hdf5 file refer to which can be use to initi other hdf5 objects, ensur they all have the same type
what are dimens scale in h5pi dataset that describ how the indic into other dataset should be interpret
how do you creat a dimens scale in h5pi to creat a scale for an axi 0 on dset , creat a 1d dataset scale_x the same length as the axe in question convert the scale dataset to a scale use dset.dims.create_scal attach the scale to the axi use dset.dims[0].attach_scal
how mani dimens scale can a h5pi dataset axi have as mani as you want
what the most import thing to rememb when use h5pi with the multiprocess modul the state of the hdf5 librari is inherit from the parent process
how should program use h5pi when they'r multiprocess don't have ani file open when you invok multiprocess featur have subprocess open file as read-on after the process has been creat have subprocess write to differ files, then merg the file when finish
how do you launch a python mpi program mpiexec -n 4 python main.pi
what the main restrict on mpi parallel for hdf5 modif to file metadata must be done collect includ opening/clos a file, creat new hdf5 objects, and moving/copi hdf5 object
what it mean that an oper is collect in mpi that everi process make a call to the oper and multipl collect oper must be call in the same order in each process
what are the two flavour of io in mpi program collect io independ io
what happen in mpi when a call that should be collect isn't made by everyone, or is made out of order by someon it undefined.
what a common problem with mpi synchron and hdf5 if you write to a hdf5 file befor a barrier, and read from it afterward there no guarante that the write will actual have been commit befor the read goe ahead
what are the two usual way to deal with the interact between h5pi io oper and mpi synchron primat enabl the atom flag, which reduc per anc in return for greater consist don't pass data between process via the file - use mpi message-pass capabl
when a h5pi object has more than one parent, what will obj.par return whichev parent is consist with the path in obj.nam
what a communic in mpi it a group of process that can communic with eachoth
what a rank in mpi it the uniqu identifi of a process in a communic
what a tag in mpi it the uniqu identifi of a messag
how are python logger name structur hierarchically; the logger name foo.bar is a child of the logger foo
which python logger name should you usual use logging.getlogger(__name__) sinc this will automat organ the logger into the same hierarchi as the modul
what the advantag of hierarchically-structur logger in python by default, event to child logger will be propag to the parent logger
how do log level work in python messag with a level lower than a logger level will be ignor for a child logger, this mean the messag won't be pass to the parent for a root logger, this mean the messag won't be process
what are the two keyword argument to python logging.debug and similar method exc_info which if true will attach except in ation to the messag extra which can be use to add arbitrari object to the logrecord creat by the event
how do filter work with python log librari messag will be pass to each filter in turn; if ani one of them evalu to false, it'll discard the messag
how do handler work with python log librari messag are pass to all handler associ with a logger and propag to the logger parent (if it has ani and it set to propagate)
what are atter in python log librari they eat logrecord object and return string that can be output
what doe {} do in a python at string the first {} s to the first argument, the second {} s to the second, etc
how can you automat convert an argument x to a at string to str(x) {x!s}
how can you automat convert an argument x to a at string to repr(x) {0!r}
how can you get an attribut of an argument from a python at string {0.weight} will get the weight attribut of the argument
if a list is pass to a python at string, how can you get the 5th element {key[5]} will get the fifth element of the argument
what the first half of the layout for a python at string [[fill]align][sign][#]
what the second half of the layout for a python at string [0][width][,][.precision][type]
in the python at string layout, what doe fill signifi the charact to use to fill the space around the valu when it align
in the python at string layout, what can be use for align , to left-align , to right-align = , to pad a number with zero between sign and digit ^ , to center-align
in the python at string layout, what can be use for sign + , to a sign for both posit and negat number - , to a sign onli for negat number a space, to use a space for posit number and a - for negat
in the python at string layout, what doe # signifi it for integers; it'll prefix the number with 0b, 0o, 0x depend on whether it binary/octal/hexadecim
in the python at string layout, what doe , signifi whether a thousand separ should be use with larg number
in the python at string layout, what doe type signifi for numerics, it indic how the output should be at - as hex, with an exponent, as a percentage, etc
when estim $d$ paramet linear depend on $n$ measur that are subject to gaussian nois of standard deviat $\sigma$, what the expect residu error $\epsilon_\text{res} = \sigma \sqrt{1 - d/n}$
when estim $d$ paramet linear depend on $n$ measur that are subject to gaussian nois of standard deviat $\sigma$, what the expect estim error $\epsilon_\text{est} = \sigma \sqrt{d/n}$
what the differ between the residu error and the estim error residu error is the distanc of the estim valu from the measur (corrupted) valu estim error is the distanc of the estim valu from true valu
what an easi way to test whether an estim algorithm is incorrect take the some true measur $\bar x$ and distort them with some small nois to $x$ then estim them as $\hat x$ assum the model surfac near $\bar x$ is planar, the pythagorean ula $ x - \bar x ^2 x - \hat x ^2 + \bar x - \hat x ^2$ should hold
if $x$ has covari $\sigma$, what the first order approxim to the covari of $f(x)$ $j\sigma j^t$ where $j$ is the jacobian of $f$ at $\mbb{e}(x)$
given a paramet surfac defin by an affin $f$ and a random measur $x$ near that surface, what the precis of the paramet estim $j^t \lambda_x j$ where $\lambda_x$ is the precis of $x$
what the number of essenti paramet of a parameter the parameter $f$ defin a surfac in measur space and the number of essenti paramet is rank of $f$ jacobian
given a paramet surfac (possibl over-parameterized) defin by a general $f$ and a random measur near that surfac $x$, what the covari of the paramet estim let $j$ be the jacobian of $f$ and let $s_p$ be a surfac that local orthogon to the nullspac of $j$ then the covari of the paramet is $\sigma_p = (j^t \sigma^{-1}_x j)^+$ where $\sigma_x$ is the covari of $x$ $(^+)$ is the pseudo-invers
what the focal plane in a pinhol camera it the plane $z = f$ on which the imag fall
what anoth name for the imag plane the focal plane
what the camera center of a pinhol camera the point through which the project is made
what anoth name for the optic center of a pinhol camera the camera center
what the principl axi of a pinhol camera the line perpendicular to the focal plane that pass through the camera center
what the principl plane of a pinhol camera the plane parallel to the focal plane that pass through the camera center
what the principl point of a pinhol camera the point where the principl axi meet the focal plane
what doe $p$ denot when discuss pinhol camera the camera project matrix which take world point to imag point
what do $x, x$ denot when discuss pinhol camera $x$ is a world point $x$ is an imag point
how is the focal length repres in the camera project matrix $k = {bmatrix}f &amp; &amp; &amp; f &amp; &amp; &amp; 1 {bmatrix}$
how is an offset of the principl point from the imag origin repres in the camera calibr matrix if the principl point has coordin $(p_x, p_y)$, $k = {bmatrix}f &amp; &amp; p_x &amp; f &amp; p_i &amp; &amp; 1 {bmatrix}$
what the camera coordin frame the coordin system for world point creat by set the origin to the camera center set the $z$ axi to the principl axi with the imag plane sit at $z = f 0$ between the camera center and the world
how is a specif camera coordin frame relat to the world coordin frame a camera with center $\tild c$ rotat by $r$ is repres by $x_\text{cam} = [r -r\tild c] x$
how is a world point in the camera coordin frame relat to the correspond imag point $x = k[i 0] x_\text{cam}$
what doe $k$ denot when discuss pinhol camera the camera calibr matrix
how are non-squar pixel captur in the camera calibr matrix if the pixels-per-unit distanc are $(m_x, m_y)$, $k = {bmatrix}m_xf &amp; &amp; m_xp_x &amp; m_yf &amp; m_yp_i &amp; &amp; 1 {bmatrix}$
what the skew paramet of the camera calibr matrix $s = k_{12}$, which is almost alway zero
in term of the project matrix, what a finit camera one with a non-singular $3 3$ on the left
how can you identifi the camera center from a project matrix it the generat of the matrix right nullspac
what the geometr interpret of the column of a camera project matrix the first three column are the vanish point of the world coordin axe the last column is the imag of the world origin
what the geometr interpret of the row of a camera project matrix the first two row give the plane that map to the imag axe the last row give the principl plane
how can you deduc the principl axi from the camera project matrix if $m$ is the left $3 3$ submatrix of $p$ and $m^3$ is it third row then $\det(m)m^3$ is the principl axis, point toward the front of the camera
how can a camera project matrix be decompos into intern and extern paramet by appli the qr decomposit to the left $3 3$ submatrix $m$; the orthogon matrix is the rotat and the upper-triangular matrix is the calibr matrix
what should you be care about when choos an imag coordin system that you either have to preserv handed or deal with a flip of one of the axe
intuitively, how an affin camera creat by increas the focal length and the distanc from the subject at the same rate until you reach infin this suppress perspect effect
intuitively, what the effect on point of approxim a project camera with an affin camera point are move radial away from the principl point base on their depth
given a camera project matrix $p$ and a line in the imag $l$, what the set of world point that map to that line $p^tl$
given a camera project matrix $p$ and a conic in the imag $c$, what the set of world point that map to that conic the cone $q = p^t cp$
what the contour generat of a surfac $s$ with respect to a camera the set of point $\gamma \subset s$ such that ray from the camera center are tangent to the surfac
what the appar contour of a surfac $s$ with respect to a camera the imag $\gamma$ of the contour generat of the surfac
given a camera project matrix $p$ and a quadric in the world $q$, what the imag of the quadric outlin the conic $c$ given by $c^* = pq^* p^t$
what the effect on imag point of rotat the camera without translat it it appli a conjug rotat $x^\prime = krk^{-1}x$
geometrically, how do imag captur by the same camera with the same camera center relat they'r all relat by a plane project (think about how onli the focal plane can move if the center is fixed)
how can the calibr matrix be interpret as a tran ation it the tran ation that take a ray through the camera center $d$ to an imag point $x = kd$
how can you calcul the angl between two ray from a camera in term of their imag point $ \theta = \frac{\langl \pi_1, \pi_2 \rangle_{\omega}}{ \pi_1 _{\omega} \pi_2 _{\omega}}$
what the imag of the absolut conic in term of the calibr matrix $\omega = (kk^t)^{-1}$
if a line has direct $d$ in the camera coordin frame, what will it vanish point in the imag be $kd$
when calibr a camera from a singl image, what part of the scene are usual use as constraint orthogon line orthogon line and a plane planar object of known size
what the geometr interpret of a zero-skew constraint when calibr a camera it equival to say that the imag $x$ and $y$ axe should be orthogon
intuitively, what the calibr conic it the imag of a $45^\circ$ cone whose axi is the principl axi and whose apex is the camera center
what the use of the calibr conic it close relat to the iac, but has real-valu point
algebraically, what the calibr conic $c = k^{-t} {bmatrix} 1 &amp; &amp; &amp; 1 &amp; &amp; &amp; -1 {bmatrix} k^{-1}$
how can the calibr conic be interpret it an ellips whose center is the principl point while the axe length give the aspect ratio time the focal length and the axe direct reflect the skew
what the baselin of two camera the line join their camera center
what'r the epipolar plane of two camera they'r the pencil of plane whose axi is the baselin
what'r the epipol of two camera they'r the intersect of the baselin with the two imag plane
what an epipolar line the line at which an epipolar plane intersect an imag plane
what constraint doe epipolar geometri put on where a point in one imag can appear in the other if $x$ lie on the epipolar plane $\pi$ in one imag then the correspond point $x^\prime$ must lie on the same plane in the other imag
what the permut symbol $e_{i_1i_2\dotsb i_n}$ which is $0$ if the indic don't cover $1, \dotsc, n$ $+1$ if the indic an even permut $-1$ if the indic an odd permut
what doe $([a]_ )_{ik}$ denot in multiview geometri the skew-symmetr matrix ed from the element of $a$, $([a]_ )_{ik} = e_{ijk}a^j$
how can the determin of a matrix be written in term of the permut symbol $\det a = \frac{1}{n!} e^{i_1\dotsb i_n} e_{j_1\dotsb j_n} a_{i_1}^{j_1} \dotsb a^{j_n}_{i_n}$
how can a cross product be written use the permut symbol $a b = [a]_ b$
what the map repres by the fundament matrix of two camera it the map from point in one imag to epipolar line in the other
how can epipol be interpret as project the epipol in one camera is the imag of the other camera center
how can the fundament matrix of two camera be written in term of project matric $f = [e^\prime]_ p^\prime p^+$ where $e^\prime$ is the epipol in the second camera imag and $p^+$ is the pseudoinvers
how should relat chang betwen two variabl be measur alway use log(x/y) satisfi statist desiderata jstor.org.ezproxy.rice.edu/stable/2683905
for a camera at the world origin and anoth with rotat $r$ and translat $t$, what are the epipol $e = kr^tt$ $e^\prim = k^\prime t$
given the fundament matrix $f$ of camera $p, p^\prime$, what the fundament matrix of $p^\prime, p$ $f^t$
how can you identifi the epipol from the fundament matrix of two camera they'r the left and right null-vector of the matrix
for a camera at the world origin and anoth with rotat $r$ and translat $t$, what the fundament matrix $f = k^{\prime -t} [t]_ rk^{-1}$
geometrically, what'r the epipol of two camera relat by a pure translat they'r the imag in each view of the vanish point of the `rail that the scene seem to move along
what the horopt curv of two camera the set of world-point that have the same coordin in both imag
what the geometr interpret of the symmetr part of the fundament matrix it describ the steiner conic
what the geometr interpret of the skew-symmetr part of the fundament matrix it the point whose polar intersect the steiner conic at the two epipol
what the steiner conic of two camera it the imag of the horopt
if two pencil of line on a 2d plane have a homographi between them, what the of the set of intersect between correspond line a conic
how do you use the steiner conic to find the epipolar line $l^\prime$ in one imag of a point $x$ in the other the line through $e, x$ in the first imag meet the steiner conic at $e$ and a second point $x_c$ the line through $e^\prime, x_c$ in the second imag is the epipolar line $l^\prime$
given a pair of canon cameras, what the fundament matrix if $p = [i 0]$ and $p = [m m]$ then $f = [m]_ m$
what kind of bilinear do skew-symmetr matric correspond to an altern
what are the basic properti of an altern $x \cdot x = 0$ $x \cdot y = -y \cdot x$
how can a pair of canon project matric be recov from a fundament matrix $p = [i 0]$ and $p^\prime = [[e^\prime]_ f e^\prime]$ where the epipol solv $e^{\prim t}f = 0$
what a normal camera matrix the project matrix with the effect of the calibr matrix removed, $k^{-1}p$
intuitively, what the essenti matrix of two camera it the fundament matrix of two normal camera
algebraically, what the essenti matrix of two canon cameras, one at the origin and the other with translat $t$ and rotat $r$ $e = [t]_ r$
what'r normal coordin in the context of two-view geometri the normal coordin of an imag point $x$ are $\hat x = k^{-1}x$
how the essenti matrix character in term of pair of imag point $\hat x^{\prime t} e \hat x = 0$ where $\hat x$ are normal imag coordin
how are essenti matric character in term of singular valu a $3 3$ matrix is an essenti matrix iff two of it singular valu are equal the third is zero
how mani degre of freedom doe the essenti matrix have five; three from the translat three from the rotat minus one for the scale ambigu
given an essenti matrix and one camera at the origin, how mani possibl second camera are there four; two twist pairs, of which one pair with translat $+t$, one pair with translat $-t$
given an essenti matrix and one camera at the origin, how can the correct second camera be identifi from the four possibl choic by find the one in which the point is in front of both camera
which point can't be triangul from two imag the point on the baselin between the camera
what the 3d reconstruct ambigu when use multipl calibr camera a similar - a rotation, translat and scale
what the 3d reconstruct ambigu when use multipl uncalibr camera a project
what the 3d reconstruct ambigu when use multipl camera that are calibr except for their focal length a similar
what the 3d reconstruct ambigu when use multipl camera with an unknown but fix calibr that are relat by a translat an affin
intuitively, how do you refin a project reconstruct of a scene into an affin reconstruct by find the plane at infin (which mean find three point at infinity)
intuitively, what the infinit homographi it the map $x^\prime = h_ fti x$ that take a point $x$ in one imag to the point $x$ at which it ray meet the plane at infin and then project $x$ the point $x^\prime$ in the second imag
what kind of reconstruct doe knowledg of the infinit homographi between two camera enabl a affin reconstruct
what a simpl way to find a rank-defici matrix that close to a full-rank matrix take the svd and then zero the smallest eigenvalu
which point correspond are degener when tri to deduc the fundament matrix of two camera when the point and camera center all lie on a (possibl degenerate) rule quadric when the camera center are the same
what imag rectif find a tran ation of each imag so that the epipolar line run parallel to the $x$-axi (which mean that dispar between the imag are in the $y$-axi only) while minim the distort appli to the imag
how is a homographi between two camera induc by a plane by project point from the first camera onto the plane $\pi$ then project the point on the plane onto the second camera
algebraically, what the homographi induc by two camera and a plane if the project matric are $p = [i 0], p^\prime = [a a]$ and $\pi = (v 1)$ then $h = a - av^t$
algebraically, when doe a homographi $h$ repres a tran that correspond to a fundament matrix $f$ when $h^tf$ is skew-symmetr
given two cameras, how can the `depth of a point be calcul relat to a plane in a scene find the homographi $h$ that correspond to the plane then for ani pair of points, $x^\prime = hx + \rho e^\prime$ where $\rho$ give the `depth relat to the plane
how is the infinit homographi describ as a plane-induc homographi it the homographi $h_ fty$ induc by the plane at infin
how can the infinit homographi be thought as a map between imag point it map correspond vanish point between imag
what are the two ambigu that scene reconstruct from two affin camera suffer that project camera don't necker revers ambigu bas-relief ambigu
what the necker revers ambigu for affin camera reconstruct rotat and object and rotat it mirror in the opposit direct are ident from an affin camera perspect
what the bas relief ambigu for affin camera reconstruct rotat a distant object a small amount and rotat a close object a larg amount are indistinguish from an affin camera perspect
how are project matric usual denot when discuss the trifoc tensor $p = [i 0]$ $p^\prime = [a_j^i]$ $p^{\prime\prime} = [b_j^i]$
how mani degre of freedom doe the trifoc tensor have 18 11 degre for each camera matrix minus 15 for the project world frame
how is a 3-point repres in tensor notat as a vector $x = (x^1, x^2, x^3)$
how is a 3-line repres in tensor notat as a covector $l = (l_1, l_2, l_3)$
given the project matric for three camera where one is $p = [i 0]$, what the trifoc tensor $\mathcal{t}^{jk}_i = a^j_i b^k_4 - a^j_4 b_i^k$
what the valenc of the trifoc tensor 3, with two contravari indic and one covari
how is a matrix written in tensor notat $a$ is $[a_j^i]$
how are correspond line in the second and third view of a scene transfer to the first $l_i = l_j^\prime l_k^{\prime\prime} \mathcal{t}^{jk}_i$
how is a point transfer from the first view to the third view of a scene via a plane defin by the second view $x^{\prime\prim k } = l_j^\prime \mathcal{t}^{jk}_i x^i$
what the basic point-line-lin correspond encod by the trifoc tensor $l^\prime_j l^{\prime\prime}_k \mathcal{t}_i^{jk} x^i = 0$
what the line-line-lin correspond of the trifoc tensor $l^\prime_j l^{\prime\prime}_k \mathcal{t}_i^{jk} (l_r e^{ris}) = 0^s$
what the point-point-point correspond of the trifoc tensor $(x^{\prime j} e_{jpr}) (x^{\prime \prime k} e_{kqs})\mathcal{t}_i^{pq} x^i = 0_{rs}$ for the three view of a singl world point
what the trifoc plane the plane contain the three camera center
how are the epipol of a three-camera configur denot $e_{ij}$ is the project of camera $j$ center onto camera $i$
for which line and point do the trifoc correspond hold three view of the same point three view of ani line that pass through that point
what the gauss-newton matrix $g = j^tj$ where $j$ is the jacobian
what the general gauss-newton matrix if the loss for a system is $h(\theta) = \sum_{x, y} l(y, f(x, \theta))$, then $g = \frac{1}{ s } \sum_{x,y} j_f^t h_l j_f$ where $j_f$ is the jacobian of $f$ and $h$ is the hessian of $l$
what the intuit use of the general gauss-newton matrix it approxim the hessian for the system it a robust approxim when base on subset of the data
what the natur gradient of a system $\tild \nabla h = f^{-1} \nabla h$ where $\nabla h$ is the gradient and $f$ is the fisher in ation
what the intuit interpret of the natur gradient it the direct of steepest descent with respect to metric defin by the kl diverg from the current locat
how can adagrad optim be interpret as natur gradient method it estim the diagon of the fisher matrix and use that to estim natur gradient step
what the main theoret advantag to natur gradient optim the path it describ is invari to reparameter
what spike time depend plastic how neuron are suspect to learn; the synapt weight is reduc if there a post-synapt spike just befor a pre-synapt spike and it increas if there a post-synapt spike just after a pre-synapt spike
what a submodular function one such that $f(a) + f(b) f(a b) + f(a b)$ for all $a, b$ ed from some ground set $v$
what a modular function (wrt submodularity) one such that $f(a) + f(b) = f(a b) + f(a b)$ for all $a, b$ ed from some ground set $v$
how can submodular function captur fix cost ex: cost to drive to two store and buy a thing at each is greater than the cost of buy both at the same store
what the diminish return definit of submodular if $a b$ and $v b^\prime$ then for ani submodular function $f(a v) - f(a) f(b v) - f(b)$ ie the valu of ad $v$ to a set drop as the set get bigger
what a pseudo-boolean function $f: \mbb{b}^n \rightarrow \mbb{r}$
how do supermodular function relat to submodular function $f$ is supermodular iff $-f$ is submodular
how submodular relat to monoton it doesn't; submodular doesn't impli monoton nondecreasing monoton nondecreasing doesn't impli submodular
what a polymatroid function a function which is normal nondecreas submodular
what it mean for a function on set to be normal $f(\emptyset) = 0$
how can submodular function be decompos into a polymatroid function if $f$ is submodular $f = f_p - m$ where$f_p$ is polymatroid and $m$ is modular
what a subaddit function $f(a) + f(b) f(a b)$
how doe subaddit relat to polymatroid a polymatroid function is subaddit
what $f(j a)$ in the context of submodular the gain $f(j a) = f(a \{j\}) - f(a)$
how can submodular be state in term of gain for all $j$, $f(j a)$ is a nonincreas function of $a$
what a common class of submodular function defin in term of concav if $g_i$ are concav and $m_i$ are modular and nonneg then $f = \sum g_i \circ m_i$ is submodular
what a set system $(v, \mc{i})$ where $v$ is a finit ground set $\mc{i}$ is a set of subset
what an independ set system a set system $(v, \mc{i})$ which contain the empti set and is subclus
what a subclus set system a set system $(v, \mc{i})$ such that if $i \mc{i}$ all subset $j \subset i$ are $j \mc{i}$
what a matroid set system an independ system $(v, \mc{i})$ such that if $i, j \mc{i}$ and $ i = j + 1$ then there an $x i \backslash j$ such that $j x \mc{i}$
what the rank function of a matroid $r(a) = \max_{x \mc{i}} a x $
is the rank function of a matroid submodular yup
is entropi submodular yup, over subset of the variabl involv
what the principl submatrix of a matrix and a set of indic it the matrix $m_a$ ed by remov the row and column of all indic not in $a$ from $m$
what a chunk in lua a sequenc of statement
how can you enter multipl statement on one line in lua \ {a = 1 b = a*2} or altern \ {a = 1; b = a*2}
how larg can lua chunk be arbitrarili large; sever megabyt is just fine
how can you run the lua interpret in inter e mode \ {lua -i}
how can you run a file from a lua script \ {dofile('filename')}
how are single-lin comment denot in lua -- comment go here
how are comment denot in lua --[[ comment go here --]]
what a common trick to quick comment/uncom code in lua use comment and uncom by chang the open symbol to ---[[
what the null valu in lua nil
what happen if you assign nil to a global variabl in lua it effect delet the variable.
how can you write a lua file that can be execut as a script at the command line make the first line in the file #!/usr/local/bin/lua
how can you execut a string of lua code from the commandlin lua -e "print("hello world")"
how do you load a librari into the lua interpret at the commandlin lua -llibnam will load libnam
in lua inter e mode, how can you print the valu of an express = expr
where doe the lua interpret usual load it option from an environ variable, lua_init which it'll interpret either as a chunk of code or, if start with a @ , a filenam
how do lua script receiv argument from the commandlin as the global variabl arg
how do you get a type of a valu in lua type(v)
what are the eight basic type in lua nil boolean number string userdata function thread tabl
what are the falsi valu in lua fals nil
what'r lua numer type number , a 64 bit float
how can you write hexadecim constant in lua 0xa.bp2 where .b is the fraction part p2 is a binari expon
what an 8-bit clean system one that correct handl 8-bit charact encod
what the length oper in lua #x work on tabl and string
how can you denot a one-lin string liter in lua s "s"
how can you denot a multi-lin string liter in lua [[s]] [=[s]=] [==[s]==] etc
what lua \z escap sequenc it for use in string literals, and will skip all subsequ space when interpret the liter data = "abc\z def"
what the string concaten oper in lua s .. t
how doe lua coerc string into number it tri to intellig interpret the string as a number tri not to use this though
how can you interpret a string as a number in lua tonumber(s)
how can you convert an object to a string in lua tostring(n)
what a tabl in lua an associ array
how do you creat an empti tabl in lua {}
what are the two way to index into a tabl in lua t["attr"] t.attr
are tabl in lua zero or one index one-index by convent
what happen if you index in an uniniti element of a lua tabl it evalu to nil
what a sequenc in lua a tabl index with valu assign to all key in a rang $1$ through $k$
if t is a table, what #t in lua ani integ n such that t[n] is not nil t[n+1] is nil
how the modulo oper defin in lua a == math.floor(a/b)*b + a % b
what the not-equ oper in lua ~=
how doe equal test work in lua if the two valu have differ types, they'r not equal if the two valu have the same type, it deleg to the type
is string concaten in lua left or right associ right associ
what the tabl constructor in lua for list num = {'one', two', three'} will popul a tabl with key 1, 2, 3
what the tabl constructor in lua for key which are posit number or proper identifi tabl = {one=1, two=2, three=3} tabl = {1="one", 2="two", 3="three"}
how do you delet an element from a tabl in lua x[y] = nil
what the tabl constructor in lua for key which aren't proper identifi tabl = {["-"]="sub", ["+"]="add"}
where can you use semicolon in lua constructor anywher you'd use a comma convent is to use them to delimit differ part of the constructor
how do you assign to multipl variabl in a singl statement in lua a, b = 1, 2
in what order doe lua carri out a multipl assign it evalu all the valu first then carri out the assign so x, y = y, x swap the valu
what happen with multipl assign in lua when there are a differ number of variabl to valu the number of valu is adjust to the number of variabl with extra variabl be assign nil and extra valu be discard (go left-to-right)
how do you creat a local variabl in lua local x = 1
what the scope of a local variabl in lua the where it creat
what'r the three kind of s in lua the bodi of a control structur the bodi of a function a chunk
in the lua inter e compiler, how do you creat chunk of more than one line wrap the line in do , end
how'd you write a do-whil loop in lua repeat , until
how'd you write a multipl condit in lua if cond then s elseif t els u end
how'd you write a while loop in lua while cond do s end
what a use differ in the scope of a loop in lua from other languag the scope of a local variabl declar insid a repeat loop includ the condit
how'd you write a numer for loop in lua for i = start, stop, step do s end the step can be omitted; it'll be assum to be 1
how do you creat an infinit for loop in lua use math.hug as the upper bound
how can you escap a lua loop earli break
what the scope of the loop variabl in a lua numer for loop local (it implicit declar so)
what the effect of chang the loop variabl in a lua numer for loop unpredictable. don't do it.
how'd you write a generic for loop in lua for x in iter do s end
how can you iter over all key-valu pair in a lua tabl pairs(t)
how can you place a return in the middl of a in lua by surround the follow code with do , end
where can you place a return in lua at the end of a
how do you defin a label for use with goto in lua ::labelname::
where doe the scope of a local variabl end in lua on the last non-void statement of the where the variabl is defin
what the use of the non-void-stat rule with respect to goto in lua label are consid null statement so you can jump to a label that appear at the end of a , regardless of whether it defin a local variabl
what'r the rule about which label you can jump to use goto in lua the label must be visibl you can't jump out of a function you can't jump into the scope of a local variabl
when can a function call be made without parenthes in lua when the function take a singl argument that either a liter string or a tabl constructor
what the colon call convent in lua o:f(x) is the same as o.f(o, x)
how do you defin a function in lua function fname (args) s end
how do you defin default argument in lua have a paramet call x then in the function, defin x = x or default_x if no valu is pass to the parameter, it'll be nil by default, which mean it'll be assign default_x in the function
when will a lua function which return more than one valu actual return more than one valu when the call is the last express in a multipl assign argument to anoth function tabl constructor return statement of anoth function
what happen if a lua function that return more than one valu is not the last express in a list of express it'll onli return the first valu
how can you call a lua function with a runtime-depend number of argument use table.unpack
what the vararg express in lua ... , which can appear at the end of a paramet list and collect the extra argument ... can then be use insid the function as an express
how do you iter over the vararg of a lua function by convert them to a tabl use {...} if there are no nil argument table.pack(...) if there are
how do you creat a function which take name argument in lua have the function take a singl argument, a tabl and pass a tabl use the f{a=b, c=d} syntax
how do you creat a function in an express in lua f = function (x) return 2*x end
how can you creat a function and assign it to a key of a lua table, all in one line function t.x (x) return 2*x end will creat the function and store it in t["x"]
how do you write a recurs function in a restrict scope in lua defin the variabl befor the function local f f = function() return f() end in global scope, function f (args) syntax doe this implicit
doe lua optim tail call yup
what'r two common situat in lua where a tail call isn't a tail call if a function f end with return (g(x)) , becaus lua will have to adjust the number of s g(x) , becaus lua will have to discard the
how do you compil and run a string in lua load(s)()
what compil refer to in lua precompil
what scope doe lua load compil chunk with respect to global scope by default
how doe lua load interpret independ chunk as the bodi of an anonym function with a variabl number of argument which are pass via ...
what a binari chunk in lua a compil chunk of code
how can you direct load a function from an arbitrari c librari in lua f = package.loadlib(path, funcname)
how do you make an assert in lua assert(cond, message) which trigger if cond is fals and return the of cond
what are the two way to guard against nil return in lua surround the call with assert use f() or default_v
how do you catch error in lua surround the call in a protect call ok, msg = pcall(f())
how do you rais an error in lua error(x, level) where x is the error valu and level indic what depth in the call stack the fault is at
what xpcall in lua it a variant of pcall that take a messag handler that is call befor the stack unwind (destroy in ation) when xpcall return
what the debug.debug messag handler in lua it launch a prompt where the error was rais
how do you creat a coroutin in lua co = coroutine.create(f)
what'r the main differ between coroutin and thread coroutin are collaberative; onli one coroutin run at a time execut is onli surrend when the run thread request
how do you get the status of a coroutin in lua coroutine.status(co)
how do you yield in a lua coroutin coroutine.yield()
how do you resum a lua coroutin s = coroutine.resume(co, args)
what the normal state of a lua coroutin it the state where it not suspend or dead, but it not run either (typic becaus it resum anoth coroutine)
what the differ between symmetr and asymmetr coroutin symmetr coroutin both resum and yield with the same call asymmetr coroutin use differ call to resum and to yield
what'r semi-coroutin can refer to asymmetr coroutin or more commonly, coroutin that can onli yield from the bottom of their call stack (ie python generators)
what doe coroutine.wrap(f) do in lua it creat a coroutin and return a function that will resum it whenev it call
what a datafil in lua a plain lua sourc file though typic it consist of mani call to some small set of functions, like book{"authorname", "title", "publisher"}
what the deriv of $u(x)\cdot v(x)$ wrt a vector $x$ $\frac{d u}{d x} v + \frac{d v}{d x} u$
if $u(x)$ and $x$ are both vectors, is $\frac{du}{dx}$ a vector or a covector covector
what the geometr interpret of the jacobian it give the linear approxim to the function at a point
what the pseudoinvers of a diagon matrix $d^+_{ii} = 0$ if $d_{ii} = 0$ $d^+_{ii} = 1/d_{ii}$ otherwis
how can the pseudoinvers be defin in term of the svd $a^+ = v d^+ u^t$
how do you defin all (if any) solut to a linear system use pseudoinvers $x = a^+ b + (i - a^+a)w$ with the famili of solut parameter by $w$
what geometr properti do the normal equat $(a^ta)x = a^tb$ encod about a least-squar system that the residu in the least squar solut must be orthogon to the column of $a$
when is $a^ta$ invert when $a$ has full column rank
what'r the normal equat for a non-euclidean norm given by $c$ $(a^t c a) x = a^t cb$ equiv $a^tc (ax - b) = 0$
which $x$ minim $ ax $ subject to $ x = 1$ the right singular vector of $a$ correspond to the smallest singular valu
what the geometr interpret of the constraint $cx = 0$ that $x$ must lie in the orthogon complement to the rowspac of $c$
what $c^ $ denot in constrain minim the orthogon complement to the rowspac of $c$
how the orthogon complement of the rowspac of a matrix $c$ defin in term of the svd it the set of right singular vector correspond to zero singular valu
what approxim reduc newton iter to gradient descent approxim the hessian with $\lambda i$ where $\lambda$ is the invers step size
what'r the augment normal equat in levenberg-marquardt $(j^t j + \lambda i) x = -j^t\epsilon$ where $j$ is the jacobian of the object at the current point $\lambda$ is the damp factor $\epsilon$ is the residu
qualitively, how is the damp factor adjust in levenberg-marquardt if a step increas the error, then the damp factor is increas if a step decreas the error, then the damp factor is decreas
what kind of step will be taken when the damp factor is larg in levenberg-marquardt a short, gradient-descent-lik step
what the second, less common set of augment normal equat for levenberg-marquardt $(1 + \lambda)j^t j x = -j^t\epsilon$ where $j$ is the jacobian of the object at the current point and $\lambda$ is the damp factor $\epsilon$ is the residu
what the main conceptu differ between momentum optim method and second-ord method momentum method acceler along low-curvatur direct second order method acceler along low-curvatur directions, but alsod \emph{decelerate} along high curvatur direct
what the impossibl about vanish gradient in rnns ani rnn which can store one bit of in ation indefinit and is robust against nois will suffer from vanish gradient
what the theorem relat regular object to constrain problem if $f$ and $r$ are smooth then for everi local minimum $x^*$ of $\min f(x)$ s.t $r(x) \lambda$ there is a $\lambda^\prime$ such that $x^*$ is also a local minimum of $f(x) + \lambda^\prim r(\theta)$
what real-tim recurr learn an altern way to train rnns online, but requir quadrat space + time
what the valu in skip connect in rnns it reduc the number of nonlinear between a hidden unit and past in ation
what are the three gate that a lstm add to a rnns input gate output gate forget gate
how is the cell state in an lstm updat $m^\prime = m \odot f + i \odot i^g $ where $i$ is the input gate $f$ is the forget gate $i^g$ is a nonlinear'd combin of the previous timestep output and this timestep input
how is the output of a lstm memori cell defin $o \odot m$ where $o$ is the output gate $m$ is the memori
what do the gate in a lstm cell get as input the previous layer output the input to the current layer the previous cell state
how should the forget gate in a lstm be initi with large, posit bias (usual 5ish)
what the idea in truncat bptt run the network forward $k_1$ step to step $t$ then run bptt for $t$ down to $t-k_2$ step where $k_1 \gg k_2$
what the usual levenberg-marquardt heurist let $\rho = \frac{f(\theta + \delta) - f(\theta)}{q(\delta)}$, where $q$ is a local approxim to $f$ at $\theta$ if $\rho \frac{3}{4}$ then reduc damping, $\lambda \leftarrow \frac{2}{3} \lambda$ if $\rho \frac{1}{4}$ then increas damping, $\lambda \leftarrow \frac{3}{2} \lambda$
in \emph{numer optimization}, what do $\mc{e}$ and $\mc{i}$ denot the set of indic for the equal and inequ constraint respect
what the fundament lemma of the calculus of variat if $ t_\omega f(x) h(x) dx = 0$ for all compact, smooth $h$, then $f$ is ident zero
what the intuit use of the euler-lagrang equat a function satisfi the euler-lagrang equat is a stationari point of $s(q) = t_a^b l(t, q, q^\prime) dt$
what the definit of the covari basi in tensor calculus $\mb{z}_i = \frac{ \mb{r}(z)}{ z^i}$ where $z^i$ are the coordin and $\mb{r}$ is a point in space parameter by $z$
what are the contravari compon of a vector in tensor calculus the $v^i$ in $\mb{v} = v^i \mb{z}_i$ where $z_i$ the covari basi
how the covari metric tensor defin in term of the covari basi $z_{ij} = \mb{z}_i \cdot \mb{z}_j$
how is the dot product defin in term of the covari metric tensor $\mb{u} \cdot \mb{v} = z_{ij} u^i v^j$
how is the contravari metric tensor defin in term of the covari metric tensor $z^{ij}z_{jk} = \delta^i_k$
how is the contravari basi defin in term of the covari basi $\mb{z}^i = z^{ij} \mb{z}_j$
how is the length of a vector defin in term of the metric tensor $ \mb{v} = \sqrt{z_{ij}v^i v^j}$
how are the compon of a vector calcul in tensor calculus $v^i = \mb{v} \cdot \mb{z}^i$
what the definit of the christoffel symbol in term of the covari basi $\gamma^k_{ij} = \mb{z}^k \cdot \frac{ \mb{z}_i}{ z^j}$
what the definit of the christoffel symbol in term of the contravari basi $\gamma^k_{ij} = -\mb{z}_i \cdot \frac{ \mb{z}^k}{ z^j}$
how the christoffel symbol defin in term of the metric tensor $\gamma^k_{ij} = \frac{1}{2} z^{km} \left( \frac{ z_{mi}}{ z^j} + \frac{ z_{mj}}{ z^i} - \frac{ z_{ij}}{ z^m} \right)$
which indic is the christoffel symbol symmetr in the lower pair; $\gamma^k_{ij} = \gamma^k_{ji}$
what the intuit interpret of the christoffel symbol it give the coeffici of the decomposit of $\frac{ \mb{z}_i}{ z^j}$ in term of the covari basi $\mb{z}_k$
intuitively, what a variant in tensor calculus an object which can be construct use the same rule in ani coordin system
what an exampl of a tensor calculus object that isn't a variant the jacobian sinc it reli on two coordin system
when is a variant $t_i$ a covari tensor when it chang as $t_{i^\prime} = t_i j_{i^\prime}^i$ when you move from the unprim to the prime coordin system
how is the jacobian defin in tensor calculus for the jacobian from coordin $z^i$ to $z^{i^\prime}$, $j^i_{i^\prime} = \frac{ z^i}{ z^{i^\prime}}$ where $z^i$ denot a map in the numer and a coordin in the denomin
when is a variant a contravari tensor when it chang as $t^{i^\prime} = t^i j^{i^\prime}_i$ when you move from the unprim to the prime coordin system
how are the jacobian for invers tran s relat in tensor calculus $j^i_{i^\prime} j_j^{i^\prime} = \delta^i_j$ $j^{i^\prime}_i j^i_{j^\prime} = \delta^{i^\prime}_{j^\prime}$
how doe the covari basi chang under a chang of coordin $\mb{z}_{i^\prime} = \mb{z}_i j^i_{i^\prime}$
what the flavour of a tensor the number of covari and contravari compon
intuitively, what a quotient argument in tensor calculus `if $a = bc$ and $a, c$ are tensors, then $b$ must be too
what the order of a tensor the number of compon
is the kroneck delta an invari no! it geometr equival to the metric tensor
is the deriv of a scalar field a tensor yup
is the deriv of a covari tensor field also a tensor nope
which set of tran ation are tensor defin with respect to it depend usual it with respect to ani chang in coordin system, but smaller class of tran ation (like affin tran s) are possibl
at an intuit level, what the contract theorem the contract of a tensor is a tensor.
what anoth name for the flavour of a tensor it index signatur
which tensor are invari tensor of order zero.
what the gradient of a scalar field in tensor notat $\nabla f = \frac{ f}{ z^i} \mb{z}^i$
is $\frac{ f}{ z^i}$ for a scalar field $f$ covari or contravari covari
what the use of a dot $t^i_{\cdot j}$ in tensor calculus it spread out the indic so that they remain in order when juggl indic
how do you rais an index in tensor calculus by contract with the contravari metric tensor $t^j = t_i z^{ij}$
what do you get when you lower the contravari index of $\delta^i_j$ the covari metric tensor, $z_{ij}$
whi doe the metric tensor appear so rare in tensor calculus becaus it invis includ via index juggl
how is the diverg of a tensor $t_i$ defin in term of the covari deriv $\nabla_i t^i$
how is the laplacian of a scalar field $t$ defin in term of the covari deriv $\nabla^i \nabla_i t$
what the effect of the covari deriv on the flavour of a tensor it add a covari index
what doe the covari deriv reduc to in the euclidean basi old fashion partial differenti
what the definit of the covari deriv of a contravari tensor $v^i$ $\nabla_j v^i = \frac{ v^i}{ z^j} + \gamma^i_{jk}v^k$
is the christoffel symbol a tensor nope.
what the definit of the christoffel symbol with respect to a contravari basi $\gamma^k_{ij} = -\frac{ \mb{z}^k}{ z^j} \cdot \mb{z}_i$
what the definit of the covari deriv of a covari tensor $v_i$ $\nabla_j v_i = \frac{ v_i}{ z^j} - \gamma^m_{ij} v_m$
what the covari deriv of an invari $f$ $\nabla_i f = \frac{ f}{ z^i}$
how doe simpl input/output work in lua you set the current open stream with io.input / io.output you read/writ string with io.read / io.writ
what are the two normal way to serial number to disk in lua io.write(x) to write a decim number io.write(string. at('%a', x)) to write an exact hexadecim number
how do you normal serial a string in lua io.write(string. at('%q', s)) the at specifi %q will escap the string
what a metat in lua a tabl associ with a valu that defin various metamethod (behaviours) for that object
how are metat map to valu in lua tabl and userdata instanc can have a metat per instanc other type have one metat per type
how do you get or set the metat of a tabl in lua getmetat setmetat
how do you get or set the metat of a valu other than a tabl in lua you can't from lua; you have to go via c but you probabl shouldn't do it anyway
how do you give a tabl a (+) oper in lua creat a __add entri in the metat
how doe lua deal with a + b when a, b have differ metat it look for __add on a 's metat first, and use it if it find it it look for __add on b 's metat next, and use it if it find it els it throw an error
how do comparison metamethod work in lua partial order can be defin via __lt , __gt , etc but if onli __lt is available, lua will infer the other
what the restrict on the equal metamethod in lua if two valu are differ types, the equal metamethod won't even be call
how can you prevent third parti from chang the metat of a valu in lua by set __metat to a valu other than the actual metat getmetat will then return that value, and setmetat will throw an error
when the __index metamethod call in lua when a tabl is index into with a miss valu if __index is set, then __index(table, k) will be return
what are valid valu for the __index metamethod in lua a function which take a tabl and a key anoth table, in which the miss key will be look up (this is syntact sugar)
how do you usual implement inherit in lua by set __index to the parent tabl
how can you access a lua tabl without invok __index rawget(table, key)
what the __newindex metamethod in lua it like __index , but use for assign to empti posit in the tabl rather than get from them can be set to a function or anoth tabl
how can you assign to a tabl in lua without invok __newindex rawset(table, key, value)
which variabl in lua can be use to access the global environ _g
how doe lua handl global variabl behind the scene chunk are compil in the scope of an upvalu (local variable) _env ani free variabl name x is translat to _env.x by default, when the compil chunk is loaded, _env is set to the global environ
how do you import a modul in lua var = requir "module_name" requir take a string and return a tabl
what a modul in lua some code that can be load through requir which creat and return a tabl
where are load modul list in lua package.load
how do you forc requir to reload a modul by set package.loaded.modnam = nil
what doe requir do behind the scene in lua it check the packages.load tabl to see if the modul has alreadi been load els it search for a lua file match the modul name, then load amp; run the file els it search for a c file match the modul name and a luaopen_modnam function, and load amp; run it with loadlib either way it store the in packages.load and return the valu
what function name doe requir search for when look through a match c file it strip the modul name up to the first hyphen and convert . s to underscor requir "version-modname.submodname" look for luaopen_modname_submodnam
how doe requir decid where to search for lua modul it check the package.path string and replac everi in the string with the modul name this is necessari becaus ansi c doesn't have a concept of directori
how is the package.path variabl initi in lua in order of preference, it initi it from the environ variabl lua_path_5_2 the environ variabl lua_path a compiler-defin default
what doe ;; indic in a lua path variabl ;; will be replac by the compil default
how doe requir decid where to search for c modul it check the package.cpath variabl and replac everi in the string with the modul name this is necessari becaus ansi c doesn't have a concept of directori
what are searcher in lua they'r a way to hook into the machineri of requir
how do you write a lua modul write a lua script that creat a tabl put thing into the tabl return the tabl
what a packag in lua a tree of modul
what requir 's default behaviour when deal with name like parent.child in lua it convert the . into a path separ and check that path
what the syntact sugar for defin a method in lua function classname:methodname(args) self.test = arg it get a hidden argument self automat
how are class initi usual defin in lua function classname:new() o = {} setmetatable(o, self) self.__index = self return o
what a weak tabl in lua it a tabl which will be ignor by the garbag collector when decid what object are garbag
what the __mode of a weak tabl in lua it set which refer from the weak tabl are weak refer k mean key are weak v mean valu are weak kv mean both key and valu are weak
which valu can be weak referenc in lua onli object strings, number and co are exclud
what an ephemeron tabl in lua it a tabl with weak key and strong valu the refer to a valu is onli strong if there a strong refer to the associ key this mean the valu will be destroy even if it refer the key intern
what the metamethod for a final in lua __gc
when are object mark for final in lua when the object is given a metat with a non-nil __gc so don't make __gc non-nil after the metat has been assigned!
in lua, in what order will the final of cyclic referenc object be call revers of the order they were mark for final in
what lua bit-manipul librari bit32
what is the differ btwn multipl and multivari regress multipl appli to the number of predictor that enter the model (or equival the design matrix) with a singl outcom (y response), while multivari refer to a matrix of respons vector se: for variate', i would say this is a common way to refer to ani random variabl that follow a known or hypothes distribution, e.g. we speak of gaussian variat xi as a seri of observ drawn from a normal distribut (with paramet μ and σ2). in probabilist terms, we said that these are some random realize of x, with mathemat expect μ, and about 95% of them are expect to lie on the rang [μ−2σ;μ+2σ] . http://stats.stackexchange.com/questions/2358/explain-the-difference-between-multiple-regression-and-multivariate-regress
what lua regex librari it doesn't have one built in; it has it own string match standard instead
what the main differ between lua print and write print automat appli tostr to all it argument
what'r the possibl argument to io.read in lua *a read the whole file *l read upto the next newlin *l read upto and includ the next newlin *n read a number 5 read a string of at most 5 char
how doe lua io.read indic the end of the file it return nil
how doe the complet io model work in lua you get a file handl with f = io.open(path, mode) you call f:read and f:write
how do you get a handl to the current open input file in lua f = io.input()
how do you creat a temporari file in lua f = tmpfile() it'll be destroy when the program end
which function in lua handl time os.tim convert a tabl to unix time os.dat convert unix time to a tabl
how do you run a system command from a lua program os.execute(command_string)
what the problem with use lua debug librari it violat some otherwis use truth about lua program like not be abl to see local variabl from outsid their scope
what the main introspect function in lua debug.getinfo(x) where x is an object or a stack level
how doe stack level index work in lua 0 is the function itself (usual debug.getinfo ) 1 is the function that call debug.getinfo etc
how can you access the local variabl of a function from a lua debugg debug.getloc
how can you set the local variabl of a function from a debugg debug.setloc
how can you set the non-loc variabl of a function from the lua debugg debug.setupvalu
how do you inspect the work of a coroutin from the lua debugg all the introspect function in debug take an option coroutin argument
what are the four hook support by lua debugg c , a function call r , a function return l , a line i , a number of instruct
how do you set up a debug hook in lua debug.sethook(event, func)
what structur damp in rnns ad a term to the object $s(\theta) = d(h(\theta), h(\theta_0))$ that measur the distanc between the hidden state sequenc under the current parameters, $h(\theta_0)$, and the hidden state sequenc under some propos paramet $h(\theta)$
what are hessian-fre optim method optim method that tri to exploit curvatur in ation without explicit comput the hessian
what an isol minim a point of a function which is the \emph{only} local minim in some neighbourhood
what a model function in numer optim it a function $m$ that approxim the object $f$ within some small trust region
when is the newton step valid when the hessian exist and is positive-definit
what the secant equat $b^\prime(x^\prim - x) = \nabla f(x^\prime) - \nabla f(x)$
what the use of the secant equat it give in ation about the hessian without actual have to comput the hessian
how do nonlinear conjug gradient method compar to newton method newton method converg faster cg method don't need to store matric
what the general of the step in a conjug gradient method $p^\prime = -\nabla f(x) + \beta^\prim p$ where $\beta$ is such that $p, p^\prime$ are conjug
what doe it mean for an optim problem to be poor scale movement in one direct produc much larger chang in $f$ than in anoth direct
which of steepest descent and newton is scale-invari newton
when design a quasi-newton method, what a suffici condit on the hessian approxim $b$ for the step to be guarante to be in a descent direct $b$ is posit definit sinc then $p \cdot \nabla f(x) 0$
algebraically, what the armijo condit that in line-search optimization, the step size should satisfi $f(x + \alpha p) f(x) + c_1 \alpha p \cdot \nabla f$ for some small $c_1 10^{-4}$
intuitively, what the armijo condit it ensur that the step taken in line-search optim lead to a suffici larg reduct in object proport to the step size
what anoth name for the armijo condit the suffici decreas condit
algebraically, what the curvatur condit in numer optim in line search, the step size should satisfi $ c_2 p \cdot \nabla f(x) p \cdot \nabla f(x + \alpha p)$ where $c_2 [0.1, 0.9]$
intuitively, what the use of the curvatur condit it ensur that the gradient at the destin of the step is suffici larg compar to the gradient at the origin
what'r the wolf condit in numer optim the combin of the armijo and curvatur condit
what the differ between the wolf condit and the strong wolf condit the curvatur condit is replac by $ p \cdot \nabla f(x + \alpha p) c_2 p \cdot \nabla f(x) $ which ensur the step end near a stationari point
what the rate of return of a stock with price $s$ $k_s = \frac{s(1) - s(0)}{s(0)}$
what doe it mean for an asset to matur at time $t$ with face valu $v$ at time $t$, the asset is convert into valu $v$
what a portfolio in financi mathemat a linear combin over a set of asset
what the assumpt of divis in financi math that you can own a fraction of an asset this is almost achiev in the real world by deal with veri larg volum
what the assumpt of liquid in financi math that you can buy or sell arbitrarili larg quantiti of an asset
how is short sell reflect in a portfolio by a negat coeffici
how is a short posit in risk-fre asset usual realis by borrow cash with the interest rate be determin by the asset price
how is a short posit in stock usual realis by borrow the stock from someon and sell it at a later time, you buy the stock back and return it to the owner
what it mean to close a short posit in stock to repurchas the stock and return it to the owner
what the solvenc assumpt in financi math that the wealth of the investor must alway be nonneg
what an admiss portfolio one satisfi the solvenc condit
intuitively, what the no-arbitrag principl that you cannot make risk-fre profit with no initi invest
formally, what the no-arbitrag principl everi admiss portfolio with zero initi valu is certain to have zero valu in the futur
what the one-step binomi price model both the risk-fre asset $a$ and the riski asset $s$ start with valu $s(0) = a(0) = 1$ at time $1$, the risk-fre asset has valu $a(1)$ and the riski asset has valu $s^u$ with probabl $p$, and valu $s^d$ with probabl $1-p$
in the one-step binomi price model, what constraint doe no-arbitrag put on the riski asset price at time $1$ $s^d a(1) s^u$
what the classic definit of the risk of a portfolio the standard deviat of the rate of return
what a forward contract an agreement to buy or sell as riski asset at a futur \emph{deliveri date} for a present, fix \emph{forward price} the money is exchang at the deliveri date
what a long forward posit a forward contract to buy an asset
what a short forward posit a forward contract to sell an asset
in a portfolio, what the valu contribut by $z$ forward contract $z(s(1) - f)$ where $s(1)$ is the futur price and $f$ is the forward price
what are carri cost in financi math the cost of hold an asset storag cost for commodities, interest rate for currenc (negative) dividend for stock (negative)
intuitively, how doe the no-arbitrag principl determin the forward price of forward contract if the forward price is too high, you could borrow money to buy the asset now, then take a short forward position. when the contract closes, you can use the forward price to repay the loan. if the forward price is too low, you could sell short the asset, invest the money, then take a long forward position. when the contract closes, you can use the asset from the forward contract to close the short.
what a call option a contract give the holder the \emph{option} to buy an asset for a fix \emph{strik price} at a futur time
what a put option a contract give the holder the \emph{option} to sell an asset for a fix \emph{strik price} at a futur time
what'r the two main differ between forward contract and option forward contract commit you to buying/sel at a futur date forward contract requir no payment when they'r made
what it mean to replic an option it mean to construct a portfolio that will match the valu of the option at the exercis time
what it mean to price or valu an option it mean to calcul the cost of the replic portfolio at the current time
what a perpetu in financi math a sequenc of payment of a fix amount to be made at equal time interv and continu indefinit
what an annuiti in financi math a finit sequenc of payment of a fix amount to be made at equal time interv
algebraically, what the present valu factor of an annuiti at an interest rate of $r$, $pa(r, n) = \sum^n_1 \frac{1}{(1+r)^n}$ which can be simplifi use the geometr sum
what the sum of the first $n$ term in a geometr seri $\sum^n r^i = \frac{1- r^{n+1}}{1 - r}$
what the ula for continuously-compound interest $v(t) = e^{tr} p$ where $r$ is the interest rate and $t$ is the time
what the logarithm return of an asset $k(s, t) = \ln \frac{v(t)}{v(s)}$
when are two compound interest method equival when the growth factor over one time unit are the same
what a $\sigma$-algebra a $\mc{f} \subset \mc{p}(\omega)$ which contain the empti set is close under complement is close under union
what a probabl measur a function $\mb{p} \colon \mc{f} \rightarrow [0, 1]$ on a $\sigma$-algebra $\mc{f}$ such that $\mb{p}(\omega) = 1$ $\mb{p}( ^n_1 a_n) = \sum^n_1 \mb{p}(a_n)$
what a probabl space a tripl $(\omega, \mc{f}, \mbb{p})$ $\omega$, the ground set $\mc{f}$, the $\sigma$-algebra $\mbb{p}$, the probabl measur
what the lebesgu measur $\mc{l}$ it the probabl measur on $[0, 1]$ that set the probabl of $[a, b]$ to $b-a$
what a borel algebra a $\sigma$-algebra generat by open set
what doe $\mc{b}[0, 1]$ denot in measur theori the borel set on $[0, 1]$
what are the borel set of a borel algebra the set of set in the algebra
defin generat model a model for random generat observable-data valu w: typic given some hidden parameters. it specifi a joint probabl distribut over observ and label sequences.
what $d$ usual repres when discuss distribut algorithm the diamet of the network
what the basic, al framework for model the evolut of a distribut algorithm in term of configur $\mc{c}$, a set of configur $(\rightarrow)$, a transit relationship on $\mc{c}$ $\mc{i}$, a set of initi configur
what the differ between a state and a configur when discuss distribut algorithm a state is local to one process a configur is global
how are event relat to configur transit in distribut algorithm everi transit is associ with a send, receive, or an intern event of some process
when is a process an initi in term of a distribut algorithm when the first event in that process is a send or an intern event
formally, when is a distribut algorithm central when there is exact one initi
what an assert in term of distribut algorithm a predic on the configur
when is an assert a safeti properti in the context of distribut algorithm if it true in everi reachabl configur
what an invari in the context of distribut algorithm an assert that true in all initi configur and is preserv by transit
what an execut in the context of distribut algorithm a sequenc of configur that start in a valid initi state and such that each sequenti pair of configur is relat by a transit
what a live properti in the context of distribut algorithm an assert which eventu hold for all execut
what a fair execut in the context of distribut algorithm either a finit execut or an infinit execut where everi event that can occur in an infinit number of configurations, occur an infinit number of time (this exclud thing like an infinit all-head sequenc of coin tosses)
what the causal order of a distribut algorithm $a b$ if and onli if $a$ must happen befor $b$
how is the causal order of a distribut algorithm generat it the smallest order such that event within each process are total order send event preced correspond receiv event transit hold
what are concurr event event which are not causal order
what a comput in the context of distribut algorithm it the set of all causally-valid order of event in the algorithm
what a logic clock a real-valu function $c$ such that for all causal order event $a b$ $c(a) c(b)$
what lamport clock the logic clock that map an event $a$ to the length of the longest causal chain termin at $a$
how is lamport clock implement let $k$ be the clock valu of the previous event in the process if $a$ is an intern or send event, $lc(a) = k+1$ if $a$ is a receiv event and $b$ is the correspond send event, $lc(a) = \max\{k, lc(b)\} + 1$
what a common issu with lamport clock it can falsli order concurr events; ie there may be concurr $a, b$ such that $lc(a) lc(b)$
what the fundament properti of the vector clock $vc(a) vc(b)$ exact when $a b$
how is the vector clock implement for $n$ processes, $vc(a) = (k_0, \dotsc, k_{n-1})$ where $k_i$ is the largest lamport clock valu of an event $a^i$ in process $i$ such that $a^i a$
what are basic and control algorithm in distribut comput the control algorithm provid some specif servic (like collect garbage) for the basic algorithm
what a consist snapshot in distribut algorithm a represent of a configur from the same comput even though that specif configur may never have occur
in term of events, when is a snapshot consist when for each presnapshot event $b$, all $a b$ are also presnapshot and a basic messag is includ in the snapshot if and onli if the send event is presnapshot and the receiv event is postsnapshot
in distribut snapshot algorithms, what a presnapshot event an event that happen befor the local snapshot of the process is taken
what the use of the chandy-lamport algorithm to snapshot system with fifo channel
in the chandy-lamport algorithm, how is the state of channel $pq$ assess it all the messag receiv by $q$ after it get the snapshot marker from someone, but befor it get the snapshot marker from $p$
at a high level, how the chandy-lamport algorithm work a marker messag is propag through the network when a process get it first marker message, it take a local snapshot and propag the marker messag to all it neighbour it termin at a process when the marker has been receiv from all neighbour
what are the two kind of messag in the lai-yang algorithm a basic messag with true or fals append a control messag sent after the local snapshot to everi neighbour
in the lai-yang algorithm, when doe a process append fals to basic messag and when doe it append true fals until it made it local snapshot true afterward
how is the channel state comput in the lai-yang algorithm the state of $pq$ is all the messag receiv by $q$ that end fals after $q$ has taken it local snapshot
what the content of the control messag in the lai-yang algorithm how mani fals messag $p$ has sent into channel $pq$
when doe a process take a snapshot in the lai-yang algorithm when it receiv a control messag from it neighbour
what the purpos of the lai-yang algorithm to take snapshot of system without need fifo channel
what are the three typic learn problem concern map input to label sequenc sequenc classif segment classif tempor classif
what the sequenc classif problem assign the whole sequenc to a class
what the segment classif problem classifi each predefin segment of a sequenc
what the tempor classif problem segment a sequenc and classifi each segment
how is per anc on tempor classif problem usual evalu use the edit distanc between the estim and the actual label
what the high level idea in a bidirect rnn run a rnn through a sequenc then anoth through the revers sequenc and then feed the ation on each element from both rnns into a third, feedforward network, which produc the output
what the sequenti jacobian of a rnn $j^{tt^\prime}_{ki} = \frac{ y_k^t}{ x_i^{t^\prime}}$ where $y_k^t$ is the $k$th output at time $t$ and $x_i^{t^\prime}$ is the $i$th input at time $t^\prime$
what the mel scale a perceptu scale of acoust frequenc
when implement neural networks, how can the backpropag implement be check by perturb each weight by $ 10^{-5}$ and use the central differ ula to numer calcul the gradient
how is the complex cepstrum of a signal defin $\mc{f}^{-1} \circ \ln \circ \mc{f}$
what the use analyt properti of the complex cepstrum of a signal if a signal is generat as $c(\omega) = a(\omega)b(\omega)$, the cepstrum of $c$ is the sum of the cepstrum of $a, b$
in grave work on speech labelling, what were the nonlinear of the lstm $[0, 1]$ sigmoid on the gate $[-2, 2]$ sigmoid on the input to and output from the cell
how a connectionist tempor classif layer implement augment the output languag with a blank symbol have the network predict a `path over the augment languag map path onto label by remov repeated, sequenti symbols, then blank symbol sum over probabl of augment sequenc to get probabl of origin sequenc
what the advantag of use blank symbol in connectionist tempor classif layer it stop the network from be burden with have to predict a specif symbol dure paus in the data
what'r the two gate in a gate recurr unit $r$, the reset gate $z$, the updat gate
what do the gate in a gate recurr unit get as input current input previous state
what the effect of the reset gate in a gate recurr unit when it $ 0$, the previous hidden state is ignor when comput the potenti updat to the hidden state
what the effect of the updat gate in a gate recurr unit when it $ 1$, the previous hidden state is preserv when it $ 0$, the hidden state is recalcul on the basi of the previous hidden state and the input
what a gate feedback rnn a rnn with stack layer where each layer connect to the one abov it and a sigmoid $g^{i \rightarrow j}$ gate the (additional) transfer of in ation from layer $i$ to layer $j$
what curriculum learn give a model simpl exampl first, then increas difficult one
defin onlin learn (ml) expos your learner to data one data point at a time https://scottlocklin.wordpress.com/2014/07/22/neglected-machine-learning-ideas/
what the covari deriv of $t_j^i$ $\nabla_k t^i_j = \frac{ t^i_j}{ z^k} + \gamma^i_{km}t^m_j - \gamma^m_{kj}t^i_m$ ie contravari compon have a posit term and covari compon have a negat term
when doe the covari deriv commut in euclidean space
what the product rule for the covari deriv $\nabla_k (t^i u_j) = t^i (\nabla_k u_j) + (\nabla_k t^i) u_j$
what the metrinil properti of the covari deriv the co/contravari metric and basi vector vanish under the covari deriv
what the algebra consequ of the metrinil properti of the covari deriv metric tensor (and so free indices) can be push into or pull out of a covari deriv
how is the contravari deriv defin $\nabla^i = z^{ij}\nabla_j$
how is the riemann curvatur tensor defin in term of the covari deriv $r_{mij}^k t^m = (\nabla_i \nabla_j - \nabla_j \nabla_i) t^k$
how the riemann curvatur tensor defin in term of the christoffel symbol $r_{mij}^k = \frac{ \gamma^k_{jm}}{ z^i} - \frac{ \gamma^k_{im}}{ z^j} + \gamma^k_{in}\gamma^n_{jm} - \gamma^k_{jn}\gamma^n_{im}$
when doe the riemann curvatur tensor vanish in euclidean space
what anoth name for the riemann curvatur tensor the riemann-christoffel tensor
what $e_{i_1 \dotsb i_n}e^{i_1 \dotsb i_n}$ $e_{i_1 \dotsb i_n}e^{i_1 \dotsb i_n}= n!$
how the 3-dim delta system defin in term of the permut symbol $\delta^{ijk}_{rst} = e^{ijk} e_{rst}$
how can the determin of a 3-dimension matrix be written in term of the delta system $a = \frac{1}{3!} \delta^{ijk}_{rst} a_r^i a_j^ a_k^t$
what the intuit definit of a delta system it $+1$ when the upper and lower indic are an even permut of eachoth $-1$ when they'r an odd permut and zero otherwis
how is the cofactor of a 3 dimension matrix defin in tensor notat $a^i_r = \frac{1}{2!} \delta^{ijk}_{rst} a^s_j a^t_k$ ie the definit of the determin with $a^i_r$ remov
how is the determin of a matrix $a^i_j$ common written in tensor calculus $a$
what the matrix invers properti of the cofactor $a^i_r a^r_m = a \delta^i_m$
what the volum element in tensor calculus $\sqrt{z}$ where $z$ is the determin of the covari metric tensor
how doe the determin $z$ of the covari metric tensor tran under a coordin chang as a weight-2 relat invari
what the voss-weyl ula $\nabla_i t^i = \frac{1}{\sqrt{z}} \frac{ }{ z^i} \left(\sqrt{z} t^i\right)$ where $\sqrt{z}$ is the volum element
when is $t^i_j$ a relat tensor of weight $m$ when it tran s as $t^{i^\prime}_{j^\prime} = j^m t^i_j j^i_{i^\prime} j^j_{j^\prime}$ where $j$ is the determin of the jacobian
what an absolut tensor in tensor calculus a relat tensor of weight zero
if $a^i, b_j$ are relat tensor of weight $m, n$, what the relat weight of $a^ib_i$ $m+n$
what the relat weight of the covari levi-civita symbol $e_{ijk}$ when interpret as a relat tensor $-1$
when interpret as a relat tensor, what the relat weight of the determin of a relat covari tensor $a_{ij}$ of weight $m$ $2 + mn$, where $n$ is the dimens
what the definit of the covari levi-civita symbol $\varepsilon_{ijk} = \sqrt{z} e_{ijk}$
in what way do the levi-civita symbol chang under a tran under an orient preserv tran , they'r absolut tensor
how can the covari permut symbol be relat to the contravari permut symbol through index juggl it can't.
what anoth name for a hermitian matrix a self-adjoint matrix
what the complex analogu of symmetr matric self-adjoint matric
what a normal matrix one such that $a^* a = aa^*$
which class of matric are diagonaliz the normal matric
in term of it eigenvalues, when is a normal matrix self-adjoint exact when the eigenvalu are all real
do the normal matric a group nope. not over addit or multipl at least
what the complex equival of an orthogon matrix a unitari matrix
what a unitari matrix one such that $u^*u = i$ ie the adjoint is the invers
what the complex number analogu of skew-hermitian matrici pure imaginari number
at a high level, how doe the isomap algorithm work find the neighbour of each point construct a neighbourhood graph comput the distanc between all pair of node use mds to emb the distanc
what the use of the isomap method it a nonlinear dimension reduct method
what the high-level idea in the t-sne algorithm construct a distribut over high-dim point pair such that similar pair have high probabl construct a distribut over low-dim point pair such that nearbi pair have high probabl minim the kl-diverg between the two
what the hash trick in machin learn map featur vector $x$ to hash string $\phi$ use $\phi_k(x) = \sum_{i:h(i)=k} x_i \xi(i)$ where $h \colon \mbb{n} \rightarrow [1, k]$ and $\xi : \mbb{n} \rightarrow \{-1, +1\}$ are hash function
what the use properti of the hash string comput by the hash trick it approxim preserv inner products: $\mbb{e}_\phi[\phi(x) \cdot \phi(y)] = x \cdot y$
what teacher forc in machin learn when a model is predict a sequenc dure train mistak are correct befor the next charact is predict
what the hidden state alloc hypothesi when appli naiv curriculum learn to neural network the optim strategi on the early, easi exampl is to use all the state avail which leav no room for the extra state need when comput later, harder exampl
what beam search in the context of recurr neural network when make predict with rnns beam search mean keep the $k$ highest-prob output sequenc at each step
what an imag salienc map for a neural network it the map of how import each pixel is to the predict class label
when ad auxiliari supervis to deep networks, where should the auxiliari supervisor be instal where the gradient tend to vanish ie everi 3-4 layer in alexnet
how should the auxiliari supervisor loss be weight when train a deep network start at $0.3$ and decay each epoch as $1 - e/e$ where $e$ is the current epoch and $e$ is the total number of epoch
what the hypercolumn of a pixel in a cnn the set of ation of all neuron which are `abov that pixel
what the storag of a torch tensor it a represent of the memori that back the tensor
how do you make a copi of a torch tensor y = x:clone()
how doe torch tensor relat to floattensor, doubletensor , etc tensor is an alia for whatev the default type of tensor is
how do you creat a torch tensor with more than four dimens torch.tensor(torch.longstorage({1, 2, 3, 4, 5, 6}))
how do you cast a torch tensor to a float tensor x:float()
how do you take a slice of a torch tensor t[{1, {2, 4}}] is equival to the python slice [1, 2:4]
what the null slice in torch x[{{}, 2}] is equival to python [:, 2]
what doe x:gather(i, js) do in torch it return a tensor whose lead shape match js and whose element are taken by index into dimens i of x use the element of js
what doe x:scatter(i, js, y) do in torch it set the element in x correspond to js along dimens i to the valu of y
what are the three function for appli an arbitrari function to one or more torch tensor s apply, map, map2
what are the two way to break a torch tensor up into a tabl of tensor split, chunk
how can you share paramet between two modul s in torch module1:share(module2, param_name)
what a modul in torch the abstract class that all layer are deriv from
what the parallel contain in torch it appli it i th child modul to the i th slice of the input along some dimens and concaten the s alon some other dimens
what a criterion in torch an abstract class which repres a loss function
what'r the four main method of a torch modul forward backward zerogradparamet updateparamet
what zoutendijk theorem in numer optim if $\nabla f$ is lipschitz continu and bound below, and the step of a descent scheme satisfi the wolf condit then the step also satisfi the zoutendijk condit
what the zoutendijk condit in numer optim $\sum \nabla f_k ^2 ^2 \theta_k fty$ where $\theta_k$ is the angl between the step and the direct of greatest descent
when the function is posit definite, what the rate of converg of newton method quadrat
in term of the secant matrix $b_k$, what the theorem concern the superlinear converg of quasi-newton method if the function is smooth and convex, then a quasi-newton method will converg \emph{exactly} when $\frac{ (b_k - \nabla^2 f(x^*))p_k }{ p_k } \rightarrow 0$ ie when $b_k$ converg to $\nabla^2 f(x^*)$ along the step direct
what the differ between the frobenius and nuclear norm frobenius is $\sqrt{\sum \sigma_i^2}$ nuclear is $\sum \sigma_i$
what the $\delta$ with minimum frobenius norm such that for a symmetr $a$, $a+\delta$ is semiposit definit $\delta = q \text{diag}(\tau_i) q^t$ where $a = q \lambda q^t$ $\tau_i = -\min(0, \lambda_i)$
what the $\delta$ with minimum euclidean norm such that for a symmetr $a$, $a+\delta$ is semiposit definit $\delta = \tau i$ where $\tau = -\min(0, \lambda_{\min})$
how the choleski decomposit compar to the lu decomposit choleski is faster lu can alway be use
how doe the ldl decomposit compar to the choleski decomposit ldl decomposit avoid take squar root ldl decomposit sometim exist for indefinit matrices, where choleski doesn't
what the of the indefinit factor of a symmetr matrix $a$ $pap^t = lbl^t$ where $p$ is a permut matrix $l$ is unit lower-triangular $b$ is a -diagon matrix with s of $2 2$ or $1 1$
what are the two phase of a typic step-length select heurist bracketing, which find an initi interv contain an accept step length selection, which refin the interv until the step length is found
when appli line search method to quasi-newton algorithms, what should the initi trial step length be $1$ sinc if the termin condit are satisfi there, various rate-of-converg theorem will appli
when pick a step size dure line search, what a typic maximum number of trial evalu $10$
what the ratio $\rho$ often use by trust-region method $\rho = \frac{f(x) - f(x + p)}{m(0) - m(p)}$ where $p$ is the step and $m$ is the model
how do trust region method usual respond to various valu of $\rho$ $\rho 1$ mean the trust region should expand $\rho \lesssim 1$ mean the trust region should be preserv $\rho \ll 1$ mean the trust region should shrink
what the cauchi point $p^c$ in trust-region numer optim it the minimum point of the model along the direct of steepest descent insid the trust region boundari
in a chmod 3-digit code, how should you interpret the \emph{value} of each digit each digit is the encod of a three-digit binari string repres read, write and execut priviledg ( r=4, w=2, x=1 )
in a chmod 3-digit code, how should you interpret the \emph{position} of each digit user, group, other
how do you add or remov a singl privileg use chmod chmod u+x to add execut permiss to the user
what doe have execut permiss on a directori mean in linux it mean you have pass-through privilege; ie you can enter the directori
defin cross tabul aka a conting tabl eg use in r
defin principl of common caus if two variabl are associated, then either one caus the other or they both share a third common caus associ = e.g., correl from reichenbach 1956; has intuit appeal but it al stand is ambigu from http://fmwww.bc.edu/ec-p/wp689.pdf
what is the theoret differ between a linear regress with two variabl and a pearson correl on the same variabl in a linear regress you have an independ and depend variable, so all of the error is in one of the variables, wherea the other variabl valu are fixed, while in the pearson correl there could be error in both of the variabl http://stats.stackexchange.com/questions/22718/what-is-the-difference-between-linear-regression-on-y-with-x-and-x-with-i "use our tradit loss function, we are say that all of the error is in onli one of the variabl (viz., y). that is, we are say that x is measur without error and constitut the set of valu we care about, but that y has sampl error. this is veri differ from say the converse."
what a termin semicolon mean in matlab that the won't be print in the command window
what the variabl \ {ans} in matlab it the variabl that unassign express will be store in in the command window
how do you creat a matrix constant in matlab \ {a = [1 2 3; 4 5 6]} comma or space for column semicolon or linebreak for row
how do you continu a statement over two line in matlab s1 ... s2 also automat concaten string
how do you receiv multipl output in matlab [x, y] = f()
what the easiest way to call a function with no paramet in matlab type just the function name
when a matlab function return multipl outputs, how can you ignor each one after the first x = f() will store onli the first output in x
when a matlab function return two outputs, how can you ignor the first [~, y] = f()
how can you test if a variabl exist in matlab exist('varname')
what command-funct dualiti in matlab if f take onli string and you don't need it returns, f('a', b') is the same as f a b
what a dot befor an oper signifi in matlab an array oper instead of a matrix oper
how do you test if an array is empti in matlab isempty(a)
how do &amp; and &amp;&amp; differ in matlab &amp;&amp; is the short-circuit version
what do i, j repres in matlab the imaginari unit
what the structur of an if statement in matlab if expr1 s1 elseif expr2 s2 els s3 end
what the structur of a switch statement in matlab switch , case , otherwis
what the structur of a for statement in matlab for varnam = array ... end
how'd you match against a regular express in matlab regexp(string, pattern, match')
what a dynam regular express in matlab a regular express which contain a chunk of matlab code
what a cell array in matlab an array for non-numer data
what the use of a comma-separ list in matlab it can be use to pass multipl variabl at onc
is matlab row or column major column major
how do you mass-assign to a comma separ list in matlab [csl1] = deal(csl2)
how can you get a comma-seper list from a matlab cell array index use {} c{5} to get the fifth column as a csl
how do you creat an empti cell array of a specif size in matlab cell(1, 4) will creat a 1-by-4 cell array
for a matlab array a , what doe a(:) get a column array view onto a
how do you generat a step sequenc in matlab 1:10:2
how can you enter multipl statement on a singl line in matlab separ them with a comma
how'd you make a cell array liter in matlab c = {1 2 3}
how do you write a single-lin comment in matlab % comment
how do you write a comment in matlab %{ comment %}
what the differ between index an array with () and [] in matlab () can take a vector of indic to access
what the not oper in matlab ~x
how do struct relat to array in matlab struct are a kind of array
what the differ between matric and array in matlab a matrix is a 2d array
how do you test for a nan in matlab isnan(x)
how can you get the datatyp of an object in matlab class(x)
what a logic array in matlab a boolean array
how do you map a function over the content of an array in matlab arrayfun(@f, a)
how can you get a summari of an arbitrari object in matlab whos(a)
how do you test whether everi item of a matlab logic array is true all(a(:))
how do you escap quotat mark in string in matlab use a doubl quotat mark
how do you concaten string in matlab use the concaten operator, [s t]
in matlab, what the differ between use strcat and [s t] to concaten string strcat will strip trail space
what a charact array in matlab an array of characters, with the dimens be the number of char
how do you at a string in matlab sprintf
how do you compar string in matlab strcmp
what a categor array in matlab an array of valu drawn from a finit base set possibl with an order
how can you mimic enum in matlab use a categor array with the protect flag set
what an ordin categor array in matlab a categor array with a sens of order on it element
what a tabl in matlab a way to repres columnar data
how do you delet a row from an array in matlab t[5] = []
how do you access a column of a matlab tabl t.colnam
what the differ between index a matlab tabl with (), {} () return a tabl {} return the extract data
with respect to a matlab table, what are properti content of t.properti includ variabl names, descriptions, units, etc
in a matlab struct, how do you set a field with a runtime-defin name s.(fieldexpr) = val
when can two matlab struct be put in the same struct array when all their field have the same name
how'd you creat an anonym function in matlab f = @(args) expr
what a map in matlab a dictionari
how do you write a map liter in matlab containers.map({k1, k2}, {v1, v2})
how are class constructor defin in matlab by defin a method with the same name as the class
how are packag defin in matlab they'r folder that begin with +
how can you list all the properti of a matlab class properties('classname')
how do you test object for equal in matlab isequal(x, y)
how can you get document for a matlab class doc('classname')
how do you declar a class in matlab classdef to open the class properti , method , event section within
how do you declar a function in matlab function = f(args) then assign to whatev you want return
how do you delet an object manual in matlab delete(x)
what a code section in matlab a chunk of code delimit by %% titlenam which can easili be run as a whole
what a persist variabl in matlab a variabl in a function mark persist that maintain it valu between call
how do you access anoth workspac in matlab use the evalin, assignin function
how a privat function defin in matlab by place it file in a folder call privat
what should you be care about when defin array of anonym function in matlab space will be interpret as column separ so enclos express in parenthes
how can you count the number of input or output argument receiv in matlab they'r assign to nargin, nargout
how can you receiv a variabl number of argument in matlab use an argument call varargin which will be a cell array with one arg per cell
how do you support a variabl number of output in matlab by name the output variabl varargout which should be a cell array with one output arg per cell
in matlab, how can you assert that the number of input argument is within a certain rang use narginchk
how can you implement type check in matlab by use the validateattribut function
how can you pass a null valu to some paramet of a matlab function f(~)
how can you implement option argument in matlab use an inputpars
what p-code in matlab an obfusc matlab at
how do you catch error in matlab try, catch me, end
how is the varieti of an error defin in matlab by the mexcept 's identifi field
what the usual at of a matlab error identifi component:mnemon where compon is matlab or simulink or anoth product compon is what varieti of error it is
how do you usual creat an error object in matlab mexception(identifier, message)
how do you rais an except in matlab throw mexception(identifier, message)
how do you usual do cleanup work in matlab x = oncleanup(@f) when x is destroyed, f will run
how do you issu a warn in matlab warning(identifier, message)
how are name argument usual pass in matlab as a pair of success argument f('argname', arg)
how can you get an array of handl to all the local function in matlab localfunctions()
what the differ between e risk and total risk activ risk is the risk due to the decis made by the portfolio manag
what residu risk in portfolio manag the compon of the return that uncorrel with the benchmark return
what an indiffer curv in portfolio manag the tradeoff curv between risk and return with constant loss
intuitively, what the in ation coeffici in portfolio manag the correl between forecast and (residual) return ie a measur of the skill of the forecast
intuitively, what breadth in e portfolio manag the number of independ time per unit time that in ation from forecast can be appli
what the fundament law of e manag $\text{ir} = \text{ic} \cdot \sqrt{\text{br}}$ where $\text{ic}$ is the in ation coeffici and $\text{br}$ is the breadth
how the in ation ratio of a portfolio defin in term of the residu return in e portfolio manag $\text{ir}_p = \alpha_p / \omega_p$ where $\alpha, \omega$ are the mean and volatil of the residu return
what are prefer in e portfolio manag what kind of invest the client would like usual couch in term of high risk/low risk
what an equiti in financ a secur repres an ownership interest
intuitively, what alpha in financ ex ante: a forecast of the residu return ex post: averag of the realiz residu return
how doe alpha relat to score $\alpha = \text{volatility} \cdot \text{ic} \cdot \text{score}$
what a score in e portfolio manag a zero-mean, unit-standard-devi represent of a forecast of a residu return
what the use of defin the alpha in term of ic and score in e portfolio manag if the score contain no in ation, then $\text{ic} = 0$, so the alpha will be zero
what capm stand for in e portfolio manag capit asset price model
how is the market portfolio of the capm usual implement via a value-weight index of domest equiti (nyse composite, etc)
what the beta of a portfolio $p$ in the capm $\beta_p = \text{cov}(r_p, r_m)/\text{var}(r_m)$ where $r_p$ is the excess return on the portfolio and $r_m$ is the excess return on the market portfolio
what are excess return in the context of the capm total return minus the return on a risk-fre asset over the same period
what the beta of the market portfolio in the capm 1
what the beta of a risk-fre asset in the capm 0
what the realiz beta in the capm it the $\beta_p$ that aris as the slope when you construct a linear regress of the portfolio excess return vs the market excess return
how the residu return $\theta_p$ defin in the capm $r_p = \beta_p r_m + \theta_p$ ie it the part of the return not correl with the market
how the residu varianc $\omega_p^2$ defin in the capm $\sigma^2_p = \beta_p^2 \sigma^2_m + \omega_p^2$
what the foundat statement of the capm $\mbb{e}[\theta_p] = 0$ where $\theta_p$ is the residu return
what'r the weak, semistrong and strong s of the effici market hypothesi weak: can't outper the market use histor data semistrong: can't outper the market use public data strong: can't outper the market
what the secur market line in the capm the line relat beta of a portfolio to capm-deriv expect return of the portfolio
what the intercept of the market secur line the return on risk-fre asset
what the sharp ratio in e portfolio manag $s_p = \mbb{e}[r_p]/\text{std}[r_p]$
what ex-ant mean befor the event
what are the two import characterist portfolio in the capm $c$, the minimum-vari portfolio $q$, the portfolio with the greatest sharp ratio
in term of characterist portfolios, how can the capm be state that the portfolio with the highest sharp ratio is the market portfolio
in e portfolio management, how the exposur of a portfolio to an attribut defin $a_p = \mb{a} \cdot \mb{h}$ where $\mb{a}$ is the vector of asset attribut and $\mb{h}$ is the vector of portfolio hold
given an attribut vector $\mb{a}$ in e portfolio management, how do you calcul the characterist portfolio $\mb{h}_a = \lambda \mb{a} / \mb{a} ^2_\lambda$ where $\lambda$ is the precis matrix of the excess return
what the intuit definit of the characterist portfolio associ with an attribut vector it the portfolio with minimum risk and unit exposur to the attribut
how are the portfolio on the effici frontier in capm ed as linear combin of the minimum-risk and market portfolio
what doe $\sigma_x c$ repres for a covari matrix $\sigma_x$ the covari of the linear combin $c \cdot x$ with respect to $x$
what doe $\lambda_x d$ repres for a precis matrix $\lambda_x$ it give the linear combin over $x$ which has covari $d_i$ with $x_i$
what the varianc of the characterist portfolio for attribut vector $\mb{a}$ $\sigma^2_\mb{a} = \frac{1}{ \mb{a} ^2_\lambda}$
how is the portfolio with the maximum sharp ratio calcul in the capm it the characterist portfolio of the expect excess return
what semivari the varianc of the return below the mean
what doe $\sigma_p$ repres in e portfolio manag the standard deviat of the return on portfolio $p$
what the rule of thumb for the risk in a larg portfolio whose stock have risk $\sigma$ and pairwis correl $\rho$ $\sigma_p \sigma \cdot \sqrt{\rho}$
is return varianc addit over time for most asset classes, yes sinc return accross period are uncorrel
what the track error of a portfolio aka the e risk the standard deviat of the e return
what a typic rang of residu risk for larg us equiti 20 to 35
what a typic rang of beta for larg us equiti 0.8 to 1.35
what the problem with estim a full covari matrix for $n$ stock from $t$ histor time period the rank of the covari matrix is $\min(n, t-1)$ and common $t \ll n$ mean the covari matrix is singular
how the excess return decompos in a linear structur risk model $\mb{r}(t) = x(t)\mb{b}(t) + \mb{u}(t)$ where $\mb{r}$ is the vector of excess return $x(t)$ is the factor loading, give the exposur of each stock to each factor $\mb{b}$ is the factor return $\mb{u}$ is the specif return
how the covari of the excess return decompos in a linear structur risk model $\sigma = x f x^t + \delta$ where $x$ are the factor load $f$ is the factor covari matrix $\delta$ is a (usual diagonal) specif covari matrix
what are the three categori of factor that usual turn up in structur risk model respons to extern influenc cross-sect comparison of asset attribut internal/statist factor
what a macrofactor in e portfolio manag an econom forc express as a risk factor
what are descriptor in e portfolio manag they'r the specif measur that contribut to risk indic
what are risk indic in e portfolio manag measur of stock exposur to various invest theme volatility, liquidity, size, etc
what the differ between excess, residu and e risk excess is with respect to a risk-fre asset residu is with respect to the market/benchmark activ is due to the manag decis
what are the three categori of e risk inher risk is part of the benchmark intent risk is taken on know incident risk is a side-effect of the e posit
what a factor portfolio with respect to a structur risk model it a portfolio that has unit exposur wrt one factor zero exposur wrt all other minim risk
how is the margin contribut to total risk defin in a structur risk model $\text{mctr} = \frac{ \sigma_p}{ \mb{h}_p}$
how are cash hold usual repres in hold vector implicit as $\mb{h}_0 = 1 - \sum_{i 1} \mb{h}_i$
how can the expect return be decompos into four compon $\mbb{e}[r] = 1 + i_f + \beta \mu_b + \beta \delta f_b + \alpha$ $i_f$ is the time premium $\beta \mu_b$ is the risk premium $\beta \delta f_b$ is the except benchmark return $\alpha$ is the expect residu return
what the time premium in e portfolio manag it the return the investor get with part with the stake for a unit period (so it the return on a risk-fre asset)
what the except benchmark return in e portfolio manag it the expect differ between the futur excess benchmark return and the long-term histor return
what a typic rang of valu for the risk premium for equiti market 3 to 7
what a basi point in financ one hundredth of one percent
what a benchmark time strategi in e portfolio manag a strategi of buy or sell depend on price movement predict
what are the three reason for use mean and varianc to select portfolio return are normal distribut the investor has a quadrat util function quadrat util function can mimic ani util function over a short period of time
how expect util usual defin in e portfolio manag $u[p] = f - \lambda \cdot \sigma^2$ $f$ is the expect excess return $\lambda$ is the avers to risk $\sigma^2$ is the varianc of the portfolio
what a normal valu for the total risk avers paramet $\lambda_t$ in portfolio manag $\lambda_t 0.75$
what the e beta in portfolio manag $\beta_\text{pa} = \beta_p - 1$
what the time compon of the value-add in portfolio manag $\beta_\text{pa} \delta f_b - \lambda_\text{bt} \beta_\text{pa}^2 \sigma_b^2$ $\delta f_b$ is the except benchmark return $\beta_\text{pa}$ is the e beta $\lambda_\text{bt}$ is the benchmark time risk avers
what the residu compon of the valu add in portfolio manag $\alpha_p - \lambda_\text{r} \omega_p^2$ $\lambda_\text{r}$ is the residu risk avers
what are the two reason that e beta $\beta_\text{pa}$ is typic close to zero becaus time risk avers $\lambda_\text{bt}$ is typic veri high becaus the except benchmark forecast $\delta f_b$ is typic close to zero
when are e and residu return ident when benchmark time is avoid and $\beta_p = 1$
how doe the residu return of a portfolio relat to alpha $\theta_p(t) = \alpha_p + \epsilon_p(t)$ $\alpha_p$ is the averag residu return $\epsilon_p$ is the mean-zero random compon
what the alpha of the benchmark portfolio $0$, sinc it the expect of the residu return
what are typic valu for ex-post in ation ratio $-0.5$ to $+0.5$
what a typic ex-ant residu risk in e portfolio manag $ 5 $
how the ex-ant in ation ratio of an e portfolio manag defin as the maximum $\text{ir}_p$ over all possibl portfolio
whi is it import to onli compar in ation ratio across the same time horizon becaus expect return and varianc tend to grow linear with the time horizon so the ir grow as $\propto \sqrt{t}$
what the certainti equival return in portfolio manag anoth name for the valu add
how doe the value-add relat to the in ation ratio and risk avers of an e manag $\text{va}^* = \frac{\text{ir}^2}{4\lambda_r}$ where $\text{ir}$ is the in ation ratio $\lambda_r$ is the residu risk avers
what special about the in ation ratio of the characterist portfolio of the alphas, $a$ it the maxim in ation ratio over all portfolio
how is the portfolio $q$ with maxim sharp ratio relat to the characterist portfolio $a$ of the alpha it a linear combin of $\mb{h}_a$ and $\mb{h}_b$, the benchmark portfolio
how the in ation ratio of a portfolio relat to the sharp ratio $\text{sr}^2 = \text{sr}_b^2 + \text{ir}^2$ where $\text{sr}_b^2$ is the sharp ratio of the benchmark
what the portfolio $a$ usual stand for in e portfolio manag the characterist portfolio of the alpha
how can the alpha of a set of stock be written in term of the margin contribut to residu risk $\mb{\alpha} = \text{ir} \cdot \mb{mcrr}_q$ where $q$ is the portfolio with maxim sharp ratio
how can the in ation ratio of a portfolio be defin in term of the in ation ratio of portfolio $a$ $\text{ir}_p = \text{ir}_a \cdot \text{corr}[\theta_p, \theta_a]$ where $a$ is the characterist portfolio of the alpha
what $e_p$ typic denot for a portfolio $p$ the fraction invest in riski asset
how is the portfolio $q$ with maxim sharp ratio most easili construct it the characterist portfolio of the expect excess return $\mb{f}$ but scale to be fulli invest in riski asset
how the in ation ratio relat to the residu risk of the characterist portfolio of alpha $\omega_a = \frac{1}{\text{ir}}$
what the characterist attribut of the portfolio $c$ in e portfolio manag $\mb{e}$, the all-on vector
what the intuit interpret of the portfolio $c$ in e portfolio manag it the fulli invest portfolio with minimum risk
how the in ation ratio defin in term of the alpha $\text{ir} = \mb{\alpha} _\lambda$ where $\lambda$ is the precis of the residu return
what'r the three major assumpt under the fundament law of e manag the manag must be hypercompet the bet must be independ each bet is made with the same skill (ic)
what hypercompet in e manag it mean the manag exploit in ation optim and can accur assess how well she can exploit in ation (her ic)
in the in ation model of e portfolio management, what doe $\mb{x}$ typic repres the whiten residu returns, with $\mb{\theta} = \mb{a}\mb{x}$
in the in ation model of e portfolio management, what doe $\mb{y}$ typic repres the whiten signals, with $\mb{z} = \mb{e}\mb{y}$
in the in ation model of e portfolio management, what doe $\mb{z}$ typic repres the standard signal
what the general method for find a posit given some signal $\mb{z}$ maxim the value-add \emph{condit on} the signal
what hutchinson trick $\text{tr}(a) = \mbb{e}[z^t a z]$ and the rhs can be estim by sampl from a rademach distribut
what a rademach distribut uni on $\{+1, -1\}$
how are valu multipl relat to risk-adjust expect $\mbb{e}^*_s[\text{cf}(s)] = \mbb{e}_s[\nu(s) \cdot \text{cf}(s)]$ where $\mbb{e}^*$ is the risk-adjust expect $\text{cf(s)}$ is the cashflow of outcom $s$ $\nu$ is the valu multipl
what are the constraint on valu multipl $\nu(s)$ in valuat theori $\nu(s) 0$ $\mbb{e}[\nu(s)] = 1$ $\nu(s) \propto r_s(s)$ where $s$ is the portfolio with minimum second moment of total return
how doe the risk-adjust expect relat to the covari of the cashflow and valu multipl $\mbb{e}^*[\text{cf}(s)] = \text{cov}[\text{cf}(s), \nu(s)] + \mbb{e}[\text{cf}(s)]$
under the assumpt that there are no arbitrag opportunities, what the ula that defin the valu multipl $\nu(s)$ for ani asset $n$ and futur time $t$ $p_n = \mbb{e}_s\left[\nu(s) \cdot \frac{p_n(s)}{p_0(s)}\right]$ where $p_0(s)$ is the futur price of the risk-fre asset in state $s$ and $p_n$ is the initi price of asset $n$
how portfolio $s$ typic defin in valuat theori it the portfolio over riski and risk-fre asset that minim $\mbb{e}[r^2]$, the second moment of total return
how are valu multipl defin in term of the minimum-second-mo portfolio $s$ $\nu(s) = \frac{r_s(s)}{\mbb{e}_s[r_s(s)]}$
how can the expect excess return of a portfolio be relat to the covari with portfolio $s$ $\mbb{e}[r_p] = \frac{\text{cov}[r_p, r_s]}{\mbb{e}[r_s]}$ where $s$ is the portfolio that minim the second moment of total return
what the operational/financi divis of a firm the oper part creat the valu the financi part deal with dividend and invest cash flow between the two part
what the modigliani/mil conjectur about corpor financ dividend and financ polici don't affect the share price all the valu come from the oper side of the firm
what the idea in the dividend discount model the valu of a stock is the risk-adjust expect of it dividend
what are the two main way to calcul the present valu of a futur cash flow either appli a discount rate differ from the risk-fre interest rate or use the risk-fre discount rate, but use risk-adjust expect rather than regular expect
what the price of a stock deliv a constant dividend $d(1)$ when discount at constant rate $y$ $p(0) = \frac{d(1)}{y}$
under the simpl model of capit appreciation, how is the dividend growth rate express $g = (1- \kappa) \rho$ where $\kappa$ is the payout ratio and $\rho$ is the return on reinvest earn
what a compani payout ratio the fraction of earn paid out as dividend
what the golden rule of the dividend discount model $g$ in, $g$ out if the growth predict is garbag then the stock price predict is garbag
what an impli growth rate in valuat theori it the growth rate calcul by assum the stock is fairly-priced, and work back from there
how do impli growth rate tend to compar to forecast growth rate forecast growth rate tend to be higher than impli growth rate
what the basic idea in the three-stag dividend discount model growth forecast are usual more accur at 1-4 year than the long term so you should interpol between the forecast and the impli growth rate as you move further into the futur
how is the dividend discount model use in intern rate of return mode to caclul except return use the dividend and current market price to solv for the intern rate of return $y_n$ for each stock use the intern rate of return to estim the benchmark excess return $f_b$§ use $y_n$ and $f_b$ to calcul $\alpha_n$
how is the dividend discount model use in net-present-valu mode to caclul except return use the dividend stream, beta and expect benchmark return to calcul a discount rate $y_n$ and solv for a fair market price $p^\text{model}_n$ which can be compar against the current market price $p^\text{market}_n$
what compar valuat valu stock by compar the compani to it peer over some set of attribut
what the idea in returns-bas analysi tri to predict the return on some stock from some set of compani attribut and consid the error to be price error rather than model error
what the naiv forecast in e manag the in ationless one that lead to the benchmark portfolio
how the ula for calcul a refin forecast from a raw forecast aris form a joint gaussian of the excess return $\mb{r}$ and the raw forecast $\mb{g}$ where $\mu_\mb{r}$ is the naiv forecast then the refin forecast is $\mu_{\mb{r} \mb{g}} - \mu_\mb{r}$
what a typic rang of `good in ation coeffici in e manag 5 to 10
fundamentally, whi is forecast risk harder than forecast return becaus with typic in ation ratio of 5 -10 $\sigma_{\mb{r} \mb{g}} \sigma_\mb{r}$ for jointly-distribut gaussian return $\mb{r}$ and forecast $\mb{g}$
what are the two standard test for ani new stock forecast techniqu feed it random data feed it simul data with a hidden relationship
what the use of arch and garch model in financ to forecast volatil and correl
should raw forecast score \emph{usually} be standard across stock or across time across stock there usual a strong correl between score volatil and stock volatility, so there usual not much reason to standard across time
what are the two step in in ation analysi in e manag turn the predict into portfolio evalu the per anc of the portfolio
what an event studi in e manag investig how rare event (like chang in ceos) affect share price
what the half-lif of in ation in portfolio manag the time it take for the in ation ratio deriv from that in ation to drop to one-half it valu
what are the two way in which two correl signal can be combin in portfolio manag diversification: if they'r strong signal but weak correlated, averag them to reduc nois hedging: if they'r weak signal but strong correlated, take the differ to reduc nois
how doe the correl between lag strategi relat to their in ation coeffici $\text{corr}[\theta_k, \theta_0] = \frac{\text{ir}_k}{\text{ir}_0}$ where $\theta_k$ are the return of the $k$-period lag strategi and $\text{ir}_k$ is the in ation ratio of the $k$-period lag strategi
what a lag strategi in portfolio manag it a strategi that use $k$-period-old data
how doe combin old and current in ation increas the in ation ratio in e manag by reduc the volatil it doesn't increas the alpha of the strategi
what the fundament interact between real and finan asset in corpor financ financi asset are sold to pay for real asset real asset produc cash flow that support the financi asset
what a secur in financ a tradeabl financi asset
naively, what an invest decis in corpor financ a decis about the purchas of real asset
naively, what a financ decis in corpor financ a decis about the sale of financi asset
what capit expenditur in corpor financ it invest in an asset that will be use beyond the current financi year
what a capit structur decis in corpor financ the choic between debt and equiti financ and specif kind of debt and equiti
what a close held compani a compani where the share aren't trade publicly, but instead held privat by a few peopl
what are typic the three most senior financi role in a compani cfo under the cfo, a treasur and a control
what the fundament role of a financi manag to act as an intermediari between the financi market (investor hold financi assets) and the firm oper (real assets)
when should a financi manag reinvest rather than pay dividend when the reinvest will achiev a greater rate of return than can be achiev by the sharehold invest in the market at the same risk
what the cost of capit in financi manag the rate of return investor would get from invest in the market at the same risk which need to be beat by ani of the corpor invest project
whi is the object of a financi manag to maxim the firm sharehold valu becaus the sharehold can themselv manag the risk of their invest and the time of payout through the financi market onli thing they can't manag is the valu of their shares!
what are agenc cost in financ cost aris from manag work toward their own end rather than the sharehold
how the present valu of an invest in a project calcul in corpor financ it the return of the invest discount by the return on an equally-riski invest in the market
what the rule of 69 in financ a continuously-compound invest take $69.3/r$ period to doubl
what the rule of 72 in financ a discretely-compound invest take $72/r$ period to doubl
what a bond coupon the interest on the bond
what the face valu of a bond the amount at which the issuer pay interest (and which is - for most bond - repaid when it matures)
what the differ between us treasuri bills, note and bond bill matur in a year or less note matur in ten year or less bond matur in more than ten year
how do interest rate chang with bond price opposit rise interest rate mean fall bond price
how the durat of a bond calcul $\text{duration} = \frac{1}{\text{pv}} \sum t \cdot \text{pv}(c_t)$ where $c_t$ is the cashflow at time $t$ and $\text{pv}$ is the present valu of the bond
how the yield of a bond defin it the interest rate that explain the bond price in term of it cashflow ie the $y$ such that $\text{pv} = \sum \frac{c_t}{(1+y)^t}$
how the volatil of a bond defin in term of the durat $\text{volatility} = \frac{\text{duration}}{1+y}$
how the volatil of a bond defin as a deriv $\frac{d\text{pv}}{dy} = -\text{volatility}$ so a 1 chang in the yield mean -volatil chang in present valu
what'r spot rate in the context of bond they'r the rate $r_t$ at which cash flow $t$ year should be discount so if $r_4 = 4 $, then a cashflow in four year has present valu $c_4/1.04^4$
what the term structur of interest rate in the context of bond how spot rate chang as you move further into the futur
what a strip bond one that make a singl payment
what the yield curv in the context of bond the relationship between yield and matur date
how doe the term structur reflect belief about interest rate an upward-slop term structur indic interest rate are expect to rise a downward-slop term structur indic interest rate are expect to fall
how doe uncertainti about interest rate affect the term structur high uncertainti lead to an upward-slop term structur (sinc long-term bond are more heavili impact by chang in interest rates)
what an index bond a bond whose payment are depend on inflat
how do expect about inflat general affect real compar to nomin interest rate real interest rate remain relat constant nomin interest rate adjust to match
what are investment-grad bond bond with a rate of bbb or abov
what are junk bond bond with a rate bb or below
what the `primari market for share direct sale of share from a compani to investor
what the differ between a dealer market and an auction market in an auction market, transact are between investor in a dealer market, a dealer/market-mak intermedi trade
what'r limit order in auction market an order to buy/sel at a certain price, when possibl
what'r market order in auction market an order to buy/sel at the best price avail
what `ttm stand for in stock report trail twelv month
what p/e stand for in stock report price to earn (per share)
how doe `stock differ from `share stock is usual use to describ ownership of ani compani share is usual use to describ ownership of a specif compani it a linguist differ at most
what ep stand for in stock report earn per share
what the dividend yield of a stock the ratio of the dividend to the price
what the net book valu of a compani the total valu of all it assets, minus the total valu of all it liabil
what gaap stand for in financ general accept account principl
what going-concern valu the valu ad when a group of asset are organ into a busi
what p/b stand for in stock report price-to-book valu (per share)
what are growth stock and cash stock growth stock are bought in the hope of capit gain cash stock are bought for the dividend
what about a compani make it a growth stock rather than a cash stock that a substanti part of it share price is deriv from futur invest
what the free cash flow of a busi the amount of cash the busi can pay out to investor after pay for all growth invest
what a valuat horizon when valu a firm, it present valu can be calcul as free cash flow out to a valuat horizon plus the valu of the firm at the horizon
whi it import to calcul the valu of a busi in multipl way becaus ani singl way can be veri sensit to error
what a good way to set the valuat horizon pick the period that the market is like to fall into competit equilibrium sinc beyond that growth opportun becom much rarer
what the book rate of return of a project $\text{book rate of return} = \frac{\text{book income}}{\text{book assets}}$
what the payback rule in financ project should be judg on how long they'll take to cover their initi invest
what the intern rate of return of a project the discount rate which lead to a net present valu of zero.
what'r the problem with use the irr to judg a project when find the irr, the equat sometim has zero or mani root irr can give flaw s when compar mutual exclus project irr can't account for time-vari cost of capit
what capit ration in corpor financ when there onli a fix amount of invest available, and it need to be distribut between project
what'r the beta of each asset with respect to the characterist portfolio of attribut $\mb{a}$ $\mb{a}$
what the definit of the partial correl $\rho_{xy\cdot z}$ if $r_{x}$ is the residu of a linear model of $x$ use variabl $z$ then $\rho_{xi \cdot z} = \text{corr}(r_x, r_y)$
how can the precis matrix be interpret in term of partial correl $\rho_{x_i x_j \cdot x_{-ij}} = -\lambda_{ij}/\sqrt{\lambda_{ii} \lambda_{jj}}$
how can partial correl be interpret in term of a gaussian distribut if $x, y, z$ are normal then $\rho_{xi \cdot z} = 0$ iff $x$ is independ of $y$ given $z$
how can the maximum sharp ratio be defin in term of the expect excess return $\mb{f}$ $\text{sr}_q = \mb{f} _\lambda$
what the goal when rescal alpha in e portfolio manag to give them a standard deviat of $\text{volatility} \cdot \text{ic}$, as they should have if the score have standard deviat $1$
what a rule of thumb for choos which alpha to trim in e portfolio manag anyth abov $3\sigma$ should be investig
what factor neutral in e portfolio manag recent some alpha so the factor has alpha $0$
what the rule of thumb for estim annual transact cost $\text{annu transact costs} \frac{\text{round-trip cost}}{\text{hold period}}$
how do transact cost constrain the alpha in an optim portfolio $-\text{sc} \text{mcva}(\alpha) \text{pc}$ where sc is the sale cost and pc is the purchas cost so it onli make sens to buy or sell when the alpha chang to push the mcva outsid this band
what are screen in portfolio construct it a method for portofolio construction: rank the stock by alpha then buy the first $n$ in the list and sell the last $m$ to equal weight
what stratif in portfolio construct appli a screen in each of sever categori
when is the long-on constraint most damag when the univers has mani asset when the volatil of each asset is low when the strategi has high risk
what the usual model of the distribut of weight in a portfolio benchmark log-norm
what market impact the addit cost of trade more than one unit of a stock sinc the lowest offer might onli be for a hand of unit
what the implement shortfal approach to measur the cost of trade the cost of trade is the differ between a paper portfolio and the actual portfolio
what doe vwap stand for in financ volume-weight averag price
what the idea in the inventori risk model of transact cost transact cost are proport to the risk that the liquid supplier take on by facilit the trade take into account trade volum and stock volatil
what the rule of thumb for estim trade cost it cost one day volatil to trade one day volum
how purchas turnov defin between a current and a desir portfolio $\text{to}_p = \sum_n \max(0, h^\prime_n - h_n)$
how turnov defin between a current and a desir portfolio $\text{to} = \min (\text{to}_p, \text{to}_s)$
what the rule of thumb about value-add when turnov is restrict you can achiev 75 of the value-add with 50 of the turnov
how can transact cost be interpret in term of the value-add/turnov curv if transact cost are constant across asset then the point on the value-add/turnov curv where the deriv equal the transact cost is the optim point to move to
how can trade toward a portfolio be phrase as an optim problem $\text{utility} = \alpha_\text{short} - \lambda \psi_\text{short}^2 - \text{mi}$ where $\alpha_\text{short}, \psi_\text{short}$ are alpha and risk accumul by the trade and $\text{mi}$ is the market impact. these term can all be written as integrals, and the hold function over $h(t)$ optim against them
how the geometr averag return defin in portfolio manag it the $g$ such that $(1 + g)^t = \prod_t r(t)$ where $r(t)$ is the return for period $t$
how the averag log return defin in portfolio manag it the $z$ such that $e^{z\cdot t} = \prod r(t)$ where $r(t)$ is the return for period $t$
how the arithmet averag return defin in portfolio manag it the $a$ such that $1 + a = \frac{1}{t} \sum r(t)$
how do the arithmetic, geometr and exponenti rate of return compar $\text{exp} \text{geo} \text{ari}$ $z g a$
what the idea in per anc attribut in portfolio manag use the factor model develop for predict return to evalu the per anc of differ factor which can then be t-test
what the differ between strateg and tactic asset alloc in portfolio manag strateg alloc establish a target alloc tactic alloc deal with the variat around that target
what the differ between domest and global asset alloc domest alloc choos between stocks/bonds/cash/etc global alloc choos between geograph equity/bond market
whi should the approach to asset alloc be differ to the approach to asset select alloc has onli 3-20 asset to choos between select has hundr or thousand
whi doe asset alloc usual involv time-seri asset model rather than select cross-sect model becaus correl between asset is much lower
how do you convert portfolio return between currenc $r(\text{u.s.} 0, t) = \frac{(\text{usd}/\text{gbp})(t)}{(\text{usd}/\text{gbp})(0)} \cdot r(\text{u.k.} 0, t)$
when deal with intern investments, how should the excess return be calcul when move between countri $r_p(\text{u.s}) = r_p(\text{u.k}) - r_\text{usd}(\text{u.k}) + \sigma_\text{p, gbp}(\text{usd})$ so the excess return on $p$ for a us investor are the excess return on $p$ for a uk investor minus the excess return on usd for a uk investor plus the covari between $p$ and gbp for a us investor
what are bsk and com in asset alloc com is a composit currenc bsk is the portfolio of currenc that determin the makeup of com
what the usual way to take a benchmark time posit in a portfolio by take a posit in futur contract for the benchmark this avoid ani risk and transact cost involv in use stock to take an e beta
what a typic rang of ic for benchmark time $0.05$ to $0.1$
how doe gradient descent work gradient descent optim the paramet of linear regress equat (y = theta_0 + theta_1*x_1 + theta_2*x_2 + ... + theta_n *x_n). it find the minimum of the cost function (like least squar cost) by descend in the direct of the minimum (gradient =0) by find partial deriv with respect to each theta and take step proport to the negat of the gradient.
what is a newton method of optim (nonlinear conjug gradient descent) it local approxim a nonlinear function by quadrat functions, find it minimum and move in that direct to find the minimum of the origin function
what is adagrad it is a weight optim algorithm for neural networks. the intuit behind it is that if the data point is more rare, it more predict (like tf/idf). so, if some particular thing produc huge chang in weights, we want to make it more import
what is cross-entropi cross-entropi is a measur of correct of prediction, e.g. of a neural network. it is comput like this: -( (ln(probabl of a class)*0) + (ln(0.3)*1) ) so, it return how sure was the network about the correct label.
what is bias and varianc what is a tradeoff between them bias is an error that an algorithm make from the expect value. high bias s in underfit (the model is too simple). varianc is error from the sensit to small fluctuat (the model is too complex). high varianc caus overfitting.
what is a kernel trick and how doe it work kernel are similar function that are appli to featur vectors. an output of appli a kernel function is an m-dimension vector, where m is amount of data point in the data set. it return a desir output for the ground truth data point (0 or 1). then, paramet theta are learn for each of the output (1..m) of the kernel function. therefore, we can learn non-linear hypothesis.
what are the main step of machin learn 1- question 2- input data 3- look for featur 4- implement algorithm 5- solv for alg. paramet 6- evalu
“the combin of some data and an ach desir for an answer doe not ensur that a reason answer can be extract from a given bodi of data.” ―john tukey  
what are the attribut of good featur 1- they could lead to data compress 2- retain relev in ation 3- creat base on expert info. systems.
common mistak that we commit 1- tri to autom 2- not pay attent to data specif quirk 3- throw info. away.
the best machin alg. should be 
predict is about tradeoff 1- interpret vs. accuarci 2- speed vs. accuraci 3- simplic vs. accuarci 4- scalabl vs. accuarci
what is the in-sampl error the error you get on the same dataset that you use to build your model
what is the out of sampl error the error you get when you tri your algorithm on new data set and it call generlize error
what we are tri to fit, in everi dataset fit the {{ signal and leav the noise}} to avoid the overfit  
what is bad about build high accuraci predictor we might end up captur the nois as well
what are [in general] the step of predict studi design 1- defin your error rate 2- split your data into train and test 3- on the train set pick a feature. 4- pick a predict function 5- appli one time to the test set (if there is no valid required) 6- if it requir appli to test set and refin
sampl preferr split. 60% train and 40% test or (20+20) test and valid
what are the type of error relat to predict 1- positive: algorithim decid that you are in the class 2- negative: algorithim decid that you are not in the class 3- true: you are actaulli in the class 4- fals you are not in the class ----------------------------------------------------------------------------------
what is the sensit the probabl of get posit test person give actual deseas pr (posit test deseased) the good of our test procedur
specif pr (negat test no desease)
posit predict valu pr (deseas posit test)
negat predict valu pr (no deseas negat test)
reciev oper charachterist roc curv 1- in the predict you usual give a probabl other that 01 answers. (i.e. p( of be alive)). depend on the cutoff you will get differ s. 2- fall-out is (1-specificity) [how mani of the case you are missing] 3- these curv can tell you how good a test can distiguish between a patient and a healthi person for exampl
the area under the roc curv tell you how good a test in distinguish between good and bad s. 
cross valid  
what cross valid is use for probabl the simplest and most wide use method for estim predict error is cross-validation.
k-fold cross-valid process 
what is the cv approach 
what are the s of cv 1- random sampl 2- k-fold sampl 3- leave-one-out
some consider of cv 
caret packag [how to fit a model ] set.seed(32343) modelfit - train(typ ~.,data=training, method="glm")
spam example: predict predict - predict(modelfit,newdata=testing)
test the accuraci by use the {{ confus matrix}} confusionmatrix(predictions,testing$type)
data slice and split intrain - createdatapartition(y=spam$type,p= 0.75 , list=false) [exampl on the split ratio you need to implement for the data]
how to creat fold use caret packag fold - createfolds(y=spam$type, k=10 , list=true,returntrain=false) [number of fold in the dataset]
how to perfrom resampl in dataset use caret packag 
what are the metric to measur the error of the predict 
 this is a detail review for the caret packag
the {{ train}} function can be use to ** evaluate, use resampling, the effect of model tune paramet on per anc ** choos the "optimal" model across these paramet ** estim model per anc from a train set  
what is the first step in tune the model - (line 1 in the algorithm abov is to choos a set of paramet to evaluate. for example, if fit a partial least squar (pls) model, the number of pls compon to evalu must be specified.
what is the first step in tune the model (line 1 in the algorithm abov is to choos a set of paramet to evaluate. for example, if fit a partial least squar (pls) model, the number of pls compon to evalu must be specified.
onc the model and tune paramet valu have been defined, {{ the type of resampling}} should be also be specified.  
what are the sampl techniqu avail with train command in caret packag k-fold cross-valid (onc or repeated), leave-one-out cross-valid and bootstrap
by default, {{ simpl bootstrap}} resampl is use the sampl  
what are the pre-process command can be pass to the train command 1- center 2- scale 3- na imput 4- spatial sign applic 5- featur extract
two level of covari creation 
predict with tree 
what is the basic algorithm for predict with tree 
what is monoton tran ation 
what are the main step of bag [bootstrap aggregation] 
bag exampl 
random forest step 
naiv bay p(i x1..xn) = p(x1..xn y)*p(y)/p(x1..xn). the idea is to find the most probabl class given all the features. assum that each featur is independ which is insane, but work pretti well in some cases. especi in nlp. fast. doesn't need much train data. don't trust predict probabilities. differ flavor - gaussian assum that the liklihood of featur has a gaussian distribution. multinomi assum liklihood of featur has a multinomi distribut (bow). bernoulli is for bivari features.
linear regress goal is min of squar error e(yi-xi*b). this can be solv by (xtx)^-1(xty). assum independ features. high bias, low variance. if featur standardized, beta weight reflect featur importance. assum error are independent. constant variance. assum normal of errors, non-stochastic. if assumpt hold. linreg is blue - best, linear, unbiased, estimator. (gauss markov theorem)
ridg regress l2. form of regular use by glm models. penalti for larger beta weight - the l2 norm which is the matrix multipl of weights. alpha paramet determin strength of regularization. solv by (xtx+alpha*i)xty. - same complex of linear regression. can set alpha with cross validation. shrink weight to a hyperspher around 0. can be thought of as a gaussian prior of 0 for each weight with width of gaussian determin by alpha (narrow as alpha grows).
lasso regress l1. better for find sparser beta weight vector (although doe not explicit gain spars ones. that would be l0). add l1 penalti to least squar problem. l1 is sum of absolut values. alpha control the amount of regularization. usual use coordin descent to fit.
multi-task lasso fit a lasso regress model to multipl problem simultaneously. use multipl y (problems) can move the model closer to ground truth. use both l1 and l2.
elast net model with l1 and l2. spars and reduc weight sizes. especi use for correl features.
least angl regress (lars) good for high dimension data (especi when more column than rows). just as fast as forward select and has same complex as linear regression. base on iter refit of residuals, which make it sensit to noise. iter fit data at each iter the beta weight are push in the direct of the featur correl with the residuals.
orthogon match pursuit forward select algorithm, but approxim the l0 norm, so it tri to find a spars solution. at each step, includ the featur that is most correl with the residuals.
linear discrimin analysi p(y=k x) = p(x y=k)*p(y=k)/p(x). assum p(x y) is gaussian. most bias of classifi algorithms. how doe it compar to naiv bay assum all class share a common covari matrix (i believ this is how it differ from logist regression. check with hasti though). can use shrinkag when have a bad featur to sampl ratio.
kernel ridg regress ridg regress with the kernel trick. similar to support vector regress but with squar error instead of svm error. faster than support vector regress when the dataset is relat small sinc this can be solv in close .
support vector machin effect in high dimension spaces. effect when number of featur exceed the number of sampl (but not too much) onli use a subset of train exampl to fit the data - memori efficient. flexible. doe not provid probabl estimates. in multi-class classification, use one against one approach so has to train mani models. quadrat program problem scale between o(#features*#samples^2) and o(#features*#samples^3). can use kernel function to explor non-linear spaces. use c paramet to regularize. it control the number of support vector i think. check this out in hastie.
nearest neighbor method find closest k item to a given point. distanc measur should be defined. high non-parametr and space requir scale linear with dataset size. in classification, each of k item vote as to which class the given item should be. can use radiusneighorsclassifi too, which count the number of item in a radius from the given item.gener chosen when data are relat uni ly distributed. as k goe up, the model lose flexibility. other tweak includ weight item by their distanc from the given item. in regression, the item is assign to the mean of k. k-d tree and ball tree are method that tri to approxim k-neighbor but without brute forc check each item in dataset.
decis tree non-parametr model that predict target variabl from simpl decis rules. deeper tree = more flexibility. easi to interpret-boolean logic. don't need to normal data/can handl categor data (doesn't care about number magnitude). train is ~o(log(training_points)). use "pruning" to prevent over-fitting. not good for unbalanc classes. hard to find the global minimum of the model. like to overfit when mani features. # of sampl need to popul the tree doubl for each extra depth increas leaf size increas bias
ensembl model combin predict of multipl models. general improv generaliz of models. averag method averag predict of multipl model (bagging). these method improv generalizability, so use to reduc overfitting. this work best with model that tend to overfit (full decis trees). boost method - model are built sequenti and the goal is to reduc the bias of previous models. work best with model that tend to underfit (shallow decis trees).
bag method use to improv generalizability, so best with model that often overfit data (deep decis trees). the idea is to averag the predict of multipl models. draw random sampl from the data and feat a model to each random sampl of the data. typic sampl is done with replacement.
random forest bag method special for trees. each tree is built by sampl data with replacement. also, when split nodes, the split chosen is not the best split among all features, is the best split given a random subset of features. this increas model bias, but averag of the model make up for the flexibility. paramt = number of tree (more tree = better but expensive) and max_featur (number of featur to use when split nodes, less featur is more bias). cross-valid with out of bag score.
featur import in random forest the rank (i.e., depth) of a featur in decis node rough reflect it importance. featur at the top of the tree are more important. thus, the fraction of sampl they control is their relat importance. averag this fraction of sampl across tree reduc varianc in predictions.
adaboost base on the idea of fit a sequenc of weak learner (model that are onli slight better than random) on modifi version of data. predict are then combin as weight major vote. so what do i mean by modifi version of data! at first, the weight of each train sampl is 1/n, but the weight chang such sampl that previous model miss are more import when creat the futur model (and vice versa). the idea is futur model concentr on exampl that were miss by previous ones. paramt = number of estim (number of models) and learn rate (how final model weight in final combination). weak learner are usual decis stumps, but can make more complex.
gradient [tree] boost boost for arbitrari loss functions. can handl mix data types, great off the shelf, robust to outliers, but suffer with scalabl (have to train model sequentially). paramet = n_estim (number of trees), max_depth (depth of trees), and max_leaf_nod (how mani leaf nodes), and learn rate. can handl interact up to the level of the tree depth. shrinkag (learn rate) use to reduc overfitting.smal learn rate mean each new model contribut less to the decisions, and these more special (more flexible) model contribut less to flexibility. can choos n_estim with earli stopping, but this interact with learn rate - lower learn rate requir more estimators.
featur select *varianc threshold - remov featur with varianc x. *univari select - select the k "best" featur where best could be associ with target (assum no interactions). *recurs featur elimin - remov featur whose weight are less than some threshold. *forward select algorithm (least angl regression),
label propag semi-supervis method for general label of known data to unknown data. clamp paramet basic control how much we trust the label data (can we consid that some are label incorrect ) assum that we have a good spatial sens of the data (similar label item appear near each other in our new space).
gaussian mixtur model fit through expect maxim - basic assign label then find mean of differ labels. assum data generat from a mixtur of gaussians. similar to k-means, but with in ation about covari structur of data. can use bic to estim the optim number of clusters. covari can be: spheric - same amount of varianc in each direct (k-means), diagon - differ featur have differ amount of varianc across cluster (on constraint is all varianc assum independent), full - no constraint (so can have correl variance), tie - all cluster have same variance.
variat bayesian gaussian mixtur maxim lower bound on model evid rather than data likelihood. use em algorithm, but add regular by use priors. concentr paramet control how like the model is to find more or less compon (low value=few components). compon model as from a dirichlet process. slower than gmm, but let the model have more input in how mani compon to select.
isomap (isometr mapping). form of manifold learning. extens of multi-dimension scale or kernel pca. seek a lower dimension embed that maintain geodes distanc between points. complex is o(n**2)
local linear embed basic a seri of local pcas which are combin for a global non-linear embedding. complex o(n**2) regular is a problem when the number of neighbor is greater than input dimens - rank deficiency. modifi lle design to combat this.
multi-dimension scale use for similarity/dissimilar data (in general) and model this as distances. come in two flavors. metric - tri to maintain actual distanc and nonmetr which tri to maintain the rank of data rather than distanc - x is closer to y than z.
t-sne t-distribut stochast neighbor embedding. probabl the most popular. treat affin of data point as probabilities. focus on local cluster which is nice when the data is compris of multipl manifolds. kl diverg of joint probabl in origin and embed space minim by gradient descent - not convex so differ starts/restart get differ answers. paramet = perplex - number of local neighbor to consid in algorithm (more neighbor = more linear bias), learn rate, iterations, angl (again whether to emphas local or global structure).
k-mean separ data into k group with equal variance. minim the within cluster sum of squar (distanc from mean of cluster). assum spheric clusters. reli on distanc measures, so curs of dimension can be a problem. use a simplifi em algorithm. nonconvex, so multipl start needed.
affin propag send messag between pair of item ask how similar pair of item are. end when a key number of exemplar item are chosen. these are the cluster centers. choos number of cluster itself. prefer paramet control how mani cluster are created. o(n**2*t) where t is number of iterations. memori complex is o(n**2).
hierarch cluster famili of cluster method that build nest clusters. either merg or split cluster depend on method. heirarchi of cluster are a tree (root has all data). leav onli have one item. agglom cluster mean start from bottom up. ward minim sum of squar differ within all clusters. linkag method minim the distanc between pair of clusters. use distanc metrics.
dbscan model data as area of high densiti (core samples) separ by area of low densiti (non-cor samples). this is a high flexibl model. min_sampl govern number of point need to find a set of core sampl (a cluster). ep is the maximum distanc between these core samples.
silhouett score use for test unsupervis models. compar mean distanc between a sampl and all sampl in same cluster to the mean distanc between a sampl and all sampl in the next nearest cluster. bound between -1 and 1. 1 is the goal. work better for spheric cluster methods.
calinski-harabaz index ratio of between cluster and within cluster dispersion. basic distanc of item from the center of their cluster compar to the distanc of cluster center from center of data. bd/wd so higher score is better. again, better for method assum spheric clusters.
pca decompos dataset into orthogon components. whiten z-score data. sklearn includ a method for estim the number of meaning components. find eigenvector with largest magnitud eigenvalues.
kernel pca similar to pca but attempt to find non-linear explan of data. throw data through kernel then per s pca. how the kernel is chosen....
singular valu decomposit similar to pca (squar root of eigenvalu are singular values). for find orthonorm basi (independ factors). easier to comput than pca.
ica separ multivari signal into (maximally) independ addit subcomponents. idea is to separ superimpos signals. whiten befor using! can also be use for dimension reduction... not too complex so i should add to this.
matrix factor for matrix with non-neg values, can be use to find lower dimension representation. minim 1/2e(x-wh)**2. expand this!
latent dirichlet alloc generat probabilist model for collect of discret data. topic modeling... draw the topic from dirichlet(n) for each topic. for each document draw from a dirichlet(alpha). for each word in a document, draw a topic from a multinomi and draw a word from a multinom govern by the output of the topic dirichlet. p(topic, document,and word in a topic word, topic distribution, and document distribution).
novelti detect compar new datum to exist data (assum that exist data doe not have outliers) and comput probabl of this new datum aris from same generat as previous data. one class svm often used. rbf is common kernel (gaussian data).
outlier detect similar to novelti detection, but doe not presuppos clean start data. goal is to separ data produc by desir generat from data generat by...someth else. one way to do this is to fit a gaussian to central mode and assum other data generat by a nois process. then estim mahalanobi distanc (how mani standard deviat from data). isol forest can estim outliers. this count the number of split between max and min in order to isol feature. basically, doe an item tend to be eccentric. especi use when data generat by multipl process (bimodal).
kernel densiti estim histogram is simplest . can think of it as kernel smoothing. appli a gaussian kernel to histogram of data. bandwith of kernel determin bias flexibl trade-off. with wider kernel smooth the data more and give more bias. mani differ kernel available.
overfit model fit both generat process and addit noise...th model will not general well to futur data becaus this data share generat process but not noise.
underfit each model assum a generat process under the data, and this assumpt is virtual alway wrong. the question is how wrong is this assumption. in the case of linear regression, if the data are not a linear project of the input than this assumpt can be veri wrong, but the model can't "learn" this. the model "bias" insist on it generat process.
cross valid split the data k time and train k models. we can then look at variabl in test set predict and averag model ability. can use to fit hyper parameters, but alway use hold out data!! rememb pre-process is basic a hyper parameter.
grid search algorithm for identifi optim hyper paramet (rememb test set data!!). exhaust is tri everi possibl valu in some rang of step size x. this is fine, but random hyper paramet valu tend to work better. it isn't hurt by ad parameters, can help with find the distrubit of loss amount given hyper parameters. obvious pick the "accuracy" or desir outcom is import and accuraci isn't alway best.
precis and recal precis is fals posit rate. recal is fals negat rate.
hing loss loss function use by svm, onli consid item where the model guess incorrectly.
log loss use by logist regress and neural nets. sum(y*log(p)). we use log becaus we like addit and we don't want numer underflow.
matthew correl coeffici correl for binari class
roc graphic plot depict per anc as decis threshold varies. plot fraction of true posit vs fraction of fals positives.
f1 score measur of both precis and recal 2*(precision*recall)/(precis + recall)
explain varianc 1-(varianc y-y_hat)/variance(y)
mean squar and mean absolut error sum(y_i-y_hat_i)**2 (or just absolut value)
r**2 (y_i-y_hat_i)**2/(y_i-y_bar_i)**2
one-hot encod binari tran ation of categor variables. one column for each uniqu valu in the category.
bow each distinct word in corpus has column. number of time that word appear in sampl is the valu of that word column. if bi-gram then each distinct pair of word has a column...ect.
tf-idf term frequency, invers document frequency. algorithm for down weight frequent word in a corpus. the valu is term_frequ (# time in document) * 1+ log(1+total_document_#)/(1+#_docs_with_term). this multipli term is then normal by the euclidean norm.
min-max scale scale all data between 0 and 1 by subtract minimum and divid by the max.
normal subtract mean and divid by standard deviation. goal is to end with mean of 0 and standard deviat of 1.
featur binar threshold featur and put everyth abov as 1 and below as 0 (or vice versa)
polynomi featur linear model can fit non-linear data if the data are project into a non-linear space. an exampl of this is squar input featur or ani other non-linear tran ation
miss valu we often want to replac miss valu rather than throw away the same point. how do we replac the miss valu without make too strong of an assumpt about what the valu would be in this place often replac with mean, median, or mode.
random project use random matric you can dramat reduc the dimension of data while larg maintain the distanc between points. i don't understand how this works...
cosin similar l2 normal dot product of vectors. call cosin becaus l2 normal project vector onto a unit sphere. this then becom the cosin of the angl between point denot by the vectors.
radial basi function exp(sigma**- x-i **2). x and y are input vector and sigma is a gaussian kernel with varianc sigma.
bay theorem p(a b) = (p(b a)*p(a))/p(b). often hardest part is find p(b). the goal is to add differ condit of p(b a) together.
maximum likelihood estimt maxim (or minimize) log-likelihood sum(log(p(x_i theta).
central limit theorem (1/sqrt(n))*sum(x_i-mu) ---- as n goe to infin --- n(0,sigma**2)
p-valu probabl that we could observ this effect under the null-hypothesis.
how to test differ in proport z-statist - normal and see how mani standard deviat away from expect effect.
test regress coeffici t-statist
good of fit chi**2 statist
anova f-statistic. use for test differ in categor defin data. factorsxlevels. equival to regress wtih dummi variables. between group sse / within group sse is the key here.
conting tabl chi**2 statist
model comparison f-statist
qq plot quantile, quantil plot. can use to visual similar of distributions. veri use when visual normal of error
wilcoxon rank-sum test between group, pairwise, nonparametr test.
sign rank test within group, pairwise, non-parametr comparison
kruskal-w non-parametr between group anova (similar to wilcoxon)
nlp pipelin tokenization, stemming, lemmatization, remov stop words, remov punctiuation, bow/tf-idf, n-gram, word embed
word2vec neural network train embed that seek to guess word by it context (or vice versa) which basic mean learn the condit word probabilities, but in a smaller space. produc embed that seem to carri in ation about word mean - the differ between king and queen is similar to man and woman.
bootstrap basic non-parametr confid interv estimation. the idea is repeat sampl subset of the data and per the analysi (find mean/run classifier/whatever). by look at the distribut of score from this analysi on sampl data, we can get an idea of the variability. i typic then tri to come up with a null distribut to compar my bootstrap distribut to.
jack-knif similar to boot-strap, but the subset are of size n-1 and there are n subset (so run the analysi onc for everi sampl and leav that sampl out). work well when not much data, but i wouldn't use it except for extrem circumstances.
kolmogrov smirnov test test for evalu whether a sampl is not normal. critic for look at regress residuals, data befor t-tests, ect. basic use it all the time.
power analysi if we assum we have effect size x with a chosen alpha and beta (power) value, we can then estim how mani sampl we will need. the key here is we want to maxim the probabl of find an effect (power) and realli the onli way we can chang this is sampl size. we can also kinda chang effect size, but less so. also practic of small effect size should be taken into account. doe the busi model care about the size of the effect peopl like to use g power.
neural network famili of models. segment function approximators. basic oper by do a seri of logist regress on the data. the hardest part of them is how to train the models. they have a ton of paramet (need ton of data) and have a segment seri of layers, which the error signal has to be pass-back through. peopl "back propogr the errors." all train is done in stochast gradient descent becaus the optim problem is non-convex. it actual a super weird problem. turn out local minima are not realli the problem. saddl point are harder to traverse. for this reason, peopl have assign a number of backprop techniqu that tri to give the loss function "momentum." realli interest stuff. larg thought of as local pattern matching. great for imag recognit - convolut neural nets. get better at language. ture complete, so can solv ani problem. the question is more how to direct the learn of the model - convolut layer constrain learn toward a solut we know is better for visual problems. recurr net contrain learn toward solut we believ better for sequenti inputs. all these model could be implement as fulli connected, but we direct the model toward better solutions. can also regular the models. dropout is super useful. help the model learn sparser solutions, which also tend to be more generalizable. l2 regular of weight is also relat common, but dropout is so success and dead simpl that peopl use it.
binomi distribut of bernoulli (yes/no) trials. paramet are n (trials) and p (probabl of success). mean is n*p. varianc is np(1-p)
poisson exponenti but varianc equal the mean. discret probabl distribution. probabl of a given number of event over a fix interv of time.
gaussian normal distribution. mean is mu. varianc is sigma squared. the pdf is (1/sqrt(2*pi*sigma**2))*e**(-(x-mu)**2/2*sigma**2) the mode, median, and mean are all the same.
bernoulli special case of binomial, but with a singl trial.
beta distribut conjug of prior distribut of binomi distribution. continu in the interv between 0 and 1. so great for repres probabilities. control by paramet alpha and beta. mean = alpha/(alpha+beta) varianc = (alpha*beta)/((alpha+beta+1)*(alpha+beta)**2)
collabor filter basic idea is if two peopl agre on one issue, they are more like than chanc to agre on anoth issue. the idea is that i have a matrix of user by product and i want to find a user embed and a product embed which when matrix multipli can replic the origin matrix. the great part about this isn't the part of the matrix that are replicated, but it the previous empti cell that now have a user x product predict in them. the hard part about this is come up with the embed matrices, but this can be train through gradient descent. it basic a simpl autoencoder. you would train the net by creat two embed matrices. dot multipli them and compar to origin matrix. can valid by leav some data out and then check whether the model was correct regard this data.
suffici statist condit neccessari and suffici condit for [$$]t(x)[/$$] to be suffici for [$$]p(x \theta)[/$$] is that [$$]p(x \theta) = f(t(x),\theta)g(x)[/$$]. this extend to ani number of paramet and statistics.
posterior given x and given suffici statist t(x) posterior are the same for ani prior distribution!
exponenti famili definit a densiti [$$]p(x \theta)[/$$] which can be written in the [$$]p(x \theta) = exp\{t(x)\phi(\theta)\}g(\theta)h(x)[/$$], where [$$]\{g(\theta)\}^{-1} = t_x \exp\{t(x)\phi(\theta)\}h(x)dx[/$$], is call exponenti family. the function [$$]\phi(\theta)[/$$] is call natur parameter.
given random sampl from fix densiti [$$]p(x \theta)[/$$], a famili [$$]\mathcal{f}[/$$] of distribut over [$$]\theta[/$$] is close under sampl wrt [$$]p(x \theta)[/$$] iff for ani sample... [$$]p(\theta) \mathcal{f} ightarrow p(\theta x) \mathcal{f}[/$$] this is equal to say [$$]\mathcal{f}[/$$] is a conjug prior for [$$]p(x \theta)[/$$]
conjug prior of exponenti famili and correspond posterior if [$$]p(x \theta)[/$$] is exponenti family, then [$$]p(\theta) \propto \exp\{a\psi(\theta)\}g(\theta)^b[/$$] is conjug prior with posterior [$$]p(\theta x) \propto \exp\{\psi(\theta)(\sum t(x_i) + a)\}g(\theta)^{n+b}[/$$]
posterior given likelihood [$$]x no(\theta, \epsilon)[/$$] with known precis [$$]\epsilon[/$$] and prior [$$]\theta no(\mu_0, \epsilon_0)[/$$] for n sampl [$$]x_i[/$$] [$$]p(\theta {x}) no(\frac{n \epsilon \hat{x} + \epsilon_0 \mu_0}{n \epsilon + \epsilon_0}, n \epsilon + \epsilon_0)[/$$] posterior precis = n * data + prior precis posterior mean: weight averag of prior mean [$$]\mu_0[/$$] and data mean [$$]\hat{x}[/$$] with weight equal to the weight precis
posterior of normal with known mean 0 (subtract mean from data if it non-zero). if [$$]x_i no(0, \epsilon)[/$$] and [$$]\epsilon ga(\alpha / 2, \beta / 2)[/$$] then [$$]p(\epsilon x) = ga((\alpha + n)/2, (\beta + \sum x^2_i)/2)[/$$]
posterior given normal likelihood with unknown mean [$$]\mu[/$$], precis [$$]\epsilon[/$$] todo
mixtur of/compound prior of exponenti famili with densiti [$$]p(x \theta)[/$$] prior [$$]p(\theta) = \alpha p_1(\theta) + \beta p_2(\theta)[/$$] with [$$]p_1(\theta)[/$$] and [$$]p_2(\theta)[/$$] conjug prior and [$$]\alpha + \beta = 1[/$$]. then posterior is [$$]p(\theta {x}) = \alpha'p_1(\theta {x}) + \beta'p_2(\theta {x})[/$$] with [$$]\alpha + \beta = 1, \alpha = \frac{\alpha p_1( {x})}{\alpha p_1( {x}) + \beta p_2( {x})}, p_i(\theta {x}) = \frac{p( {x} \theta)p_i(\theta)}{p_i( {x})}[/$$]
jeffrey prior. with random sampl x from densiti [$$]p(x \phi)[/$$], we choos the prior as [$$]\pi(\phi) \propto h(\phi)^\frac{1}{2}[/$$] where [$$] h(\phi) = - t_x p(x \phi) \frac{\delta^2}{\delta \phi^2} \log p(x \phi) dx[latex][/latex][/$$]
predict distribut with [$$]x_1, \ldots, x_n[/$$] and [$$]y[/$$] as random sampl from distribut [$$]p(x \theta)[/$$] and prior [$$]p(\theta)[/$$], predict distribut is [$$]p(i x) = t p(i \theta, x)p(\theta x) d\theta[/$$] [$$]= t p(i \theta)p(\theta x)d\theta[/$$]
prior predictive/margin likelihood a predict distribut base on a prior rather than a posterior: paramet [$$]\theta[/$$] of likelihood have been integr out (marginalised) so that ing distribut depend onli on prior parameters: [$$]p( {x}) = t p( {x} \theta)p(\theta) d\theta[/$$]
implement paramet constraint two equival way 1. integr into posterior: with [$$]\omega[/$$] as unconstrain paramet space of [$$]\theta[/$$] and constraint [$$]c: \theta \omega_c[/$$] it follow that [$$]p(\theta c,x) = p(\theta x) \frac{pr[c \theta, x]}{pr[c x]}[/$$] 2. build into prior [$$]p(\theta)[/$$]
bay estim point estim of posterior accord to loss function: [$$]argmin_t t l(t,\theta)p(\theta {x})d\theta[/$$] t is bay estim
bay estim for squar and absolut error loss for [$$]l(t,\theta)=(t-\theta)^2[/$$], bay estim is [$$]t = e[\theta x][/$$] for [$$]l(t,\theta)= t-\theta [/$$], bay estim is the posterior median
credibl interv (a,b) for [$$]\theta[/$$] given posterior [$$]p(\theta x)[/$$] (a,b) is a [$$]100 \cdot (1-\alpha) [/$$] credibl interv for [$$]\theta[/$$] if [$$] t_a^b p(\theta {x})d\theta = 1-\alpha[/$$] also call bayesian confid interv
highest posterior densiti interv (hpd) (a,b) is a [$$]100 \cdot (1-\alpha) [/$$] hpd interv if (a,b) is a [$$]100 \cdot (1-\alpha) [/$$] credibl interv and [$$]p(a {x}) = p(b {x})[/$$] multimod posterior: hpd can be union of multipl intervals!
bay factor for hypothes [$$]h_0, h_1[/$$] ratio of margin likelihoods. if [$$]h_0[/$$] and [$$]h_1[/$$] have support over the paramet interv [$$]\theta_0, \theta_1[/$$], then the factor is [$$]\frac{ t_{\theta \theta_0} p( {x} \theta) p_0(\theta) d\theta}{ t_{\theta \theta_1} p( {x} \theta) p_1(\theta) d\theta}[/$$]
interpret of bay factor valu b 1: support h0 b [$$]10^{-\frac{1}{2}}[/$$]: slight evid against h0 b [$$]10^{-1}[/$$]: moder evid against h0 b [$$]10^{-2}[/$$]: strong evid against h0 b [$$]10^{-2}[/$$]: decis evid against h0
handl nuisanc paramet if [$$]\theta_1[/$$] are paramet of interest, [$$]\theta_2[/$$] nuisanc parameters, just integr out [$$]\theta_2[/$$]: [$$]p(\theta_1 {x}) = t p(\theta_1 \theta_2, {x})p(\theta_2 {x})d\theta_2[/$$]
linear model definit with [$$] {y}[/$$] vector of n observations, [$$]e[ {y} {\theta}] [/$$] vector of p parameters, a as known n x p design matrix, and n x n dispers matrix c ([$$]c^{-1}[/$$] is precis matrix): [$$]e[ {y} {\theta}] = a {\theta}[/$$] [$$]e[ {y} {\theta}] = t {y} p( {y} {\theta}) d {\theta}[/$$]
multivari normal densiti function n-dimension observ y with mean [$$]e[ {y}] = {\mu}[/$$] and variance-covariance/dispers matrix [$$]v = ( {y} - {\mu})( {y} - {\mu})^t[/$$] if densiti is [$$]p( {y} {\mu}, {v})=\frac{1}{\sqrt{(2\pi)^n v }} \exp\{(-\frac{1}{2}( {y}- {\mu})^t v^{-1} ( {y} - {\mu})\}[/$$] [$$] v [/$$] is determin of [$$]v[/$$]
linear model with uni prior with [$$] {\mu} = a {\theta}, v=c[/$$] (a, c known): [$$]p( {y} {\theta}) \propto \exp \{(-\frac{1}{2}( {y} - a {\theta})^t c^{-1} ( {y} - a {\theta}) \}[/$$] with least squar estim [$$]\hat{ {\theta}} = (a^t c^{-1} a)^{-1} a^t c^{-1} {y}[/$$] this becom [$$]p( {y} {\theta}) \propto \exp \{ (-\frac{1}{2}(( {\theta} - \hat{ {\theta}}) a^t c^{-1} a ( {\theta} - \hat{ {\theta}}) \}[/$$] with uni prior, [$$]p( {\theta} {y}) \propto p( {y} {\theta})[/$$], so posterior of [$$] {\theta}[/$$] is a normal densiti with mean [$$]\hat{ {\theta}}[/$$] and dispers matrix [$$](a^t c^{-1} a)^{-1}[/$$]
two stage linear model definit [$$] {y} {\theta_1} n(a_1 {\theta_1}, c_1)[/$$] where [$$]a_1, c_1[/$$] are known, and a normal prior [$$] {\theta_1} n( {\mu}, c_2)[/$$] where [$$] {\mu}, c_2[/$$] are known.
lindley-smith theorem given a two stage linear model, the posterior of [$$] {\theta_1}[/$$] is [$$]n(b {b}, b)[/$$] where [$$]b^{-1} = a_1^t c_1^{-1} a_1 + c_2^{-1}[/$$] [$$] {b} = a_1^t c_1^{-1} {y} + c_2^{-1} {\mu}[/$$] margin distribut of [$$] {y}[/$$] is [$$]n(a_1 {\mu}, c_1 + a_1 c_2 a_1^t)[/$$] least-squar estim of [$$] {\theta_1}[/$$] is [$$](a_1^tc_1^{-1}a_1)^{-1} a_1^tc_1^{-1} {y}[/$$]
matrix lemma for ani matric [$$]a_1, c_1, c_2[/$$] of appropri dimens for which the invers as state in the exist we have [$$]c_1^{-1} - c_1^{-1}a_1(a_1^tc_1^{-1} a_1 + c_2^{-1})^{-1}a_1^t c_1^{-1} = (c_1 + a_1 c_2 a_1^t)^{-1}[/$$]
three stage linear model with [$$]a_1, a_2, c_1, c_2, c_3, {\mu}[/$$] known: [$$] {y} {\theta_1} n(a_1 {\theta_1}, c_1)[/$$] [$$] {\theta_1} {\theta_2} n(a_2 {\theta_2}, c_2)[/$$] [$$] {\theta_2} n( {\mu}, c_3)[/$$]
posterior of [$$] {\theta_1} {y}[/$$] for three stage linear model [$$]n(d {d}, d)[/$$] with [$$]d^{-1} = a_1^t c_1^{-1} a_1 + (c_2 + a_2 c_3 a_2^t)^{-1}[/$$] [$$] {d} = a_1^t c_1^{-1} {y} + (c_2 + a_2 c_3 a_2^t)^{-1} a_2 {\mu}[/$$]
lemma on the invers of a certain sort of pattern matrix [$$] (a i_n + b j_n)^{-1} = \frac{1}{a} i_n - \frac{b}{(a+nb)a} j_n[/$$] for [$$]a 0, b -(a/n)[/$$], where [$$]i_n[/$$] is [$$]n n[/$$] ident matrix and [$$]j_n[/$$] a [$$]n n[/$$] matrix of one
exchang assumpt given data point [$$]y_i n(\theta_i, \sigma^2)[/$$], exchang assum that our belief about the differ [$$]\theta_i[/$$] are the same, thus from a common distribution. if we think this common distribut is normal ([$$]\theta_i n(\mu, c_2)[/$$]), we end up with two stage linear model.
taylor approxim of function [$$]f(x)[/$$] at point a [$$]tf(x; a) = \sum_{n=0}^{ fty} \frac{f^{(n)}(a)}{n!} (x - a)^n[/$$]
posterior approxim with taylor approxim assum posterior [$$]p(\theta y)[/$$] is unimod and rough symmetric, we can approxim it with a normal centr at it mode [$$]\hat{\theta}[/$$] with taylor expans of [$$]\log p(\theta y)[/$$] at point [$$]\hat{\theta}[/$$]: [$$] \log p(\theta y) = \log p(\hat{\theta} y) + (\theta - \hat{\theta}) [ \frac{d}{d\theta} \log p(\theta y) ]_{\theta=\hat{\theta}} + \frac{1}{2}(\theta - \hat{\theta})^2 [ \frac{d^2}{d\theta^2} \log p(\theta y) ]_{\theta=\hat{\theta}} + \ldot [/$$] ignor higher-ord term and note that first deriv is 0, we have [$$]p(\theta y) c \cdot \exp \{-\frac{1}{2}i(\hat{\theta}) (\theta-\hat{\theta})^2 \}[/$$] with [$$]i(\theta) = [- \frac{d^2}{d\theta^2} \log p(\theta y) ] [/$$] thus posterior is approxim with normal with mean [$$]\hat{\theta}[/$$] and varianc [$$]i(\hat{\theta})^{-1}[/$$]
gibb sampl with model with likelihood [$$]p( {x} \theta,\delta,\phi)[/$$] and prior [$$]p(\theta,\delta,\phi)[/$$] base on mcmc,which build a markov chain so that the distribut of sampl valu in the chain tend toward the actual one. gibb sampling: we onli need to find a way to split the variabl of interest into group so that we can sampl from the condit distribut of each given all others: [$$]p(\theta \delta,\phi, {x}), p(\delta \phi,\theta, {x}), p(\phi \theta,\delta, {x})[/$$] algorithm: 1. choos initi estim [$$]\theta_0, \delta_0, \phi_0[/$$] 2. given current estimates, simul new valu 3. return to step 2 often burn-in of length l to remov depend on initi values, and take everi k sampl after that to reduc serial correl effects. posterior mean/median/hpd can be determin base on sample. kernel densiti estim can give us smooth approxim to this sample. predict densiti of new observ y can be estim by averag [$$]p(i \theta_i,\delta_i,\phi_i)[/$$] over sampl i
constraint on paramet in gibb sampl can alway use reject sampl technique: pick initi valu so that constraint are fulfilled, then when sampl from the conditionals, resampl repeat until the sampl fulfil the constraints. this may need mani simulations. more effici to sampl direct from an appropri constrain distribut if avail
handl miss data in gibb sampl treat miss data as extra set of parameters: if we knew miss data, we can simul from condit distribut of paramet and vice versa. therefor introduc condit distribut for miss data given paramet and per normal gibb sampling.
standard normal and convers to standard normal standard normal cdf: [$$]\phi(z) = t_{- fty}^z \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}t^2}dt[/$$] conversion: let x be a normal with mean [$$]\mu[/$$], standard deviat [$$]\sigma[/$$]. then: [$$]f_x(x) = pr(x x) = pr(z \frac{x-\mu}{\sigma})[/$$]
gamma function definit [$$]\gamma(n) = (n-1)![/$$]
expect and properti [$$]e[g(x)] = t_{- fty}^{ fty} g(x)p(x)dx[/$$] montonicity: if x and y are rvs so that [$$]x y[/$$] then [$$]e[x] e[y][/$$] linearity: [$$]e[x+y] = e[x] + e[y], e[ax] = ae[x][/$$]
covari [$$]\sigma_{xy} := e[(x-e(x))(y-e(y))] = e[xy] - e[x]e[y][/$$]
varianc and properti [$$]var(x) = \sigma_x^2 = e[(x-e[x])^2][/$$] [$$]var(x) = e[x^2] - e[x]^2[/$$] [$$]var(x+a) = var(x)[/$$] [$$]var(ax) = a^2 var(x)[/$$] [$$]var(ax+by) = a^2var(x) + b^2var(y) + 2abcov(x,y)[/$$]
integr by part [$$] t uv = uv - t u'v[/$$]
quotient rule [$$](\frac{u}{v}) = \frac{u'v - uv'}{v^2}[/$$]
chain rule [$$](f(g(x))) = f'(g(x)) \cdot g'(x)[/$$]
recurs integr by part (tabular method) wanted: [$$] t u(x)v(x) dx[/$$]. assum u has zero as n-th derivative. write down u and v next to each other, then below the deriv of u and the integr of v. solut is then uv - u'v^{2} + u''v^{3} (start with first row of u, second of v, then altern signs)
log-trick [$$]\frac{df}{d\theta} = f(\theta) \frac{d}{d\theta} \log(f(\theta))[/$$]
univari normal densiti function [$$]p(x \mu, \sigma^2) = \frac{1}{\sqrt{2\pi}\sigma} \exp \{- \frac{1}{2}\frac{(x-\mu)^2}{\sigma^2} \}[/$$]
beta function [$$]b(a,b) = \frac{\gamma(a)\gamma(b)}{\gamma(a+b)}[/$$]
find a% hpd interv for a normal with mean [$$]\mu[/$$], varianc [$$]\sigma^2[/$$] look at which z has the a(z) (area under the standard normal) that is closest to [$$]1-\frac{1-a}{2}[/$$], then the interv is [$$]\mu +- z \sigma[/$$]
what is a sigmoid function and what is it use for sigmoid is a special case of logist function, with midpoint set to 0 and width of 1. it is often use as an ation function for a neuron.
what is stochast gradient descent in ordinari gradient descent, we use all data to updat our weights, in stochast descent, we comput gradient on small batch and descent gradually.
what is the differ between l1 and l2 regular l1 (lasso) norm add as a penalti term to object function to minim sum of absolut valu of the parameters: regularization_parameter*∑(w) l2 (ridge) minim sum of squar valu of parameters: regularization_parameter*∑(w^2) l1 norm function is linear and reduc the size of paramet much quicker and therefor s in mani paramet equal 0. therefore, it also has inbuilt featur select l2 norm is more use in most case and there an analyt solution, therefor it faster to compute.
what is chain rule and how is it appli in machin learn chain rule is a rule that help to comput a deriv of a function of a function. for instance, we deriv a function sin(2x+5). first we comput sin'(2x+5) = cos(2x+5), then we comput (2x+5)'=2. therefor sin(2x+5) = 2cos(2x+5). in machin learning, we use it to back propag in a neural network.
how doe backpropag and weight optim work backpropag comput the deriv of the cost function (for example, quadrat loss) with respect to everi weight in the network. then optim step updat the weight in the direct opposit the gradient proport to second deriv (slope steepness)
how can i comput the size of the output featur map for a convolut layer if i have filter of size fxf, stride s, and pad p we can comput the spatial size of the output volum as a function of the input volum size (w), the recept field size of the conv layer neuron (f), the stride with which they are appli (s), and the amount of zero pad use (p) on the border. you can convinc yourself that the correct ula for calcul how mani neuron “fit” is given by (w−f+2p)/s+1. for exampl for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. with stride 2 we would get a 3x3 output.
what is hing loss and when is it particular use hing loss is less aggress than rmse loss, ulat as loss = max(0, 1-t*y) where t is the true label (+-1) and y is a prediction. hing loss is use when there are outlier becaus then the outlier do not get penal that harsh and don't skew the prediction.
what is laplac rule of success the probabl of a = (a + 1) / (a + !a + 2), basically, assum you'v observ one of each possibl state alreadi
logist function \[\sigma(t) = \frac{1}{1+e^{-t}}\]
central limit theorem for ani random variabl with finit mean \(\mu\) and stddev \(\sigma\): random sampl of the mean converg to a normal distribut with mean \(\mu\) and stddev \(\frac{\sigma} {\sqrt n}\).
what are the assumpt behind linear regress 1. relationship is actual linear (depend variabl is a linear combin of independ variables) 2. error are independent, 3. normal distributed, 4. constant varianc (5. non-collinear sampl is not a requir of linear regress as a technique, but of ols, one method of solution)
what doe the p-valu of a coeffici in linear regress tell you the chanc the variabl doesn't matter. (technically: given that the coeffic is zero, the probabl you erron find it non-zero)
what is the interpret of a coeffici in a linear regress model the mean chang in the depend variabl for a unit chang in that independ variabl (i.e., the slope)
how is r 2 interpret in linear regress explain variance. the proport of varianc in the depend variabl explain by the independ variables. \[r^{2} = 1 - \frac{\sum{\text{residuals}^2}}{\sum{\text{variance} * n}} = 1 - \frac{\sum{(y_i - \hat{y}_i)^2}}{\sum{(y_{i} - \bar{y})^2}}\] (also call coeffici of determination)
what are some method of deal with class imbal weight : minor instanc higher metric : precision, recall, f1, normal accuraci (kappa), auc... resampl : upsampl small class or downsampl larg one interpol : generat synthet sampl by interpol or smote (randomly-weight combin of neighbors) bag : divid major class into n subset of same size as minority, train n classifiers, combin
explain the bias-vari tradeoff bias : get wrong answers, usual from underfit varianc : high variabl answer (from small input changes), usual from overfit for a given model it usual imposs to reduc both. ensembl and mixtur can. low bias \(\leftrightarrow\) high accuraci \(\leftrightarrow\) high varianc \(\leftrightarrow\) overfit \(\leftrightarrow\) poor general low varianc \(\leftrightarrow\) low accuraci \(\leftrightarrow\) high bias \(\leftrightarrow\) underfit \(\leftrightarrow\) good general
explain bag combin sever model to reduc variance. ( b ootstrap ag gregating) train multipl model on subset of the data and combin outputs, by e.g., averag or major vote. usual use "strong," low-bia models.
explain boost combin mani model to reduc bias. sequenti train models, weight mispredict sampl higher in the next model. combin model weight by accuracy. gradient boost train each model to predict the error of the previous model (the gradient of the loss function). usual use "weak," high-bia models.
how do decis tree work recurs split the data into group base on most discrimin feature; each leaf give a prediction. to build: comput "purity" metric on label for all split of all featur (e.g. gini impurity, in ation gain) take best-scor split, creat child node for each subgroup recurs on each child node, stop at some desir level of puriti
bay theorem \[p(a b) = \frac{p(b a)p(a)}{p(b)}\]
what is logist regress linear regress squash to the rang [0, 1] with a logist function to do binari classification. \[l(x) = \frac{1}{1 + e^{-xw + b}}\] \(x\) = vector of independ variabl \(w\) = learn weight vector \(b\) = learn bias
logit (log-odds) function \[logit(p) = \ln\frac{p}{1 - p}\] invers of the logist function.
when can logist regress fail too mani variabl colinear sampl empti class sampl are perfect separ
how can you evalu the s of logist regress \(\chi^2\) between label and predictions. wald statistic: ratio of each coeff to it std err, squared; \(\chi^2\) with 1 dof. likelihood ratio: \(-2\ln \left( \frac{\text{prob weight is 0}}{\text{prob weight is }} \right) \) for each weight compar to \(\chi^2\) with 1 dof usual suspects: cross-valid of accuracy, precision, recall, f1, auc...
how doe k-mean cluster work start with k random sampl as centroid assign all point to nearest centroid recomput centroid (vector averages) repeat until stationari
how do you choos k in k-mean cluster the "elbow method:" plot ratio of between-group varianc to total varianc (or silhouett coefficient) vs. # clusters, pick the elbow in the graph.
how doe dbscan cluster work find area of high densiti (mani neighbors) grow cluster of point with at least m neighbor within distanc ep also attach edg points, exclud outlier
how doe spectral cluster work emb pairwis distanc matrix in a low-dimension space (pca) then use k-mean
how doe hierarch cluster work bottom up (agglomerative) start with all point as cluster merg close cluster (distance: sum-squared, mean, max) repeat until target # cluster remain.
how do you evalu cluster without ground-truth label compar within-clust distanc to between-clust distances. example: silhouett coeffici
what is a/b testing, statist speak how do you do it, at a veri high level signific test (hypothesi testing). tri two version of something, compar their metric with a signific test.
signific test for binari (binomial) outcom (e.g., convers out of visitors) \(\chi^2\) with 1 d.o.f. or, for small sampl or unbalanc classes, fischer exact test base on the hypergeometr distribution.
signific test for a continu (normal-ish) variabl (e.g., spend per user) welch t-test
signific test for multipl count (multinomial) data (e.g., number of each product purchased) \(\chi^2\) with \(k - 1\) d.o.f. for \(k\) counts/classes.
what are the assumpt for (unpaired) generic t-test independ sampl normal distribut (non-norm work fine for larg n)
one-sampl t-test statist ula \[t = \frac{\text{differ of means}}{\text{std err of mean}} = \frac{\bar{x} - \mu}{\sigma / \sqrt{n}}\]
welch t-test statist ula (two-sampl (independent), unpaired, unequ sizes, unequ variance) \[t = \frac{\text{differ of means}}{\text{std err of mean}} = \frac{\overline{x_1} - \overline{x_2}}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}}\]
what is a p-valu (in signific testing) the probabl that, given no significance, you mistaken find significance.
what is the danger of repeat signific test (the multipl comparison problem) how do you handl it some of the test will erron exceed the threshold for signific by random chance. the bonferroni correction: just divid your signific level by the number of test \[ \alpha^* = \frac{\alpha}{m} \]
what is the problem with stop an a/b test as soon as you see a signific how do you avoid it you may get fals signific s, as you are effect run mani signific test (the multipl comparison problem). set the test sampl size(s) in advanc to get the power you need and don't stop early.
what is regularization, and what problem doe it address ad in ation to a model, often constraints, to make a problem well-defin and/or avoid overfitting.
what is l2 regular and how doe it work ad a term for the l2 norm of the weight to the loss function of a model. penal larg weight to reduc varianc and overfitting. also known as ridg regression; a special of tikhonov regularization.
what is l1 regular and how doe it work ad a term for the l1 norm of the weight to the loss function of a model. penal a larg number of (non-zero) weights, for featur select and simpler models. (the l1 norm is a convex approxim to the l0 norm (the number of nonzeros), which is what you actual want.) also known as lasso regression.
how doe gradient descent work minim the loss function with respect to the weight by take small step in the opposit direct of the gradient. \[w \leftarrow w - \eta \nabla j(w) \tex {weight -= learn rate * gradient} \]
how doe stochast gradient descent work like gradient descent, but at at each step, comput the gradient and updat the weight use onli one or a few (mini-batch) train samples. the sampl are random shuffl or sampled. more time and space efficient.
how doe k-nearest neighbor work train store sampl in a kd-tree or similar. predict then find the k nearest neighbor and take the major of their label for classification, or averag them for regression.
what is the curs of dimension as the number of dimens increases... volum scale exponentially, requr exponenti more sampl data to learn function in the space. distanc metric lose usefulness; relat distanc between near and far point approach zero. distanc to the center or mean increases; "everi point is an outlier" ani partit of sampl becom linear separ (cover theorem), lead to overfitting.
what is the definit of log loss (a.k.a. cross-entropy), and whi is it use \[ -\frac{1}{n}\sum_{i=1}^n {y_i\log(p_i) + (1 - y_i)\log(1 - p_i)} \] binari classif metric, measur similar of two probabl distributions, punish extrem confidence. natur error function for logist regression.
defin precis \[ \frac{\text{tru predict positives}}{\text{al predict positives}} \] \[ \frac{tp}{tp + fp} \]
defin recal (sensitivity) true posit rate \[ \frac{\text{correct predict positives}}{\text{actu positives}} \] \[ \frac{tp}{tp + fn} \]
defin specif true negat rate \[ \frac{tn}{tn + fp} \]
defin f1 measur f1 = \( 2 \frac{precis * recall}{precis + recall} \) the harmon mean of precis and recal
defin the roc curv plot of true posit rate against fals posit rate. (tp = recal = sensitivity, fp = 1 - specificity)
defin auc area under the roc curve. the probabl that a random posit sampl score higher than a random negat sample.
how doe a random forest work bag multipl decis trees. plus "featur bagging," select the split featur from a random subset of all features, to reduc correl between tree
what is statist power what factor influenc power probabl that a test find signific given it is actual there. the signific level (\(\alpha\)), desir effect size, and number of sampl affect the power.
how doe hadoop work hadoop is an open-sourc implement of googl distribut file system and mapreduce. the data is store in hdfs and process there use mapreduc
what is svd singular valu decomposit is a dimension reduct technique. factor a matrix \( x = u\sigma v^t \) where \[ {align} x &amp;= m n u &amp;= m m &amp;&amp; \text{orthogonal} \sigma &amp;= m n &amp;&amp; \text{diagonal, the singular valu of } x v^t &amp;= n n &amp;&amp; \text{orthogonal} {align}\] by take onli the largest singular valu (and correspond vecotr of \(u\) and \(v\)), you obtain a low-rank approxim to \(x\) (minim the frobenius norm between \(x\) and \( u\sigma v^t \) ). the number of non-zero singular valu is the rank of \(x\) (# linear independ columns, dimens of space span by row or cols).
what is the roc curv receiv oper charactaristic, it plot the sensit (true positives) as a function of the fall-out (fals positives), for a binari classifi
law of total probabl \[ {align} \text{p}(a) &amp;= \text{p}(a b) + \text{p}(a \overline{b}) &amp;= \text{p}(a \mid b) \text{p}(b) + \text{p}(a \mid \overline{b}) \text{p}(\overline{b}) {align}\] in general, for a partit of the sampl space \( b \): \[ \text{p}(a) = \sum_i \text{p}(a b_i) = \sum_i \text{p}(a \mid b_i) \text{p}(b_i) \]
probabl multipl rule (joint probability) \[ {align} \text{p}(a b) = \text{p}(a \mid b) \cdot \text{p}(b) = \text{p}(b \mid a) \cdot \text{p}(a) {align} \]
binomi distribut the probabl of exact \(k\) success in \(n\) independ trials, each with probabl \(p\) \[ {n \choos k} p^k(1-p)^{n-k} \] \[ \mu = np \] \[ \sigma^2 = np(1 - p) \]
poisson distribut the probabl of exact \(k\) independ event in an interval, given an averag of \( \lambda \) event per interv \[ \frac{\lambda^k e^{-\lambda}}{k!} \] \[ \mu = \lambda \] \[ \sigma^2 = \lambda \]
geometr distribut probabl of \(k\) independ trial to reach one success, each with probabl \(p\) \[ (1 - p)^{k - 1}p \] \[ \mu = \frac{1}{p} \] \[ \sigma^2 = \frac{1 - p}{p^2} \]
negat binomi distribut probabl of \(k\) success befor \(r\) failures, each independ with probabl \(p\) \[ {k + r - 1 \choos k} p^k(1 - p)^r \] \[ \mu = \frac{pr}{1-p} \] \[ \sigma^2 = \frac{pr}{(1-p)^2} \]
hypergeometr distribut probabl of \(k\) success in \(n\) draw without replac from \(n\) item contain \(k\) success \[ \frac{\binom{k}{k} \binom{n - k}{n - k}}{\binom{n}{n}} \] \[ \mu = n\frac{k}{n} \] \[ \sigma^2 = n{k\over n}{(n-k)\over n}{n-n\over n-1} \]
normal distribut \[ \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left ( \frac{x - \mu}{\sigma} \right )^2} \]
defin linear independ for vector a set of vector \(v_i\) is linear independ iff the equat \[ \sum_i {a_i v_i} = 0 \] has onli the trivial (all-zero \(a_i\)) solution. can be determin by gaussian elimin on the abov equation.
defin orthogon for vector the dot product is zero.
defin the span of a set of vector the set of all linear combin of the vectors.
defin the rank of a matrix the dimens of the space span by the columns; the number of linear independ vector among the columns.
defin the eigenvector and eigenvalu of a matrix the eigenvector of a matrix \(a\) are the vector \(x\) such that \[ a x = \lambda x \] i.e., multipl by \(a\) act like scalar multipl by \(\lambda\), the eigenvalu correspond to \(x\).
defin a basi of a vector space a set of linear independ vector that span the space, i.e., the number of vector equal the dimens of the space.
state as mani equival of the invert matrix theorem as you can. an \(n n\) matrix \(a\) is invert if and onli if... the equat \(ax = 0\) has onli the trivial solut \(x = 0\). the determin of \(a\) is nonzero. the row and column of \(a\) are linear independent. the row and column of \(a\) span and a basi for \(\mathbb{r}^n\). \(x \to ax\) is one-to-on and onto. the transpos is invertible. the invers of \(a\) exist and is unique. \(a\) has rank \(n\). the null space of \(a\) is \(0\). \(a\) has \(n\) nonzero eigenvalu and singular values.
how do support vector machin work svms find the hyperplan of maximum distanc (margin) from the nearest sampl of each class (the "support vectors"). for non-separ data, we minim a combin of the distanc from the margin (for misclassifi samples) and the invers of the margin size. svms can find non-linear hyperplan in higher dimens via the kernel trick, sinc it onli make use of distanc (dot products) between samples.
how is the (soft-margin) loss function deriv for support vector machin the hyperplan bound the (hard) margin are \( w \cdot x - b = 1 \) for weight and bias vector \(w\) and \(b\). the distanc between them is \(2 \over w \), so minim \( w \) maxim separation, subject to the constraint that the sampl lie on the correct side of the margin: \[ y_i(w \cdot x_i - b) \ge 1 \iff {cases} w \cdot x_i - b \ge +1 &amp; \qquad y_i = +1 w \cdot x_i - b \le -1 &amp; \qquad y_i = -1 {cases} \] for the soft-margin (not linear separable) case, we introduc the hing loss \( \max(0, 1 - y_i(w \cdot x_i - b)) \), and minim the averag hing loss over all sampl plus the invers margin size: \[ \left [ \frac{1}{n} \sum_i \max(0, 1 - y_i(w \cdot x_i - b)) \right ] + \lambda w ^2 \] this can be solv as a constrain quadrat program problem with standard techniqu like conjug gradient, or minim with (sub-)gradi descent or coordin descent.
what is princip compon analysi pca "rotates" data such that featur becom linear independ and order by variance, use for decorrel featur or dimension reduction.
how is pca relat to svd the right-singular vector of \(x\) are the princip compon - the eigenvector of the covari matrix \(x^tx\) the compon weight (eigenvalues) are the squar of the singular values. given the svd of \(x = u\sigma v^t\), the princip compon are the column of \(v\) and their weight (eigenvalues) are the squar of the singular valu \(\sigma\). pca can be comput by svd. after mean-cent each column, pca find the matrix of eigenvector \(w\) and diagon matrix of eigenvalu \(d\) of the covari matrix \(x^tx\), such that \(x^tx = wdw^{-1}\). \[ {align} x &amp;= u\sigma v^t x^{t}x &amp;= v\sigma u^t u\sigma v^t &amp;= v\sigma^2v^t &amp;&amp; \text{sinc } u \text{ is orthogonal} &amp;= v\sigma^2v^{-1} &amp;&amp; \text{sinc } v \text{ is orthogonal} &amp;= wdw^{-1} {align} \]
how do you decid the number of compon to keep in pca plot explain varianc vs. number of compon and pick a point of diminish returns. the explain varianc is the sum of the retain eigenvalu divid by the sum of all eigenvalues.
defin covari \[ {align} \operatorname{cov}(x,y) &amp;= \operatorname{e}{\big[(x - \operatorname{e}[x])(i - \operatorname{e}[y])\big]} &amp;= \operatorname{e}\left[x y\right] - \operatorname{e}\left[x\right] \operatorname{e}\left[y\right] {align} \] for discret data \(x_i\) and \(y_i\): \[ \frac{1}{n} \sum_i{(x_i - \bar{x})(y_i - \bar{y})} \] the entri of the covari matrix of \(n\) observ of \(k\) discret variabl \(x\): \[ c_{jk}=\frac{1}{n}\sum_{i=1}^{n}\left( x_{ij}-\bar{x}_j \right) \left( x_{ik}-\bar{x}_k \right) \]
what are some techniqu for featur select univari ranking: choos the best featur as rank by variance, correl with labels, mutual in ation, etc. forward select : find the best singl featur via cross-validation. then add each remain feature, one at a time, to find the best pair of features. repeat, ad one featur each iter until "good enough." recurs featur elimin : use a model that give weight to features, train with all features. remov the lowest-weight features, repeat until satisfied. dimension reduct : pca, svd built-in : some algorithm have featur select built in: lasso, decis tree
what is bootstrap comput a quantiti of interest mani time on random subsampl of the data to estim uncertainti or reduc variance.
what are the limit of pca sensit to the scale of featur assum featur with less varianc are less import assum (gaussian) varianc accur character featur assum featur are orthogon onli per s linear tran ation (but see kernel pca) onli remov linear correl
defin cosin similar the cosin of the angl between two vectors. \[ (\theta) = \frac{a \cdot b}{ a b } \] 1 for parallel vectors, 0 for orthogon vectors, -1 for opposit direction.
how doe naiv bay classif work comput the probabl of each class given a sampl use bay rule, and return the most like class. for class \(c_k\) and sampl \(x = (x_1, ..., x_n)\), \[ {align} p(c_k \mid x) &amp;= \frac{p(x \mid c_k)p(c_k)}{p(x)} &amp;&amp; \text{bay theorem} &amp;= \frac{p(c_k) p(x_1 \mid c_k) \cdot p(x_n \mid c_k)}{p(x_1) \cdot p(x_n)} &amp;&amp; \text{assum independ } x_i {align} \] for categor features, you can tabul the probabl directly. for continu features, a normal distribut is usual assumed; for count features, a multinomi distribut is appropriate.
give some exampl of power-law distribut word frequenc in natur languag (zipf law) incom popul of citi connect in social network
describ power-law distributions. whi are they difficult to work with \[ f(x) = cx^{-\alpha} \] the mean and varianc are infinit for valu of \(\alpha \le 2\) or \(3\) (resp.), which are common, invalid typic assumpt of normality.
how do you determin the sampl size need to measur a given differ with a certain confid view it as a \(t\)-test of signific of the differ \(\delta\): \[ \frac{\delta}{\sigma \over \sqrt{n}} \gt t \] then \[ n \gt \frac{t^2 \sigma^2}{\delta^2} \] where \(t\) is the critic valu for the desir level of confid (with high d.o.f.), or equivalently, the correspond standard normal deviate, e.g. \(95 \to t = 1.96\).
what is a quick ula for confid interv of normal-ish data \[ \mu z\sigma \] or in general \[ \text{ statist } \text{ critic valu } \text{ std dev of the statist } \] \(z\) is the critic valu of the test statist correspond to the desir confid (e.g., \(z = 1.96\) for \(95 \) confid with normal data), and \(\sigma\) is the stddev of the statist (e.g., the sem = \(\sigma \over \sqrt{n}\) for a mean), not the popul .
defin condit probabl \[ p(a \mid b) = \frac{p(a b)}{p(b)} \]
instanc input. vector of valu of attributes.
concept classification. function we care about. essenti the entir set of exampl of the label, membership test.
target concept classif learning. the correct set of exampl which are the target answer.
hypothesi class classif learning. the set of all possibl function be considered.
sampl classif learning. train set. set of input exampl with labels.
candid classif learning. concept which may be target concept.
test set classif learning. set of instanc with labels. use to measur per anc of candid concept.
defin the silhouett score \[ \frac{\text{nearest cluster dist - within cluster dist}}{\max(\text{nearest cluster dist, within cluster dist})} \] averag over all sampl
whi is the central limit theorem import it let you approxim most ani random variabl with a normal variable, and quantifi the error of the approximation.
decis tree split the dataset on an attribut at each node until all exampl remain have the same label
demorgan law a and b = !(!a or !b)
what is the comput complex of pariti function o(2^n) - it is a hard (superexponential) function
how mani possibl decis tree are there for n binari attribut and binari label the total number of decis tree possibl are 2^2^n, which is basic infinite. this mean decis tree are incred expressive, and find the right one can be hard without a heuristic.
what is the id3 decis tree algorithm greedi approach to creat a decis tree. pick best attribute, split examples, if not perfect sort recurse.
what is a common "best attribute" in decis tree in ation gain. reduct of random base on split on an attribute. the differ of the entropi in the origin set and the sum of weight entropi of the ing sets.
what is the ula for entropi - σ_ v (p(v) * logp(v))
how doe entropi chang when an even label set is split into two even label set it doesn't chang at all, even if the two set are differ sizes.
what are the two main bias of algorithm search in a space restrict bias, prefer bias
what is restrict bias the bias we get when we pick a solut method which reduc our hypothesi space.
what is prefer bias prefer bias tell us which hypothes in the total space our algorithm prefers. all heurist impart a prefer bias.
what are the main two induct bias in the id3 algorithm prefer tree with good split at the top prefer correct tree over incorrect tree prefer shorter tree over longer tree (consequ of first)
what is a good way to prevent overfit with decis tree prune the tree, collaps node up the tree as long as the ing error in the cross valid set is low.
how do you handl the output of a decis tree for regress the leav must vote somehow. an averag or linear fit should work.
what is regress to the mean in the height of children the slope of the line map parent height to children height against the averag is 2/3. becaus it is less than 1 it is an exampl of regress to the mean.
how do you repres distinct choic in regress bit vector work well, not impli and order
what is the ation of a perceptron the sum of the input multipli by their weight
what is the perceptron rule how to set the weight of a singl unit (and threshold value, by ad a bias term of 1). deltaw_i = learn rate * (y_desir - y_estimated) * x_i the perceptron rule will linear separ linear separ data.
gradient descent delta w_i = learn rate * (label - ation) * x_i gradient descent can converg even if the data is not linear separ gradient descent will converg to local optima
sigmoid function 1 / (1 + e^-a)
what are the hidden layer in a neural net all of the layer not includ the input layer or the output.
back propag comput benefici organ of the chain rule / gradient desent
optim weight momentum higher order deriv random optim penalti for complex penalti for larg weight
what is the restrict bias of neural network with suffici complex network structur you can represent: boolean function continu function arbitrari function (with at least 2 hidden layers)
what is the prefer bias of neural net by start with low random initi weight prefer low complex explan prefer correct expan
for logist regression, what function is use to tran label probabl to recast the problem as linear regress when can't this method be used, and what to use instead the logit function \[ logit(p) = \ln\frac{p}{1 - p} \] this is undefin for \(p = 0\) or \(1\), so have to use iter method like gradient descent.
in what sens is dimension reduct by pca optim the basi ed by the \(k\) largest princip compon (eigenvectors) minim the least-squar project error in \(k\) dimensions.
what doe pca comput (in matrix-theoret terms) the eigenvector of the covari matrix (the princip components).
what is the \(\chi^2\) test, and how is it comput compar an actual discret distribut \(a_i\) to an expect distribut \(x_i\). \[ \chi^2 = \sum^k_{i=1}{\frac{(x_i-a_i)^2}{x_i}} \] compar to critic valu from tabl with \(k-1\) d.o.f. the error (differences) must be independ and normal distributed.
defin correl covari normal by the standard deviat \[ corr(x,y) = \frac{cov(x, y)}{\sigma_x \sigma_y} = \frac{\operatorname{e}{\big[(x - \operatorname{e}[x])(i - \operatorname{e}[y])\big]}}{\sigma_x \sigma_y} \] for discret data \(x_i\) and \(y_i\): \[ \frac{\sum (x_i-\bar{x})(y_i-\bar{y})} {\sqrt{\sum (x_i-\bar{x})^2 \sum (y_i-\bar{y})^2 }} \]
what are the advantag of decis tree interpret no featur engin necessari nonlinear model provid featur import weight predict is effici
what are the disadvantag of decis tree prone to overfit high varianc hard to learn some simpl function (parity, xor) axis-parallel decis boundari (not great for smooth boudaries)
what are the advantag of svms can model known non-linear relationship with kernel can find "best" model for given hyperparams, sinc error function has global minimum
what are the disadvantag of k-nearest neighbor sensit to featur scale suffer from class imbalance; weight can help suffer from curs of dimension (all point basic same distanc in high dimensions). model size grow with data; must store all sampl
what are the advantag of naiv bay classif interpret effici compact model work with small data
what are the disadvantag of naiv bay classif assum featur are independ (given the class) assum a distribut for continu featur (usual normal) doe not handl spars data well fixed-s model; diminish return with more data
what are the disadvantag of svms slow and larg in both train and prediction; don't scale well not great with multiclass problem
describ a project or research you'v done your respons here
describ a challeng you face and how you overcam it your respons here
describ a mistak or failur in your previous experience, and how you overcam it your respons here
describ someth you'v enjoy in your previous experi your respons here
describ a conflict in your previous experience, and how you handl it your respons here
describ someth from your previous experi that you would do differ your respons here
describ a time when you took initi your respons here
describ a time when you work with other to solv a problem your respons here
describ a chang in project scope or schedul in your previous experience, and how you handl it your respons here
how doe an artifici neuron (perceptron) work appli an ation function to the weight combin of it inputs. \[ y = f \left( \text \sum w_i x_i \right) \] common ation function are linear, step, sigmoid, tangent, rectifi linear, etc.
how doe a (fully-connected, feedforward) neural network work a neural network is compos of layer of neuron that reciev the same inputs, appli an ation function, and produc an output. the output of one layer becom the input of the next. neural network are train via backpropagation, which iter updat the weight in the direct that minim error at the output (often via gradient descent).
how doe backpropag work backpropag iter updat the weight of a neural network to minim the error between the actual and desir outputs. each weight get updat in the opposit direct of the deriv of the loss function with respect to that weight (the gradient). the deriv at each layer depend on the deriv of all success layer (between it and the output), so the weight updat are calcul from the output back: backpropagation.
what is the kernel trick whi is it use the kernel trick can comput the s of vector dot product after non-linear map into high-dimesion space use onli dot product in the origin low-dimenson input space. it find high-dimension non-linear descript of data at low comput cost. if \(\phi: \mathbb{r}^n \to \mathbb{r}^m\) map vector to a higher-dimension space (\(m \gt n)\), then the correspond kernel function \(k: \mathbb{r}^n \mathbb{r}^n \to \mathbb{r}\) \[ k(x,y) = \langl \phi(x), \phi(y) \rangl \] act as a dot product in higher-dimension map space, but can be comput use onli low-dimension dot product \(\langl x,i \rangle\).
between-factor one way anova: purpose, ss partition, f f=1 doe not prove mean are equal but fail to find evid that the group mean are the same
valu of an anova table: ss, df, ms, f, p-valu
what to do if the omnibus f test reject h 0 - at least one group differ from the other groups, base on one or more effect (main/interaction) further inspection: - visual inspect (no statist proof) - multipl comparison (test or ci´s) 1. plan - contrast 2. post hoc comparison find the group which differ increas f reject h 0
anova: assumpt 1. independ observ 2. in each group the score are normal distribut (qq-plots; skew and kurosis) 3. in all group equal varianc - (check sampl varianc (or sd´s): max/min 2 is ok - levene´ test: be catious, use of a signific test to confirm h 0
experi have 3 characteristics: manipul of treatment level (research control natur and time of each level) random assign of case to levels; group (remov bias, averag out differ among cases) control of extraen variabl (on treatment level chang dure experiment) - when all three hold, differ in score are attribut to differ in treatment
how to control extran variables: hold them constant counterbal their effect turn them into extra factor
between-subject designs: in ation differ due to treatment are test between group of subject case are random assign to treatment level differ case in everi level between-subject (each subject appear in one treatment only)
factori design (between subject designs) treatment level are determin by more than one factor main effect of each factor, and interaction(s) interact effect between a and b (differ in factor a depend on which group b you are in) differ may differ between a 1-3 on b
factori anova usual more than one factor (defin differ groups): a*b groups, and main effect and interact effect can be test whi sever factor : (design or substant reason, statist resion i.e. reduct of error variance, interplay between variables)
sourc of variance: identifi them list each factor as sourc examin each combin of factor (complet cross - includ interact as source) when effect is repeated, with differ instances, at everi level of anoth factor - includ factor as sourc example: factor a, b, subject s a, b, axb, and s differ s, at each level of a and b: a, b, axb, and s(ab) - includ subject (measur multipl times) as factor explain varianc
what is bag fit a model to a subset of the data, do this n times, and averag the n model to model the entir dataset
what is a weak learner a hypothesi which doe better than chanc on ani distribut of the data
what is vc dimens vaprix-chervonenki dimension. the largest number of point that a hypothesi space can shatter
what doe shatter mean a hypothesi space shatter a dataset if it is capabl of label all of the data correct for all possibl label
what is a hypothesi space a set of mani hypotheses.
comput complex how much comput effort is need for a learner to converg
sampl complex batch - how mani train exampl are need for a learner to creat a success hypothesi
mistak bound onlin learn - how mani misclassif can a learner make over an infinit run
consist learner a learner such that the estim label is consist for all exampl in the train data
version space the version space of s is the set of all hypothesi in the hypothesi space which are consist with the sampl data s.
what is pac learn probabl approxim correct learn
what are the paramet in pac learn c: concept class l: learner h: hypothesi space n: h size of hypothesi space d: distribut over input 0 =epsilon =1/2 error goal 0 =delta =1/2 certainti goal probabl (1-delta) approxim (epsilon) correct (error_d(h)=0)
when is a concept class pac learnabl c is pac-learn by l use h iff learner l will, with probabl 1-delta output a hypothesi h h such that error_d(h) = \epsilon in time and sampl polynomi in 1/epsilon, 1/delta &amp; n.
epsilon-exhaust version space vs(s) is epsilon exhaust iff for all hypothesi in the version space vt(s) error_d(h) = epsilon (all hypothes remain have error below epsilon)
haussler theorem bound true error as a function of the number of sampl drawn from a distribution. let error_d(h_1, ..., h_k in h) epsilon probabl of this error remain after m sampl is pr(h_i consist with c on m examples) = (1-epsilon) ^ m probabl of at least one hypothesi with greater error consist with c no m exampl is then... = k*(1-epsilon)^m = h (1-epsilon)^m = h * e ^ (-epsilon * m) do some math show m = (1/epsilon) * (ln( h ) + ln(1/delta))
what is the kernel trick becaus not everyth is linear separable, we can use the kernel trick to project the data into a higher dimension space by replac the dot product similar term in the svm weight calcul with a better similar function use domain knowledge.
what is the radial basi kernel e ^ -( x - y ^2 / (2 sigma^2))
what is the mercer condit basic that the measur behav similar to a distance. this definit need improvement.
what is a svm support vector machine, which is a "machine" for find "support vectors". support vector are the exampl requir to defin the boundary.
what is the margin of an svm the distanc between the hyperplan and the closest data point on either side. this commit the least to the data, while still correct classifi it. it help increas general and reduc overfitting.
whi is boost resist to overfit becaus boost essenti increas the margins. boost is veri unlik to overfit.
when doe boost overfit when the data is disturb by pink nois (uni noise). also when the under weak learner overfits.
what is the optim model for find maximum margin quadrat program
what is the unin ed prior treat all hypothes as equal like
what are bayesian network bayesian network help repres and manipul probabilist quantiti over complex spaces.
what is joint distrobut where two probabl distribut are related.
condit independ x is condit independ of y given z if the probabl distribut govern x is independ of the valu of y given the valu of z, that is is for all x,y,z, p(x = x y = y, z = z) = p(x=x z=z) or more compact p(x y,z) = p(x z)
what is the ula for independ variabl pr(x, y) = pr(x) * pr(y)
what is the probabl chain rule pr(x, y) = pr(x y) * pr(y) which also impli pr(x y) = pr(x) for independ distribut
what is a belief network / bay net / bayesian network direct graph show depend of probabl distribut
what order do you sampl probabl in, in a joint distrobut bay net topolog order, which is an order such that for everi direct edg u,v in the graph, the node u come befor v in the ordering.
what is probabl margin p(x) = sigma_i p(x,y)
how do you per classif use a naiiv bay net argmax p(v) pi_i p(a_i v)
what doe naiiv bay mean naiiv bay assum strong independ between variables. given a label you can infer the attributes, and then you can work backward from attribut to infer the label use bay rule.
what are some of the downsid of naiiv bay one unseen attribut (probabl of it is 0) in the product of all of the probabilities, reduc the whole product to 0. also it doesn't actual estim the probabl well, due to not take into joint probabilities. though it doesn't need to correct estim the probabilities, if the onli goal is classification.
what are some of reason naiiv bay is cool tractible, the gold standard (best we can possibl do) if the attribut are iid, and can per infer in ani direction. also readili handl miss attributes.
hill climb sampl point in the neighborhood, move to larger value, repeat until no larger valu exist
what are the trick to improv per anc of random hillclimb keep track of posit you'v alreadi been at, and don't reevalu them.
what is the simul annealng algorithm for a finit set of iterations: sampl a new point in neighborhood jump to new sampl with probabl given by p(x, x_t, t) decreas temperatur t p(x, x_t, t) = 1 if f(x_t) = f(x), e^((f(x_t)-f(x))/t), otherwis
what is the probabl of simul anneal end at a particular x pr(end at x) = e^(f(x)/t)/z_t (where z is a normal term)
what is the basin of attract the state space where gradient decent s in a particular local maxima
what are the import part of genet algorithm popul of individu mutat (local search) cross over (in ation sharing) generat (iter of improvement)
what is one point crossov for genet algorithms, choos a random bit and creat children with half of their in ation from opposit parents, split on that bit
what are the assumpt in use crossov in genet algorithm local of bit is import there are subspac to optim independ
what is uni crossov in genet algorithm each bit is random select from one parent (and the other child get the other parent bit)
what is the point of random optim it overcom local minima, becaus it has the option of take step which are not in the best direct
what is the basic idea of mimic direct model under probabl distrobution, and success refin this model through iter
what is the mimic algorithm generat sampl from p_thetat(x) // start at uni set theta_t+1 to nth percentil // fittest retain onli those sampl s.t. f(x) = theta_t+1 estim p_thetat_1(x) repeat
how doe mimic use depend tree to estim distrobut it use depend tree as a minim represent of relationship between variables, which are then abl to generat new sampl probabilist in time linear in the number of featur use topolog sort on the depend tree.
what is the right way to compar similar of probabl distrobut d_kl(p p_pi) = \sigma p[lg(p) - lp(p_pi)] this impli that you should pick parent which maxim the sum of the mutual in ation between everi featur and it parent
how can you creat a depend tree from sampl for mimic creat a fulli connect graph of all featur in the data label the edg of the graph with the mutual in ation between the featur find the maximum span tree
what are the practicl issu with mimic mimic doe well with structure, becaus it model it. mimic doesn't get stuck in local optima often mimic get you probabl theori built in mimic is veri time complex, though it finish in veri few iter this is veri good for fit function which are veri expens to evalu
miss data lack of respons due to sever reason such as: e.g. subject refus to particip or the answer is unread
when is a miss score a miss datum onli if an under true valu exists! important!
how doe spss deal with miss data spss would just remov the case with miss valu total exclus from the analysi
whi shall we not ignor miss data i. ineffici - loss of in ation and size - loss of power ii. bias s, depend on: a) systemat differ between respond and non-respond (depend on natur of missingness) b) proport of miss data (when compar two treatment and the bias is the same in both treatments, there is no bias in the comparison!) iii. incorrect inferenti in ation (e.g. standard errors, statist tests) - serious problem!
what is unit non-respons a sampl unit is not observed, the entir data collect fail - data are complet miss (e.g. subject did just not come)
miss by design data are partial miss not-applic items, incomplet design (e.g. if you answer question 2 with no, pleas got to question 6) predictable!
partial non-respons (time dependency) data are partial miss attrition, miss baseline, break-off dure interview, drop-out important: ask yourself what realli happen
item non-respons data are partial miss skip items, inadequ responses, in ation lost e.g. score forgotten to put on the sheet. note: if the pattern look random, this is the easiest one to deal with
q: first item to administ is select on the basi of age of child. what is the typolog of miss data miss by design
q: last item administ depend on the per anc of the child. (stop after 3 fals items) what is the miss data typolog miss by design
an item was skip and no score was given. what is the miss data typolog item non-respons
a child refus to further participate. what miss data typolog is this partial non-respons (if it was more than one item missing). if this ed in just one item that could not be scored, it would be item non-respons
what are potenti sourc of non-respons 1) mode of data collect (experimenter, telephone...) 2) questionnair (layout, wording, hard questions...) 3) respond (accident skipping, refusal, do not know answer...) 4) interview (guidance, probin, recording...) 5) data process (coding, editing...)
how doe matrix r look like indic variabl (matrix) r indic whether an element of the data matrix is observ ( r = 1) or miss (r = 0)
how is the distribut of r call the miss data mechan
what is a miss data mechan an what is it use for it a mathemat devic use for: 1) describ rat and pattern of miss valu 2) captur rough possibl relat between missing and the (unobserved) valu of the miss item 3) but it doe not captur causal relationship a miss data mechan is alway an assumption!
what are the three typolog of missing mechan miss complet at random ( mcar ) miss at random ( mar ) miss not at random ( mnar )
what doe mcar impli missing is not relat to the observ (yobs) and miss data (ymis): p(r) doe not depend on yob and ymis. no bias due to (systematic) differences. respond are a repres (sub)sampl of the population: onli loss of power (becaus of data loss) so: if mcar - the sampl is still a good repres of the popul
what das mar impli missing may be relat ot the observ (yobs) but not to the miss data (ymis): p(r) may depend on yob but not on ymis. no bias due to (systematic) differ given observ data. subgroup of repond are repres subsampl of the population. - non-repons can be predict by observ data!
q: when is data mar in a longitudin studi when the missing depend on previous measur but not on actual (and future) measurements. be careful! if further specul is need - mnar
what doe mnar impli missing is relat to the miss data (ymis) (and mayb the observ data yobs): p(r) depend on ymis. systemat bias due to (systematic) differ - it is unknown what we do not measur
what is a differ name for mnar in ativ non-respons
q:when is data mnar in a longitudin studi missing is relat to the actual measur and mayb futur measur e.g. drop out becaus of futur anticip (it was hurt and therefor there will be pain involv again in the future)
how can we tri to check our assumpt (regard miss data mechanisms) compar respond with non-respond on the observ score
in which set in mar plausibl miss by design (plan missingness) latent variables: miss with probabl 1
how can we find caus and correl that confirm our assumpt (regard miss data mechanisms) 1) descript techniqu to investig pattern of missing 2) test mcar vs. mar (test mar vs. mnar is not possible) 3) follow-up of nonrespond (rather unrealistic)
how should we handl miss data 1) avoid/ prevent it 2) collect data on reason for miss (obtain in ation about the miss data mechanism) 3) use descript techniqu and test mcar vs. mar 4) where possible, do a mar approach - direct model of observ and miss data (multilevel) - multipl imput of miss data: predict miss score -(possibl other miss data treatments)
how should we handel mnar this mechan is non-ignor and requir advanc model (but jorg doe not specifi how)
what are the two delet method and when are they applic listwis delet pairwis delet applic onli when data is mcar or mar
what is listwis delet complet case analysi = casewis deletion: delet subject for which at least 1 score is miss - this is the method spss use
what is pairwis delet avail case analysi = delet subject that miss at least 1 score necessari for the analysi at hand
when do the delet method give unbias estim onli if miss data is mcar although they are applic for mcar and mar (as jorg say dure the lecture)
how are the procedur base on imput call singl imput multipl imput
how do imput method work they substitut plausibl valu for the miss score
what are the advantag of imput method more effici than analyz complet case (more powerful) opportun to use in ation about miss score complet data can be analyz with standard procedur and softwar result in complet data set that is the same for all follow analys
what are the disadvantag of imput method sometim hard to implement (especi multivari cases) (he say dure the lectur that singl imput is easy) some (ad hoc) imput sever bias distribut and relat
what are the three singl imput method and how do they work 1. uncondit means: fill in mean 2. condit means: fill in model predict (e.g. regress imputation) 3. condit distributions: fill in draw from the distribut of ymis given yob (e.g. fill in predict plus random error)
which are the most and least prefer singl imput method least:uncondit mean middle: condit mean most: condit distribut (add nois and is therefor more realistic)
what problemat with singl imput of uncondit mean bias varianc and covari (slide 34 and the previous and follow give examples)
what is problemat with singl imput of condit mean bias covari (slide 34 and the previous and follow give examples)
what is problemat with the singl imput of condit distribut noth much. they are unbias under mar
shortcom of singl imput bias estim : mean and/or varianc and/or covari the joint distribut of the variabl is disrupt ses, p valu and other uncertainti measur are mislead becaus they do not take into account the extra uncertainti caus by miss valu imput are treat as observ sampl size is not equal to n
what doe multipl imput correct for and how repeat the imput process: paramet estim vari due to the random charact of the imput procedur and this variat can be use to correct the varianc and ses
what is the procedur of multipl imput 1. repeat imput process m times: m imput data set 2. analyz the m data set seper use standard techniqu 3. summar the s, take into account differ within dataset (uncertainti in data) and between dataset (uncertainti due to miss data and imputation)
what are maximum likelihood-bas procedur and how do they work maximum likelihood estim of a paramet is the valu of the paramet that is most like to have ed in the observ data how analyz the full, incomplet data set use maximum likelihood estim with miss data: likelihood comput for case with: - complet data on some variabl - compplet data on all variabl - two likelihood are maxim togeth
pros and con of maximum likelihood-bas procedur pro: ml base procedur give unbias paramet estim and standard error con: onli avail for linear model (e.g. regression) advantag over multipl imputation: it doe not requir the care select of variabl use to imput valu
3 step of miss data treatment in practic 1. avoid md 2. if md occurs, administ reason for md for each case 3. data analysis: - veri few md singl imput use plausibl model - more than veri few consid mechanism(s) under md - m(c)ar: likelihood-bas procedur or multipl imput -mnar: specif likelihood-bas method (pattern mixture, use of auxiliari variables)
effects: two-way anova - a main effect is the averag of the simpl effect for a certain factor (for all level of the remain factors) - main effect : most appropri interpret with no interact - not repres of simpl effect if interact is present factori design produc two type of in ation: interact effect (comparison of the simpl effects) main effect (two single-factor experiments) first look at simpl effect a on onli b1 (if same for b2… exclud interact effect)
effect sizes: p-valu measur the signific of a factor: e.g. viewcat: p .001: strong signific
effect sizes: e.g. η2 measur the size of the difference: e.g. η2 = 7811/39438 = 0.198 some other effect size measur exist, each with their own advantag viewcat explain rough 1/5th of the varianc - look at combin of p-valu and effect size to judg relev of a variabl with huge n, even the tiniest differ is signific
unequ / equal n per cell prefer : equal number of subject per cell sum of squar of effect and interact are orthogon effect are complet separated, and test are independ unequ number of subject per cell : differ way to decompos effect 1. regress approach (adjust each effect for all other effect to obtain it uniqu contribution) ('type iii ss'; spss default) 2. experiment method : estim the main effect ignor the interaction, estim the interact adjust for the main effect ('type ii ss') 3. hierarch approach : use a (theoret based) order in decompos the effect ('type i ss')
factorial- s design - factori design with minim two factor one factor serv as a so-cal ing factor - block factor intrins to the subject (e.g., sex) relat to the depend variabl (e.g., therapy-effect) - purposes: draw separ conclus about each (e.g., differenti therapy-effect in men and woman) error reduct (more power to reveal, e.g., overal therapi effects)
factor in factori anova - two type of factor experiment (i.e., condit in experiment) block (i.e., fix characterist of subjects) - factorial- s design 1. randomized- s design 2. post-hoc design
randomized- s design random s design: simplest case of two-way design - factor a: experiment factor - factor b: block factor block factor - homogen s of subject are ed beforehand to reduc within-group variabl - block factor defin s: error control there may be onli 1 observ in each cell of the design - no interact between the factor measur
randomized- s design and it advantages: advantag - reduct of error varianc - increas compar of group by assur the size are equal (make the group more homogeneous) - interact between factor can be detect alway includ ing variabl in anova, regardless of signific
post-hoc ing block after collect of the data block not initi plan in design - form s post hoc, segreg subject into homogen s - problem : often unequ sampl size - problem : data fish if ing variabl is continuous, ing caus a loss of in ation (median split are evil); consid altern - ancova - regress analysi
type of experiment design between-subject design - differ due to treatment are test between group of subject - one of more factor in the experiment design - differ case in each cell of the experiment design repeated-measur design - each case particip in two or more treatment level and is measur onc at each level - within-subject designs: differ due to treatment are test within the same set of subject
fix vs. random effect (example) studi : treatment effect factor : hospital, 3 level (groningen, leeuwarden, assen). goal of studi : are the main hospit in this region equal effect fix factor goal of studi : are dutch hospit similar; these 3 are a random sampl of all dutch hospit random factor _________________________________________________ fixed: onli the 3, no extrapol to other hospit random: extrapol to the popul of dutch hospit
matrix algebra - terminolog : matrix exampl of a matrix: (capit letter, bold)
matrix algebra - terminolog : vector exampl of a vector: (small letter, bold)
matrix algebra - terminolog : scalar exampl of a scalar: (small letter, italics)
matrix algebra - terminolog : size of a matrix number of row and number of column also called: order or dimens exampl of notation: matrix a is of order (3x2)
matrix algebra - terminolog : length of a vector length of a vector: number of element example: length of vector a is 3
matrix algebra - terminolog : transpos transpos of a matrix (vector): interchang row and column
matrix algebra - terminolog : column vector column vector
matrix algebra - terminolog : row vector row vector:
matrix algebra - terminolog : multipl multipli a scalar by a matrix:
matrix algebra - terminolog : multipl of two matric multipli two matrices:
squar matrix: - equal number of row and columns, i.e., order (dd)
diagon matrix - squar matrix, with off-diagon element equal to zero
symmetr matrix: - squar matrix where each element (i, j) is ident to element (j, i)
invers of a scalar and matrix the invers of a scalar a is equal to 1/a: invers of a matrix a is equal to a -1
inverse: detail invers of a matrix a - onli defin if a is a squar matrix. - onli defin if a is of full rank (not explain here). exampl : - note : you must be abl to check whether the given matric are the invers of each other. - you don't have to be abl to comput the invers of a matrix.
multivari data, and associ parameters: matrix 
sum-of-squares: matrix 
paramet of multivari distributions: matrix - covari (how much the varianc influenc each other) - symmetr matrix field are equal in size (covari between a&amp;b = b&amp;a)
determin (matrix) (co)vari matrix: - σ and s: popul and sampl covari matrix, respect - general variance: the determin of the sampl covari matrix - associ between the variabl (covariance) reduc the general varianc - variat (variance) in one variabl is attribut to variat in anoth variabl exampl - determinant: extrem case 1 (r = 1): extrem case 2 (r = 0): s1 factor 1.5 (r=1) between valu therefor =0 s2 (r=0) will be equal to the product 4*9
experiment control control in experiment elimin confound and minim variabl control by random : random assign control by design : control nuisanc variabl - hold constant throughout experi - counterbal effect - model as extra factor (categor iv): ing - model as covari (continu iv): ancova
whi ancova 1. elimin systemat differ (bias) between the experiment group - especi in non-experiment (observational) or quasi-experiment studi 2. reduct of within group varianc (error variance) - by make the group more homogen (cf. ing in factori designs, repeat measures) - more power
ancova in experi experi : - deal with systemat differ (bias) by random assign of subject to group - therefore, bias due to variabl outsid the experi is the same for all group - and thus ani systemat differ between group is due to manipul of the factor (under control) then whi use covari reduct of error varianc if group size are small, systemat differ (chanc differences) are more possibl use covari to adjust mean for differ ancova in other studies: - non experimental: no random assign to groups: - natur group are use this mean that ani systemat differ are due to initi differ (some variables) or due to manipul of treatment level (factors) ⇒ confound - again reduct of systemat bias by use covari to adjust (posttest) mean for initi differences: i.e. group start equal on the covati (equat the groups) - also error will be reduc
analysi of covari : covari covari - usual continu variabl (interv level); categor covari are also use (rarely) - to have an effect, the covari should be correl with the depend variabl - linear relationship (regression) (when relat clear non-linear, a tran ation could help) - use this depend to make better predict (less bias, less uncertainty) of the mean in the groups: adjust the group mean
analysi of covari : combin regress and anova 1. per a regress analysi to predict the depend variabl use the covari the residu of this analysi are correct valu of the depend variabl 2. per an anova on the correct depend variabl (residuals) to examin group (treatments) combin both step in one model
equal regress slopes: univari ancova procedur 1) comput regress line to model associ between x and y. this line has the same slope for all a groups. 2) determin the within-group varianc with respect to the regress line (within the a groups). 3) to calcul between-group varianc use differ between correct (adjusted) means. calcul correct mean use the regress line by fill in the total mean of the covari (grand mean). 4) calcul adjust mean use the regress line by fill in the mean of the covari (grand mean) 5) test treatment effect - test adjust mean µ i *: h 0 : µ 1 * = µ 2 * use f test - ss calcul via model-bas approach
ancova definition: - the analysi of covari test for differ between group by compar a descript of the data base on a singl regress line to one base on line with the same slope and differ intercept for each group - do not add covari without consider thought correl between covari caus problem advic : never use more than two, and onli use two if they are logic unrel quantiti and correl is small
assumpt in ancova: 1. independ observ - design, intraclass correl 2. normal distribut error - score (dv) in each group: yi ~ n(μ i ,σ) - skewness, kurtosis, pp-plot, histogram, boxplot 3. homogen of varianc - sampl sds, leven test, bf-test - (1 to 3 are just the usual anova assumptions) - 4. linear - the relat between y and the covari is linear. 5. homogen of regress slope - the regress line are parallel, i.e., group have equal slope 6. the covari is measur without error - import with natur group correct of violations: tran ation nonlinear ancova
violat assumpt in ancova: heterogen (unequal) regress slope indic interact between factor and covari - test the signific of the interact to investig the assumpt of homogen regress slope - thus, a signific interact effect indic unequ slope unequ slope can be model by includ the interact in the model ≠ (traditional) ancova - however, onli equal slope ensur that differ in mean are match by differ in height of the regress lines, and h0 has the same mean as in an ordinari anova
ancova in random design random assign of subject to group systemat differ between subject are more or less equal divid over group - no systemat differ in covari mean - primari effect is error reduction. when pre-exist classif is use (group are defin by classif factor) systemat differ between the group may arise: - these are partial reflect in the covari - pre-exist classif known as natur / intact group
ancova with natur group (i.e. non randomized) non-random designs: - systemat bias may exist between group that is not due to manipul of experiment factor __________________________________________________ exampl : neuropsycholog studi - two group of patient (natur groups) with brain damag - compar per anc on cognit task - per anc (partially) relat to age (covariate) - differ in per anc between group can be due to differ in brain damag but also to differ in age - what happen when ancova is use anova can give differ s than ancova ancova compar group on equival basi (with respect to age) - correct of means: describ how the data might have been if age were compar (not realistic) ancova test differ hypothesi in this situation: - test the anova hypothesi as it would be when there were no systemat differ (on age) - caus valid of linear regress model is grand mean of covari repres for all group no measur error in covari (lord´ paradox)
lord paradox investig the effect on student of the diet provid in the univers dine hall and ani sex differ in these effect depend variabl : weight of the student - pre measur and post measur of weight - two groups: men and women in example: - mean weight for both group doe not change: - mean differ score is 0 “crucial question: is group membership unrel to pre-test score ” explan ancova ask condit question: how much are the group expect to chang if they had both come from a popul with the same baselin mean that is: - the group are compar with the averag of the observ baselin means. - for weight of men and women this is unrealistic: men and women do not have equal weight (exampl of invalid ancova with natur groups)
structur analys 
k-group case: univari anova univari anova assumpt : - populations: y1 ~ n(1,), … , yk ~ n(k,) - k sampl with size n1, … , nk (n = n1+ … + nk ) - is constant in all popul test h0: µ1 = µ2 = … = µk - anova f test: f= ms b /ms w - f distribut with (k – 1) and (n - k) df
k-group case: multivari test multivari test assumpt : - vector y1, … , yk have a multivari normal distribut with mean µ1, … , µk and covari matrix σ - k sampl with size n1, … , nk - σ is constant across k group - linear relat between all dv
k-group manova: partit of (co)vari (uni/multivariate) univari anova: partit of total varianc ss total = ss bg + ss wg multivari anova for k-groups: partit of total covari (matrix) total sscp = between sscp + within sscp t = b + w
k-group manova: test statist test statist wilk λ : determin of sscp matric are general varianc - wilk λ give percentag unexplain varianc - measur of bad of fit (cf. in regress r2 is the good of fit) if b = 0 , there is no treatment effect, and λ = 1 if w = 0 , there is no within group dispersion, and λ = 0
k-group manova: distribut of λ complic approxim : - with χ2 distribution, with p(k – 1) df - with f distribution, with df that may be non-integ - spss use f, which is better for small n - f is exact for some valu of p and k (e.g., when k = 2) there are other statistics: roy largest root (base on bw -1 ) hotelling-lawley trace (base on bw -1 ) pillai-bartlet trace (base on bt -1 )
k-group manova: which statist to use – usually, statist point in the same direct – in case differ are observed: - wilks, pillai-bartlett, hotelling-lawley equal quit robust with respect to violat of the assumpt of homogen of covari matrices, provid that group size are approxim equal - in some situat roy largest root has more power, but differ with respect to power between the four statist are small - understand of differ requir knowledg of discrimin analysi (treat in cours multivari models)
k-group manova: effect size – p-valu report signific ; effect size report relev – effect size: η2 = 1 – λ - interpret similar to r 2 in regression: percentag of explain (generalized) varianc . – problem : sum of all effect η2 might exceed one . – solut : partial η2 = 1 – λ 1/s where s = min( nr. of dv's, hypothesi df effect )
roy-bargmann stepdown analysi - method for select of import dv - conceptu similar to backward elimin in multipl regress
roy-bargmann stepdown analysis: procedur procedur : 1. rank order the dv's, base on theoret considerations, or effect size in separ anova 2. do a univari anova on the most import dv - significant: select dv and go to step 3. - else: stop. 3. do ancova with next-most signific dv as dv, and the select dv as covariate. - significant: includ dv in select list', repeat 3. - not significant: stop. the select dv are “most important”
manova: type i and ii errors: nomin and actual level – terminolog - nomin α: α that you specifi (e.g., 0.05) - actual α: real α of the test (i.e., proport of test that are incorrect declar signific if you test with your specifi α) – violat of model assumpt may affect - type i error : nomin α and actual α differ - thus also nomin and actual type ii level differ
manova: two kind of type i error 1. actual α larger than nomin α: - test or test statist is call liber - consequence: too often fals reject h0 2. actual α is smaller than nomin α: - test or test statist is call conserv - consequence: less often fals reject h0
model assumpt in (m)anova 1. independ observ 2. distribut assumpt within each group anova: observ (of depend variable) follow a normal distribut manova: multivari normal distribut of depend variabl 3. (co)vari assumpt within each group anova: popul varianc are equal manova: within-group covari matric are equal 4. linear relat between all dv anova: not applic (on 1 dv)
model assumpt in (m)anova: 1. independ observ no relationship between case – effect of violation: - estim of standard error are general too small: thus test liber – detect of violation: - design: experiment, sample, data collect - intraclass correl – correct of violation: - test at more stringent level of signific : smaller nomin α level, such that the actual α will have a normal valu consequ : decreas power - much better: model relationship between cases: hierarch linear model (rep.measures: lectur 5a,5b,6)
model assumpt in (m)anova: 2. multivari normal of depend variabl manova: observ on depend variabl follow a multivari normal distribut – stringent assumption, mean : - all individu variabl univari normal distribut - ani linear combin normal distribut - all subset of variabl are multivari normal distribut – detect of violatio n (necessari but not suffici checks): - check the margin distribut of the individu variables: univari normal distribut - check bivari distribut of all pair of variables: bivari normal distribut
model assumpt in (m)anova: (2.) check multivari normal check univari normality: graph : probabl plot ( pp, qq ), histogram , box plot statist : skewness, kurtosi test : shapiro-wilk test – interpret of non-signific (use combinations) check multivari normality: 1. check margin distributions: univari normal 2. check bivari distribut − pairwis scatterplots: circl : no associ ellips : associ (the thinner, the higher the correlation) check whether scatterplot for each pair is ellipt
model assumpt in (m)anova: (2.) what if multivari normal is violat correct of violation: - tran ation of the depend variabl (separately; steven give exampl for standard variables) consequ : more difficult to interpret - collect more data (central limit theorem) - data trim (remov largest variables; outliers) - check if data contain outlier (and remove.. ) inspect standard or student residu between –3 and 3 altern to correction: use appropri model - e.g., for binari depend variable: logist multilevel model
model assumpt in (m)anova: 3. equal of within-group covari matric effect of violation: within each group: varianc and covari are equal = homogen of σ g , g = 1,…, g group effect of violation: - for equal group size actual α level are veri close to the nomin α level - for unequ group sizes: sometim f liber (larg variances, small groups) sometim conserv (larg variances, larg groups) - balanc design (all n ij equal) are veri import
model assumpt in (m)anova: 3. equal of within-group covari matric check equal and correct check equal - visual : compar observ within-covari matric - test with box m test be cautious, use of a signific test to confirm h 0 box m test veri sensit for non-norm correct of violation: - tran ation of the individu variables, to stabil the varianc (examin in spss) relationship mean and sd squar root proport (arcsin tran ation) - use differ model (i.e., multilevel model)
model assumpt in (m)anova: 4. linear relat between dv (check and correction) check : scatterplot of yi vs yj correct : - tran ation - nonparametr manova (not part of this course) - if onli one out of mani dv misbehaves: remov it
model assumpt in (m)anova: guidelin guidelin 1. check independ assumpt 2. check multivari normal 3. check equal of (co)vari with box m 4. check linear relat if not 4’ then check sampl size n and varianc (use determinants): - rough equal sampl size: actual and nomin α about the same - determinant(s) largest for smaller sampl size groups: test liber - determinant(s) largest for larger sampl size groups: test conserv
multipl comparison more than one comparison among the treatment means: - “statist minefield” - multipl test affect error rate example: i = 4 treatment test six null hypotheses: h0: µ1 = µ2, h0: µ1 = µ3, h0: µ1 = µ4, h0: µ2 = µ3, h0: µ2 = µ4, h0: µ3 = µ4 - take alpha = 0.05 for each test, than the overal error rate for six independ comparison equal 1 – (1 – 0.05)6 = 0.26 - capit on chance: fals reject h0 too often
post-hoc test general idea : when test sever (related) hypotheses, adjust critic valu of test (or correspond p valu and confid intervals) - bonferroni procedur for depend t test in multivari approach
bonferroni procedur general multipl test procedur – when per ing a number of tests, say m, at level α, the probabl of make a type i error increas (chanc capitalization) – this probabl is maxim m x α instead of per ing each test at level α, use signific level equal to α/m to guarante overal level of α
contrast in manova: – specifi comparison under contrast or via l-matrix in glm module, or make your own tran ed variabl – key idea: test combin of effect – example: 4 treatment groups, i.e., we have µ1, µ2, µ3, µ4 - research question: is treatment 1 better than treatment 2 through 4 - effect to test:
contrast in manova: coeffici for contrast 
advantag of contrasts: – work with independ contrast has mani advantag (stevens, 5.10) – various contrast coeffici specifi same contrast (1, -1/2, -1/2) (-1, 1/2, 1/2) (2, -1, -1) – to fix a choice, often (non-necessary) guidelines: - largest coeffici – smallest coeffici = 1 - first non-zero coeffici is posit
prefer test approach – the fewer tests, the better - prefer contrast over multipl comparison – post-hoc test - bonferroni - in specif case (e.g. with a refer group), specif altern better – conceptu post hoc test is similar to situat univari anova
type i error (α) chanc to reject the null hypothesis, while the null hypothesi is true - reduc alpha increas chanc of type ii error - onli way to reduc both is to increas sampl size
“ nomin α” and “ actual α” - nomin alpha is the one you chose (.05) - actual alpha is the real alpha of a certain test given a certain sampl (if all assumpt are met, and you onli per one test; actual = nominal)
type ii error (ß) chanc to not reject the null hypothesis, while the null hypothesi is not true. - this mean that base on the sample, you would assum no relat exist between the depend and independ variable, when there realli is one. - unlik the α, it not possibl to direct set the level of type ii error, but it is link to the α, along with the sampl size and effect size. - when α increases, ß will decrease.
power (1 – ß) chanc to reject the null hypothesis, while the null hypothesi is not true - increas type ii will decreas power - increas power by increas alpha, ad variabl (reduc unexplain variance)
effect size: the size of the effect, which aim to provid in ation free from sampl size, to get an idea how strong an effect is in practice. - exampl are η² for (m)an(c)ova or r² for regression. - the effect size is low, it take a larg sampl size befor this small effect is significant. - if the effect size is large, the sampl size requir to prove the exist of this effect in the popul will obvious be a lot lower.
chanc capitalization: problem with per ing mani consecut analyses, and the reason whi it not recommend - i.e. k*(k-1)/2 - for alpha .05 with 6 test equal .95 6 = .735 mean at least one error with 26.5% chanc - no way to find out which test or how mani
statist signific vs. practic significance: - statist signific is to look at the p-valu alon from a signific test, and decid whether to reject the null hypothesi base on this valu - practic signific is about whether the effect that was found in a sampl actual matter in practice, someth onli the research can decid – averag sampl sizes, statist signific usual overlap with practic signific – huge sampl size even the smallest differ is statist significant, while perhap not be practic relev - conversely, with a tini sampl size a medium effect size isn't enough to find a statist signific , while possibl still use in practic (uncertainti when general a tini sampl size of, say, 10 peopl to the population, but it might well provid an interest start point for futur research and possibl covariates) (both type of signific should be considered, use differ alpha valu to bring them in sync - larger sampl and smaller alpha)
outliers: - can major influenc s when they'r particular far away from the mean or when the sampl size is small - rule of thumb is to look for valu that are more than 3 standard deviat away from the mean (most appropri for averag sampl sizes; for larg n use 5sd) - measur to find outlier are cook distanc and mahalanobi distanc
cook distanc - measur the influenc of points, and should be smaller than 1
mahalanobi distanc also sometim tran ed to leverage, is base on how far away an observ valu is from the mean, and should be smaller than 15
proper design for proper analysis: - proper measur - what measur is assum real - get the most accur score - limit non respons - no third variabl - general to the right popul - extern motiv of researcher´ paper
experiment research: there are three way in which they differ from observ research: 1. random assign to group (remov ani third variabl statistically; with natur group it´ use to includ covariates) 2. treatment may be manipul 3. allow causal to be proven beyond reason doubt
null hypothesis: “there is no relat between the independ and depend variable(s)” - anova : ssg=0, so there is no differ between the group mean and by extens the total mean. this mean that “h 0 : µ1 = µ2 = … = µk” - match pair t-test show that the differ between the mean of the pre- and post- measur be divid by the se. when the mean of both measur are equal, 0 is divid by the se, which would lead to a t-valu of 0, so “h 0 : differ score = 0”
there are three major type of hypothesis: h 0 : all group mean are equal h 0 : differ score is equal to 0 h 0 : there is no relat between variabl h a : at least one group mean is differ h a : differ score is differ from 0 h a : there is a relat between variabl
continu variabl a variabl contain scores, such as iq or extravers
categor variabl a variabl contain group identification, like gender or educ level
ordin variabl a categor variabl with an order, like educ level
nomin variabl a categor variabl without order, like gender or favourit colour
binary/dichotom variabl a categor variabl with two groups, like gender
popul – a particular group of peopl in the world a theori is develop for
sampl – a particular group of peopl that theori is test on, to generalis to the popul
descript statist – statist that aren't meant to be generalized, but are use to summar observ valu ( mean, histogram, standard deviation, median )
inferenti statist – statist that tri to generalis valu from a sampl to a popul (signific testing, confid interval)
orthogon – independ between variabl ( r = 0 ). - orthogon actual mean “90-degre angle”, which correspond to two variabl be complet independ from each other in matrix algebra, becaus a chang on a horizont axi doesn't affect the valu on the vertic axi
sequenti / hierarch analysi – analysi for which variabl are enter in step instead of all at once, so that differ set can be test instead of just the entir model or individu variabl
linear combin – a sum of two or more numbers. even 2 + 2 = 4 is a linear combination. it an all-encompass term for two or more variabl be summar as an expect valu ( linear regress ), function ( manova &amp; discrimin analysi ), or factor ( factor analysi )
effect size – reflect the size of an observ effect for a certain test. this effect could for instanc be a relat between variables, differ between groups, or proport explain varianc
between group effect – the part of the sum of squar total that is explain by the differ between group
within group effect (residuals) – the part of the sum of squar total that is not explain by the differ between group , thus remain unexplain
between-subject effect – reflect the differ between group of unrel subject
within-subject effect – reflect the differ between multipl measur of one subject
fix factor – a categor variabl that limit itself to the list group in the popul
random factor – a categor variabl where the list group are a random represent of a bigger pool of group in the population. for instance: select 10 random hospit out of all hospit in the netherlands. multilevel analysi is especi suit to test these factors.
bivari r reflect the strength of the linear relationship between two continu or binari variabl . is part of the statist base for mani other tests, such as manova, regression, multilevel model and factor analysis. (statist 1a)
chi-squar (test) – reflect degre of associ between two categor variabl (statist 1b)
multipl r – degre of linear associ between a set of continu or binari variabl (iv), and a singl continu variabl (dv). most use as part of multipl linear regression. the set of variabl is be combin into one variabl , like the predict valu base on the linear combin of the model for regression, follow by a calcul of the correl between this variabl and the dv. (statist 2)
sequenti r – degre of linear associ between a set of continu or binari variabl ( iv ), after correct for variabl that were previous enter in the model , and a singl continu variabl ( dv ). - most use as part of hierarch linear regress . - the set of variabl is be combin into one variable, like the predict valu base on the linear combin of the model for regression, follow by a calcul of the correl between this variabl and the dv. (statist 3)
canon correl – degre of linear associ between a set of continu or binari variabl , and anoth set of continu or binari variabl . both set are combin into one variabl through a linear combin of either set, follow by a correl between both linear combinations. (multivari models)
multiway frequenc analysi – degre of associ between multipl categor variabl , when no variabl is clear defin as iv or dv , like with correlation. (multivari models)
multilevel model – overarch term for multipl type of model which all have the same goal of tri to predict the degre of associ between variabl , while take into account that some group are nest into other groups. for instance, psycholog student may be compar to sociolog students, with studi year (1, 2, 3, master) as a nest group of study. (repeat measures)
t-test to test the differ between two group of a singl independ variabl , on a singl depend variabl . (statist 1b)
oneway-anova – to test the differ between two or more group of a singl independ variabl , on a singl depend variabl . (statist 2)
oneway-ancova – to test the differ between two or more group of a singl independ variabl , on a singl depend variabl , where a (set of) continu variable(s) is ad to the model to reduc error and/or bias . (multivari &amp; repeated)
twoway-anova – (factori anova) to test the differ between two or more group of two or more independ variabl , on a singl depend variabl . (statist 2)
twoway-ancova – (factori ancova) to test the differ between two or more group s of two or more independ variabl , on a singl depend variabl , where a (set of) continu variable(s) is ad to the model to reduc error and/or bias . (multivari &amp; repeated)
hotel t² – (not ident to hotel trace for manova) to test the differ between two group of a singl independ variabl , on two or more depend variabl . (multivari &amp; repeated)
oneway-manova – to test the differ between two or more group of a singl independ variabl , on two or more depend variabl . (multivari &amp; repeated)
oneway-mancova – to test the differ between two or more group of a singl independ variabl , on two or more depend variabl , where a (set of) continu variable(s) is ad to the model to reduc error and/or bias . (multivari &amp; repeated)
twoway-manova – (factori manova) to test the differ between two or more group of two or more independ variabl , on two or more depend variabl . (multivari &amp; repeated)
twoway-mancova – (factori mancova) to test the differ between two or more group of two or more independ variabl , on two or more depend variabl , where a (set of) continu variable(s) is ad to the model to reduc error and/or bias . (multivari &amp; repeated)
repeat measur anova – to test the differ between two or more group of one or more independ variabl , on one depend variabl that has been measur multipl time . especi use with small sampl and no miss data (statist 3 &amp; repeated)
repeat measur manova / profil analysi – to test the differ between two or more group of one or more independ variabl , on one depend variabl that has been measur multipl time . especi use with lack of spheric and no miss data (repeat measures)
discrimin analysi – the mathemat opposit of a manova. - use one more continu independ variabl , one or more function are creat that optim combin all independ variabl in such a way that explain varianc is maximis while minimis the number of function . these function are use to predict group membership , defin by the depend variabl , of each participant. - similar to manova, it is possibl to expand discrimin analysi to multipl categor variabl ( factori discrimin analysi ), and like linear regress it is possibl to per a hierarch analysi ( sequenti discrimin analysi ). (multivari models)
multiway frequenc analysi (logit) - degre of associ between multipl categor variabl , with clear defin depend variabl and independ variabl . (multivari models)
logist regress – on the basi of one or more categor or continu independ variabl , the probabl to be part of each group of the depend variabl is calcul separately. the predict group membership is the group with the highest calcul probability. (statist 2)
princip compon analysi – an analysi that tri to estim w hich variabl from a set have a similar latent structur on a pure statist basi , with no room for possibl theories. (multivari models)
factor analysi – an analysi capabl of test a theori regard a latent structur under a set of variabl . unlik pca above, there is room for theori here . (multivari models)
matrix algebra - use a group of number as a singl entiti - it not possibl to divid matric
(3x2) matrix: a set of number with at least two row (horizontal) and column (vertical). - this is a (3x2) matrix as there are a three row and two columns. - the first number alway reflect the number of rows, the second alway reflect number of columns. - the notat is like a, both a capit letter and bold
(3x1) vector: one column of potenti unlimit numbers. - as this vector has three 3 numbers, it a 3x1 matrix. - the differ between a vector and a matrix is that a vector has just one column, while a matrix has more than one. - a matrix with just one row is call a transpos vector - the notat for a vector is a, a small letter , but still bold .
(1x1) scalar: a number like ani other. - most wouldn't call this a matrix, as it consist of just one number - the notat is a, small letter and ital . - sometim this notat is also use to point out one valu of a matrix. for instance, you could point to the 3 on the left-bottom site of the matrix abov with “a 31 ”, as it is the scalar of matrix “a”, on the third row, first column.
transposed: to transpos a matrix is to turn each row into a column, and each column into a row: - first row becom first colum (1,2,3) - notat of a transpos vector is a’ - notat of a transpos matrix is a’
squar matric have sever uniqu properti over matric in general i.e.: main diagonal: - top left to bottom right (a 11 ,a 22 ,a 33 ) - onli make sens in a squar matrix - import in correl matrix (diagon = 1) tract: - sum of the valu of the main diagon
triangular matrix: like the name says, the valu are all in triangular ation. this mean that the main diagon , and onli one of the side of the diagon has valu besid 0 . - the other side onli has scalar of 0. two examples:
symmetr matrix: in mathemat terms, a symmetr matrix is ani squar matrix that doesn't chang when transpos it . - this would mean both side of the main diagon are mirror of each other, so that the first row is equal to the first column , the second row is equal to the second column , and so on. - a well-known exampl is a correl matrix. - symmetr matric are common in manova, as everi sscp-matrix, covari matrix and correl matrix is symmetric.
diagon matrix: a special case of a symmetr matrix, where everi valu besid the main diagon is 0 . - as onli the score of the diagon are differ from 0, give this matrix the name “diagonal” make sense.
scalar matrix: a special case of a diagon matrix, where everi valu of the main diagon is the same , and everi valu not on the main diagon is 0 . - as the scalar is just one number, and this matrix has onli one number besid 0, the name make sens too.
ident matrix: a special case of a scalar matrix. ident mean all valu on the main diagon are equal to 1 , while the valu not on the main diagon are equal to 0 . examples: - ident matrix are use in mani ulas, as multipli with an ident matrix doesn't chang the valu but might chang the size of a matrix
summat and subtract of matrices: 1) addit and subtract work the same way as with normal number 2) addit and subtract is onli allow when both matrices, vector or scalar have the same number of row and the same number of columns. in both case the two scalar on the same posit are be sum or subtracted. this mean a + b is alway exact the same as b + a, just like 2 + 3 and 3 + 2 show the same .
multipl of matrices: each row of the first matrix is multipli with each column of the second matrix - also mean that onli the length of the row of the first matrix need to be equal to the length of the column in the second matrix . so the main rule here is: 1.) length of row in first matrix = length of column in second matrix 2.) number of column in first matrix = number of row in second matrix an example:
import of order and equal in matrix multiplication: differ from exampl for a*b: when the number of column of the first matrix is not equal to the number of row in the second matrix, the follow problem happens: - we can multipli both matric if we take the transpos of a: everi row on the left need to be multipli by everi column up top. this would lead to: - this show that matric or vector needn´t have the exact same number of row and column to multipli them - in fact, except for squar matrices, have two matric with the same number of row and column don't work for multipl (i.e. both have 4 row and 2 columns, then the number of column of the matrix is not equal to the number of row of the second matrix) the size of the matrix after multipl is equal to the number of row of the first matrix , and equal to the number of column of the second matrix . this can be shown with a small vector: we had 2 row in the first vector and 2 column in the second vector, so the end is a 2x2 matrix. as a side note: multipli a vector by it transpos alway lead to a symmetr matrix . this is also whi covariance-vari matric are necessarili symmetr if we multipli a’ by a , we get someth different: we had 1 row in the first vector and 1 column in the second vector, so the end is a 1x1 matrix, or a scalar. multipli the transpos of a vector by itself alway lead to a scalar .
divis of matrices: - it is imposs with matric - but divid by a number is the same as multipli by it invers (for mani but not all squar matric one can find the inverse) - the invers of a particular number that if you multipli the invers with that number, you'd get exact 1 (0.5 for 2) - invers onli exist for squar matric
invers of a squar matrix: - is a matrix of equal size that if multipli by the origin matrix , will lead to an ident matrix of again the same size (1 for each valu of main diagonal, 0 for all other values) - like the invers of 1 = 1, the invers of an ident matrix is an ident matrix (for the exam : import to check if two matric are their inverse, therefor multipli them, if you get an ident matrix they are each other´ inverse)
in order for matric to be multiplyable: the number of column of the first matrix need to be ident to the number of row of the second matrix
the size of the matrix after multipl is equal to the number of row of the first matrix, and equal to the number of column of the second matrix ( multipli a vector will alway give a symmetr matrix )
multipli the transpos of a vector by itself: - alway lead to a scalar (1 row in the first vector and 1 column in the second vector, so the end is a 1x1 matrix, or a scalar)
determin can theoret be understood as a singl valu to repres the matrix as a whole . for a scalar (1x1 matrix), the determin is equal to the scalar. for a 2x2 matrix, we can calcul the determin like this: so for example:
use the determin to calcul the invers we need to per three step to calcul the invers for a 2x2 matrix: 1) switch the place of a 11 (4) and a 22 (8) 2) switch the sign of a 12 (6) and a 21 (5 (posit numb er becom negative, and vice versa) 3) divid all number by the determin ( a = 2) test the procedur by multipli a by a´ (i 2 becaus of 2 one in the matrix)
three step to calcul the inverse: 2x2 1) switch the place of a 11 (4) and a 22 (8) 2) switch the sign of a 12 (6) and a 21 (5) – (posit number becom negative, and vice versa) 3) divid all number by the determin ( a = 2)
if the determin equal 0 no invers can be ed
sscp-matrix &amp; covari matrix contain in ation about: - the spread of the score for each variabl , and the degre of associ between multipl variabl - both of them are necessarili symmetr matrices, as the covari between variabl 1 &amp; 2 = covari between variabl 2 &amp; 1, like with a correl matrix
step to calcul a covari matrix: the first step is to find the differ between each observ score and the mean of that particular variabl , similar to the red, bold part of this ula: the second step is to multipli this matrix of differ score with itself . as state before, we can onli multipli two matric if the number of column from the first matrix is equal to the number of row of the second matrix. we can't multipli the matrix by itself in it current shape, as 5 is not equal to 2. but we can transpos x to fix this: the third step is to divid the sscp-matrix by the degre of freedom , in this case n – 1, as can be seen in the ula for variance. we still can't divid with matrices, but we may multipli the invers . this number is a scalar , which is uniqu in that a matrix may be freeli multipli by a scalar regardless of the size of the matrix. this multipl may be done by multipli each valu of a matrix by the scalar. in this case, n – 1 = 5 – 1 = 4, which has an invers of 0.25.
step 1 in calcul a covari matrix: ( varianc ) find the differ between each observ score and the mean of that particular variable, similar to the red, bold part of this ula: (the variabl mean was subtract from each observ value)
step 2 in calcul a covari matrix: ( sscp - matrix) is to multipli this matrix of differ score with itself . as state before, we can onli multipli two matric if the number of column from the first matrix is equal to the number of row of the second matrix. we can't multipli the matrix by itself in it current shape, as 5 is not equal to 2. but we can transpos x to fix this: multipli the transpos with the origin matrix of differ score is equal to squar all differ score in the origin ula: sum of squar is liter the sum of the squar of the differ scores, and cross-product are liter product of cross equations.
step 3 in calcul a covari matrix: ( covari matrix) to divid the sscp-matrix by the degre of freedom , in this case n – 1, as can be seen in the ula for variance. we still can't divid with matrices, but we may multipli the invers . this number is a scalar, which is uniqu in that a matrix may be freeli multipli by a scalar regardless of the size of the matrix. this multipl may be done by multipli each valu of a matrix by the scalar. in this case, n – 1 = 5 – 1 = 4, which has an invers of 0.25.
how to interpret a covari matrix (exampl case) example: covariance, and by extens the correlation, are equal to 0. when all covari are equal to 0, all variabl are independ from each other.
how to recogn a perfect correl between variabl these two variabl have a correl of 1, which can be shown in two ways: 1) anoth ula from statist 1a: all of these number can be found in the matrix, therefore: ( squar varianc !) covari = 2, sd 1 = 1, sd 2 =√4 =2 (with this ula, a standard degre of linear associ can alway be calcul base on the covari between both variabl and the varianc or standard deviat of both variables.) 2) if correl = 1, then determin will be 0: (1 * 4) – (2 * 2) = 4 – 4 = 0
the determin of a covari matrix is known as general varianc
general varianc (caution!) the general varianc decreas when the covari (or correlation) increases, and is equal to 0 when both variabl perfect correl - it might be tempt to think of it as sse or mse. but general varianc is someth els entir exampl : sse and mse are depend on the depend variable, and will alway be equal or smaller than the total sum of squar (sst) or varianc of the depend variabl (mst). the determin , however, may be larger than the varianc of the depend variable: varianc of the second variabl = 11. determin = 6.50 * 11 – 0 * 0 = 71.50, which is clear larger - this mean that general varianc is more of a general measur of associ between all variabl , independ of what is chosen as independ and depend variable, while mse and sse show the part of the varianc that is not explain by the model - onli by dedic a covari matrix to the error and total variance, calcul the general varianc of both and divid the one from the error matrix by the one of the total matrix, can we get a measur of unexplain (generalized) variance. this is exact how wilk lambda is found
t-test: purpos , formula - use to compar the score of two group on a singl depend variabl - it also use the pool standard deviat , which mean both group are assum to have an equal standard deviat in the population:
t-test: assumpt 1) independ : everi measur should be measur in equal circumst and be otherwis unrelated. if this is not true, a relationship between score will, on average, bring them closer together. this caus the standard deviation, and by extens the p-value, to decrease, so that the actual α is larger than the nomin α. 2) homoscedast : as we can see in the ula, the pool standard deviat is be used. this mean that we fill in the same valu of s p for both groups, which obvious onli work if the standard deviat for both group is reason similar. if this is violated, the s p is a bias estim of the real standard deviation, lead to inaccur p-values. 3) normal : we divid the differ between the mean by the standard error to get a t- value, which we are abl to translat to a p-valu through the t-distribution. this t- distribut is rather normal distributed, so if the actual sampl mean aren't normal distributed, estim a p-valu base on the t-distribut is inaccurate. 4) no measur error : while observ which group of a categor variabl someon belong to usual happen without error, the same can't be said for a continu variable, especi when measur with questionnaires. even though no test has a 100% reliability, we still pretend that what we'v measur is the real score of that person. this is fine when reliabl is high, but if it low it anoth sourc of error (and mayb even bias) on top of the unexplain varianc we alreadi have for our model, therefor decreas accuracy.
relationship t-test and anova - when do the independ samp t-test use the pool standard deviation, the t valu may be squar to find the f valu that would be given if the group are compar through anova - also work in revers if the anova has two group (not with more groups) - t-test can be seen as a “special case” of anova for two group (ad benefit of immedi tell us which group is larger through t-values, as they, unlik f values, can be negat )
oneway anova: assumpt independ normal homoscedast no measur error
oneway anova: h o hypothesi µ 1 = µ 2 = ... = µ k y ij = µ total + e ij state that all group mean are equal, and by extens equal to µ total - this is whi ai is left out from the null hypothesis, as it would be 0 for everi group (useless addition)
oneway anova: h a hypothesi " at least one of the group mean is differ from one or more of the other group mean " - so necessarili at least one group mean is differ from µ total so a j is need to correct account for that differ in group mean y ij = µ total + a j + e ij µ total is the grand mean of all observ score y ij is an observ valu of the depend variabl a j is the differ between the mean of group j and grand mean e ij is the residu for each individu
oneway anova assumption: independ (meaning, assessment, correction) - no relationship between the individu measur (when variabl are depend the sd, and therefor p-valu will decrease, so actual alpha is larger than the nomin alpha) - measur it through autocorrel between each observ valu and the valu measur befor that - lower the nomin alpha (get the actual back to .05); of lack of independ is due to design rm-(m)anova or multilevel analysi can be use
oneway anova assumption: normal (meaning) - as the f-test is base on a normal distribution, violat may lead to bias and inaccur p-valu (as mse is part of everi f-test of anova, the assumpt of normal main concern the residu ) - residu are assum to follow n(0, sigma); if the distribut is too flat, the p-valu will be estim too low (it´ possibl to reject the null hypothesi when it shouldn´t have been) if the distribut will be estim too high, the p-valu will be estim too high (it´ possibl not to reject the null when it should have been) - skew to either side can potenti in/- decreas the p-valu depend on how the groupmean are distribut (make it even more difficult to predict what will happen to the p-values) - central limit theorem make sure the anova is robust against violat (with larger n increas it resistance)
skew to either side can potenti increas and decreas the p-valu depend on how the group mean are distributed, which make it even more difficult to predict what happen to a p-valu when this assumpt is violat
oneway anova assumption: normal (checking) graphical: - residu can be pictur in a histogram , so that we can see whether a normal pattern is followed. (bellshape) - or by mean of a p-p plot of residu (should be close to the line)
oneway anova assumption: skew and kurtosi - skew show how asymmetr the distribut is - kurtosi show whether the distribut is too flat or a peak that too high - similar to other calcul of standard error, skew and kurtosi are consid signific when the absolut valu of the statist is over twice as high as the se - sampl size heavili influenc the se, so, ironically, when one increas sampl size to decreas concern about normal due to the central limit theorem, you actual increas the chanc to conclud that there a signific differ from normal - graph are the prefer mehtod (s&amp;k work great as descriptives)
skew interpretation: skew 0 mean the distribut is left skewed, which mean the longer tail is on the left, which mean the peak is to the right of the centre. skew = 0 mean the distribut is symmetrical. skew 0 mean the distribut is right skewed, which mean the longer tail is on the right, which mean the peak is to the left of the centre.
kurtosi interpretation: kurtosi 0 mean that the peak is too low and the distribut is too flat, so there aren't enough valu near the centre, while there are too mani in both tail of the distribut kurtosi = 0 mean that the relat amount of valu in the peak and tail of the distribut are equal to a normal distribut kurtosi 0 mean that the peak is too high and the tail are too low, so there are too mani valu near the centre, while there are not enough valu in both tail of the distribut
skew and kurtosis: statist test - also suffer from easili reject the assumpt with a larg sampl size , so be care when use them in that case - null hypothesi for ani statist test surround ani assumpt is “ this assumpt has been met ”. this is probabl the singl time where a non-signific is alway prefer - distribut of the depend variabl for all separ group were tested, which is ident to test whether the residu are normal distribut - no distribut were signific differ from the normal distribution. this mean it should be safe to assum normal for this data
oneway anova: what to do when normal is violat - if it caus by outlier , they can be delet as long as delet of case can be back up theoret - tran the score in such a way that the tran ed variabl follow a more normal distribut - increas sampl size will also increas resist to non- normality, therebi increas accuraci of the - a model that doe not assum normal can be chosen, like the logist multilevel model
oneway anova: homoscedast (meaning) - mean that the varianc and standard deviat of the residu are equal for each group ( pool standart deviat which is basic a weight mean of the standard deviat of all groups) - this mean larg group have more influenc on it than small groups. a group with 1000 particip has about 10 time as much influenc on the pool standard deviat than a group with 100 particip - if the group with 1000 particip has the higher sd, it will drag the pool standard deviat up with it, often lead to an overestim of mse (estim p-valu are too high, so an actual α smaller than the nomin α) - if the group with 1000 particip has the lower sd, it will drag the pool standard deviat down with it, often lead to an underestim of mse (estim p-valu are too low, so an actual α higher than the nomin α) - a balanc design on the other hand is more robust against a violat of homoscedast
oneway anova: homoscedast (checking) the rule of thumb is: “largest standard deviat should be smaller than 2 time the smallest standard deviation” levene´ signific test , with the null hypothesi that the standard deviat of all group are equal (sensit to sampl size, for larg sampl size the null hypothesi almost alway get rejected) - if the test is not signific , don´t reject the null hypothesi that the group have equal sd, so it´ save to not reject homoscedast (if it´ signific we would violat homoscedasticity)
oneway anova: homoscedasticity, what to do if violat - outlier may influenc the standard deviat (remove) - tran the score so that the standard deviat are more similar to each other - use a method that doesn't assum homoscedasticity, like kruskal-w
oneway anova: no measur error - need for high reliabl of test
oneway anova: outlier - rule of thumb: univari outlier 3sd´s +/- from the mean (though this might be accept for larger sampl n = 500) - cook distanc (cd 1 = not influential, cd ≥ 1 = influential) and leverag to look for bivari outlier (each measur can be test for a signific leverag at a default of α = 0.001)
oneway anova: sum of squar between group (ssg)= varianc in score explain by the differ between the group mean and the overal mean , so the varianc we can explain base on this model. we usual want this to be as larg as possible.
oneway anova: sum of squar within group (sse) = varianc in score explain by the differ between the observ valu and the mean of the group they belong to , so the varianc we cannot explain base on this model. we usual want this to be as small as possible.
oneway anova: sum of squar total (sst) = all varianc in score on the depend variabl (not s², but all variation), so the sum of the squar differ score between the overal mean and all individu scores.
oneway anova: degre of freedom between group (dfg) = degre of freedom depend on the number of group that take part in the analysis. (the number of degre of freedom can be defin as the minimum number of independ coordin that can specifi the posit of the system completely)
oneway anova: degre of freedom within group (dfe)= degre of freedom depend on the total number of particip and group that take part in the analysis. also use as the df for each possibl follow-up t-test that use the pool standard deviat
oneway anova: degre of freedom total (dft) = degre of freedom depend on the total number of particip that take part in the analysi
oneway anova: mean squar between group (msg) = simpli ssg/dfg, there is noth els this valu is need for besid be a step stone between ssg/dfg and the eventu f-valu &amp; p-value.
oneway anova: mean squar within group (mse) = (pool standard deviation)² = pool variance. -use for multipl comparison &amp; contrasts, as both use the pool standard deviat by default.
oneway anova: mean squar between group (mst) = varianc of the depend variabl ( so this is s² ).
oneway anova: f = f-value, a test statist to help translat the mean squar into a certain p-value, given that the assumpt are not violated.
oneway anova: p = p-value, the chanc that we find this , or a more extrem , given that the null hypothesi is true . - in this case, that we find these differ or bigger differ between the groups, given that all group mean are equal in the population. if p α, it statist significant.
oneway anova: general in ation - oneway-anova is often use to compar more than two group of one independ variabl on a depend variabl - core of the idea behind anova in general is to divid all varianc in the depend variabl (sst) into smaller part (for oneway-anova this mean ssg and sse) - if the varianc between group clear exceed the error variance, so if f is far abov 1, the differ between the group probabl aren't random, which mean the group realli are differ from each other - f is never a negat valu (t 2 =f) - mean that unlik the z-test &amp; t-test, we can't draw conclus about the direct of an effect from an f-valu itself, just that there is one (therefore: post hoc, and contrasts)
confid interv (ci): in general is: “ a rang of valu so defin that there is a specifi probabl that the valu of a paramet lie within it ” - so for the mean of a group, a 95% ci means: ” a rang of valu so defin that there is a 95% chanc that the mean of the group in the popul lie within it ” - this mean that when two confid interv do not or onli slight overlap (rule of thumb is 25% overlap and below), we'r reason certain the mean of the group are differ in the popul
post hoc: multipl pairwis comparison - multipl t-test fall for chanc capit k * (k - 1)/2; at alpha .05 this means: .95 6 =.735 for k = 4 i.e. alpha for 6 test is .265 - therefor use of lsd &amp; bonferroni - lsd is more liber than bonferroni - bonferroni often to conserv - use lsd for k 3 and bonferroni for more than 3 group
post hoc: lsd - effect with small number of group ( i 3 ) - by reli on the general principl that an anova has to be signific befor proceed with these post-hoc tests, and by use the mse and dfe from the anova tabl instead of just the two group it is comparing, it limit α base on theori - maintain the power of the t-test - but alpha increas too much with i greater 3 example:
post hoc: bonferroni bonferroni take the number of pairwis comparison into account: 1.) can be comput by either multipli the p-valu by the number of pairwis comparisons, so you can judg at the level of nomin α (spss) 2.) by divid the nomin α by the number of pairwis comparisons, so you can judg the calcul p-valu at the level of the new, lower level of α (statist 2) - confid interv are also affect (use new, lower alpha), therefore, wider in bonferroni
post hoc: contrast (gener in ation) - test a few specif combin of group for differ (1 vs. 4) - mean we don't need to correct for chanc capit (not so mani tests) - maintain power - have a limit number (= dfg) of contrast to work with - also make use of the pool standard deviat and dfe, so s from a repeat and simpl contrast, which are also pairwis comparisons, complet overlap with the lsd method from multipl comparison example: - group1 is sign. differ from the overal mean (p .001) - negat contrast estim and 95% ci show that the mean with the negat code was larger than the mean with posit code - i.e. mean of group 1 in the popul is probabl lower than the overal mean in the popul
post hoc: contrast (default contrasts) simple, deviation, helmert, difference, repeat - default contrast alway have one group that get a +1, while the group or group they'r be compar to spread -1 equal over themselv - may not be readili appar with deviat contrast as it contain no +1 or -1, but keep in mind that the deviat group compar a singl group (+1) to the mean of all group - simpl &amp; deviat contrast need a refer group (-)
post hoc: contrast - simpl compar all group mean to the mean of the refer group:
post hoc: contrast - deviat compar all group mean (except the refer group) to the grand overal mean:
post hoc: contrast - helmert compar each group mean with the mean of all group that follow:
post hoc: contrast - differ compar each group mean with the mean of all previous groups:
post hoc: contrast - repeat compar each group mean with the mean of the next group:
post hoc: contrast vs. multipl comparison - a huge argument in favour of contrast is that set of group can be compar instead of just pairwis comparison - contrast don't need to be correct for increas level of α - a major downsid to contrast is that not everi possibl differ is test , unless you want to risk increas α to danger level - lsd and bonferroni do test everything, but lsd is often too liberal, lead to increas α, and bonferroni is often too conservative, lead to increas ß
oneway anova: effect size - is a measur for ho w differ the group are, independ of sampl size (η² (eta squared)) - theoret ident to r², but while r is between continuous/binari variables, eta reflect the degre of associ between a categor and continu variable, like with anova - η², like r², reflect the proport explain varianc ( ssg/sst ) - η² is especi use to help give insight to how larg an effect realli is in smaller samples, as they are rare statist signific unless the effect is especi huge, or with larg sampl as near ani effect is statist signific there
twoway-anova (factori anova): general in ation - null hypothesi , assumpt and idea behind the calcul are ident to oneway-anova - now we have to watch for multicollinear - that we have two or more independ variables, interact effect between independ variabl may now be ad to the model
twoway-anova (factori anova): multicollinear - strong correl between two independ variabl (one fair accur predict the other) - onli when you care about the uniqu explain varianc of each variabl (which is admit quit often), doe multicollinear matter - "singular" complet explain by other iv (thrown out) - vif check for multicollinearity: (r 2 is the proport explain varianc of other independ variabl for variabl j, squar of the correl between the two) - rule of thumb is that vif 4 (or r 2 j .75) is fine, and vif ≥ 4 (or r 2 j ≥ .75) is not, but of course, in general we want the multicollinear to be as low as possible, lead to a vif of 1 or r 2 j of 0 if we care about the effect of each separ independ variabl - high vif still allow (m)anova, but tri to comput with/- out variabl of highest multicollinear - don´t remov more than one at the same time
twoway-anova (factori anova): multicollinear with onli categor variabl - in that case it complet depend on sampl size - when the design is balanc (tabl 1) or proport (tabl 2), there is no multicollinearity. when it is unbalanc (tabl 3), there will be multicollinear - if correl between two variabl exist, it mean we can use the score on the first variabl to say someth about the likelihood of various valu on the second variabl
twoway-anova (factori anova): calcul - the anova tabl has expand to test two more effects; both the second independ variabl and the interact effect between the first and second independ variabl
twoway-anova (factori anova): main effect if we want to know the main effect for factor a, then we compar the three level of a as if b didn't even exist. just imagin all peopl from both a1b1 and a1b2 ing a super group of a1, be compar to a2 and a3. the main effect for factor b will compar b1 and b2, as if factor a wasn't even in the model - import for multicollinear
twoway-anova (factori anova): interact effect - consid the possibl that for the condit b1, a1 has the smallest mean, a2 the largest, and a3 is about halfway between the two. then for b2, a1 has the largest mean, a2 the smallest, and a3 is about halfway between the two again. this is what an interact effect is about. - the pattern of mean for a1 is depend on the level of b. graphically, after make a mean plot, we can easili see an interact effect when the line for both level of b aren't exact parallel (parrallel line are an assumpt of ancova)
twoway-anova (factori anova): simpl main effect (simpl = one iv, one dv) - in this case we don't compar a1, a2 and a3 in general like with a main effect, but we compar them within one level of b1. so an exampl would be compar group a1b1, a2b1, and a3b1 to get the simpl main effect of a within b1. anoth exampl is to compar the group a1b2, a2b2, and a3b2 to get the simpl main effect of a within b2. a differ between these simpl main effect for a within b1 and b2 indic that an interact effect exist
twoway-anova (factori anova): tabl (spss) - correct model = ssm, dfm, msm values, which are usual reserv for regress - unlik the variabl themselves, ssm + sse = sst is alway true as all multicollinear was remov after the predict score were calcul - if there is no multicollinearity, ssa + ssb + ssab is equal to ssm, but now that there is, it won't be equal with ssm ssa + ssb + ssab - "intercept" and "total" may be ignor
twoway-anova (factori anova): sum of squar (formula for variabl a and b) 
twoway-anova (factori anova): degre of freedom (formula for variabl a and b) 
twoway-anova (factori anova): mean squar ( ula for variabl a and b) 
twoway-anova (factori anova): f-test ( ula for variabl a and b) 
twoway-anova (factori anova): p-valu ( ula for variabl a and b) 
block - ad anoth categor variabl big s are divid into smaller s, a process they call “ ing” - ing is like an ancova in that both of them add a variabl to the model to remov bias and error from the depend variabl (but ancova is for continu variables, ing is for group variabl ) - while a second variabl is usual ad to test whether that variabl is relat to the depend variable, this is not the main reason for ad a variabl when ing – most import for rm-anova: - the model will be ed for each subject, so that the differ between particip can be remov from the error when analys the variabl of interest (everi singl observ will have the mean of the particip it belong to subtract from it, so that onli the varianc within subject remain ) - these score are use as the new depend variabl for a normal anova, with the variabl of interest as independ variabl ( remov between-subject error, increas η² and power ) – random block design : per ed befor the analysi with random assign – post-hoc blockt : afterwards, look for similar sampl size
ancova: general in ation - purpos is ident to ad a ing factor (usual continu covariate) - ancova as a concept is to onli add one or two continu variabl call covariates, and no interactions, to correct for bias and/or error in the model - whenev there is an independ categor variable, an independ continu variable, and mayb even an interact between the two, ancova can be use just as well as multipl linear regress to estim these effect - the process of linear regress follow by anova can also be revers to test the signific of the covariate. first, do an anova to remov all the varianc explain by groups, then take the residu of the anova and use them as depend variabl in a linear regress with the covari as a predictor - don't add more than one or two covariates, and don't add covari that don't bring someth uniqu to the tabl to take away error or bias, as they don't contribut to the model (ad too mani covari mean they might obscur real effect, especi when deal with natur groups. even if we find signific in this case, one has to wonder if that due to differ in the actual groups, or due to the covari correct each other in such a way they accident caus signific group differ to happen)
ancova: h 0/a hypothesi when use ancova as a concept - sinc the group of the independ variabl are be correct for the covariate, we compar whether or not these adjust mean are equal to each other rather than the origin means: - the altern be that not all adjust group mean are equal to each other. as with anova, we can write this down in anoth way: as state before, we alreadi “know” the covari and depend variabl are related, henc the “ß 1 x ” term that repres this relationship is present in the null hypothesis. what we are interest in, is whether the adjust mean are differ from the overal mean, which is what a j reflects.
ancova: h 0/a hypothesi when use ancova as an analysi ancova as an analysi also test the covari for significance: - the relationship between the covari and depend variable, after correct for differ between groups, will automat be test as well:
ancova: assumpt - all assumpt for anova are also present here - addit assumpt of “ there is a linear relationship between the covari and depend variabl ” (bi use correl - linear) - everi pair need to be linear relat for correl to estim the relationship proper - becaus the concept of ancova is to add a covari without ad an interact effect, there is anoth assumpt “ the relationship between the covari and depend variabl is equal for all group ” (graphic as parallel lines) - test this statist by simpli ad the interact effect between the covari and independ variabl to the model and check for signific (should be not significant) - both assumpt may be check by creat a scatterplot between the covari and depend variable, show separ line for each group (linear, parallel pattern)
ancova: calcul - simul the s we see for the independ variabl of an ancova model by first comput a linear regress between the covari and the depend variabl (sole explain as much varianc as possible) - residu will then be use as the depend variabl of a follow-up anova, compar the mean of the residu for each group (mean of the "new" depend variabl are now adjusted) - we can calcul these adjust mean with the full regress model with µ i * = adjust mean for group i, b 0 = intercept, and b 1 = b 2 = b 3 = slope for given variabl - dummi should be fill in with either 0 or 1, base on the code and the group you'r calculating, and “covariate” should be fill in with the overal mean of the covari each time.
ancova: adjust mean experiment group are usual complet independ from covari due to random assign in groups, so their adjust mean will not differ much, if any, from their regular means. natur group are often relat to their covariate, so as part of correct bias their adjust mean could be drastic differ from their origin mean. - anova and ancova could therefor lead to differing, or even revers conclusions. it could be that one show a signific differ while the other doesn't, or that the group with the highest origin mean also has the lowest adjust mean.
ancova: lord´ paradox - when the correct of an ancova doesn't just remov bias, but add bias to an analysi _____________________________________
miss values: pattern 
miss values: mcar miss complet at random , it basic boil down to the miss data be complet unrel to ani variable, both measur and unmeasured. obviously, this is easier to assum than it is to prove. this can rare be assum in practic and there are peopl advoc onli use mar and mnar becaus there no real way to prove mcar. - make handl miss data a lot easier
miss values: mar miss at random . it relat to other measur independ variables, but not to the depend variable(s). - the clearest exampl of this is that there a control and experiment condition, and a higher percentag of peopl in the experiment condit quit. this mean that there is a correl between condit and whether someon has a miss valu or not, but it doe not necessarili mean that the peopl who are miss would score differ relat to their group on average. if they do, it would not be miss at random. if they don't, it miss at random.
miss values: mnar miss not at random . this one is the most dangerous, as appar peopl miss is not complet base on the variabl in the model. - for instance, if one would be hold a general survey to tri and predict anxieti base on sever independ variables, peopl with high anxieti would probabl be less inclin to fill in the survey than peopl with low anxiety. now, the peopl who are miss have a strong relat with what we are tri to predict so it is no longer random. whether someon answer or not is determin by intang which are not measur like they are for mar. this make it imposs to account for them and no longer random.
miss data; case delet listwis : delet all peopl from the dataset with one or more miss valu pairwis : check everi calcul separately, then use everyon who is avail for that specif calcul - easiest but least prefer - work for mcar - creat bias with mar, mnar
miss data; weight assign a certain weight to each outcome, so the data as a whole reflect the s it would probabl have with complet case - if univari or monoton miss (page 70), this could provid decent s, but not for arbitrari - one clear advantag is that this method is non-parametric, so it doesn't depend on assumptions, even though it requir a model for the probabl of a response.
miss data; singl imput : uncondit mean imput (on mcar ) - just fill in the mean of the variabl for whatev valu is missing, which has a huge theoret problem of reduc the sd of that variabl when it shouldn't be reduc - power will be retain - but produc too liber test
miss data; singl imput : hot deck (on mcar ) - consid the observ valu of the variabl as a sampl from which you random draw one. as the draw is random, both the mean and sd should be similar to the origin data - best mcar onli techniqu - like mean imputation, it doesn't take relationship with other variabl into account
miss data; singl imput : condit mean imput - like uncondit mean imputation, this one fill in mean valu for each miss data - take into account other independ variables, therebi turn it into mar - there is still an unwant side effect of reduc the sd
miss data; singl imput : predict distribut imput ( mar ) - these imput valu base on other independ variables, while also ad a random error (base on sd) to simul the sd of the variabl that is be imput - most complic of the singl imput methods, but also the best in pretti much ani case - when it choos a random error for each variable, this error probabl isn't the right one - solut is a techniqu that repeat this calcul multipl time to get closer to the real popul paramet
miss data; multipl imputation, maximum likelihood : in ation - singl imput tend to underestim the varianc of a variabl - predict distribut imput reli on the varianc stay equal by ad a random compon to the guess of miss values, but there is no guarante that this error is correct - multipl imput solv this problem by imput multipl times, so we have a number to link to this random fluctuat in the valu between imput - maximum likelihood is an altern method that doesn't tri to estim the valu to complet the dataset, it just use all avail data to estim the request paramet through various algorithms; step of get a complet dataset is skipped, in favour of calcul the outcom direct (multilevel analysi is an example)
miss data; maximum likelihood : in ation - a linear model is estim base on the case that are avail , which is use to estim all miss valu as well - the model is estim again now that all miss valu are imputed, to increas accuraci of the model - this new model is use again to improv the accuraci of the miss data imput - cycl continu until the estim valu for the miss data, along with the paramet of the linear model, remain constant - biggest disadvantag is that maximum likelihood model assum a linear model - no complet dataset will be produc by use this method, mean that you'r stuck with more complic analys which might be harder to interpret
miss data; bayesian multipl imput : in ation - work a lot like predict distribut imput (the best singl imput method), but multipl time instead of just onc - it possibl to comput a valu that proper reflect the extra random due to imput valu - go-to method for most comput program becaus it a techniqu that can alway be used, when mar or mcar may be assum - it give multipl imputations, so the entir rang of possibl valu for a singl miss valu may be explor - disadvantag is that it pretti hard to work with when be given mani differ dataset - one may find clear signific s with one imput but complet no effect with another, if the proport of miss valu is larg enough, like 10% or more - averag all imput elimin this problem, but then you have to take the extra error from the differ imput into account
multivari test - have multipl depend variabl - more suit to find signific differ between group - especi if the depend variabl are moder relat - also keep the chanc of type i error steadi at .05 - use all avail data, henc increas power - it is alway recommend to keep the number of variabl to a bare minimum - ad new depend variabl might obscur group differ for other depend variabl - power will decreas by includ bad variabl - as with ani statist test, that with increas flexibl come decreas specif of the s
oneway - manova: h 0 hypothesi - notic the first number repres dv, ad anoth dv would place it below the other in a vector the second number repres group, ad anoth group would add a new vector to the right “there is no linear combin of the four depend variabl for which the three group of the independ variabl are differ from each other” (clear) h a : “there is a linear combin of the four depend variabl for which the three group of the independ variabl are differ from each other” (unclear) - which depend variabl were need for the linear combin which group are differ for this linear combin follow up:
oneway - manova: setup 2 or more groups, 2 or more depend variabl
t 2 -test: setup 2 groups, 2 or more depend variabl
discriminant: setup provid in ation about the linear combin use by manova
anova: setup 2 or more groups, 1 depend variabl
multipl comparison: setup 2 groups, 1 depend variabl
contrasts: setup 2 or more groups, 1 depend variabl (but onli specif comparisons, not all)
hotel t²: h 0/a hypothesi (not the same as hotel trace in manova output) the null hypothesi for t² is also ident to manova, except that we can onli have two group for t². if we again presum to have four depend variables: h a : h 0 : “ there is no linear combin of the four depend variabl for which both group mean differ from each other ”. h a : “ there is a linear combin of the four depend variabl for which both group mean differ from each other ”.
hotel t²: formula this analysi is great at differenti two group from each other on multipl depend variabl , but at the same time is limit to do just that. this might not be surpris consid this is call a t²-test, but the ula is liter the squar ula for the t-test, modifi for have multipl depend variabl - when the differ between the vector increase, the valu of t² will increas along with it - when s increases, s -1 decreases, therefor the valu of t² will also decrease. - increas sampl size will also increas t², as the multipl abov the break line will increas faster than the addit below the break line.
hotel t²: formula tran ation (f) after fill in the ula to calcul the valu of t², this t² valu can be tran ed into an f-valu so a p-valu can be found afterwards. this can be done use the follow ula, with p repres the number of depend variabl and n repres sampl size : - show whi t² = f for the univari case. - fill in p = 1 make the part abov and below the break line exact equal, ing in f = t² - this ula onli work for two group spss: there is no specif t²-test button in spss, but it can be simul by select two group and run a manova. like with anova compar to t-test, a manova with 2 group will give the same f-valu (thank to that ula above) and p-valu as the t²-test. with this, we have set the final step toward a “proper” manova analysis.
oneway - manova &amp; t²: assumpt - independ of all measur - multivari normal - the ( within ) covari matrix is equal for all group - no measur error - there is a linear relat between all depend variabl (not assum for t²)
oneway - manova &amp; t²: assumpt : multivari normality: - veri strict assumpt - not onli do the separ depend variabl need to be normal distribut , but everi possibl linear combin of depend variabl need to be normal distribut as well - when there is perfect multivari normality, each separ depend variabl and each linear combin of two depend variabl will be normal distribut as well, so can at least test those - check depend variabl separ is done through the same method as anova (shapiro-wilk, skewness/kurtosis, pp/qq-plot) - for compar two depend variabl we can make a 3d-plot , or scatterplot between the two. if the variabl are normal distributed, we'll see either a circl (r = 0) or ellipsi (r ≠ 0) - central limit theorem
oneway - manova &amp; t²: assumpt : multivari normality: violat - increas sampl size to increas normal through the central limit theorem, tran ing score so they follow a more normal distribution, and remov potenti outlier all increas normal - if the non-norm is caus by the depend variabl not be continuous, take anoth approach like a multivari logist regress
oneway - manova &amp; t²: assumpt : equal of (within) covari matric - is an expans of homoscedast - test for by use the box m test : sensit to non-normality, so keep this in mind when conclud someth base on this test when you know multivari normal is violat - if the (within) covari matric aren't equal, we could tran the data to make them more equal or use a method that doesn't assum these matric are equal, like some varieti of the multilevel model
oneway - manova &amp; t²: assumpt : lack of measur error - assum that whatev we measur is the true score of someon - high level of reliabl
oneway - manova &amp; t²: assumpt : linear relationship between all depend variabl - for manova, the linear combin is base on the relationship between depend variables, so to creat a valid linear combination, we need to have measur the relationship accur - for t², the linear combin is alway “y1 – y2” regardless. - we don't care what the correl between the two measur is, we alway take the differ score . for three or more groups, take differ score is inefficient, as the number of differ score increas exponentially. this mean we onli take this approach for two group (t²) and rm-manova, so in those two case we don't care about this assumpt - group show us whether there is a linear combin of the four depend variabl for which the three group are differ from each other, and given p α, it appear there is as we can safe reject the null hypothesi
oneway - manova &amp; t²: assumpt : linear relationship between all depend variables: differ test in a table: wilk lambda as default: - ratio between the determin of the within-sscp matrix and the determin of the total-sscp matrix - as the determin of an sscp matrix can be interpret as the “generalis variance”, wilk lambda give us the “ proport unexplain (generalised) varianc ” - for two group the f test is complet accur and ident for each method
oneway - manova &amp; t²: follow up analysi to find out which group are differ for which linear combin - anova - multipl comparison - contrast
oneway - manova &amp; t²: roy-bargmann stepdown analysi - check for which depend variabl the group are signific differ - to find the set of depend variabl that explain the most varianc with the least number of variables, this stepdown analysi can be per ed first , comput a univari anova with the most signific dv (y1) (if this isn't significant, stop here, becaus no dv is signific by itself; if the dv is significant, it will becom a covari in the follow model) next , comput an ancova with the second most signific dv ( y2 ) and y1 as a covari (if this isn't significant, stop here and conclud that y1 by itself is the best model; if this is significant, y1 and y2 are both use additions) continu this process until either all dv are present in the model (like now for this example) or until a non-signific is encountered. the final model is the model that onli contain the crucial dv need for the observ effect
oneway - manova &amp; t²: follow up : contrast in the tabl above, the s of two contrast (group 1 vs. group 3 &amp; group 2 vs. group 3) are given for both depend variables. this show that for the first depend variable, group 1 &amp; 3, and group 2 &amp; 3 are both signific differ from each other. in both case the mean of group 3 is higher, as the valu of the contrast estim is negative. when comput a simpl contrast, spss use “µ1 – µ3 ” and “µ2 – µ3 ”, so if this equat s in a negat number, µ3 µ1 or µ3 µ2 depend on which contrast. for the second depend variable, µ3 µ2 again as we find a signific negat contrast estimate, but now we can't reject the null hypothesi that µ3 = µ1 anymore. the mean of group 1 and 3 probabl about the same, while the mean of group 3 is larger than 2 - to test 1 vs. 2 use bonferroni (more conservative) or lsd
oneway - manova &amp; t²: lsd - lsd is basic a t-test, use the pool standard deviat √mse and degre of freedom dfe from the anova table, instead of just the two group be compar - the fact that the anova test need to be signific befor lsd may be computed, protect lsd from inflat of type i error, but still actual α is usual larger than the nomin α, henc the test is liber
oneway - manova &amp; t²: bonferroni - bonferroni is ident to lsd in everi way, except that spss will multipli th e calcul p-valu by the number of pairwis comparison to compens for inflat of type i error. - this usual lead to the actual α be smaller than the nomin α, henc the test is conservative.
twoway - manova – two independ variabl (factori manova): import to look for - with two independ variables, we need to think about multicollinear between both independ variabl - assumpt remain the same
twoway - manova – two independ variabl (factori manova): h o hypothesi group (3 groups), factor (2 factors) and 2 depend variables: this would mean there is no effect at all, so no main effect of group, no main effect of factor, and no interact effect. the null hypothesi for just the main effect of factor is like a oneway-manova: so as can be seen, group is irrelev for the main effect of factor.
twoway - manova – two independ variabl (factori manova): spss table: - group is the onli signific effect in our model (p .001) with the proport unexplain generalis varianc equal to 3.4%, which is veri littl - factor is not signific (p = .618) with 95.9% unexplain generalis variance, which is a lot - wilk ’ lambda statist for the interact effect (just 76.1% unexplain generalis variance) make it seem like there might be an effect, but due to the small sampl size it isn't signific (p = .170)
twoway - manova – two independ variabl (factori manova): effect size: - manova use η 2 (eta squared) for measur of effect size, with the same interpret - wilk lambda can be interpret as the proport unexplain varianc , and η 2 is the proport explain varianc - due to multicollinearity, simpli sum η 2 doesn't equal the effect of the entir model (could exceed 1) - to counteract this, the “partial η 2 ” can be calculated, to remov multicollinear from the η 2 so that η 2 can't exceed 1: s = either "hypothesi df" or # of dv (take the smaller valu of the two example: the sum of these effect is .816 + .021 + .128 = .965, which is smaller than 1. this show partial η 2 is most use to learn about the effect size of the entir model, and to learn more about the uniqu effect of each variable, without overlap from other variables.
man c ova: assumpt - independ - no measur error - equal of covari matric - homogen of slope - linear relationship between all continu variabl - the onli differ worth explicit mention is that the manova-assumpt that all depend variabl are linear related, and the ancova-assumpt that the covari and depend variabl are linear related, are combin into one giant “ all covari and depend variabl should be linear relat ” - i.e. “all continu variabl should be linear related” work for (m)an(c)ova - nice exampl from p.60 onward
man c ova: h 0 hypothesi h 0 : “ there is no linear combin of the four depend variabl for which the three adjust group mean are differ from each other ” ha: “ there is a linear combin of the four depend variabl for which the three adjust group mean are differ from each other ”
man c ova: assumpt : independ just like everi other test until now, we assum that the measur of all particip are independ from each other, outsid of the group variabl we'r measuring, and that the particip didn't influenc each other. (this assumpt is gone for repeat measur after this, as have more than measur per particip mean the measur are related)
man c ova: assumpt : no measur error: just like all the test before, we assum no measur error was made when measur some continu score of the participants.
man c ova: assumpt : multivari normality: similar to manova, each separ depend variabl and all linear combin of the depend variabl need to be normal distributed. we can test this with univari ( shapiro-wilk, skewness/kurtosis, pp/qq-plot, histogram ) and bivari ( 3d-plot, scatterplot ) methods.
man c ova: assumpt : equal of covari matrices: similar to manova, it assum that the (within) covari matrix for each group is equal. we can test this with the box m test .
man c ova: assumpt : homogen of slopes: similar to ancova, the relationship between the covari and depend variabl may not differ between group , becaus then an interact effect would exist. interact effect with covari are still disallow in this conceptu idea behind mancova. - this can be test by ad the interact effect to the model, or by creat a scatterplot with a fit line for each group.
man c ova: assumpt : linear relationship between all continu variables: similar to manova, we want there to be a linear relationship between all depend variables, so a valid linear combin can be produc . similar to ancova, we want the relationship between all covari and all depend variabl to be linear, so that the coeffici measur this relationship is proper estimated. this can be check through a scatterplot between all separ pair of continu variables.
canon correlation: setup - linear relationship between two set of continu (or binary) variabl
multiway frequenc models: setup - relationship between two or more categor variabl - onli take categor variabl into account (nominal/ordinal) - "good of fit" test is per ed - test whether there is still a signific portion of unexplain varianc that could be explain next to the predict model
factor analysis: setup - a techniqu use to s ummar mani continu variabl in just a few factor . these factor will be comput base on which factor are most relat to each other
discrimin analysis: setup - a techniqu use to predict group membership base on multipl continu variabl (opposit of manova, in that it has multipl continu independ variabl and a singl categor depend variable)
discrimin analysis, factor analysis, and canon correl , all share a mathemat foundation: - all three use linear combin of the continu variabl in the model (were first common use by multipl linear regression, for which multipl independ variabl were combin into one model, which was use to comput predict scores) - we now make more than one linear combin base on the continu variabl - therefore, eigenvector
eigenvector and eigenvalu : calcul for everi squar matrix, like sscp-matric or covari matrices, it is possibl to comput an eigenvector with an associ eigenvalue. - we defin a matrix as a, an eigenvector as u, and an eigenvalu as λ ( au = λ u ) - there is one eigenvector and eigenvalu for each row (or column) of the squar matrix. so a 2x2 matrix will have 2 eigenvectors, a 3x3 matrix will have 3 eigenvectors, etc. - a covari matrix or sscp-matrix will have one row (or column) for each continu variabl in the model - combin these two points, the general rule that we can calcul one eigenvector and eigenvalu for each continu (or binary) variable, sudden make sense.
eigenvector and eigenvalu : definit let a be an n x n matrix. a scalar λ is call an eigenvalu of a if there is a non zero vector x such that ax = λx . such a vector x is call an eigenvector of a correspond to λ note : if λ is an eigenvalu of a , and x is an eigenvector belong to λ , ani non-zero multipl of x will be an eigenvector
eigenvector and eigenvalu : usag - use in the same vein as correlation, but for multipl continu variabl - base on these eigenvectors, each variabl is assign a particular weight as part of a linear combination, compar to the slope of a linear regress - use these weight and the score of each subject, the score on the linear combin of these continu variabl may be calcul for each subject, similar to the predict valu for linear regress this linear combin has differ names: linear regress : model discrimin analysi : function canon correl : dimens factor analysi : factor compon analysi (subgroup of factor analysis): compon - in all these case the linear combin that minimis unexplain varianc and maximis explain varianc is estim
eigenvector and eigenvalu : limit for all linear combin the number of possibl linear combin is equal to the number of possibl eigenvectors, so it is equal to the number of continu (or binary) variabl that are part of the linear combin - discrimin &amp; canonical, two linear combin are calcul for two differ set of variabl simultan (in this case, the linear combin with the smallest number of continu variabl defin the number of eigenvector that could be calculated) the second linear combin for the same set of continu variabl is calcul the same way as the first linear combination, but instead of use the origin data, it use the residu from the first linear combin - as residu are alway complet independ from the model, the second linear combin will be complet independ from the first one - this second linear combin will never be abl to explain a higher proport of explain varianc (first alreadi expl. max.)
eigenvalu : mean - one thing that alway true, is that a higher eigenvalu mean a higher proport explain varianc discrimin &amp; canonical: an eigenvalu mean the ratio between explain and unexplain varianc , so an eigenvalu of 1 would mean equal explain and unexplain variance, so that 50% is (un)explain by the linear combin factor &amp; compon analysis: we could divid the eigenvalu by the number of variabl in the model to calcul the propor tion explain varianc of all variabl in the model.
eigenvector and eigenvalu : spss interpret spss will alway give the eigenvalu for all three analyses, but it will not show the eigenvector - it will alway provid us with the weight ( slope ) use to calcul the optim linear combination(s). - they will show the standard weight by default, but it can also alway show the unstandard weight as desir ( read the tabl care ) - standard weight can be interpret like the ß for regress - unstandard weight is like b for regress (on the basi of these weights, we can calcul the definit score of each factor / compon / dimens / function for each subject, just like we calcul the predict score base on the model with linear regression) spss will give a structur matrix (= factor matrix = load matrix = compon matrix) is comput . - this matrix show the correl between the calcul linear combin and all separ continu variabl that were part of this linear combination. - figur out which continu variabl are relat to a particular linear combination, to interpret what exact everi linear combin is base on.
eigenvector and eigenvalu : spss calcul linear combinations, their usag depend on the method: canon : use the linear combin to calcul the linear degre of associ between two sets. discrimin : use the linear combin to predict which group subject belong to. factor : use the linear combin as factor to summaris the continu variabl in the model. compon : use the linear combin as compon to summaris the continu variabl in the model.
the size of the matrix after multipl is equal to: the number of row of the first matrix, and equal to the number of column of the second matrix.
bivari correl r: correl between a singl continuous/binari variabl and anoth singl continuous/binari variabl
multipl correl r: correl between a set of continuous/cod variabl and a singl continuous/binari variabl
canon correlation: correl between a set of continuous/binari variabl and anoth set of continuous/binari variabl - becaus the canon correl between two set can consist of the linear relationship between multipl linear combin (= dimens for canonical), the canon correl may be written as a vector instead of just a number. exampl p.115
canon correlation: find the optim linear combin : - is done through the eigenvector and eigenvalu - on the basi of these vectors, a particular dimens (= linear combination) is creat for both sets, maximis the correl between these set (measur by r ) - next , two new dimens that are complet separ from the first two dimens are created, again tri to maximis the correl between both set with the residu of the first two dimens - number of dimens we end up with is equal to the number of continu variabl in the smallest set
canon correlation: assumpt - linear - normal - homoscedast becaus of robustness, the assumpt aren't that strict, so it deal well with small to medium violations, especi with larg sampl sizes. onli with larg violat for relat small sampl sizes, or with a clear violat of linearity, problem will occur if proper action isn't taken.
canon correlation: assumpt : linear we measure, as with everi varieti of correlation, the degre of linear relationship . - if a quadrat relationship exist between the variables, it won't be found when simpli look for a linear relationship. these variabl would then need to be tran ed to a linear relationship befor canon correl (or ani other type of correlation) could accur pick up on this relationship.
canon correlation: assumpt : normal the test for signific of the correl onli work if the score of everi dimens are normal distributed. however, this is the onli part of this analysi which need normal distribut dimensions. so it isn't requir to accur measur the strength of the linear relationship. also, the test is quit robust, so a small violat of normal doe not caus problems.
canon correlation: assumpt : homoscedast the varianc of a variabl is constant over all level of combin of other variables. this one is also most import for signific testing, but like with normality, the test is quit robust, so a small violat of homoscedast doe not caus problems.
spss - canon correlation: output tabl depend : theori question (theoryscore), applic question (applyscore), and insight question (insightscore) predictor : number of studi hour (studytime), a score that quantifi motiv for academia (motivation), and an attr eness score (attr eness) - in the first dimension, the second set can explain 63.4% of the varianc in the first set, and also the other way around - om the right side of the table, we learn that the first dimens is signific (p .001), while the second dimens (p = .220), and third dimens (p = .980) are not. this mean we have the best ratio between the number of dimens and explain varianc with one dimension, so our final model is simpli the correl between the first linear combin of both sets, so r canon = .796
spss - unstandard canon correl : output tabl these tabl show the unstandard weight (b) for each variabl in both sets. given that a subject score 30 on theoryscore, 20 on applyscore, and 10 on insightscore, then their score for dimens 1 is: given that a subject score 20 on studytime, 40 on motivation, and 30 on attr eness, their score for dimens 2 is: - from these tables, we can see how to exact calcul the dimens valu for each individual, but becaus these score are unstandardized, we can't conclud which variabl are the most import for creat the dimens (especi with veri differ sd´s)
spss - "structur matrix /canon load " canon correl : output tabl - show the correl between each dimens creat for a particular set of variables, and each variabl in that set first set: all three variabl are strong relat to the first dimension, theori &amp; insight main correl with the second dimension, and appli correl with the third dimens second set: studytim and especi motiv correl with it, and they also most defin the second dimension. onli the third dimens lean heavili on attr eness. as you can probabl tell at this point, it doesn't quit matter if the load are negat or positive, onli that they are as differ from 0 as possible, as 0 impli no relationship between the dimens and the variabl in the set. you can get all “bi self” valu by squar the valu of your dimens of choic in the canon load for each variable, and then sum them, and divid by the number of variables. for instance, we can find the valu of the first dimens of “set 1 by self”:
spss - "proport of varianc explained" canon correl : output tabl "how well the various dimens are capabl of explain their own respect set of variabl (bi self) and how well they can explain the other set of variables" “set 1 by self” show the proport of varianc of the individu variabl of set 1 that can be explain by the dimens base on set 1. “set 1 by set 2” show the proport of varianc of the individu variabl of set 1 that can be explain by the dimens base on set 2. “set 2 by self” show the proport of varianc of the individu variabl of set 2 that can be explain by the dimens base on set 2. “set 2 by set 1” show the proport of varianc of the individu variabl of set 2 that can be explain by the dimens base on set 1.
discrimin analysis: general in ation - an analysi to predict in which of the group defin by the depend variabl someon belong to on the basi of a set of continu independ variabl discrimin analysi allow multipl continu independ variabl to be combin into a few l inear combin , which can be use as the predictors, creat an effici model - we use the smallest number of predictor (linear combinations) to maximis the proport explain varianc (be abl to correct predict which group subject belong to for as mani subject as possible) - creat these linear combin (= function for discrimin analysis) by first calcul the eigenvector, as was explain in the introduction. this also mean that the function (= linear combinations) for this analysi are complet independ from each other
discrimin analysis: overview of procedur - the discrimin analysi automat tran s the categor depend variabl into (number of group – 1) dummi variabl , after which the canon correl between the set of continu independ variabl and the set of dummi variabl is be calculated. this is whi the maximum number of function for discrimin analysi is equal to the smallest of either “ number of group – 1 ” or the number of continu independ variabl - we have “number of group – 1” dummi variabl , so it basic the same rule we had for canon correlation, that the number of dimens could not exceed the number of variabl in the smallest set.
descript discrimin analysi descript is meant to be a follow-up for a manova , to be use if the valu for the wilk lambda is signific . this mean we reject the null hypothesi that there is no linear combin for which the group differ, so we know there is some linear combin out there that might help explain the differ between groups. by use discrimin analysis, we can gather more in ation about this linear combin
predict discrimin analysi predict is meant as a stand-alon analysis, to check whether a set of continu independ variabl is abl to accur predict which group of the depend variabl they belong to.
discrimin analysis: assumpt mathemat the discrimin analysi is exact equal to the manova, onli the output we see is complet different. this mean that the assumpt of manova are all assumpt for discrimin analysi as well, mean that discrimin analysi is a rather strict test, especi when compar to it closest competitor: logist regress
discrimin analysis: signific test a signific test is per ed for onli one part of the discrimin analysis. that is to test whether or not the various function creat from the continu independ variabl predict a signific amount of varianc of the depend variabl . - predict a signific amount of varianc of the depend variabl would, in this case, mean that the function help accur predict group membership. exampl p.119
discrimin analysis: spss output eigenvalues: (from manova we saw that there is at least one linear combin (function) in the data for which the group are differ (p .001) - therefor discrimin analysis) - depend variabl is “education” with the group “low, medium, high” - continu independ variabl are three aspect that can be test for an exam: theori question (theoryscore), applic question (applyscore), and insight question (insightscore) ________________________________________ first function: this tabl show two function were made. this is now our maximum, as the depend variabl had three groups, so two dummi variabl were creat from that. the eigenvalu of 3.802 mean the proport explain varianc is 3.802 as larg as the proport unexplain varianc - use the canon correl , we can see that the proport explain varianc is .890² = .792, which would make the proport unexplain varianc equal to 1 – .792 = .208. 0.792 / 0.208 is equal to about 3.802, so it all fit second function: the eigenvalu is .294, with a canon correl of .476. this is way less than the first function, but still decent. we onli had 20.8% unexplain varianc after the first function, and of this 20.8%, the second function explain .476² = .227 = 22.7% of the variance. this mean that of the total explain variance, .208 * .227 = .047 = 4.7% is explain by the second function that mean the total explain varianc by both function is: proport explain varianc function 1 + proport explain varianc function 2 = .792 + .047 = .839 = 83.9%, which is veri high.
discrimin analysis: wilks´ lambda (total explain variance) wilk lambda is the proport unexplain varianc , and here we can see that function 1 and 2 togeth leav 16.1% unexplain varianc behind, which would mean 100 – 16.1 = 83.9% varianc is explain note: also notic that the wilk lambda valu below for 1 through 2 is equal to the wilk lambda valu for the manova (no coincidence, as the first valu for wilk lambda will alway be equal, regardless of the number of functions)
discrimin analysis: spss " peel-off test " first : we test the signific of everi function combined, after which we continu peel off the first function (so the one that explain most variance) from the model next : we test if the remain variabl are still signific predictor are still relat to the group variables. if “1 through 2” is significant, we know that at least function 1 is significant, but we can't say anyth about function 2 yet, as it explain less than function 1, so we don't know whether it enough to be signific - we need to test that one separ from the first function to conclud whether or not it significant. in this case, both function are found to be signific (p .001, p = .001), so both function are decent predictor of group membership.
discrimin analysis: spss - canon discrimin function/ standard canon discrimin show the unstandard weight (b) for each function. given that a subject score 30 on theoryscore, 20 on applyscore, and 10 on insightscore, then their score for dimens 1 is: this show these coeffici are excel for calcul the score for each function , but as they are not standard , we can't use them to say anyth about which valu was most import in creat the function itself. that whi we need the next tabl with the standard coefficients: standard coeffici , we can estim their relat import for estim the function itself. for function 1, theori &amp; insight were the most important. for function 2, theori was a strong negat predictor, while appli and insight were two fair posit predictors. all in all, all variabl contribut to the function creat by the discrimin analysis, so no function or variabl are consid for removal. if anything, theori seem to have the most influenc on creat the functions, with insight close behind, and appli as the least influential.
discrimin analysis: spss - structur matrix this tabl show the correl between the function and all separ independ variables. function 1 was by far the most import for predict group membership, and we can see here that insight and theori are most strong relat to this function. function 2 was not as good, but still significant, and theori is most relat to this function, with insight as a second, and appli as the least relat to the function. in this case, the standard weight and correl were rather similar, but this isn't necessarili the case, so alway analys these separately. this all mean that insight and theori are probabl more import in differenti between the differ groups. but given that the correl with appli is still decent, appli is not useless in differenti between group either.
discrimin analysis: spss - function at group centroid ( tabl ) we can calcul the score on function 1 for all peopl who score “low” on education, with the coeffici we gather from the “canon discrimin function coefficients” table. the mean score on function 1 of the group “low” is shown in the tabl above: -2.320. for “medium” and “high” we can do the same thing, and find the mean for “medium” and “high”, which are –0.015 &amp; 2.335, respectively. the fact that –2.320 – 0.015 + 2.335 sum up to 0 is becaus the mean score of all individu is alway exact 0 , and now that we have equal sampl size for each group, the unweight mean is equal to the weight mean = 0. for function 2 we can repeat all these calculations, and the tabl abov also show us those means. this mean that for the group “low”, their mean score on function 1 is –2.320, and their mean score on function 2 is 0.377. these are the coordin on the x-axi (function 1) and y-axi (function 2) of the group centroid for “low” in the graph below. the same can be said for the group centroid of “medium” and “high”. for everi individual, the differ between it and all group centroid is mathemat calcul with mahalanobi distanc . the one with the smallest distanc is the group a subject is predict to be a part of. as we can see in the graph below, the differ between centroids, especi for function 1, are so big that subject are usual predict correctly.
discrimin analysis: spss - classif result (tabl and graph combined) the combin of the tabl and graph show that function 1 distinguish the differ group pretti well, especi “low” and “high” education. function 2 doesn't differenti between “low” and “high” at all, but it doe distinguish both of them from “medium”, so less mistak are made for point around “medium” and one of the other levels. these two function togeth predict group membership pretti well, as we can see in this classif table: if the model was useless, we would still random guess 33.33% of the group membership right. so increas that to 86.7% through this model is an excel , so this model is veri abl to accur predict whether peopl receiv “low”, “medium” or “high” educations. all peopl in the “high” group were even accur predicted! so if we are look to improv the model even further, we need someth to differenti between low and medium slight better, as most mistak are in that area. though all in all, 86.7% is a veri satisfi
factor analysis: theori - goal of a factor analysi is to summar a number of continu variabl into a smaller number of factor or compon - no real independ or depend variabl for this test, just a bunch of continu variabl that we'r explor for an under link, so we may summar them in a few compon or factor repres these under link there are mani differ way to per this analysis, but there are three default way we need to know for the current exam:
factor analysis: differ between compon analysi and common factor analysi compon analysi (pca) tri to optim summar variabl as a whole common factor analysi (cfa) split the varianc of each variabl into a part that overlap with other variabl ( communal ) and a part that doesn't overlap ( uniqu ) - the uniqu part is ignored, as with factor analysis, we onli care about the part that overlap to find the under link between the variabl - good models, with much overlap between the continu variables, the choic between cfa and ca won't matter much, as they will probabl arriv to (almost) the same conclusion, as we could see dure the lectur
factor analysis: d iffer between exploratori and confirmatori exploratori liter explor the data, just to check what doe and doe not relat to each other , and how to optim summar the variabl in sever factor or components. confirmatori liter confirm a model that was creat beforehand , so it test whether this model actual fit the observ data or not
factor analysis: for the whole variabl (pca) or the overlapping, communal part (cfa), we again use eigenvector to estim the best linear combin (= factor for cfa, or compon for ca) - these maximis the explain varianc of all variabl in the model within one linear combin - next, anoth optim linear combin is calcul for the residu of the first linear combin - cycl continu until there are as mani linear combin as there are continu variabl in the model there is no definit cut-off point to decid whether or not a variabl should be included, and you'r allow to set that point yourself, but a general rule of thumb is that onli linear combin with an eigenvalu greater than 1 should be part of the final model.
pca: spss - output ; exampl "total varianc explained" the variabl are the three aspect that can be test for an exam: theori question (theoryscore), applic question (applyscore), and insight question (insightscore). second set of the canon correl are ad as well: number of studi hour (studytime), a score that quantifi motiv for academia (motivation), and an attr eness score (attr eness) first , we see a tabl show us how much varianc each compon explain in all continu variabl , and how mani compon are use for our model. - two compon with an eigenvalu larger than 1 , so the optim model with the least amount of compon explain the most variance, is two compon - for pca the order of % explain varianc is alway from highest to lowest for initi eigenvalu &amp; extract , but that may chang after rotation, even though in this case the order is still correct. in total, 65% of varianc is explain by these two components, which is a decent . (score with eigenvalu larger 1) - there is some logic behind this rule of thumb of onli includ variabl when their eigenvalu is larger than 1. there are 6 components, and if you sum the number below “total” you will find they sum to 6. each variabl on their own explain “1”. this mean that if a compon explain less than “1” of the model, it explain less than a continu variabl by itself would, so it is no longer effici to add components.
pca: spss - output ; communal “ initi ” repres the proport of varianc that is use by each variabl . as pca alway use variabl in their entirety, “initial” will alway be equal to 1, which basic never happen with cfa - that is how one could differenti between pca and cfa . " extraction" is the proport of explain varianc by both compon for each of the separ variabl . - the sum of the extract valu is equal to the sum of the two eigenvalu of the two relev components.
pca: spss - output ; scree plot the scree plot is a plot where the initi eigenvalu of the first tabl can be seen in a graph, so that we may also visual inspect the data to figur out how mani compon we would like to add:
pca: spss - output ; compon matrix/ factor matrix or load matrix in general, or structur matrix it show the correl between each compon and the separ variabl . on the basi of this table, we could interpret which variabl strong correl with each component, so we better understand what each compon consist of. compon 1 is most relat to theory, apply, insight, studytime, and motiv compon 2 is most relat to studytim &amp; attr eness. the intent behind the data was for attr eness to be complet alone, but appar studytim is negat relat to attr eness, so compon 2 is negat relat to studytime. also, the correl between the separ variabl and a compon may be squar , then sum to find the exact valu of the eigenvalu of that component: when we divid this eigenvalu of a compon by the number of variables, we end up with the proport explain varianc of that component: we can sum the proport explain varianc of both compon to calcul the total proport explain varianc , as all compon or factor of pca and cfa are alway independ as long as no obliqu rotat is per ed. the load (correl between variabl and components) for each variabl on both compon are alreadi quit polar to -1, 0 and 1, without rotat them. this mean we can alreadi decent interpret each compon without rotat the valu - it is veri clear which compon each variabl belong to, except mayb for studytim which high correl with both compon
rotat the matrix: is a procedur to get the load for each variabl as close as possibl to -1, 0 and 1 , to eas interpretation. in theory, there are infinit way to rotat data, as it can be rotat for ani number of degre between 0 and 360. - there are automat procedur capabl of find the optim rotat for you - rough estim the perfect angle, then subt chang until the valu can't be push toward -1, 0 and 1 anymor - chang the proport of explain varianc for each separ component, so much so that it is possibl for the first compon to explain less varianc than the second component, but the total amount of explain varianc alway remain equal . major possibl for rotat data are orthogon rotat ( varimax ) and obliqu rotat ( oblimin ) - in most cases, orthogon rotat is preferred, as it maintain independ between all components. - obliqu rotat tri to push the valu of the compon matrix to -1, 0, and 1 even more rigor (sacrific independ between the compon – difficulti of calcul the uniqu explain varianc for each component) sometimes, varimax can't find a proper solut as there is no great solut by maintain independence. in those cases, direct oblimin is superior as it at least allow us to interpret the compon in some way, even though they aren't independent. varimax method: rotat bare chang anything, as the valu of the origin compon matrix were alreadi relat close to -1, 0 and 1, so there wasn't much to improv upon. therefore, the interpret remain the same as for the origin compon matrix
rotat the matrix: graph here we can see that insight, apply, theory, and motiv all fit well together, with studytim most differ a littl from them on compon 2, while attr eness is pretti much on it own planet. studytim onli seem to be far away from the other point due to be random relat with attr eness, while attr eness isn't close to the other point in ani component, so that variabl truli seem to be separ from the others.
pca/cfa: spss - output; to calcul the score of an individu on a specif component, by use the tabl below: given that a subject score 30 on theoryscore, 20 on applyscore, and 10 on insightscore, 20 on studytime, 0 on attr eness, and 100 on motivation, then their score for compon 1 is:
common factor analysis: spss - output: exploratori cfa will give exact same tabl as pca , but most valu within the tabl will be differ as onli the common (communality) part is optim instead of the variabl as a whole initi is clear lower than 1, as onli the common (communality) part is now be analysed. this is a case for which exploratori cfa isn't great, as larg part of each variabl are now unique, and therefor missing. - also, studytim has a weird high extract of 0.999. - but regardless, we continu with our analysis, becaus we can pretend like all our valu are perfect. - the question in practic would be how accur our calcul would be now , and if we would even be abl to general to the population. - but in the context of this example, that doesn't matter as much. such perk to have fake data!
common factor analysis: spss - output: total varianc explain now that we work with cfa, onli the initi eigenvalu tabl is in order from high to low, both of the other can change, as is readili appar for the extract part of the tabl above. - here, we have a quit extrem case of factor 1 bare have ani communality, while factor 2 had a lot, mean it eigenvalu end up higher than factor 1. after rotating, factor 1 is again higher than factor 2, so natur order is restor in the kingdom of factors.
common factor analysis: spss - output: scree plot it will look exact the same as it did with pca, as it s the initi eigenvalu that are alway exact the same as the pca table:
common factor analysis: spss - output: factor matrix from this tabl we see that factor 1, studytim is probabl a measur error due to it weird high score, and there theori who still has a decent loading. for factor 2, motivation, insight, apply, theory, and to a lesser extent attr eness all score high. - this is clear differ from the s of pca, as in this case it most studytim that left out, while attr eness hide under the radar. we can clear see in this case that imperfect data lead to differ conclus for both methods.
common factor analysis: spss - output: rotat the matrix rotat will save this exploratori cfa, so that weird load for studytim disappears: now we see someth that resembl pca more close - everything, except for attr eness, score high on factor 1, and studytim &amp; attr eness score (relatively) high on factor 2. the exact valu are still different, but the interpret in general is the same for both rotat matric for pca and cfa.
common factor analysis: spss - output: graphic represent of rotat matrix becaus the valu for the matric are rather similar, the graph are rather similar as well. the differ between them is that now it studytim that furthest away from the group of scores, while attr eness is now a littl closer, so these two kind of switch places. but we alreadi know by now those two variabl are except variables, and that it the other 4 that main a common factor or component. these 4 variabl are close relat in the s for both pca and cfa.
common factor analysis: spss - output: goodness-of- fit test now that we onli use the communal part for creat factors, it is import that whatev we estim actual fit the data as a whole and not just the communal themselves. - the null hypothesi of this test is that the model fit the observ valu . ( we'd prefer to not reject the null hypothesis, otherwis we would need to add more factor to improv the model, or switch to anoth type of factor analysi like pca ) - p-valu is as close to be signific as possible, so there is probabl enough room to improv on this model - we could tell it wasn't perfect as attr eness and especi studytim were estim rather weirdly, but appar it was still good enough to (barely) not reject the null hypothesis, so that the model fit the data well enough.
confirmatori factor analysi by: - use the two exploratori method abov to confirm a model - simpli let spss calcul the optim model and check to see if that model fit the expectations. if it does, then that most confirm the model for your data, too. if it doe not, an (alternative) rotat or a chang in the number of components/factor could make the differ to get the model we expect to show up and confirm our model anyway.
confirmatori factor analysis: “official” confirmatori common factor analysi where one could enter the model directly. the main idea is that you have to indic which variabl you expect to be relat beforehand - imagin there a theori that say theory, apply, insight and motiv belong togeth in a factor, and that studytim &amp; attr eness are most import for a second factor - the model can then be fill in, in such a way that all variabl get a 0 on the factor they don't belong to: this way we guarante the variabl have no chanc to contribut to factor they shouldn't belong to, so that we truli test the model we'v creat base on the theori that was known to us. this is the best way to recogn confirmatori cfa compar to exploratori cfa. compar to pca, this techniqu still doesn't use variabl as a whole, so the communal tabl will still not be fill with 1 on initi like it is for pca. by use the eigenvector and eigenvalu base on the overlap (communality) part of the variance, the question mark will be optim estim - if these are far from 0, so close to -1 and 1, much of the varianc in these continu variabl is explain by the factors, so the model inde fit the theoret model we'v made beforehand - if these are close to 0, they bare explain ani variance, and so the model is probabl wrong or the data is extrem crappy. if the data is crappy, noth can be done except start again or increas sampl size - if the model is wrong and has to be develop again, an exploratori cfa or pca can be comput for the current data to check what the optim model for our data is this is especi use to find ani potenti differ between the theoret model and our current data, so we may identifi the problem with our theoret model.
what is the differ in multivari design (compar to univari design ) we have more than one depend variabl -- we can measur model associ between depend variabl
what are the advantag of ad anoth depend variabl 1. some treatment (factors) affect subject in more than one way 2. sever criterion measur will provid a more complet and detail descript of the phenomenon under investig
how do we write the h0 in manova we have one vector per group contain a mean for each depend variabl h0: vector group 1 (contain e.g. µ 11 , µ 21 when we have two dv) = vector group 2 (µ 21 , µ 22 ) µ jg the popul mean on variabl j in group g (first dv, second group)
when get spss output, where do we have to look in order to retain/reject h0 look for the f statist for the hotel t and see whether this is significant. reject = there are differ between the two vector of popul mean
whi do we use multivari analysi instead of multipl t-test reason 1 1. multipl univari test inflat overal type i error rate if alpha = 0.05 for one test, for 4 test the overal alpha is = 1-(0.95)^4 = 0.19 (maximum, assum the test are independent) -- too mani errors, too often fals reject h0 -- capit on chanc
whi do we use multivari analysi instead of multipl t-test reason 2 2. univari analys ignor import in ation: associ between depend variabl (correlation) - seper test reanalyz same varianc (share varianc analyz sever times) - individu variabl may show no signific effect, while joint the variabl do have an effect -- power : multivari test may have more statist power to detect the effect of interest
whi do we use multivari analysi instead of multipl t-test reason 3 the use of a total score (which consist of sever subtest scores): may not reveal signific effect due to a cancel out effect e.g. over-/underestim bias - cancel each other out don't use a total score!
reason for not per ing multivari analys (or rather downsid of it) 1. the techniqu do not answer all questions; still need univari test to follow up signific s (get a more complet pictur by use a combination) 2. small or neglig differ on "bad chosen" variabl may obscur real differ on other (more important) variabl (make generaliz difficult) -- parsimony! 3. arbitrarili chosen variabl (with even up to moder correl between dvs ) can decreas the power of multivari test -- high share varianc = instabl in model paramet = care select the variabl you include!
assumpt for multivari test vector y1 and y2 have a multivari normal distribut with mean mu1 and mu2 and covari matrix m (so for each vairabl that compos the multivari distribution, score are normal distribut ) m1 = m2 (m is constant across groups) two sampl size n1 and n2 (we assum that the matrix with varianc and covari coincid in both groups)
look up 2b slide 17,18 to understan how the t-test, f-test and the hotel t 2 are relat softwar tran s t 2 into f for us, but this tran ation is not exact anymor for difficult design when look at the ula one can see that if p=1 - f = t 2 p = depend variabl f and t 2 give the same p-valu
formulas: multivari comput of test statist in two-group case (slide 22) we don't need to know how to comput an inverse! but: we need to know the ula for the "multivari sp", which is s best to look a the slide: hints: s1 = matrix in group 1 (variance-covari matrix) s2 = " group 2 p = nr of dv in each group degre of freedom n - p - 1
what doe a signific overal effect mean there is at least one linear combin of depend variabl for which at least some of the group differ in popul mean this linear combin of mean seper the group (slide 24) so in a scatterplot one could see a clear differ between the group
what to do after a signific manova follow-up: which variable(s) and/or group(s) caus the effect variabl : each seper variabel - univari anova which linear combin - discriminat analysi group (lectur 3a) post-hoc procedur or contrast visual inspect (prefer with cis)
pca is aim at construct linear combin of the observ variabl with maximum varianc (call princip components). - all the varianc of the observ variabl is distribut among the princip components. - the solut to ﬁnding the princip compon is base on a decomposit of the correl (or covariance) matrix of the observ variables.
fa is differ from pca is diﬀer becaus it conceptu the divis of the total varianc of a variabl in two part : 1.) a common part which is share with at least one other variabl in the dataset (call the communal of the variable) 2.) and a uniqu part which is uncorrel with all the remain variables. - this uniqu part is typic a mix of measur error and speciﬁc (reliable) varianc that is uncorrel with the rest of the variables. an fa solut is base on ﬁnding a decomposit of the correl (or covariance) matrix of the observ variabl with the diagon valu typic smaller than 1 (or than the varianc when use the covari matrix). these diagon valu are precis the communalities.
rule of thumb for loadings: (compon matrix) for interpretation, item are consid to load on a factor when the load is larger than .32
differ orthogon (varimax) and obliqu (oblimin) rotation: ani orthogon rotat of the princip compon (use varimax or ani other orthogon procedure) preserv the total percentag of explain varianc . - however, the rotat reshuﬄ the explain varianc among the compon this properti is no longer valid for obliqu rotations. in this case the rotat compon share varianc (becaus they correlate), so one cannot just sum the explain varianc across all compon in order to ﬁnd the total explain variance. this problem resembl the similar issu in regress analys and the multicollinear property.
multiway frequenc models: general - complet unrel to eigenvalu - a multivari model, with the differ that these are all categor variabl (nomin and ordinal) - altern name for this analysi is “log-linear models”, becaus to calcul the predict score of , in this case, the natur logarithm of the expect valu , we use a linear model
multiway frequenc models: rule surround calculus with logarithm first: second: if no base valu is given, so log(y) = x , we may alway assum that the natur logarithm (ln) was used, so that it has base level e: log e (y) = x . (this mean that on the exam, if you find yourself need to calcul anyth “log”, alway press the “ln” or “e x ” button, rather than “log”) third : these rule are alway true, independ of the chosen base of log: - the log of a valu may be infinit posit or infinit negat - the log of a number larger than one is alway positive, and the larger this number is, the closer the log of that number get to posit infinity. the logarithm of a number between 1 and 0 is negative, and the closer this number get to 0, the closer the log get to negat infinity.
logarithm rules: if log a (y) = x, then: a x = y
logarithm rules: log (a * b) = log(a) + log(b)
logarithm rules: log (a b ) = b* log(a)
logarithm rules: log (a/b) = log(a * b -1 ) = log(a) - log(b)
logarithm rules: log(1) = 0, as anyth to the power of 0 = 1
logarithm rules: log (0) = undefined, as the log of everi negat number
the log of a number larger than one is alway positive, and the larger this number is the closer the log of that number get to posit infin
the logarithm of a number between 1 and 0 is is negative, and the closer this number get to 0, the closer the log get to negat infinity.
two categor variabl (pearson chi-squar &amp; expect value) - compar the number of time we see ani event occurring, so the frequenc
two categor variabl (pearson chi-squar &amp; expect value): assumpt - assumpt made are that everi is measur independ , and that there are at least five time as mani peopl as there are cell in the tabl
two categor variabl (pearson chi-squar &amp; expect value): h 0 hypothesi or in word (both null hypotheses, as the altern onli add the word “not”): h 0 : “the type of anim is unrel to what food they prefer” h 0 : “relat speaking, both anim love cat and dog food the same amount”
two categor variabl (pearson chi-squar &amp; expect value): calcul to test whether the type of anim is independ of the food they prefer, we can calcul the valu of x² by use the follow ula:
pearson chi-squar &amp; expect value: df 
odd ratio: general - when we have a 2x2 tabl - odd themselv are a reflect of how like it is to get the outcom you mention, rather than the altern - odds-ratio reflect how two group of one variabl compar on their odd of the other variabl calcul the odds-ratio may seem like a burden, but fortun there is a ula that should work everi time the tabl is structur normally: "for cats, the odd are 5/25 = 0.2. for dogs, the odd are 35/10 = 3.5. the odds-ratio would then be 3.5/0.2 = 17.5. this mean the odd are 17.5 as larg for a dog to prefer dog food, compar to the odd for a cat to prefer dog food. similarly, you could say the odds-ratio is 0.2/3.5 = 0.057 = 1 / 17.5. this mean the odd are 17.5 as small for a cat to prefer dog food, compar to the odd for a dog to prefer dog food."
three categor variabl (multiway frequenc model &amp; expect value): calcul : - extend the ula of chi sqaure: (ad gender as categor variabl – unrel to type of anim and food)
three categor variabl (multiway frequenc model &amp; expect value): what make multiway frequenc model more power than the chi-squar test while the chi- squar test was perfect functional, it is limit to test the independ of two variables. - with multiway frequenc modelling, you'r free to defin your own model. this not onli includ pick the main effect you'r interest in, but you could even includ interact effects! the expect valu when there is an interact effect can be calcul as follows: in this case, the interact between “anim type” and “food type” is included. now we can calcul the expect valu for each cell, use the model with an interact term:
three categor variabl (multiway frequenc model &amp; expect value): spss - multipl interaction: paramet estimation: this is where we can final see how multiway frequenc model use a linear combination. by proper fill in the model below, we obtain the natur logarithm of ani cell we are interest in. unfortunately, spss is a pain in this regard by not proper specifi which group is 1 and which is 2, so i will have to add this myself: we can fill in for ani cell: for instance, for cat &amp; cat food, male: or we could fill in dog &amp; cat food, femal (gender effect is so small, it practic 0):
the end goal of multiway frequenc model is: that you can test differ models, then decid on the model that has the best balanc between number of effect and keep the model as close to the observ valu as possible. - this mean that rather than just test to check for independ between two categor variables, you have the flexibl to test the strength of the contribut for ani main effect and ani interact effect . - that is whi there is no consist null hypothesis, as it depend on the effect that are in the model. - this can rang from a model without ani effect to test everi possibl effect between all variables. the onli consist part is that in everi case, the observ valu are compar with the expect valu . - the null hypothesi is that they are equal, and the altern is that they are not equal . - sinc we look for the model where the expect valu are rather close to the observ values, this is one of the few test we like to see non-signific (in this case, gender was independ from type of anim and the food they prefer, so their expect and observ valu are equal, mean that x² = 0. that mean this model is perfect, and we don't need to includ further interact to improv the model.)
multiway frequenc models: to defin exact which model we are testing, we can use the follow terms: food = [a] anim = [b] gender = [c] in our example, we had the effect of foodxanim [ab] and gender [c]. if we take an interaction- effect [ab] we also alway have to includ the main effect that are part of this interaction, so [a] and [b]. this mean the final model has the main effect of a, b and c, and the interaction-effect [ab], so [a][b][c][ab], or [ab][c] for short.
multiway frequenc models: df for the degre of freedom of this model, we need to look close at how the model is specified. we alway start with the total number of cells, then alway subtract 1 becaus at the veri least, you are alway awar of the total sampl size of all groups. this is similar to how there is alway an intercept in the regress model. next, we need to subtract “number of group – 1 = i – 1” for each main effect, “ (i – 1)(j – 1) ” for each twoway interaction, and “ (i – 1)(j – 1)(k – 1) ” for each threeway interaction, and so on. in this regard, the rule for degre of freedom are ident to anova. total number of cells: 2 * 2 * 2 = 8 subtract 1 for know the total sampl size/constant: 7 group a – 1 = 2 – 1 = 1 therefor 6 left group b – 1 = 2 – 1 = 1 therefor 5 left group c – 1 = 2 – 1 = 1 therefor 4 left ab = (2 – 1) * (2 – 1) = 1 therefor 3 left so the degre of freedom for this model = 3.
multiway frequenc model in spss to find the best model - comput program are capabl of automat select this model by use a forward (not in spss) or backward-analysi (instead of decid on the test beforehand – example)
multiway frequenc model in spss to find the best model: backward analysi a backward analysi start with all possibl effects, then continu remov the least signific one until onli signific predictor remain. - note that the rule that state we can't remov main effect that are still part of an interact effect in the model is still true here. - for our example, it would start with [abc], or fulli written: [a][b][c][ab][ac][bc][abc]. - for categor variables, this guarante 100% perfect estim of everi observ value. - now, we start remov the highest order interact , [abc], to test whether or not it signific contribut to the model. if it is, we can't remov it without ruin the predict qualiti of the model, so we won't remov it. - if it isn't, which is often the case, we will remov it and move on to two-way interactions. [ab], [ac], and [bc] are all test simultaneously. - if they all explain a signific part of the model, then the procedur stop here and the best model is the one with all twoway-interactions. - if they aren't all significant, the one with the highest p-valu is removed. - after that, the model with the remain interact is test again, to check whether someth can be deleted. - this whole process continu until no variabl remains. - the exampl abov was enter in spss, and a backward analysi was computed. - the output will be shown on the next page: for step 0 we have: [a][b][c][ab][ac][bc][abc] as our model for step 1 we have: [a][b][c][ab][ac][bc] as our model for step 2 we have: [a][b][c][ab][ac] as our model for step 3 we have: [a][b][c][ab] as our model for step 4 we have: [a][b][ab] as our model after that it find noth els to delete, so our final model was the model specifi at step 4. becaus the backward analysi remov gender, we have gain one degre of freedom sinc our calcul by hand. becaus gender didn't contribut to accur estim the expect valu in ani way, we didn't gain ani error when we remov gender from our model. as long as the p-valu is abov .05, our final model has no signific loss of error compar to the full (step 0) model. (likelihood ratio &amp; pearson are both equal valid tests, but their p-valu will not alway be identical. likelihood ratio seem slight more accurate, but as casper liter mention dure the lectur they were both equal valid, i don't think he'll make you choos for the exam.) the fact that anim gender contribut absolut noth to the model, mean we can collaps on gender. this sound rather fancy, but it simpli mean we'r forget that variabl that exists. so instead of: and now, if we want, we can run the analysi again for this model. we may onli collaps for variabl when [abc] is not significant, and [ac] and/or [bc] isn't signific either, or els the frequenc would be bias by pretend that variabl doesn't exist.
multiway frequenc model in spss to find the best model: backward analysi finally, we will test the final tabl with odds-ratio test for differ in gender. this is onli possibl for a 2x2x2 model, so we can compar the odds-ratio of two groups, like men and women: odds-ratio (animal, food) for women (a): (cat, cat food / cat, dog food) * (dog, dog food / cat, dog food) = (25 / 5) * (35 / 10) = 17.5 odds-ratio (animal, food) for men (b): (cat, cat food / cat, dog food) * (dog, dog food / cat, dog food) = (25 / 5) * (35 / 10) = 17.5 no signific differ at all, though that was obvious by the fact both or were exact equal.
multiway frequenc models: residu the residu for this model is still equal to the differ between the observ &amp; expect valu , similar to linear regression. this differ may be standardis with the follow ula: the notat in the slide use r ijk , instead of z, to emphas the r of residuals, but the outcom is a z-valu and r is pretti well-known as correlation, so i chose to use z for my notation. the letter doesn't matter for the ula, though, but now at least you know what it means. these standardis residu can be calcul for each cell, so lack of model fit can be test for each of them. the z-valu may be compar to the absolut critic valu of z* = 1.96. if the absolut valu of the residu is higher than 1.96, the model doesn't fit well for this particular cell, and ad a variabl that increas the accuraci of the expect valu of this particular cell should be considered. if all residu are lower than 1.96, the model appear to be fine.
in general, we are onli interest in the differ between condit within subject ( within-subject varianc ), and not whether or not someon has a tendenc to score higher in general. there are three main way to remov between-subject varianc that are be use by the techniqu we use dure the course: 1.) differ score between measur in time ( match pair t-test &amp; manova ) 2.) enter “subject” in a model as a random ing variabl ( anova &amp; multilevel ) 3) take the pre-measur as a covari ( ancova )
3) take the pre-measur as a covari (ancova) by enter the pre-measur as a covari , the mean of each group that part of the ancova will be equal at that point in time. also, the pre-measur will have the opportun to explain as much as varianc as it can in the post-measurement. whatev remains, is how much each group mean chang between the pre- and post-measurement. - these group mean may then be compared, to check if there were ani differ in how they changed.
2) enter “subject” in a model as a random ing variabl ( anova &amp; multilevel ) we'v learn that with anova, we can by enter a ( categor ) variabl into the model to remov error or bias from that model. this also happen when enter subject as a ing variable. “subject” will get the chanc to explain as much varianc as possibl in the model so that all between-subject varianc will be remov . the residu can then be use as the new depend variabl , in a model to compar the differ condit over time.
1) differ score between measur in time (match pair t-test &amp; manova) by take the differ scores, it onli matter how much a subject improv over time, so their origin or final score doesn't matter anymore. a subject that increas their score from a -2 to a 2, will have the same differ score as a subject that increas from 8 to 12. the between-subject variance, that the second person score 10 point higher in both measurements, is no longer there in the differ score.
the effect size (η²) work the same way for rm-anova, rm-manova and the repeat measur variant of ancova as they did for anova, manova and ancova respectively. for match pair t-test &amp; multilevel, no effect size is mention dure the lectures.
match pair t-test &amp; anova (gain score) the match pair t-test is specialis in compar two measur in time , but that is all it can do. - it correct for between-subject varianc by take the differ score between the pre- and post-measur . - next, it test whether or not the mean of the differ score is signific differ from 0, as 0 would mean noth has chang over time. - anova (gain score) doe the same thing, but also allow compar (between-subjects) group on the differ score, to check for a potenti interaction-effect.
ancova specialis in check whether two or more group have chang in vari amount between a pre- and post-measurement. - similar to anova (gain score), but use the pre-measur as a covari instead of take differ scores. - therefore, unlik anova, it can't test whether the mean of all differ score is 0, but it the best model for compar group on how much they'v chang over time.
rm-anova the first model with a wide varieti of uses, though it can't deal with miss data . - correct for between-subject varianc by ing for subject , by ad “subject” to the model as a random factor. after remov the effect of subject, a regular anova is computed. - becaus we have just one depend variabl regardless of how mani measur in time we'v done, we have to be abl to assum spheric of our data. - if we can safe assum sphericity, rm-anova is general the most power test of all.
rm-manova the second model with a wide varieti of uses, though it also can't deal with miss data. - correct for between-subject varianc by tran ing x measur in time to (x – 1) code variabl , like take differ score of each consecut pair, like the match pair t-test. - next all code variabl are enter in the model as depend variables, after which a manova is comput as usual. - now that we don't have just one depend variabl anymore, the assumpt of spheric has disappeared.
multilevel model is the most flexibl model of all, and is even abl to deal with miss data . like rm-anova, it correct for between-subject varianc by ing for subject , by ad them as a random variabl in the model. multilevel analysi is an umbrella term for mani differ models. depend on the model, the output and assumpt will be different. this will be explain extens in this part of the summary. the flexibl of multilevel allow one, in contrari to the other five mention models, to creat the entir model themselves. this increas the difficulti of compar the differ method directly, like state which of these has the strictest assumptions, as the answer depend on how the multilevel model is specified. it can be either stricter than rm-anova, or even less strict than rm- manova as it doesn't even need complet data. when, in the summary, the three main model are compared, it will alway be “in general”, so someth that is true for most multilevel models. - for instance, “in general” rm-anova has the strictest assumpt out of the three of them.
match pair t-test: in ation match pair t-test can onli compar a pre- and post-measur , and can't compar ani between-subject variabl or interact when it do that. it calcul the differ score between two measurements, and test whether they are differ from 0.
match pair t-test: h 0 hypothesi 
match pair t-test: assumpt first , the differ score has to be normal distributed, or the sampl size has to be big enough that it doesn't matter anymore, due to the central limit theorem. second , the differ score have to be independ from each other. - there is no problem of homoscedast or linear here, as the pre- and post-measur are combin into one scale anyway.
match pair t-test: exampl – spss here, the s of the match pair t-test are shown. se was remov from the tabl for better readability. if the first two valu of b2 weren't changed, so that b2 = b1 + 2 was true for everi value, the sd of the differ score would'v been 0, so it wouldn't have been abl to calcul anything. the lesson here is that regardless of the sourc of the difference, ani differ will be pick up by the match pair t-test and will be test for significance.
anova (gain score): in ation this isn't the “real” rm-anova, but an altern way to comput repeat measur when you have two measur of time, which was also shown dure the lecture. similar to match pair t-test, differ score (gain scores) are use as the depend variable. the intercept of the anova tabl will show whether or not these differ score are signific differ from 0:
anova (gain score): h 0 hypothesi (and addition) the intercept in the anova tabl is useful, and test the same hypothesi as the match pair t-test above. - but in addit to that, you'r free to add a categor variabl as independ variabl , to test whether the mean differ score is equal across groups. the hypothes for this are similar to the “normal” anova: or in words: so with this model, we can test the main effect of time like we can with the match pair t-test (do particip chang in general ), and we can test the interact effect between the differ group and chang over time (doe one group chang more than anoth ). on the next page, an exampl will be shown, with “cdifference” as the depend variable, and “group” as the independent, group variable.
anova (gain score): assumpt normaliy, homoscedasticity, independ
anova (gain score): exampl – spsss “cdifference” as the depend variable, and “group” as the independent, group variable. intercept show whether or not the mean of all differ score was signific differ from 0, so that peopl in general score signific differ on both measur in time. in this case, we see that it is (p .000), so either particip signific improv or declin over time . ancova also has an intercept, but as ancova doesn't use differ scores, the intercept is onc again useless for that model. group show whether or not there is a signific interact effect between group and the time measurements. in this case, there appear to be an interact effect (p = .015). to check whether the particip improv in general and which group chang differ from the others, we can comput multipl comparison as a follow-up analysis. as we have three groups, the lsd -procedur is chosen the tabl of mean for group the mean of the differ score for each group. all of them are positive, which mean the particip in each group score better on the post-measur than on the pre-measurement, on average. also, from the multipl comparison tabl , we can see that group 3 improv the most out of all groups, have a signific higher mean than group 1 (p = .005) and group 2 (p = .033). group 1 and group 2 don't seem to differ signific from each other, so we can't tell which group improv more in the population.
ancova: in ation the onli goal of an ancova is alway to compar groups, in this case after they are correct for a covari . but in this case, the covari happen to be the pre-measur , and the depend variabl is the post-measur , which mean you'd expect a pretti strong correl between the two. ad the pre-measur as a covari , mean ani differ between group mean on the pre-measur will be removed. next, the three group are compar on the post- measurement, as that is now the depend variable. a signific differ show that the chang between both point in time is differ for at least one group, compar to the other groups. lord paradox might be present for repeat measures, as was alreadi shown with the exampl compar weight of men and women at pre-measur &amp; post-measurement. when we correct for the pre-measur of weight, we were ad bias, instead of remov it. so with natur groups, be veri care use this repeat measur test.
(rm)ancova: h 0 hypothesi 
(rm)ancova: differ to regular ancova so the onli differ between a regular ancova and this ancova is the fact that a pre- measur is use as the covari , instead of a differ variable, but that doesn't chang anyth in practic or even in the math behind it like anova &amp; match pairs, it can onli be use for two time points, as ad a potenti third invalid the analysis, regardless of whether it consid a depend variabl or covariate. so the onli thing it can do is compar rate of improv over differ group for two time point , which seem aw limit as anova (gain score) can do that too. but as it doe this in a veri uniqu way, it is alway the best way unless one of the assumpt is violat or lord paradox threaten to happen for your data
(rm)ancova: exampl – spss if a1 and a2 are compar for the three groups: a2 = 1.5 * a1, so a1 alreadi explain 100% of the varianc of a2. this is what make ancova uniqu among all repeat measur test. use ani other method to analys this data, the absolut differ between measur would be test similar to how match pair t-test and anova did it, and it would conclud that score increas in general, and that group 3 increas more than the other groups. ancova isn't limit to the differ score (a2 – a1), but it is allow to freeli chang the weight of a1 to best estim a2 (a2 – 1.5*a1), so all varianc in a2 can alreadi be explain by a1 and all chang scores, includ the differ between groups, disappear. this will happen as long as chang score are proportional, and ancova is the onli method that take that into account. thank to the first two case (5-6 &amp; 5-8), there is still a littl unexplain varianc in the model. however, again all differ been group have alreadi been explain by the pre-measurement. the match pair t-test was signific as the post-measur was 2 point higher on average, but the ancova doesn't care about that and just explain what it can with the pre-measurement, includ that main effect of time. ancova onli measur the interact effect well, and as everi group increas by 2 on average, there is no interact effect, henc ss group = 0. the signific of b1 show that there is a veri strong linear relationship between b1 and b2. given that these are a pre-measur and post-measurement, you'd expect this relationship to be at least decent. now, the differ are a littl random, so both the covari and the group are now significant. this mean the differ weren't proport like with a, or equal like with b, but that there realli is a (more random) differ in how the group chang over time. this mean we can use multipl comparison as a follow-up analysi to find exact where these group differ that can't just be explain away by the pre-measurement. unlik the anova for gain score previously, the mean in the “estimates” tabl aren't the differ scores, but the adjust mean of the post-measur after correct for the pre- measurement. though also in this case, all the general conclus remain the same, though ancova is probabl more accur given that ani bias between group due to differ on the pre-measur is removed. group 3 has the highest adjust mean of all groups, have a signific higher mean than group 1 (.005) and group 2 (.023). the mean we see are the mean on the post-measur after be correct for the pre-measurement, so while the mean itself repres their score on the post- measurement, their differ are complet due to differ in chang over time, as they all use “19.2” (shown right below the “estimates” table) as a pre-measur score. group 1 and 2 don't seem to differ signific (.216).
(rm)ancova: conclus ancova is onli use for compar differ between two time point for various groups. - by allow the pre-measur to explain as much as it can in the covari , the calcul aren't bound to (a2 – a1), but the weight of a1 can chang to anything, for instanc (a2 – 1.5 * a1). - it will then compar the differ in chang between groups, someth which ancova is superior in as long as there no lord paradox or violat assumption. - anova with gain scores, rm-anova &amp; rm-manova can all test differ between group through as well, but in essenc they all take differ score between two time point as a default, like the match pair t-test did. - rm-anova &amp; rm-manova allow for contrasts, but find the optim estim of a1 yourself is near impossible.
rm-anova: in ation this analysi can includ more than two measur in time , more than one within- subject variabl , an d between-subject variabl in a singl model, instead of just the effect of “time” and an interaction-effect. - split the varianc between and within subject (like multilevel analysis) - allow rm-anova to use differ varianc depend on which effect need to be test - we are onli interest in differ between the measur within subject , we don't care about some peopl general get higher score than other when their chang in score (30 – 40 vs. 0 – 10) is the same - rm-anova correct for this by ad “ subject ”, or whatev name is use for the variabl that link each measur to each subject, as a random variabl to the model (this effect will be calcul as part of the model, so the varianc it explain will be remov from all other part of the model) - becaus subject are random select from a population, it a random variable, unlik “condition” which usual isn't a random sampl from a popul of conditions. ad “subject” is what separ rm-anova from “normal” anova - we are allow to add as mani measurements, within-subject &amp; between-subject variabl as we pleas - rm-anova will general have the most power, given that no assumpt are violat (rm-anova, rm-manova, multilevel)
rm-anova: dis / - advantag rm-anova has the most power and provid us with much readili usabl in ation, but also has the strictest assumptions: - we expect the score between subject to be independ (so not all measurements, as we alreadi know repeat measur are related, and that fine), and we assum normal distribut residu - lack of measur error - if we add a between-subject variabl to the rm-anova, we even get the assumpt of homoscedast again, so that the differ group have the same sd in the population. - for within-subject variabl , there is the (new) assumpt of spheric . this one is also about variance, similar to homoscedasticity, but as it now a within-subject variabl we need to look at it in a slight differ way.
rm-anova: spheric (assumption) in rm-anova everi score from each time point is combin into one depend variable: - a pretti strong assumpt to just say that all of these measur from differ moment in time are actual all the same variabl - can be check by compar the varianc of all possibl pair of differ scores, which should all be equal - a consequ of the more technic actual assumpt that the ( tran ed ) covari matrix onli contain covari of 0, and all varianc are equal to each other (in case of violation: rm-manova, multilevel model)
rm-anova and rm-manova have extra limitations: - rm-anova and rm-manova onli work for complet data , while all multilevel model can deal with miss valu as all paramet are calcul through the maximum likelihood procedur - rm-anova and rm-manova, like their non-rm counterparts, use categor independ variabl (this mean that when ad a “time” variable, it has to be a categor variable, such as “pre-measurement, post-measurement, follow- up” while it can't be “time in day after start of research”. - multilevel method is base on linear regress , we'r free to add continu variabl like “time in day after start of research” to the model as a predictor/independ variable.) in brief : when there is no fix set of measur over time or when you have miss data, multilevel is superior. when you do have a fix set of measur and (almost) no miss data, rm-anova is use when you can assum sphericity, and rm-manova when spheric is violated.
three differ effect can be test with rm-anova: between-subject effects, within-subject effects, interact effect
rm-anova: test for spheric spss will automat test for spheric use mauch w test . right below the tabl it say “test the null hypothesi that our data has perfect sphericity”, but with more technic terms. like with all other signific test for assumptions, we would like this to be non-signific , becaus then we don't have to reject the null hypothesi that the assumpt is met. (violat here)
rm-anova: how to deal with violat of spheric within rm-anova, by correct the degre of freedom of the f-test: this correct is comput by multipli the df with epsilon (ε): greenhouse-geiss (too conserv ) or huynh- feldt (too liber ) the rule of thumb is: - to use greenhouse-geiss when the mean of both epsilon is lower than 0.7 , - to use huynh-feldt at 0.7 or higher . the mean here is (0.528 + 0.696) / 2 = 0.612, so lower than 0.7, so the greenhouse-geiss correct is used.
rm-anova: spss – exampl the first rm-anova tabl given to us by spss after comput repeat measures, show both the within-subject effect and the interact effect with at least one within-subject effect, as they all use the within-subject error “measurement” is the within-subject main effect , test whether the four time point of c1, c2, c3 or c4 all have the same mean ( test of flat - when there is no main effect of time, the line in a graph would all be flat) the hypothes are: - for two measurements, the p-valu of this test is equal to that of a match pair t-test, or anova with gain score (as it check for differ between “the differ between groups”, it hard to write an algebra null hypothesi for it, unless there are just two time points) “measur * group” is the interact between the within-subject and between-subject effect, which show if the general pattern of time is the same for all three group ( test of parallel - when there is no interaction, the line in a graph would all be parallel). - as soon as one part of the interact is a within-subject effect , it will alway be in the within-subject tabl for two, we use the same null hypothesi as the anova with gain score had: greenhouse-geiss correction: gg-correct has a p-valu of .007 for measur , which show that there is a signific differ between the time point . this mean we reject the idea that the mean score of each time point is equal for each time point in the population. as with normal anova, we can't tell which mean is different, but we now know there at least one. the interaction-effect show a p-valu of .323, mean that no signific interact - effect was found base on the data. this mean we can't reject the idea that there is a differ in their pattern in the population. - so now that we know there is a main effect and no interaction-effect, we could do sever follow-up to get a better view of what exact is go on, like make a graph : - hint at main effect with measur 2 higher than the other three in time, and measur 1 seem have the lowest mean (clear differ between the patterns, especi with group 3 increas more)
rm-anova: graph analysis: - main effect , with measur 2 have a higher mean than the other three measur in time, and measur 1 seem have the lowest mean - clear differ between the pattern , especi with group 3 increas more between measur 1 and 2, and decreas more between measur 2 and 3, compar to the other group a signific quadrat trend would mean that the score would clear rise between two time measur , but also clear drop between two other time measurements. - this happen pretti clear for our data, with the massiv increas between measur 1 and 2 and the clear decreas between measur 2 and 3. - this mean we will probabl find a signific quadrat trend. - also indic which trend could potenti be significant. a signific linear trend would lead to a clear differ between the first and last measur . - though line at measur 4 are slight higher than measur 1, the differ isn't large, so there is probabl no signific linear trend for this data a signific cubic trend mean the score rise-fall-rise, or fall-rise-fall. - we kind of see this pattern emerg for group 2, and becaus the fall between measur 2 and 3 doesn't continu between measur 3 and 4, as the effect straighten out. - this mean there is a small cubic effect, most depend on sampl size and signific level to conclud whether it signific or not. a signific trend for the interaction-effect mean that there is a differ between group with regard to the trend. - a signific linear interact trend point to one group have a strong linear trend, while anoth doe not. - or one might have a posit linear trend, while anoth has a negat trend. - these help provid some nuanc to the general trend provid by the main effect, so that we may provid some in ation about each separ group of the model.
rm-anova: graph trend: main effect main effect , with measur 2 have a higher mean than the other three measur in time, and measur 1 seem have the lowest mean. - there are reason clear differ between the patterns, especi with group 3 increas more between measur 1 and 2, and decreas more between measur 2 and 3, compar to the other groups.
rm-anova: graph trend: linear trend - a signific linear trend would lead to a clear differ between the first and last measurement. - though line at measur 4 are slight higher than measur 1, the differ isn't large, so there is probabl no signific linear trend for this data.
rm-anova: graph trend: quadrat trend would mean that the score would clear rise between two time measurements, but also clear drop between two other time measurements. - this happen pretti clear for our data, with the massiv increas between measur 1 and 2 and the clear decreas between measur 2 and 3. - this mean we will probabl find a signific quadrat trend.
rm-anova: graph trend: cubic trend mean the score rise-fall-rise, or fall-rise-fal . - we kind of see this pattern emerg for group 2, and becaus the fall between measur 2 and 3 doesn't continu between measur 3 and 4, as the effect straighten out. - this mean there is a small cubic effect, most depend on sampl size and signific level to conclud whether it signific or not.
rm-anova: graph trend: interact effect trend for the interaction-effect mean that there is a differ between group with regard to the trend . - a signific linear interact trend point to one group have a strong linear trend, while anoth doe not. - or one might have a posit linear trend, while anoth has a negat trend. - these help provid some nuanc to the general trend provid by the main effect, so that we may provid some in ation about each separ group of the model.
rm-anova: tabl trend analysis: (within-subjects) the quadrat trend of measur was the strongest, and clear signific (p .001), there was a smaller, but still reason cubic trend of measur which turn out to be bare signific (p = .043), and a linear trend that was most absent (p = .539). for both linear and cubic, there doe not seem to be a differ between groups, but quadrat is around the level of α, indic a potenti small effect. we could see this from the graph too, with the increas and decreas around measur 2 be larger for group 3 than for the other groups, though in the end it wasn't found to be signific (p = .057). even if we did find signific in this tabl specifically, it wouldn't be wise to reject the idea that there are no differ between the quadrat trend in the population, as the general interact effect we test two page back wasn't signific either (p = .323).
rm-anova: tabl trend analysis: (between-subjects) – spss between-subject effect ( test of differ in level ) - a between-subject effect would look like one line be abov the other in a graph, henc the name. - this is exact like the normal anova, in that it doesn't take away varianc between subjects, and just compar sever group on the depend variable, or in this case the averag of all measur per subject. - this also mean the assumpt of spheric is gone for test this specif effect, and is replac by “the usual” homoscedast . from the graph, we could see that group 3 had the highest mean, follow by group 2, and that group 1 had the lowest mean. but is this a signific differ that what we can find out here. the hypothes are: becaus this between-subject variabl has 3 groups, we use the lsd correct for the multipl comparison follow-up. as these three group are compar on the averag level of all four measurements, the mean differ will be differ from when we did this as part of the anova (gain score) and ancova exampl before. the top tabl show that there is inde a signific differ between the three groups, so we may reject the idea that the mean of all three group is equal in the population. - but from just that table, we can't conclud which group is differ from the other groups. - from the graph we know which group has the highest and lowest mean, but we don't know whether they are all signific differ from each other or if onli the group with the lowest and highest mean are differ from each other. - to this end, we can use the multipl comparison tabl to provid us with the answer to all these questions. both group 2 (8.12 point higher) and group 3 (12.23 point higher) have a signific higher mean than group 1, but group 2 and group 3 do not differ signific (p = .127).
differ between rm-manova and rm-anova: the main differ is that rm-anova assum spheric , while rm-manova doe not manova has multipl depend variabl , while anova onli has one multipl measur in time are all consid to be one depend variabl for rm-anova, while multipl measur in time are consid to be separ depend variabl for rm- manova but if we were to add the four measur in time as our depend variables, they wouldn't be correct for between-subject variance. to solv this, the differ score between measur in time are used, and due to have more than two measur in time we can use the logic of code variabl for linear regress on our depend variabl to test specif effects.
rm-manova: exampl (dependent) code variabl proceed to creat three code variabl ( alway one less than number of measur ) to compar various groups. depend on the code we choose, we will test differ attribut or effects. for instance, if you would take dummi code with c4 as refer group, then everi group will be compar to c4. however, dummi code isn't one of the two most common use ones. they are: repeat contrast this is like comput multipl match pair t-test after each other, as for all three contrast we compar two measur in time. that is whi an rm-manova for just two measur in time will have the exact same p-valu as a match pair t-test would have. the other common contrast is the polynomi contrast , to test for linear, quadratic, cubic, … trend . this one is especi use for effect over time:
rm-manova: example; multivari test (spss) a manova for both contrast type will now be comput (r = repeat , p = polynomi ): - differ code variabl have no impact on the total explain varianc of the model (repeat &amp; polynomi contrast will show the same general effect) intercept : for contrasts, we want to know whether the contrast estim differ from 0, and the intercept is a test to compar a valu such as the grand mean or, in this case, contrast estim to 0, so they coincid
rm-manova: example; repeated/polynomin tabl for the contrasts: from the repeat tabl we learn that there is a signific differ between measur 1 and 2 (p .001), which we also saw in the graph for rm-anova, which show a clear increas from measur 1 to measur 2. after that the line went down more gradually, so the differ between measur 2 and 3 (p = .101) and measur 3 and 4 (p = .504) were not found to be significant. _____________________________________ from the polynomi tabl we can see whether we have a linear trend (no, p = .536), a quadrat trend (yes, p .001), and cubic trend (almost, p = .053). ( hey, wait! these look like the p-valu we found for the rm-anova, the within-subject contrast tabl with these trend to be precise! surprise: both time we'v test the same effect, but in a differ way. ani differ between these s and the one in that tabl are round error ) but there is anoth interest point to these tables. there are no predictor in our model now, so the entir model, intercept &amp; error, for observ effect is shown above. our model is literally: observ valu y at time t of subject i = intercept (mean) of time t + error at time t of subject i this is the secret to whi rm-manova has so few assumpt . - the onli thing we assum is that there is a mean for each time point, all varianc between the score besid that mean is includ in the (random) error. - this technic mean we explain 0% varianc , as the intercept isn't part of the explain variance, so 100% is unexplained. (the multilevel analogu to this rm-manova model is call the “fulli multivari model”. this doesn't chang anyth about rm-manova itself, but it use to know for the multilevel part of this course, so keep this in mind for the multilevel section)
this is the secret to whi rm-manova has so few assumptions: the onli thing we assum is that there is a mean for each time point, all varianc between the score besid that mean is includ in the (random) error. this technic mean we explain 0% variance, as the intercept isn't part of the explain variance, so 100% is unexplain - multivari normal is still an assumpt here, as even the intercept is test with an f-test. - we also have the assumpt of independ between subject - no measur error the final assumpt of normal anova, linear relationship between depend variables, is gone as the depend variabl are now all code variables. as always, we onli need a linear relationship between continu variables, as there is no sens in tri to find linear relationship between codes.
rm-manova: h 0 hypothesi 
rm-manova: spss – output - hope they won't use this exampl for the exam, as the answer whether or not it signific depend on the chosen manova-method - wilk lambda , we can conclud that the interact effect is not significant, as the differ between the pattern in the observ data is just bare not big enough to generalis to the popul with certainti (if it was, we could use the differ in trend to help us pinpoint where this differ is, most like to the differ in quadrat effect around measur 2) - the main effect is significant, similar to rm-anova, which mean at least one time point is differ from the other on averag
rm-manova: follow up analysi rm-anova, they share their complet follow-up analysis, with no differ anywhere, so it will not all be repeat again
for two measur in time, with no between-subject variabl p-valu of match pair t-test = p-valu of rm-anova = p-valu of rm-manova
for two measur in time with one or more between-subject variabl intercept anova (gain scores) = main effect time rm-anova = main effect time rm-manova group effect anova (gain scores) = interact rm-anova = interact rm-manova
carry-ov effect is a phenomenon that with repeat measures, the previous condit influenc the condit after it , for instanc due to drug stay in blood circulation. this might be hard to prevent, like with experiment drugs, but is alway unwanted. to counter this, increas the time between condit might help, though that allow patient to chang more between measurements, and random the order might help to counter-bal ani carry-ov effect.
match pair t-test has a uniqu advantag in use a t-distribution, mean that the direct of an effect becom immedi clear from it be posit or negative. rm-anova &amp; rm-manova use f-values, that don't show this direction. except for that, rm-anova &amp; rm-manova are complet replacements, so if you understand rm-anova and rm-manova, you'll never have to comput match pair t-test or anova (gain scores) again! ancova and multilevel remain unique, due to the covari at the pre-measur and the insan flexibl of multilevel analysis.
multilevel modelling: assumpt normal &amp; independ of (all) residu (like rm-manova)
what multilevel analysi essenti does: summar the distribut of score into: grand mean of all, and varianc score to indic the spread in scores.
multilevel analysi vs. linear regress (exampl 1) linear regress without an independ variable: independ continu and code variabl can be ad to this model, to test to which degre they reduc the error in estim the observ values. the proport explain varianc can be measur with r² . the higher this number is, the more proport of varianc is explain multilevel also allow for multilevels: the error of all individu measur , which we saw for linear regression, is now replac by two separ error term - both b 0j and e ij are unexplain varianc in this model. - this addit error term mean that we can make variabl target one error term specifically. we can have it attempt to reduc error in level 2, level 1, or both, depend on what we expect a certain variabl to do it import to have these subgroup whenev necessary, as we have assum independ residu for everi test we'v learn about, includ multilevel models. when subject are part of differ group that score differ on the depend variable(s), and these differ group aren't part of the model, this assumpt is violat ( imagin check how well student per on a physic test in secondari school, and we don't take into account that this group of student have two differ teachers. when both teacher are equal competent, there is no problem as then the mean score for both teacher would be the same. the model for each student would then look like: ) the teacher be exact equal isn't veri like though, so it is wise to the effect of “ teacher ” to remov that part of the error from the model. in this case, we could also use anova to compar both teachers. however, a linear regress model with “teacher” as a code variabl take this differ into account in an ident way. the code will be –0.5 and +0.5, so contrast coding, for the sake of consist in interpretation: note how the altern notat of the anova hypothes return for linear regression. for the null hypothesi (both teacher are equal), b 1 is not part in the model. now that there is a mean differ for both teachers, so the altern hypothesi , b 1 is part of the model. we can also write down the model abov with multilevel notation:
essenti differ between linear regress and multilevel model the onli differ is that “ b 1 c j ” from the regress model is replac by “ b oj ” for the multilevel model. - b 1 show us the differ between teachers, and automat test this differ for signific as part of the linear regression, which is useful.
multilevel analysi vs. linear regress (exampl 2) the test score for a physic test are also relat to intelligence, so it is decid to also measur iq. this also make the comparison between teacher more fair, as one teacher may have student with a higher averag intelligence, thus caus the differ between teachers. at this point, we could do an ancova with intellig as a covariate, but for this case we may also use linear regression. iq is center to creat iqc, to keep the interpret as consist as possible: note how the altern notat for ancova-hypothes return for linear regression. for the null hypothesi of b 2 , that iq is unrel to test score, it is not part of the model, like on the previous page. now that we expect iq to be relat to test score, so h a , it is part of the model. the abov model may again be rewritten in multilevel : with regard to notation, multilevel analysi is now start to be superior. as you might have noticed, the first number after ani sign indic which paramet of student i it affecting, and the second number which paramet for teacher j it affecting. - b 00 is the overal mean, so the intercept for both groups. - b 0j is still relat to the intercept for students, as that depend on the mean of the teacher they belong to, but the teacher are free to differentiate. - b 10 doesn't influenc teacher score directly, becaus it not his/her iq we'r deal with, but it is part of the model for the individu level of students. - for them, it slope b 1 , a slope to reflect the relationship between iq and their physic test score.
multilevel analysi vs. linear regress (exampl 3) ad an interact effect for teacher * iq the abov model may again be rewritten as a multilevel model:
multilevel analysi vs. linear regress (exampl 4) ad teacher to the model (now 8 in total) linear regression: the abov model may again be rewritten as a multilevel model: this model is ident to the model for two teachers! this is one of the two biggest strength of multilevel analysis, how it can summar various effect of larg quantiti of group with just the mean and varianc , instead of the test each differ in full detail with the linear regress approach. it specif and attent to detail is a great strength of linear regress for simpler models, but now the more general approach of multilevel is more useful. multilevel win this one!
multilevel analysi vs. linear regress (exampl 5) ad "experi in years" of the teacher to the model: - linear regress stop work multilevel model gleefulli continu in multilevel , becaus as state before, the offici notat is realli clear: all possibl for test effect are ed in the model above: intercept or overal mean: y 00 main effect for students: y 10 main effect for teachers: y 01 interaction-effect between teacher and students: y 11 note how y is use for each fix factor ( u = unknown, random ), and that the first number show what it mean for students, and the second number show what it mean for teachers. unexplain varianc between teachers: u 0j unexplain varianc in the slope for iq between teachers: u 1j differ between teacher in general translat to a differ group mean for differ students, so that whi unexplain varianc between teacher start with a 0: u 0 , like b 0 for linear regression. differ between the slope of iq between teacher mean it relat to the varianc in y 10 , so to symbol this, the first number is a 1: u 1 multilevel model is the onli techniqu we'v learn about, at this point, that can includ both a level 2 effect (experience) and a level 1 effect (iq), and can simultan calcul separ valid coeffici for both effects. this is the second major strength of a multilevel model, as we are allow to take everi effect into account when calcul the other effects.
multilevel analysi vs. linear regress (exampl 6) miss p.104 teacher and all their relat effect will now be complet remov from the model, so we end up with a simpl model that linear regress can handl again: now let see what happen when everi student repeat this test multipl time over a year so we can track their progress:
multilevel model: theori - we can have more than one sample, which may be a subgroup of the other sampl - therefore, more than one error, which we add as anoth random variabl aggreg , onli test on level 2 (hospitals, teachers), work well for a small number of group and a relat larg number of level 1 unit by includ it as a fix factor , as we'v seen in the previous pages. however, this doe mean we lose most in ation on level 1, which might not alway be desir disaggreg , so ignor level 2 and onli test level 1 , is onli allow if there are no differ between the group at level 2, or the assumpt of independence, present for everi test, will be violat two-stag regress , a linear regress separ for each group at level 2 . work fine statistically, but is a lot of work when the number of group increases, and mean you have to somehow combin s to reach a general conclus rm-(m)anova is a valid altern that is even prefer for complete, fit data, but onli work for repeat measur and 2 levels, with separ measur in time on level 1 and subject on level 2. this mean it not as general applic as multilevel is. multilevel is also the onli one that can handl a continu variabl to indic the time a measur took place.
multilevel model: random intercept individu at level 1 have differ intercepts, depend on which group j they belong to. - this follow from the notation, as the intercept is shown with a 0 in “ ß 0j ”, and it depend on j is shown as the j in “ ß 0j ”. - all els is error in level 1 “ e ij ” for level 2 , we can see that the intercept of level 1 is depend on the overal mean ( y 00 ) that veri fit includ two 0's, as it is also the intercept of level 2, and the error term defin as the differ between the overal mean and the mean of each separ group. as we can see here, both level 1 and level 2 have their own error. u 0j is the varianc between the differ mean of each group at level 2, so the random of the intercept we use for level 1. that is what make this the “random intercept model”. ad u 0j to the model mean we expect the mean of each group to be different.
multilevel model: random intercept model with a slope let go back to the exampl of 8 teacher with students, but divid the student into two group with “old learning” and “new learning”. we can add this effect to the model through use dummi code , for instanc give old a 0 and new a 1, similar to linear regression. the bottom part show whi this is not consid a random slope ula, as the slope is fix at y 10 , as no (random) compon is ad to defin chang between groups. this mean we assum that the mean differ between old and new is the same for each teacher, which is quit strict. if you think this assumpt may be violated, it smart to add a random compon to check:
multilevel model: random intercept &amp; random slope note how a “ j ” was ad to ß 1 to reflect the effect of learn now chang depend on which teacher j an individu is part of. - now that the differ between both group is free to vari for each teacher, we no longer assum that the differ between the group is equal for each teacher. - this show that it is possibl to add or chang paramet in your model, to increas complex but reduc assumpt of a particular model.
multilevel model: repeat measur with repeat measures, we consid the sampl of all measur to be at level 1 , and a sampl of subject at level 2 . - becaus we now assum the measur to be a random sampl belong to a subject, similar to a class of student “belonging” to a teacher, it more logic follow that not everi sampl of measur within subject need to be equal in size, similar to how not everi class has an equal size. - just like some teacher have 8 students, and other have 5, now some subject have 8 measurements, and other have 5. also, to reflect we now work with repeat measures, the offici notat chang from “individu i, group j” to “measur t, individu i”. this is simpli convention, and has no influenc on how the model is defined. so the model above: turn into (given that the dummi is now use to distinguish group of measurements): multilevel is most similar to rm-anova with regard to how it deal with between-subject varianc . rm-anova has a set of measurements, from which the between-subject varianc ( u 0i for multilevel), is remov from the error as a whole. - as we can see from comput both test in spss, the random intercept model and rm-anova will use the exact same error for test the differ between measur in time, which also lead to ident f and p-values. the differ is that multilevel can handl data in a more flexibl manner, includ ad a paramet to creat a random slope model. - it doesn't requir each subject to have had the same number of measurements, and it doesn't even requir the measur to be defin by a categor variable. - becaus the paramet are estim through maximum likelihood, it can also deal well with miss data. - but maximum likelihood also has a drawback, we can´t calcul how much explain “variance” a model has, onli how much is still unexplain (compar the unexplain varianc of our model to an empti model where everyth is unexplained) - we may then compar both unexplain variances, to determin the proport explain varianc by our model. this empti model is call the “fulli multivari model”: for the record, the lectur say “ satur model ” as a refer to that now there is no individu error ( e ti ) anymore, but onli error part of the model, defin as u ti . - this is also the reason it is refer to as the “fulli multivariate” model, as everi observ valu can be found without fail with this model, mean e ti is 0. as this model is complet random, it explain 0% of the variance, make it a nice reference. just like we choos differ code variabl for rm-manova to test differ effects, like the repeat contrast &amp; polynomi contrast, we can test sever model for repeat measures. a linear relationship between two or more measur in time: so the differ between each time point is exact ß 1i mean it predict increas is linear. of course, not everyth in practic has a linear relationship, as we could even see in this veri summari when onli quadrat and cubic were (almost) significant, but it is the onli one that definit need to be understood for the exam, so learn it well. the next one are probabl not part of the exam, but might help to increas insight: a quadrat relationship between two or more measur in time: with the quadrat term ( ß 2i ) test for a potenti quadrat relationship over time. a piecewis linear function , given that we have 12 measur (t 1 to t 12 ), divid into 3 equal parts: these exampl of piecewis linear function and quadrat relationship are to show that it is possibl to test pretti much anyth as part of multilevel models. the greatest strength of multilevel model is flexibility. but what is most import for the exam, is be abl to understand and write down the model for a linear relationship. when compar linear regress vs. multilevel we could see all the differ type of variabl we could add to a multilevel model, such as a level 2 variable, level 1 variabl and interact between both of them. in repeat measures, this would mean a between-subject effect, within-subject effect and interact between both of them, respectively. after that, we'v learn the other notat where we split the regress model in a level 1 and level 2 section. finally, we'v learn a view basic model and notat for repeat measures, and how it is link to rm-anova and rm- manova. now, as the final part of this summary, we will look at the spss output for multilevel analysis, and we'll be complet done!
multilevel model: repeat measur ; a linear relationship between two or more measur in time: so the differ between each time point is exact ß 1i mean it predict increas is linear. of course, not everyth in practic has a linear relationship, as we could even see in this veri summari when onli quadrat and cubic were (almost) significant, but it is the onli one that definit need to be understood for the exam, so learn it well. !!!!!!!
multilevel model: repeat measur ; spss output the model we use to analys this is the random intercept &amp; random slope model the null hypothesi for multilevel modelling: all paramet are be compar to 0, similar to linear regression: the spss output is: everi coeffici is now call “estimate” as it is mere estim through maximum likelihood. multilevel, unlik linear regress and anova, doesn't work through ordinari least squares. the fix effect are test the same way they are with linear regression, by calcul a t-valu (estim / se) to then convert into a p-valu depend on the df. the random effects, varianc and possibl covariances, will now also be test against 0 (unlik all previous models) with the wald z test (estim / se). in this case, the linear effect of measur ( y 10 ) is signific (p = .014), but all other paramet are not. from that we can conclud the grand mean doesn't differ signific from 0 ( y 00 ), and doesn't chang signific between differ subject ( u 0i ), that this linear relationship we found doesn't chang much between subject ( u 1i ), but that the linear slope in general ( y 10 )probabl doe differ from 0 in the population. final the individu error that remain ( e ti ) is also not significant, mean that this model explain a lot as it is. but with just 18 measurements, that a pretti normal to find.
bonus – exampl of the 3-level rm-model, with a linear effect of time as a coefficient, that is differ for differ group (random intercept &amp; slope model): 
varianc / covari matrix (3 variables): 
correl (between variabl a and b): 
sscp matrix (sum of squar – cross products, 3 variables): 
contrast (4 groups, a = weight, µ = group mean): 
determin of a (1 x 1 or 2 x 2) matrix determin of a scalar (1 x 1 matrix) is equal to the number itself: so det[5] = 5
wilk lambda 
degre of freedom anova: 
regress model use in ancova 
correct / adjust mean in ancova 
null hypothes 
null hypothes 
binomi distribut function: 
multinomi distribut function (optional): 
basic mathemat ula concern logarithms: 
formula for comput expect valu for 2 variables: 
formula for comput expect valu for 3 variables: 
formula for comput degre of freedom for 2 variables: 
how to calcul degre of freedom for 3 variables: for the degre of freedom of this model, we need to look close at how the model is specified. we alway start with the total number of cells, then alway subtract 1 for know the total sampl size, even if we have no other variabl in the model. next, we need to subtract “number of group – 1 = i – 1” for each main effect, “(i – 1)(j – 1)” for each twoway interaction, and “(i – 1)(j – 1)(k – 1)” for each threeway interaction. in our example, we had the effect of foodxanim [ab] and gender [c]. if we take an interaction- effect [ab] we also alway have to includ the main effect that are part of this interaction, so [a] and [b]. this mean the final model has the main effect of a, b and c, and the interaction-effect [ab], so [a][b][c][ab], or [ab][c] for short. total number of cells: 2 * 2 * 2 = 8 subtract 1 for know the total sampl size: 7 group a – 1 = 2 – 1 = 1 therefor 6 left group b – 1 = 2 – 1 = 1 therefor 5 left group c – 1 = 2 – 1 = 1 therefor 4 left ab = (2 – 1) * (2 – 1) = 1 therefor 3 left so the degre of freedom for this model = 3.
null hypotheses: 
spss calcul linear combinations, their usag depend on the method: canon : use the linear combin to calcul the linear degre of associ between two sets.
spss calcul linear combinations, their usag depend on the method: discrimin : use the linear combin to predict which group subject belong to.
spss calcul linear combinations, their usag depend on the method: factor : use the linear combin as factor to summaris the continu variabl in the model.
spss calcul linear combinations, their usag depend on the method: compon : use the linear combin as compon to summaris the continu variabl in the model.
which test can we use for a repeat measur design with k = 2 pair t-test or ancova k=2 would e.g. be a pre-test, post-test design
which test can we use for a within subject design with k 2 within-subject anova e.g. pre-test, post-test, follow-up
ex of rm: doe a treatment work how would the design look like per anc across k condit simpl design: k=2: pre- and post-test measur of one-group sampl remember: within-subject design and singl group repeat measur are differ term for this same model
ex of rm: per anc across time. how would the design/research question look like how doe the effect of a treatment develop over time at least k= 3 time points!
what would be an exampl of a mix between-within subject design doe the effect of therapi from pretest to posttest differ between men and women
doe the effect of therapi from pretest to posttest differ between men and women what would be the between and what the within factor, and how mani level do they have between factor: gender (2 levels) within factor: time one within design and one between design
doe the effect of therapi from pretest to posttest depend on gender and educ level how mani (between/within) factor and level do we have between factor: gender, educ level within factor: time we do not know how mani level each factor has from the descript alon
what is the problem when use multipl pair t-test for each pair of time point and under which condit is it ok to do this ok for k=2 not ok for k 2, because: - multipl hypothesi test (name k-1 tests) - inflat alpha -reduc power (also when adjust alpha) -disregard associ between more than two measur = use within-subject anova or profil analysi
what is the h0 for a pre-test post-test deisgn when have onli one group (no between factor) h0: µ(difference) = 0 popul mean of differ score is 0 equival to a match (paired) t-test and within subject anova with 1 within subject effect
what do we use as a covari in a repeat measur ancova the pre-test the post-measur is regress on the pre-measur -- impli work with a correct mean -- the post-test mean µ 1 is correct for the pre-test mean µ 0 use linear regress
when use differ scores, what is assum for ß 1 we assum ß 1 to be = 1 but this is usual not the case. in ancova, ß 1 is estim optimally, this reduc the error varianc and increas power (keep in mind that ancova cannot alway be used!)
when is it ok to use ancova and when shall we use differ score lord paradox - when it is an experiment design ( random alloc to groups) - at the popul level no differ in pre-measur between group - when data is mcar 1 group: equal of regress slope - use ancova ancova and gain score analysis: - test same hypothesi and estim same group differ - ancova provid more power and precis than anova on gain scores, becaus the error varianc is smaller (thus preferred) __________________________________________________________________________________________________________________ if we have an observ design/quasi-experiment design/natur group and/or miss data which is not mcar -mean pre-measur are not necessarili the same -anocva compar group assum that they are equival on pre-test (therefor problematic!) -differ score analysi compar group as is (lead to differenct s than ancova here) - use differ scores! the crucial question to ask yourself: is group memebership unrel to pre-test score yes - ancova no - differ score analysi
understand output: how doe the output look like when we test h0: µ(difference)= 0 a pair t-test was use test whether the mean differ score is 0 (no difference). if the test is significant, we know that there is a signific mean chang (e.g. differ between pre- and post-test scores)
the sum of the extract valu is equal to the sum of the two eigenvalu of the two relev components.
understand output: how doe the output look like when we test h 0 : µ(d1) = µ(d2) = µ(d3) = µ(d4) four group analysis, anova on differ score (between subject anova) look at intercept : the intercept test if the popul mean of the outcom variabl is 0 (which is our time effect!) look at the categori (e.g. viewcategori is sesam street example), if this is significant, we know that at least one group differ
understand output: how doe the output look like when we test h 0 : µ1* = µ2* = µ3* = µ4* and which method was use four group analyisi ancova the post-test score is outcom variabl and the pre-test score is our predictor if viewcat (our groups, between factor) is no significant, we reject the h 0 : µ1* = µ2* = µ3* = µ4*
how doe a mix design k=2 time point look like we have a within subject factor with k = 2 level and a between subject factor (group) g group (g 1) - mix design: we can use anova on differ score test two hypotheses: h0: µ(diff) = 0 (time effect) h0: µ(d1) = µ(d2)..= µ(dg): group * time effect equival to within-subject anova , test 1 within-subject effect and 1 (or 1) between subject effect(s)
understand output: mix design k=1 where to look to which: h0: mu(diff) = 0 h0: mu(d1) = mu(d2)..= mu(dg) intercept is our within subject (time effect), if signific - reject h0: mu(diff) = 0 viewcat( group ) is our between subject effect, if signific - reject h0: mu(d1) = mu(d2)..= mu(dg)
how doe a mix design, k 2 time point look like and which model is appropri we have within subject factor with k 2 level and between subject factor(s) possibl analyses: either within-subject anova - consid repeat measur as a -design - ing factor subject or: profil analysi (l4) multilevel analys (l5 and further)
how doe a design work we the subject to remov within-subject variabl from error varianc we consid differ sampl mean across condit (e.g. across 4 differ drugs) and differ btw the subject (which we want to kick out)
whi can't we use a between subject anova for a within-subject design (although it would work in spss) imagin you have 5 subject and each of them is treat with 4 drug - we have a depend between our observ within one subject, due to the specif properti of that subject. subject must be regard as a ( ing , i.e. random) factor! spss doe not know this -- we would have less power
how doe the within-subject anova work we consid subject as a seper factor: ing (we then have two factors!) the univari approach split within-group variabl ss sk into two parts: 1) interact individu differ with treatment sssk 2) individu differ due to subject ss s we run an anova and add e.g. drug as a factor as well as subject the error term is the interact effect between treatment&amp;subject
compar between- and within-subject anova how are the ss partit between-subject anova: ss total = ss k + ss s(k) = between +within ss total = ss k + ss s +ss sk = between + individu differ + interc individu differ with treatment interact term treat as error term we onli have a singl combin of subject&amp;treat
understand output: within-subject anova with factor subject factor s ( subject ss ): measur consistend differ between subject that affect subject mean - if factor s is significant, the averag reaction time differ across subject - if this effect is 0, the within subject anova is the same as a regular anova factor k ( treatment ss ): within-subject effect and therefor requir within-subject error term error ms : interact ms , relfect the extent to which subject respond differ to treatment
assumpt in rm anova - independ observ (i.e. across subjects) - normal of residu - f k 2 (on more than two measur occas involved): spheric : at the popul level it hold that for all differ variabl between all pair of k repeat measures, the varianc are equal example: we have 3 drugs, therefor we have the differ score 1-2 2-3 1-3 the varianc of all these score need to be equal!
how can we test the spheric assumpt mauch test test the h0 that spheric hold (we hope to not reject)
what are the problem with the mauch test sensit to departur from normal lack of sensit to small violat
what to do when spheric is violat in rm anova f would be too liber (reject fals too often) 1) use a within-subject anova (univari anova) with epsilon correct or profil analysi (l4) rm multilevel analysi (l5)
what doe the epsilon correct in rm anova do it adjust the degre of freedom 2 methods: (µ 0.7) - greenhous &amp; geisser - huyn &amp; feldt epsilon is the extent to which the covari matrix deviat from sphericity: dfs of f are multipli by epsilon
when to use greenhous &amp; geisser vs. huynh &amp; feldt gg: correct is quit conserv (df becom too small) hf: correct is quit liber (df becom too large) for larg n, they are usual the same for small n, gg is safer
rm-anova assumes: spheric
spheric violat (in rm-anova) rm-anova with epsilon correction, or : profil analysi = repeat measur m anova
rm-anova – test provid by spss 
rm-anova – correct test 
profil analysi – type of design 1 within-subject factor (e.g., time) test of flat (= main effect of time) 1 within-subject factor (e.g., time) and 1 between-subject effect (e.g., gender) test of flat (= main effect of time) test of parallel (= interact time x gender) test of differ in level (= main effect of gender)
exampl of profil (graphs) 
profil analysi - 1 within-subject factor profil analysi with 1 within-subject factor equal analyz k – 1 tran ed measur with manova cf. t test for depend sampl (pair samples): analyz differ score di = yi1 – yi2
tran ed measur manova on the (k – 1) tran ed variabl – particular linear combin of k origin measur h 0 : mean of the tran ed variabl equal to 0 e.g.,: t1 =y1 - y2: t2 =y3 - y2; μ(t1)=μ(t2)=0 – k – 1 tran ed variabl may or may not be dependent, henc it is not assum that the tran ation take care of all depend between measur contribut by one subject
tran ed variabl – which k-1 tran ed variabl have to be use – mani equival choices, not all orthonormal, some via dummi contrasts: - polynomi tran ation (contrasts) are standard in spss - differ score of consecut time point – invari property: multivari test statist is the same for equival choic example: – 3 repeat measures: polynomi tran ation – coeffici orthonorm yes, becaus … 1. cross-product : (–0.707) * 0.408 + 0 * (–0.816) + 0.707 * 0.408 = 0 2. length of coeffici : (–0.707)2 + 0.7072 = 1 0.4082 + (–0.816)2 + 0.4082 = 1
assumpt in rm-manova – rm-manova is manova on tran ed variabl – so, if you know the assumpt of manova, you know the assumpt of rm-manova … back to assumpt of manova
profil analysi with between-subject factor exampl : differenti effect of treatment for panic disord – dv: scl90 1 between-subject factor treatment (3; cgt, ssri, combi) 1 within-subject factor time (4; pre, mid, post, fu) – research questions: a. doe the scl score decreas across time - test of flat b. which treatment is most effect - test of parallel (interact time*treat)
rm-(m)anova - techniqu to choos depend on spheric assumpt 
(m)anova on tran ed variabl - techniqu to choos depend on spheric assumpt 
hierarch linear model (hlm) multilevel = special regress model - cluster or hierarch nest data not necessarili the same amount of observ per group - more level possibl
multilevel model – sampl are drawn at two (or more) level - observ are nest : - level 1 observ are depend within level 2 unit - assumpt of independ observ is violat model relat between group model relat within group – repres within-group and between-group relat in a singl model: - conceiv of the unexplain variat within and between group as random variability: random coeffici
back to basics: linear regress 
recall: assumpt in linear regress independ observ linear relat between y and x error term has constant varianc σ2 error term e is normal distribut with mean 0 and varianc σ2, independ of x error term repres random differ between observations, summar by the varianc
multilevel model graph notat (for 2 levels) j = 1, …, n group (level 2 units) i = 1, …, nj individu in group (level 1 units) depend variabl y i y ij predictor at level 1 (individuals) x i x ij !! !!
altern method aggreg : analyz mean at level 2 (ignor level 1) - statist ok if sampl size at level 1 larg research question is complet at the higher level (loss of in ation, shift of meaning) ecolog fallaci = fallaci of aggreg (alker, 1969); use aggreg data to infer at individu level disaggreg : analysi of all variabl at level 1 (ignor level 2) - not ok ignor depend structur lead to overestim n and underestim of standard error (and incorrect tests) atomist fallaci (hox, 2002); use individu data to infer at aggreg level two-stag regress analysis: - ok 1 – for each group (level 2 unit) separ regress 2 – analyz the group-depend coeffici however, inefficient: small n per level 2 unit, loss of precis analysi of varianc (repeat measures): - ok less flexible: need balanc design, no more than 2 level
multilevel is ideal solut – not separ regress per group j, but one model in which differ in coeffici (for groups) are model use random coeffici - coeffici are fix or random - explain varianc at two level
random intercept model 1.differ between individu 2. differ between group - β 0j : intercept vari over group - γ 00 : mean intercept in popul of group - var (β0j) = var (u0j) : varianc of intercept reflect rang of differ across group in intercept
random slope model 1. differ between individu 2. differ between group - β 0j and β 1j : intercept and slope vari over group - γ 00 and γ 10 : mean intercept and slope in popul of group - var (β 0j )=var( u 0j ) and var (β 1j )=var (u 1j ): varianc of intercept and slope indic rang of plausibl differ - also cov( β 0j, β 1j ): covari between intercept and slope
multilevel model (graphs) 
random intercept and random slope model assumptions: (u 0j , u 1j ) and e ij independ u 0j ~n(0,τ 0 2 ) u1j~n(0,τ 1 2 ) eij ~n(0,σ e ) which paramet are be estim
two formul of a multilevel model 
predictor in multilevel analysi (example) example: pupil (level 1) nest within class (level 2) predictor at level 1 : x ij e.g., age, sex, iq predictor level 2 : z j - e.g., year of experi of teacher, number of pupil within classes, - special: mean score of level 1 variabel (e.g., mean iq score in class) random slope model to predict languag test score (y) from iq (x)
statist test in the multilevel model fix regress coeffici usual t-test: (estimate/standard error) or likelihood ratio test random coeffici likelihood ratio test = devianc test onli for nest model
fix or random effects: exampl example: employe nest within compani a. 500 employe from 40 compani b. 500 employe from 3 compani multilevel analysi a . yes - or anova with compani as random factor b . no - use ordinari regress - or anova, with compani as a fix factor
fix or random effects: depend on – focus of statist infer (research question) – the natur of the set of n group (generalizability) - pupil within school - inhabit within countri
fix or random effects: limit by – sampl size at all random effect level - e.g., 2 levels, n j individu within i (i=1,…,i) groups: 1. number of group 2. magnitud of the group sampl size n j
two-level structur of measur within individuals: level 1: measur - equal occas across individuals, or differ time point - explanatori variables: time and time-depend variabl level 2: individu - explanatori variables: individu characterist cross-level interact : - time by individu characterist - do male show a differ pattern across time than femal
research question typic involv change/develop – level 1 (intra or within-individual) - how doe outcom change/develop over time – level 2 (inter or between-individual) - can differ in the chang be model or predict how mani measur – minim 2 - depend on research interest
from subjects-group to time points-subject 
composit model – time points-subject 
random slope model vs. fulli multivari model 
(m)anova repeat measur vs. multilevel analysi 
fix part - random part fix part: model for µ t , i.e., mean at time point t possibl depend on covari e.g., µ tg with g=1,2 group random part: model for within-covari matrix of time point possibl depend on covari within-covari matric (e.g., random group effect)
fix part of multilevel model for rm equal measur occas – equal measur occas - e.g., pretest, posttest, follow-up – model via dummi code unequ measur occas – vari time points, but associ via time - e.g., bodi length, measur at various time point between 2-10 year of age – model via function of time
function of time model : popul of curv – data are order accord to some under dimens (here: time) – statist model - determin adequ class of function fi - investig how they depend on explanatori variabl exampl : linear function of time, random intercept and slope, no explanatori variabl linear function often inappropri use of polynomi function (with r m)
differ class of function polynomi function (with linear function as a special case) piecewis linear function linear function at given interval, with 1 interv use
random part of multilevel model for rm 1. compound symmetri = random intercept model assumpt : all varianc of and covari between measur are equal example: note: compound symmetri is stricter than spheric
random slope model – individual-depend deviat u 0i and individual-depend rate of increas u 1i – in between compound symmetri and fulli multivari model exampl :
fulli multivari model satur model: perfect fit of covari matrix use as a benchmark no level 1 varianc exampl :
random intercept and slope: exampl : sampl of 103 children, born into low-incom famili depend variable: cognit per anc ( cog ) independ variables: program (0 = no program; 1= yes program) intens intervent program to increas cognit per anc age (measur time points: at 1, 1.5, and 2 of age) model as a variabl occas design, with a linear function in the model, (age–1) is used, for interpret purpos question : how doe the individu cognit per anc chang over time: is there a time effect 2. are there differ in chang between children in both group with and without intervention: is there a program effect over time t = 1, 2, 3 time point (at age 1, 1.5, and 2) i = 1, … , 103 children random intercept ( β 0i ) for child i: depend on program repres estim valu of cog of child i at age 1 random slope β 1i for child i: depend on program repres estim rate at which child i chang over time fix effect γ 00 : popul averag of individu intercept in no program popul γ 10 : popul averag of individu slope in no program popul γ 01 : differ between popul averag of individu intercept in no program popul and yes program popul γ 11 : differ between popul averag of individu slope in no program popul and yes program popul model on level 1: model on level 2:
null hypothes exampl : sampl of 103 children, born into low-incom famili depend variable: cognit per anc (cog) independ variables: program (0 = no program; 1= yes program) intens intervent program to increas cognit per anc age (measur time points: at 1, 1.5, and 2 of age) model as a variabl occas design, with a linear function in the model, (age–1) is used, for interpret purpos on the fix part of the model 1. between subject : effect of program at age of 1 there is no program effect at age 1, h0: γ01 = 0 no differ in intercept between group 2. within subject : effect of time ( age ) there is no time effect for no program, h0: γ10 = 0 no differ in slopes: horizont line (over time) 3. within subject : interact effect there is no interact effect, h0: γ11 = 0 effect of time is the same for both group
discret depend variabl – so far: linear multilevel regress (mr), for contin data, normal distribut level 1 residu – also avail - logist mr, for dichotom data - poisson mr, for count data … - but, more complic than linear mr
which test to use identifi design of the studi between -subject factor(s), within -subject factor(s) covari (s) (= continu predictor) depend variable(s) (dvs) decid which aspect(s) from 1. to includ in one analysi
univariate, covari (x), between-subject effect only: anova
univariate, covari (√ - assumpt hold), between-subject effect only: ancova
univariate, covari (√ - assumpt violated), between-subject effect only: regress model , with group (via dummi ) and covari as predictor, possibl with interact between both
univariate, covari (√ - assumpt violated), between-subject effect + possibl between-subject effect rm-multilevel model
univariate, covari (√ - assumpt hold), between-subject effect + possibl between-subject effect rm-ancova or rm-mancova, or rm-multilevel model
univariate, covari (x), between-subject effect + possibl between-subject effect rm-anova or rm-manova, or rm-multilevel model
multivaraite, covari (x), between-subject effect + possibl between-subject effect not treat in the cours
multivaraite, covari (√ assumpt hold 1 ), between-subject effect + possibl between-subject effect 1 linear relationship and equal of regress line across group not treat in the cours
multivariate, covari (√ assumpt violated), between-subject effect + possibl between-subject effect not treat in the cours
multivaraite, covari (√ assumpt violated), between-subject effect onli multilevel regress (not treated)
multivaraite, covari (√ assumpt hold 1 ), between-subject effect onli 1 linear relationship and equal of regress line across group mancova
multivariate, covari (x), between-subject effect onli manova
structur analys - no rm 
structur analys - no rm 
structur analys - repeat measur 
manova: h 0 (example) – 3 dv's, 1 between-subject factor, with 2 level (e.g, gender) – test of main effect of gender with µ jg the popul mean on test j (j = 1,2,3) for gender g (g = 1,2)
rm-(m)anova: h 0 (exampl 1) – 1 dv – 1 within-subject factor, with 4 level (e.g., drug) – test of main effect drug: with µ tj the popul mean on a proper tran ed variabl of drug j (see lectur 4b)
rm-(m)anova: h 0 (exampl 2) 1 dv 1 within-subject factor, with 4 level (e.g., drug) 1 dv 1 within-subject factor, with 4 level (e.g., drug) test of main effect drug: with µ tj the popul mean on a proper tran ed variabl of drug j (see lectur 4b) test of main effect gender: with µ f the popul mean of femal across all four level of drugs, and µ m of male test of interact drug and gender: with µ tj , f the popul mean on a proper tran ed variabl of drug j of the femal
multilevel model for chang – regress model to address two differ question about chang simultan – model on two level - level 1 model : individu growth model how doe outcom chang over time ( within-subject ) - level 2 model : model for differ between individu can we explain differ in chang over time ( between-subject ) - paramet : fix or random, and interpret
exampl – lab class: willet &amp; singer: research question ______________________________________________ - sampl of 82 children - depend variable: alcohol use ( alcus ) - independ variable: no/y alcohol parent ( coa : 0/1) - three measur time points: age 14, 15, and 16, center at ( age_14 ) the research question here is whether individu trajectori of alcohol use dure adolesc differ accord to the histori of parent alcoholism. question : 1. how doe the individu alcohol use chang over time: is there a time (age) effect ( main effect time ) 2. are there differ in chang between children with and children without an alcohol parent: is there a parent effect over time ( interact level2 )
exampl – lab class: willet &amp; singer: level 1: model for individu chang ‘ 9.2. formul the level 1 compon of the multilevel model: the individu growth model assum that there is a linear relationship between age_14 and alcohol use. give an interpret of the paramet .’ ______________________________________________ - sampl of 82 children - depend variable: alcohol use ( alcus ) - independ variable: no/y alcohol parent ( coa : 0/1) - three measur time points: age 14, 15, and 16, center at ( age_14 ) the research question here is whether individu trajectori of alcohol use dure adolesc differ accord to the histori of parent alcoholism. i = 1, …, 82 children; j = 1, …, 3 measur (at 14, 15, 16 years) model the chang over time ( age ) for each individu ( child ): intercept β 0i (for child i): popul valu (accord to model) of alcus at age 14 slope β 1i (for child i): popul rate (accord to model) at which child i chang over time assumpt : epsilon ij ~ n(0, σ epsilon 2 ) (sometim briefli σ 2 )
exampl – lab class: willet &amp; singer: level 2: model for differ in chang 9.3. formul the level 2 compon of compon of the multilevel model: the model for interindividu differ in chang (i.e., differ between individuals), assum that the intercept and the slope of the level 1 model have a linear relationship with coa. give an interpret of the parameters. ______________________________________________ - sampl of 82 children - depend variable: alcohol use ( alcus ) - independ variable: no/y alcohol parent ( coa : 0/1) - three measur time points: age 14, 15, and 16, center at ( age_14 ) the research question here is whether individu trajectori of alcohol use dure adolesc differ accord to the histori of parent alcoholism. γ 00 : initi alcohol use at age 14 for coa = 0 (popul average) γ 10 : rate of chang for coa = 0 (popul average) γ 01 : differ in initi use at age 14 between coa = 1 and coa = 0 γ 11 : differ in rate of chang between coa = 1 and coa = 0 u 0i : deviat between popul intercept and individu intercept u 1i : deviat between popul slope and individu slope note: u0i and u1i may be dependent:
exampl – lab class: willet &amp; singer: h 0 to test ______________________________________________ - sampl of 82 children - depend variable: alcohol use ( alcus ) - independ variable: no/y alcohol parent ( coa : 0/1) - three measur time points: age 14, 15, and 16, center at ( age_14 ) the research question here is whether individu trajectori of alcohol use dure adolesc differ accord to the histori of parent alcoholism. - general : ‘h 0 : γ.. = 0’ - what doe it mean e.g., γ 10 : rate of chang for coa = 0 if γ 10 = 0, then for coa = 0 the rate of chang is 0, so no time effect
exampl – lab class: willet &amp; singer: singl composit model 9.3. combin the level 1 and level 2 model algebra into one singl composit model by substitut the constant term and slope term in the level 1 model by the model specif of the level 2 model. ______________________________________ ________ - sampl of 82 children - depend variable: alcohol use ( alcus ) - independ variable: no/y alcohol parent ( coa : 0/1) - three measur time points: age 14, 15, and 16, center at ( age_14 ) the research question here is whether individu trajectori of alcohol use dure adolesc differ accord to the histori of parent alcoholism. 
exampl – lab class: willet &amp; singer: exampl - test fix effect ______________________________________ ________ - sampl of 82 children - depend variable: alcohol use ( alcus ) - independ variable: no/y alcohol parent ( coa : 0/1) - three measur time points: age 14, 15, and 16, center at ( age_14 ) the research question here is whether individu trajectori of alcohol use dure adolesc differ accord to the histori of parent alcoholism. note: spss estim paramet for coa=0
exampl – lab class: willet &amp; singer: exampl - test random effect ______________________________________ ________ - sampl of 82 children - depend variable: alcohol use ( alcus ) - independ variable: no/y alcohol parent ( coa : 0/1) - three measur time points: age 14, 15, and 16, center at ( age_14 ) the research question here is whether individu trajectori of alcohol use dure adolesc differ accord to the histori of parent alcoholism. 
equal measur occas – equal measur occas e.g., pretest, posttest, follow-up – model via dummi code
unequ measur occas – vari time points, but associ via time - e.g., bodi length, measur at various time point between 2-10 year of age – model via function of time
exampl – lab class: willet &amp; singer: function of time ____________________________________________ - sampl of 82 children - depend variable: alcohol use ( alcus ) - independ variable: no/y alcohol parent ( coa : 0/1) - three measur time points: age 14, 15, and 16, center at ( age_14 ) the research question here is whether individu trajectori of alcohol use dure adolesc differ accord to the histori of parent alcoholism. three measur time points: age 14, 15, and 16 function of time:
exampl – lab class: willet &amp; singer: dummi code ____________________________________________ - sampl of 82 children - depend variable: alcohol use ( alcus ) - independ variable: no/y alcohol parent ( coa : 0/1) - three measur time points: age 14, 15, and 16, center at ( age_14 ) the research question here is whether individu trajectori of alcohol use dure adolesc differ accord to the histori of parent alcoholism. three measur time points: age 14, 15, and 16 d1: valu 1 for age 15, 0 otherwise; d2: valu 1 for age 16, 0 otherwis intercept β 0i : valu for child i of alcus at age 14 slope β 1i (for child i): rate of chang for child i of alcus between age of 14 and 15 slope β 2i (for child i): rate of chang for child i of alcus between age of 14 and 16
key issu for an analysi plan sampl design: nest data – analys as fix or random effect outcome(s) of interest: dv(s) 1 dvs - a univari analysi per dv, or joint in a multivari model predictor(s) of interest – confirmatory: primari interest, hypothesi driven – exploratori – type of relationship between predictor in analysi miss data – causes, mechanism(s) (nmar, mcar, mar), threat to generaliz
how doe word2vec work word2vec learn a vector represent of a word from text corpus. it is a shallow neural network, that tri to creat similar vector for word in close neighbourhood with each other (5-15 word apart), and dissimilar vector for word that do not cooccur. it doe so by ad a tini bit of a vector from one word to the other when pull closer and, of the opposit vector when pull apart.
what is adam, how doe it work adam is an extens of stochast gradient descent, which combin the advantag of adagrad and rmsprop, it is adapt moment estim (adapt learn rate for each parameter, base on the histori of it previous gradient mean and varianc (first and second moments). it make learn faster.
what is the differ between analyt and numer gradient how are they comput the numer gradient is an approxim of the gradient, without actual comput a deriv of a function. it is comput simpli by pick a small constant δ, which is ad to a function as so: numerical_gradi = f(x+ δ) - f(x) / δ . by do this we find the approxim direct and rate of the function change. to find the exact direct for a particular x_i, we need to comput the deriv and substitut valu of x_i in a ula. this is not alway possibl (some functions, like hing loss for example, are not differentiable. so, we comput the subgradient, which is a of analyt gradient.
what is the differ between gru and lstm gru doesn't have a separ cell state.
learn improv with experi e at some task t with respect to per anc measur p
least mean squar rule find target function that optim mean squar error 1. select train exampl at random 2. calcul valu of learn function v'(b) 3. comput error e = v_train(b) - v'(b) 4. updat weight for each board feature: w_i = w_i + epsilon + e
design choic train experience, target function, represent of learn function, learn algorithm
data mine - need to suppli exampl (labeled, typical, adequ distributed, not show clutter) for learn 1) hybrid systems: explicit model + ml 2) learn as much as possibl from unsupervis learn - onli few label exampl - visualization/acces
concept boolean function that is true for appropri entiti concept learn ~ binari classif (see lectur 08)
learn task - instanc x: e.g. days/min ... - target function c (boolean) - train exampl d - hypothesi (representation) h -- need: h elem h such that: "all x elem d: h(x) = c(x)"
induct learn hypothesi ani hypothesi found to approxim the target function well over a suffici larg set of train examples, will also approxim the target function well over other exampl
general h2 is more general than h1 if everi instanc that is classifi posit by h1, is also classifi posit by h2
find- 1. h = most specif hypothesi (o) 2. for each posit exampl x: - if attribut ai not satisfi by h - replac ai in h by next most general constraint that is satisfi by x 3. output h
find- complaint - learn onli from pos exampl - when is learn finished/target learn - might get sever possibl h, depend on order of exampl - no detect of inconsist train data + maxim specif h pick - induct bias: specif h prefer and hypothesi render all exampl negat unless unless the opposit is entail by it prior knowledg - veri strong!! no use of neg exampl
consist a hypothesi h is consist with a set of train exampl d of target concept c, iff h(x) = c(x) for all train exampl
version space version space vs_hd with respect to hypothesi space h and train exampl d is the subset of hypothes from h consitst with all train exampl in d
list-then-elimin find hypothesi by list all possibl and elimin inconsist one 1. vs 2. for each (x, c(x)) in d: - if(h(x) != c(x)) - remov h from vs 2. output vs
list-then-elimini properti - onli applic for finit vs - enumer annoy - comput complet vs - if remain vs big - need other measur to pick h - vs empti - inconsist exampl - impract - induct bias
general boundari g of vs_hd is set of maxim general member
specif boundari s of vs_hd set of maxim specif member
candid elimin - defin version space by two boundari (g,s) - g learn from negative, s from posit exampl - add minim general (code see folder) - narrow down vs gradual - size = miss info - vs can be consult to select most in ativ exampl (50:50 pos:neg)
induct bias - minim set of assert need for a system to induct infer b from a - b is knowledg necessari to deduct infer not from exampl but a priori knowledg d_c ^ x_i l(x_i, d_c) == *b* ^ d_c ^ x_i = l(x_i, d_c)
entropi - ula e(s) = -p+ * log_2(p+) - p- * log(p-) - p* proport of pos exampl c differ values: e(s) = - sum(i=1 ... c) p_i * log(p_i)
entropi - measur of "impurity" - number of bit need to encod class (+ or -) - optim code: p*log(p) bit
gain - ula gain(s,a) = e(s) - sum(v elem values(a)) e(s_v) * s_v / s - e(s): befor evalu - sum: after evalu - fraction: proport of exampl belong to s_v
gain want to maxim gain - minim entropi after evalu to get highest gain
decis tree - classifi data by sequenc of interpret step - domain: discret target function describ as attribut valu pair - branch = conjunct of attribute-valu - disjunct hypothesi - want smallest tree (use gain as select attribute)
id3 - find best order of attribut by distribut over the exampl - alway select attribut with best gain as long as exampl not perfect classifi - search space of possibl decis tree - output: singl hypothesi (tree) - no backtrack - local maxima problem - robust against nois - disadvantage: all exampl need for everi step induct bias: short tree prefer (occam razor) - high gain at root prefer - pseudo-cod see folder
c4.5 extens of id3 - better against overfitting, for continu valu attributes, attribut with mani values, attribut with cost, miss attribut
problem of decis trees/id3 overfitting, continu valu attributes, attribut with mani values, attribut with cost, miss attribut
overfit fit to train set too high, no general anymor - avoid by: stop grow if split not signific anymor - after grow tree: rule post prune & reduc error prune
rule post prune - build tree - write down everi branch as logic rule - remov ani presupposit that improv per anc on test set - sort final rule by accuraci (new order of application) - whi : * no entir subtre - more sensit * everi attribut (also root) prunabl * better readabl
reduc error prune - grow tree - while(p anc of tree on test data++) { * check all node for effect of prune on test data * remov node with biggest improv * remov subtre of node and assign most common attribut valu as new leaf } * produc smallest version of accur tree * remov node produc by nois * but: few data - better rule post prune
gainratio - attribut with mani valu - gain select it (e.g. day) - no good general - gainratio = gain(s,a)/splitinfo(s,a) - splitinfo(s,a) = - sum(i=1...n) si / s * log2 si / s - entropi with respect to attribut valu - si subset of s for which a has valu v
decis tree - miss attribut - use exampl with miss valu anyway but do: * assign most common valu of a to node * assign most common valu of a of other exampl with same target valu * assign probabl pi to each possibl valu vi of a - assign fraction of pi exampl to each miss value-nod
caus of outlier - technical/measur error * cut off measur - high concentr of valu at boundari - unexpect "true" effect (model by 2 overlay distributions) - high varianc in data (distribut with high flanks)
outlier detect - what is normal/in normal rang z-test zi = xi - mu (or median) / sigma - zi 3 or 3.5 = outlier * rosner test
rosner test while(new outlier found) { * calcul mu/median and sigma * find xi with largest zi * if(zi 3) {remov xi} } variation: remov k outliers, ! no new median/mu/sigma calcul for all those
miss valu - can't throw data away - would make much of set useless 1) substitut mean/median !artifact 2) (linear) regress !artifici cluster along regress line 3) estim distribut of data + generat acc. to distribut !use onli complet data 4) em-algorithm - use complet and incomplet vector
em algorithm aim: estim of hidden valu problem: h depend on unknown distribut theta solution: * estim distribut theta_t * maxim q with respect to theta = averag over hidden valu - theta_t estim will converg to local max, that is hope close to real theta (problem: local search) - trade in h depend of i for theta depend of q - code: see note
metric 1) symmetry: d_ij = d_ji 2) coincid axiom: d_ij = 0 i=j 3) triangl equation: d_ik + d_kj = d_ij - impli non-neg of distanc
distanc measur approx of semant relat sinc onli numer access to machin - euclidean - citi block - malahanohobi (use covari matrix c) - maximum distanc (king moves) - ham - p-norm!
nomin scale problem: other topolog as r^n - embed in r^n map nomin attribut to real valu {wood, metal, stone} - {(1,0,0), (0,1,0), (0,0,1)} - problem of mani attribut valu n: - choos random vector from r^m 1 - high dim. like orthogon
jaccard index - use for similar of binari scale - j(a,b) = # common elem/# all elem = a ^ b / a u b - distanc function: j_d(a,b) = 1 - j(a,b)
cluster distanc measur d_min = min d(x, y) d_max = max d(x,y) d_mean = 1/( x * y )* sum d(x,y) d_centroid = d(1/ x * sum x, 1/ y * sum y)
bias cluster - all c. algorithm have bias - size/shap of prefer cluster - should be: adjust by model paramet - usually: onli algorithm paramet changeabl - bias has to be infer from algorithm - hierarch cluster solv scale problem with differ scale solut
hierarch cluster agglom divis
agglom cluster - everi point own cluster - merg cluster recurs bottom-up - pseudocod see folder - o(n^3), optmin o(n^2)
divis cluster - all point one cluster - divid recurs top-dow - o(2^n), optmin o(n^2)
linkag criteria - singl linkag - complet linkag - centroid - ward min varianc - averag link - minimum energi
singl linkag - shortest distanc between two cluster - d_min use
complet linkag - longest distanc between two cluster - d_max use
averag linkag - averag distanc between cluster - d_mean
centroid cluster - distanc between centroid use (cluster onli repres by centroids) - real valu attribut requir for comput - cluster with more member domin merg centroid
ward minimum varianc - merg clusters, so that increas of total varianc is minim e = sum(al clusters) sum(al point in cluster) (x - mu_i)^2 mu_i = 1 /c_i * sum(al point in c_i) x - optim based, but implement by distanc measur d_ward = d_centroid(x,y)/(1/ x + 1/ y ) - prefer cluster with same number of member - robust against noise, but not outlier
minimum energi cluster - don't onli evalu inter-clust difference, but also cluster size (small preferred) - ula on paper d_energi = 2*d_mean(x,y) - d_mean(x,x) - d_mean(y,y)
hierarch cluster - properti - ani distanc measur usabl - onli need distanc matrix (not data) - no paramet - ing dendogram offer altern solut (has to be analyzed) - cut off at differ level of dendogram might be necessari for compar cluster - outlier fulli incorpor
optim base cluster what should good cluster look like - put idea in measur e to assign good valu to certain partion of data
basic maxim algorithm - optim base cluster initi data in cluster somehow while (stop not reached) { * choos exampl x at random, denot cluster as c(x) * random select target cluster c_i * comput chang of good function e [...] * if (delta e 0) put x from c(x) to c_i * els put x from c(x) to c_i with probabl e^{beta delta e) * beta increas
properti - basic maxim algorithm - may be caught in local optima - escap by prob in els statement - depend on initi cluster - simul annealing: initi small beta allow downhil steps, increas beta- more unlik
optim function (more detail see cards) minimize: * tr(w) = sum(i=1...d) lambda_i(w) - minim varianc (small, round clusters) * det(w) = product(i=1...d) lamda_i(w) - minim volum of averag cluster, no compact guarante (cluster of similar shape) * product(i=1...n) det(a_i)^ c_i - look at individu cluster to avoid ident shape of all cluster - problem: for d dimensions, need at least d+1 data/clust otherwis det=0 maximize: * tr(bw^-1) = tr((a - w) * w^-1) = tr(aw^{-1} -1) - small tr(w) yield small cluster - larg tr(b) yield big var of cluster center
cluster and compress data set repres onli by cluster center - transmit cluster center + for each point onli cluster it belong to
k-mean - divid data set into k cluster c1 ... ck - cluster repres by mean w1 ... 1k - minim quadrad error: e(d, wi) = 1/ d * sum(i=1...d) xi - w_m(xi) ^2 with w_m(xi) = arg min(j) xi - wj - iter k-means: * random choosen refer vector * assign data to best match * updat refer vector by shift them to mean of cluster * assign data to best match ....
k-mean properti - local search, solut not uniqu - greedi optimization: local optima, depend on initi condit - worst case: empti cluster - one paramet k = # cluster - implicit defin scale/shap of cluster - fast - color compression: 2^{3x8} bit = 16,7 mio color can be depict by k=100 (100 colors)
soft cluster - describ data by probabl distribut p(x) - data point assign to cluster use probabl - no hard boundari
hard cluster cluster are set of data points, describ by set or their mean - disjoint - each point assign to singl cluster - no uncertainti
mixtur of gaussian generat data: (- p(x) as whole) - each gaussian repres separ cluster: select one gaussian with probabl g_k - generat random x with prob n(x, mu, ck) a priori probabl that random exampl from d is in volum v of data space: p(x elem v) = integral(v) p(x) dx a priori probabl that random exampl from d belong to cluster k g_k a posteriori probabl that given data point belong to cluster k p*k(x) = g_k * n(x, mu_k, ck) / sum(i=1...k) gi * n(x, mu_i, ci) find best mixtur of gaussian to fit given data set d - find paramet {g_k, mu_k, c_k} by em
properti - em for mixtur of gaussian - local optimum - comput more expens than k-mean - precaut against collaps of a gaussian on singl data point necessari - k-mean can provid use initi for mu_k, local pca for c_k - k! equival solut
evalu of cluster - no general method - tests: * differ data subset * distribut of averag distanc in k-neighbor cluster * compar inter- and intra-clut differ * compar two clusterings: (#pair seper in 2 clusterings)/(#al pairs) = probabl that random choosen pair is treat the same in both cluster
conceptu cluster employ idea of cluster and decis tree learn for unsupervis classification/categor - class not a prior fixed, ed by exampl - bias: prefer of categori ation
cobweb most well known algorithm for conceptu cluster - unsupervis and increment learn - probabilist represent (gradual assign to classes) - no a priori fix # of categori motivation: (most from drawback from id3): * artifici threshold * disjoint learn (tree building) and applic phase * each learn step divid data onli along one dimens of attribut space * categori defin by proposit logic
principl of categor * categori ation strong connect to prototyp concept - birch more typic tree than palm (for us) * basic level categori (middle) acquir first furnitur kitchen chair
famili resembl theori categori definit base on similar in complex way, not by hard necessary/suffici condit
cobweb realize global util function - determin # of categories, # of hierarch levels, assign of object to categori categori c1 ... cn, attribut ai with valu v_ij: s = 1/n sum(n=1...n) sum(i,j) p(ai = v_ij) * p(ai = v_ij cn) * p(cn ai = v_ij) - interpret see card learn = new exampl insert into tree and tree modifi in such a way, that s is max
curs of dimension - combinator explos - n^d point necessari to sampl d-dimensionla space - dens sampl imposs for larg d - no real data will ever "fill" space - pair of random vector like to: * have similar distanc * close to perpendicular - high d: almost all volum on this unit cube surfac layer
intrins dimension # independ paramet necessari to describ data - depend on problem, not represent - presenc of hidden structur can be recogn from d_intrins - data concentr along manifold of dimension much lower than embed represent space - manifold ~ space made from distort piec of an euclidean r^n, != subspac
descript dimension # paramet use in unprocess data set - depend on represent
aim of dimension reduct - find local dimension in data manifold - new coordin system to repres (and project) data manifold - get rid of empti part of space of descript paramet - new paramet more meaning
principl compon analysi find subspac that captur most of the data varianc - unsupervis - given data set d = {x1 ...}, xi elem r^d with zero mean = sum(i=1... d) xi = 0 pca find m eigenvector = principl compon - features, recept fields, filter kernels,... doe not creat structure, onli make exist structur access
pca - mathemat basi ( ula see cards) vector pi are m eigenvector of the autocorrel matrix c with largest eigenvalu lambda_i - c symmetr and real - lambda_i are real and there is an orthonorm basi of eigenvector - expans of x use all eigenvector would xield xi without error - obtain approxim zi of xi, onli m expans after the m e = sum(i=1...d) (zi - xi)^2 approxim z_i of x_i is project of x_i onto m-dimension sub-spac span{p1 ... pm} of r^d residuum is delta*x_i = sum(j = m+1 ... d) a_ij * p_j (left out dimensions/eigenvector varianc of residuum is sigma²(delta*x) = sum (i = m+1 ...d) lambda_i, i.e. the mean approxim error is the sum of the left out eigenvalu
pca - find m eigenvector look for a jump in spectrum of eigenvalu - suitabl cut-off number problem: larg eigenvalu also possibl due to noise/inappropri scale
local pca find cluster centers, then comput pca individu for each cluster - better: iter improv posit of cluster center and local pcas problem: for continuous, nonlinear (non-clustered) distribut (e.g. curves) - local pca no continu descript of manifold - better: principl curv problem: differ cluster possibl on continu distribut - entir differ local project system
princip curv nonlinear extens of pca - data project onto principl curv to obtain linear order issues: *find appropri dimension * find appropri flexibl (parameterization) of the fit function (underfit vs reason vs overfit) - addit smooth constraint should be use general approxim of pca to parameter surfac x(a1, ..., am; w) of m dimensions, vector w summar n paramet which determin the shape of the surfac - found by minim e = 1/2 * intergr ( x- x(a1, ..., am; w) )² * p(x) dx m paramet ai(x) defin point on surfac of x, that best match x and have to be comput for each x individu
princip curv - find w e.g by gradient descent - problem: each step requir integr over all data - solution: stochast approxim = make downhil step onli wrt singl paramet
kohonen adapt rule addit smooth constraint ad for adapt of princip curv - delta w_kl = epsilon * (x - w_kl) alon is winner take it all rule (on best match adapted) constraint see flashcard
scatterplot matrix project on two of the dimens and all combin in a matrix
glyph map each dimens onto paramet of a geometr figur - perciev as whole - requir train - re-numb of axe - new train *star glyph *parralel coordin (koordinatensystem) * chernov faces: paramet map to facial featur
project techniqu - don't repres full data, onli project to select dimens - e.g. pca for visual - problem: structur of data disregarded, onli variance: visual may be unstructur
project pursuit -project onto 2-3 directions, where data exhibit interest structur = variance, non-gaussian, cluster - procedure: 1) select 1-3 dimens (pca ...) 2) project onto those, get densitiy p(x) of project data 3) comput index function (how interest p(x) ) 4) maxim index function - search for better direct
project pursuit: indic - measur deviat of a distribut (normal to 0 mean and varianc 1) from n(x) maxim * friedman-turkey: i_ft = integr p²(x) dx - minim if p is parabol function similar to normal * hermite: i_h = integr (p(x) - n(x))² dx - minim if p std normal distribut * natur hermite: i_nh = interg (p(x) - n(x))² * n(x) dx - like i_h but stronger weight to center * entropy: i_e = integr p(x) logp(x) dx - minim if p std normal distribution, note: not -p*log(p)
index for find cluster cluster distributions: - more short distanc between pair of valu (for ident variance) - i(k) = s(k) * p(k) maxim - k = project vector - s(k) = std deviat along k (still big variance) - p(k) = averag point distanc - onli comput up to certain scale, defin by f(.) and cut off valu r
project pursuit: densiti comput maxim requir estim of densiti p(x) of project data - kernel densiti estim (parzen windows) - orthonorm function expans
multidimension scale aim: find manifold of low dimens such that project the data onto this manifold preserv the structur as good as possibl - what doe structur mean important: distanc between point find map that distanc are well preserv sammon stress measure: - minim e.g. by gradient descent with respect to map paramet
formal neuron - iti = x (spike frequency) - d input - x elem r^d - each input x_i weight by number w_i - weight input sum up to obtain ation s = sum (i=1...d) w_i * x_i = w*x - output y - sigmoid ation function y = y(s)
activ function - sigmoid * fermi * grossberg *tanh * threshold
unsupervis learn - no teacher - no label exampl - effect code in learn rule
supervis learn - teacher - label exampl (class ...) - learning: map input to label as output (class) weakly/semi-supervis learn popular: pre-structur data by unsupervised, use label spare after unsupervis
reinforc learn - "weak teacher" - reach goal state is be told by teacher - way has to be found on it own
hebbian learn "cell that fire together, wire together" - correl of fire lead to increas of weight unsupervis
hebb rule increas weight w_i accord to the product of the iti of the neuron and the input delta w_i = epsilon * y(x*w) * x (stepsiz epsilon)
hebb rule: limit weight growth no forgetting, sinc weight never decreas - decay term - dynam normal to w = 1 - explicit normal to w = 1 - oja rule use weight decay ~y² formula see card - stimulus will be forgotten if not present anymore, other will becom more domin
oja rule delta w = epsilon * y(x*w)(x-y(x*w)*w)
effect of hebb rule on pair of neuron with decay term after repeat present of sampl of the same set - equilibrium - averag over time step t w_ij doe not chang anymor t = t = 0 t = eps/lamdba * t - w_ij proport to correl of two neuron iti
effect of hebb rule on weight vector - converg to eigenvector c with largest eigenvalu - w along direct of largest varianc hebb neuron affect by all input (not like winner take it all from kohonen net) - for w small, close input has less effect than same input stretch by c 1: c*x - w direct itself acc to varianc
habitu anti-hebb rule delta w = - epsilon * y(x^t * w) * x - weight vector becom orthogon to a repeat stimulus x - no ation from this stimulus sinc x*w = 0 - w filter out new stimuli - sever stimuli x1...xn n onli w-compon orthogon to span{x1, ..., xn} remain - onli stimuli xev with v orthogon span{x1, ..., xn} can "pass" fiter = ate neuron
extract more pcs use hebb rule singl hebb-neuron extract v_1 (w in direct of most variance) - want more pcs 1) success applic of singl hebb neuron * extract v_1 * project d to space orthogon to v_1 x_i = x_i - (x_i * v_1) * v_1 * extract v_2 by same method ... - each neuron correct input - singl neuron less train 2) sanger method * chain later coupl neurons, each hebbian * onli first neuron unfilt input * second has input "minus" direct of first weight vector , third minus first two w ... - each step complet chain train - beginning: later input still with pcs with larg eigenvalu in them - but: no need to detect end of train of singl neuron sinc all train togeth
later interact - hebb rule sanger method with chain of neuron - each neuron receiv input filter by predecessor
perceptron - most promin for neural network - build of mlp - supervis learn - input: x^i - linear ation s - output y as threshold function - can onli solv linear separ task (on two side of hyperplane) - small hypothesi space - more step until converg
perceptron train rule - train iter - label data set x^n e r^d - target valu t^n delta w = epsilon * (t - y) x - epsilon learn rate - converg if epsilon small and task solvabl decis surfac repres by perceptron: d-1 dim hyperplan orthogon to w - posit side: xw 1 - rest 0 - rule can be deriv from minim mean squar error by gradient descent
xor problem not linear separ - solv by distort of input space - linear tran ation * axi not x1, x2 but f(x1,x2) - solv by ad third dimens x3 = x1 * x2
perceptron: batch and increment mode batch mode = stochast approxim of mean squar error to get to train rule delta w = epsilon * sum(i=1...d) (t^i - y(x_i)) * x_i increment mode: gradient descent onli wrt one exampl at a time delta w = epsilon (t^i - y(x_i)) * x_i
mlp - input x^i - hidden layer l_h - output layer l_h+1 - weight read backwards! - output o_i(k) - ation function sigmoid, becaus linear tran would be substitut with singl linear tran - need soft step, becaus backpropag need differenti error function same as for simpl perceptron - problem: comput all deriv from w_ik(m,n) to output layer
mlp - squash map the incom in ation to much smaller rang
backpropag initi weight random while(!stop criterion) { * propag input x forward to obtain output y * for(al output dimens i) { -- calcul error between target and actual output } * find weight "responsible" * updat weight }
backpropag properti - weight from k to k+1 adapt in proport to 1) ation of neuron in prev layer o_i(k) 2) weight error it caus at the output epsilon * delta(k+1) - stochast approxim to gradient descent error function e (one sampl at a time) - minimization: * comput expens * suffer from numer local minima * difficult to termin (overfit vs good minimization) induct bias: smooth interpol - given by network architectur and gradient descent search
backpropag - local minima avoid: - repeat train (diff init random weights) - annealing: add noise, eg.g everi n learn step w_ji(k+1, k) = w_ji(k+1, k) + t * zeta_ji(k+1, k) - zeta is gaussian random variabl with averag = 0 - t = temperatur denot amount of nois ad (decreas dure training) - need more learn step with anneal
backpropag - step size adapt - increas epsilon in flat region and decreas in steep terrain - use epsilon+ 1 and epsilon- 0 and multipli with old epsilon
momentum problems: - minim stop in local minima due to small epsilon - valley of e: oscil due to larg epsilon solv by momentum - avoid abprubt chang of direct and increas effect step size in flat region
backpropag - weight decay - larg weight problematic: make neuron too sensit to input - binari ation addit quadrat regular term in error funtion avoid too larg weight - linear weight decay in learn rule
funahashi & cybenko f: arbitrari bound continu map can be approxim arbitraili precis by one hidden layer + sigmoid ation function + linear output function c: anoth sigmoid hidden layer - ani function approx
mlp architectur first layer defin hyperplan second layer defin and (one class in the other) third layer defin convex area (sever class insid one other)
issu with mlp architectur how mani layers/nod in each layer - no restrict but: - intrins dim of input will be reduc to min of neuron in a hidden layer - more node - facili of better representation, but no "invention" of new info - hidden layer - discov featur of input data
neural architectur - fulli connect net - spars connect net - diagon matrix - tridiagon matrix - lower/upp triangular matrix - block-diagon matrix (optional: spars rest of matrix) - symmetr (hopfield)
feedforward network - onli zero weight in upper triangl - acycl direct graph - e.g. mlp - output: comput by updat neuron from input to output side - "timeless"
recurr network - connect matrix has non-zero weight in both triangl - cycl - imposs to find order of neuron for updat as in ffnetwork - onli usabl by defin dynam - continu time (diff. equations) - discret time step (synchronous/al neuron at onc vs asynchron updating) - state: vector compromis all neuron ation
phase space space of all possibl state of a network (all possibl ation vectors)
instanc base learning/nearest neighbor algorithm - train = memorize/stor train exampl - applic = for unknown input x, find best match x^n of train sampl - output: t^n - local
k- nearest neighbor find set s of k nearest neighbor of store exampl - discret valu output: vote among k nn - real valu output: mean of k nearest neighbor y = 1/k * sum(i elem s) t_i
properti of nearest neighbor approach - nn: hard boundari of class assign (voroni-tessel cells) - k-nn: continu transit - choic of k depend on local intrins data dim - "training" = fast/memory-intensive/no wast of info/no paramet - applic = mayb slow/sensit to error and nois
distanc weighetd k-nn nearer neighbor more import - y = 1/ (sum(i elem s) w_i) * sum(i elem s) w_i * t_i weight w_i = invers distanc 1/ x - x_i
local weight regress idea: better approxim of y(x) by comput fit function in area surround sampl - what fit function (linear, quadratic... ) - what error function * squar error over k-nn s * error over entir d with error weight by decreas distanc function k * combin
local vs non-loc - mlp not local (adapt of singl weight base on singl exampl might influenc entir net) - "death" of neuron - major influenc local: wrt input space - output comput individu for diff region of input space, adapt onli local effect
radial basi function - global approx of target function by linear combin of local approx - relat to distanc weight regress + neural network - repres map of in- to output (like mlp)
architectur of rbfn - singl layer of neuron - all same input - ation depend on match between input and neuron- weight - ation function unimod (gauss...) = kernel function = "area of responsibility" in input space - neuron contribut to vector valu output by their weight - high ate = more contribut - output function = local function with "compact support"
task of rbnf train 1. suitabl "centers" / input weight - use examples/clust on input part of exampl zeta_i = x_i 2. suitabl radii of influenc - radius = distanc to nn control by gamma sigma_i = gamma * min (k != i) zeta_i - zeta_k 3. output weight - perceptron like train rule: delta w_i = epsilon * (t-y) k_i( x - zeta_i )
rbf vs mlp adapt step: *rbf: onli per anc in respons input area affect *mlp: may chang all weights/per anc on all data architectur paramet * rbf: # basi function * mlp: # layers, #neuron adapt paramet * rbf = clustering, radii, stepsiz * mlp: stepsize, momentum ... - rbf general easi to interpret
self- organ map - motiv low level signal data - abstract, symbol high level represent - concept ation - filter (relev vs irrelevant) - find "structure" = relat between concepts/prototyp -- topolog preserv map from signal to higher level
som - spatial arrang of neuron - all same input - competition: neuron at locat s with best match weight w_s "wins" = highest excit - adapt weights, later interact caus neighbor neuron to adapt too excit over layer caus by later interact - unimod function: h_rs = exp(- r-s ² / 2*sigma) adapt rule/kohonen rule: delta w_r = epsilon * h_rs * (x-w_r)
som adapt rule adapt rule/kohonen rule: delta w_r = epsilon * h_rs * (x-w_r) - hebb rule with decay term - epsilon*w_r (forgetting) and grid-dist weight h_rs local
som adapt procedur 1. get input x random 2. find best match neuron at s such that w_s*x w_r*x (r!=s) 3. adapt * foral r: delta w_r = epsilon * h_rs * (x-w_r) 4. decreas step size epsilon and size sigma of adapt region - more node in region of dens data (later interact = excit within short range, inhibit over long range)
som practic use - princip curv - cluster analysi - visual
semi-supervis learn - unsupervis learn to find structur in data - subsequ label (supervision) of alreadi found structur
vision 2.0 simpl system, configur by anybodi for arbitrari task 1. captur unlabel imag 2. extract standard featur of all imag 3. som to find structur in featur space 4. visual 5. label assign to cluster of relat imag 6. train classifi
classif - assign discret class to object/fact/person/.... - attribut repres by featur vector x elem r^d - output: natur number c(x) assign to c differ class - c_i(x) discrimin functions: c(x) = arg max(i) c_i(x)
bay classifi classifi input to minim expect cost * p(c x) = n * p(x c) * p(c) - n = normal - p(x c) = prob densiti that class c has featur x - p(c) a priori prob of class c - overlap class can be repres - uniqu assign not possibl
problem bay classifi - probabl unknown - need to estim - complet knowledg might be necessari or not (depend on distribution) 1.) estim p(x c) and p(c) from data, then do classifi 2.) construct approx of classifi from data
euclidean classifi discrimin found from center of mass of the class r(x) = w * x with w = - - linear - not local - fast "training" with no paramet - sensit to outlier
lda - class have ident a priori probabl - gaussian distribut with mean and and covari matric ^+ = ^- = ^ r(x) = w * x with w = ^-1 ( - ) ^ =
quadratic, polynomi classifi quadratic/polynomi discrimin - polynomi almost like mlp
nearest neighbor classifi implicit seperatrix, defin by neighbor - local - no general + paramet - no train but memori
support vector machin 1. comput hyperplan (linear separatrix) - base on: exampl (support vectors) close to class boundari - margin maxim - slack variabl to deal with overfit
svm kernel trick - trick to solv non-linear problems: * project data into higher dimens (everi problem linear seperable) * project of this hyperplan to origin data space is non-linear separatrix - onli inner product of x_a * x_b need to be comput without actual represen of x_a * x_b in h - mercer condit
mercer condit - over all rang of interg != infin - lime can be 0
reinforc learn agent within environ that - has percept - can per action aim: per optim action depend on percept to achiev some goal
agent p er anc e nviron a ctuator s ensor
markov decis process - discret state from finit set s - discret time t - discret action a successor state and reward depend onli on current state and current action, not earlier one s_t+1 = delta(s_t,a_t) with delta = successor function r_t+1 = r(s_t, a_t)
step in develop a machin learn applic collect data. prepar the input data analyz the input data train the algorithm test the algorithm
describ the knn algorithm. we have an exist set of exampl data, our train set. we have label for all of this data--w know what class each piec of the data should fall into. when we'r given a new piec of data without a label, we compar that new piec of data to the exist data, everi piec of exist data. we then take the most similar piec of data (the nearest neighbors) and look at their labels. we look at the top k most similar piec of data from our known dataset; this is where the k come from. (k is an integ and it usual less than 20.) lastly, we take a major vote from the k most similar piec of data, and the major is the new class we assign to the data we were ask to classify. should normal data when deal with differ ranges. calcul the distanc use euclidean norm.
describ the decis tree algorithm. to build a decis tree, you need to make a first decis on the dataset to dictat which featur is use to split the data. to determin this, you tri everi featur and measur which split will give you the best s. after that, you'll split the dataset into subsets. the subset will then travers down the branch of the first decis node. if the data on the branch is the same class, then you'v proper classifi it and don't need to continu split it. if the data isn't the same, then you need to repeat the split process on this subset.
describ entropi in in ation theori 
what the differ between bag of word and set of word model bag of word model count the number of occurr of a token and set of word onli count the existence, 1 or 0, of a token.
describ bootstrap aggreg or bag it is a techniqu where the data is taken from the origin dataset s time to make s new datasets. the dataset are the same size as the original. each dataset is built by random select an exampl from the origin with replacement. by "with replacement" i mean that you can select the same exampl more than once. this properti allow you to have valu in the new dataset that are repeated, and some valu from the origin won't be present in the new set. after the s dataset are built, a learn algorithm is appli to each one individu- ally. when you'd like to classifi a new piec of data, you'd appli our s classifi to the new piec of data and take a major vote.
whi are svms call machin they'r call machin becaus they generat a binari decision; they'r decis machines.
what is boost boost is a techniqu similar to bagging. in boost and bagging, you alway use the same type of classifier. but in boosting, the differ classifi are train sequentially. each new classifi is train base on the per anc of those alreadi trained. boost make new classifi focus on data that was previous misclassifi by previous classifiers. boost is differ from bag becaus the output is calcul from a weight sum of all classifiers. the weight aren't equal as in bag but are base on how success the classifi was in the previous iteration. there are mani version of boosting, but this chapter will focus on the most popular version, call adaboost, short for adapt boosting.
describ the adaboost algorithm. adaboost is short for adapt boosting. adaboost work this way: a weight is appli to everi exampl in the train data. we'll call the weight vector d . initially, these weight are all equal. a weak classifi is first train on the train data. the error from the weak classifi are calculated, and the weak classifi is train a second time with the same dataset. this second time the weak classifi is trained, the weight of the train set are adjust so the exampl proper classifi the first time are weight less and the exampl incorrect classifi in the first iter are weight more. to get one answer from all of these weak classifiers, adaboost assign valu to each of the classifiers. the valu are base on the error of each weak classifier. the vector d is important. it hold the weight of each piec of data. initially, you'll set all of these valu equal. on subsequ iterations, the adaboost algorithm will increas the weight of the misclassifi piec of data and decreas the weight of the proper classifi data. d is a probabl distribution, so the sum of all the element in d must be 1.0. to meet this requirement, you initi everi element to 1/m.
what is a confus matrix a matrix with actual (rows) vs predict (columns) s
what is a problem with linear regress one problem with linear regress is that it tend to underfit the data. it give us the lowest mean-squar error for unbias estimators. with the model underfit, we aren't get the best predictions.
what is a way to reduc the mean squar error for linear regress a techniqu known as local weight linear regress ( lwlr ). in lwlr we give a weight to data point near our data point of interest; then we comput a least-squar regress lwlr use a kernel someth like the kernel demonstr in support vector machin to weight nearbi point more heavili than other points. the most common kernel be the gaussian kernel.
what the problem with lwlr the problem with local weight linear regress is that you need to "carri around" the dataset. you need to have the train data avail to make predictions.
what doe it mean for a matrix to not be fulli rank if we have more featur than data point ( n m )
describ princip compon analysis. in pca , the dataset is tran ed from it origin coordin system to a new coordin system. the new coordin system is chosen by the data itself. the first new axi is chosen in the direct of the most varianc in the data. the second axi is orthogon to the first axi and in the direct of an orthogon axi with the largest variance. this procedur is repeat for as mani featur as we had in the origin data. we'll find that the major of the varianc is contain in the first few axes. therefore, we can ignor the rest of the axes, and we reduc the dimension of our data. we can get these largest variabl valu by take the covari matrix of the data- set and do eigenvalu analysi on the covari matrix. onc we have the eigenvector of the covari matrix, we can take the top n. when the pca is appli to this dataset, we can throw out eigenvectors. the top n eigenvector one dimension, and the classif problem be- will give us the true structur of the n most import features. we can then multipli the data by the top n eigenvector to tran our data into the new space. note: some peopl use the number of princip compon that will give them 90% of the varianc or other use the first 20 princip components.
induct learn hypothesi ani hypothesi that approxim the target function well over a suffici larg set of train exampl will also approxim the target funciton well over the unobserv exampl
candid elimin algorithm converg converg to target concept if no error exist and target function exist in h
induct bias find- the target concept c is in h and that the solut is a maxim specif hypothesi
induct bias candid elimin the target concept c is in h
entropi e(s) = sum(-pi*log(pi))
in ation gain gain(s,a) = e(s) - sum( abs(sv)/abs(s)* e(sv))
recall, precis "recal = tp / (tp + fn) precis = tp / (tp + fp)
" "suppos a comput program for recogn dog in photograph identifi eight dog in a pictur contain 12 dog and some cats. of the eight dog identified, five actual are dog (true positives), while the rest are cat (fals positives). precis and recal " the program precis is 5/8 while it recal is 5/12
roc curv x - fp, y = tp
id3, characterist "id3 search a complet hypothesi space in an incomplet way induct bias in id3 is prefer for smaller tree overfit is an import issu and can be avoid with prune method
" bay rule p(a b) = p(b a)p(a)/p(b)
learn as probabl estim p(h d) = p(b h)p(h)/p(d)
hmap, hml "hmap = argmax p(d h)p(h) hml = argmax p(d h)
" "is hmap the most probabl classifi for a new instanc " no it is not, the bay optim classifi is.
bay optim classifi bay optim classifi = argmaxv sum( p(vi xi,hi)*p(hi d))
chain rule probb p(x1,..xn) = pi_1^n p(xi x1,..xi-1)
x is condit independ of y given z p(x,i z) = p(x y,z)p(i z)=p(x z)p(i z)
naiv bay assumpt and classifi "p(a1,a2...,an vj,d) = pi p(ai,vj,d) vnb = argmax p(vj,d)pi p(ai vj,d)
" nb_learn(a,v,d) "for each vj in v: p(vj d) = estim for each attribut ak: for each ai in ak: p(ai vj,d) = estim
" linear least squar solut "(x'x)^-1x't assum gaussian condit distribution, not robust to outlier
" fisher criterion "w = sw^-1(m2 - m1) wo = w'm sw = sum (xn-m1)(xn-m1) + sum(xn-m2)(xn-m2)
" perceptron train rule delta wi = lr (td - od) xi,d
logist regress "p(c1 x) = g(w'x+w0) g(t) = 1/ (1+e^-t)
" linear basi function model y(x,w) = w'phi
linear regress wml wml = (phi'phi)^-1phi't
multiclass logist regression, cross entropy, softmax "softmax = e(aj)/sumj(e(aj)) cross ent = - sumn sumk ( tnk * ln(ynk))
" what is a kernel "a distanc function k(x,x') =0 typic symmetr and non-neg linear = x'x poli = (bx'x+g)^d rbf/gauss = e(-b x-x ^2) sigmoid = tanh(bx'x+g)
" kernel regress "primal: w = (x'x-lamdai)x't dual: alpha = (xx + lambdai)^-1t, w = x'alpha y(x,w) = sum alphai * xi'x = sum alphaik(xi,x) alpha = (k + lambdai)^-1t
" svm regress min 1/2 w ^2 + c * sum( s- + s+) s 0 ti ti = y(xi,w) - e - s-
" svm classif "min 1/2 w ^2 + csum ti(w'x + wo) = 1 - s
" one versus all, one versus one "one versus all - c binari classifi one-versus-one: c(c-1)/2 classifi
" parametr non parametr "parametric: model has a fix number of paramet non parametric: number of parametr grow with amount of data
" knn "find k nearest neighbour of test input x assign x to the most commont label among the major of vote
" knn kernel trick xa - xb ^2 = xa'xa + xb'xb - 2xa'xb
boost boosting: use a weak (or base) learner to build simpl rules, then combin these weak rule into a singl predict rule that will be more accur than each singl rule. ym(x) = sign(sum( alpha_m * ym(x) )
adaboost "init wn = 1/n for m =1 ... m: train a weak learn ym(x) with weight error functon: jm = sum wn i(ym(xn) != tn) evalu em = (sum wn i(ym(xn) != tn) / sum wn alpham = ln ( (1-em)/em) updat weight wn = wn exp(alpam i(ym(x) != tn)) ym(x) = sign(sum alpham * ym(x) )
" adaboost pros con "advantages: fast, simpl and easi to program no prior knowledg about base learner is requir no paramet to tune (except for m) can be combin with ani method for find base learner theoret guarante given su cient data and base learner with moder accuraci issues: per anc depend on data and the base learner (can fail with insu cient data or when base learner are too weak) sensit to nois
" cost function neural net "typic negat log likelihood - maximum likelihood principl j(theta). = - ln ( p(t x)) with sigmoid: g(t) = 1 / (1+exp(-t) j(theta) = softplus((1-t)a), a=w't + b the softplus onli satur when given correct answer
" softmax y = softmax(a) = exp(ai)/sum(exp(ai))
relu, sigmoid, tanh "relu = max(0,a) sigmoid = 1/(1+exp(-a) tanh = 2sigmoid(2a) -1
" backprop "forward: for k =1 to l: a = w*h + b h = f(a) backward: g = derji for k = l, l-1, .. 1: g = g x f'(a) - elementwis derb = g derw = g(h^(k-1)) g = w'g
" sgd, sgd momentum, "theta = theta - lr grad j momentum v = gamma*v + lr grad j theta = theta - v
" nesterov momentum "v = gamma*v + lr grad j(theta-gamma*v) theta = theta - v
" rmsprop "dw, db sdw = b2sdw + (1-b2)dw^2 sdb = b2sdb + (1 - b2)db^2 w = w - lr * dw/sqrt(sdw + e) b = b - lr * db/sqrt(sdb + e)
" wout cnn wout = floor( (win - f + 2p)/s + 1)
properti of cnn "translat invari - an object can appear anywher in the imag not tran ation invari - cannot fix rotation, scale etc, data augment weight share
" "cnn properti kernel, padding, stride, recept field" "kernel - matrix correspond to filter depth - number of filter stride - step of slide kernel recept field - 2d dimens of kernel
" what doe a cnn layer consist of "a convolut part (kernel and biases) a non linear part ( ation function) a pool part (max-pool)
" number of paramet in a cnn layer (fxfxdepth + bias)nc
what is pca use for "dimension reduct data compress (lossy) data visual featur extract
" pca step "to per pca in a m-dimension project space, with m comput the mean of the data x ̄ comput covari matrix of the dataset s find m eigenvector of s correspond to the m largest eigenvalu
" pca varianc maxim "xbar = 1/n sum xn varianc after projection: (1/n) sum (u1'xn-u1'xbar)^2 = u1'su1 s = (1/n) sum (xn - xbar)(xn - xbar) max var subject to u1'u1 = 1 max u1'su1 + l1(1-u1'u1), der = 0 su1 = l1u1 - u1'su1 = l1
" pca algorithm " full eigenvalu decomposit of s (slow) 2 e cient eigenvalu decomposit - onli m eigenvector 3 singular valu decomposit of center data matrix x
" markov properti "a- onc the current stat is known, the evolut of the dynam system doe not depend on the histori of states, action and observ - the current state contain all the in ation need to predict the futur - futur state are condit independ of past state and past observ given the current state - the knowledg about the current state make past, present and futur observ statist independ
" markov process is a process that has the markov properti
mdp(x,a, s,r) "x is a finit set of state a is a finit set of action s : x x a - x is a transit function r: x x a - r is a reward function
" one state mdp solv "mdp(x0, a, s, r) r(ai) det and known: pi^* = argmax r(ai) r(ai) det and unknown: tri all ai then select highest r(ai) non-det and known: pi^*(x0) = argmax e[r(ai)]
" q-learn train rule "q(s,a) alpha = 1/ ( 1 + visits(x,a)
" sarsa learn rule "q(s,a) = ""q(s,a) alpha = 1/ ( 1 + visits(x,a)
"""  
what is concept learn "infer a boolean-valu function from train exampl c(x) - target function h(x) - estim of h over x goal is to find the best hypothesi h that predict correct valu of h(xj) xj not in d
" more_general_than_or_equal_to hk (hj = hk) iff (for all x) (hk(x)=1) - (hj(x)=1)
hj is more_general_than hk (hj hk) iff (hj = hk) and (hk not hj)
find_ "init h to most specif hypothesi in h for each posit train instanc x in d for each attr_const ai in h if constrain h in h is satisfi by x do nothin replac ai in h by the next more general constrain that is satisfi by x
" consistent, version space "consisten(h,d) = (for all x in d) h(x)=c(x) version space: {h in h consistent(h,d)}
" candid elimin algo "g s for each train ex d: if d pos: remov fom g ani hyp inconsisten with d for each hyp s in s that is not consisten with d: remov s from s add to s all minim general h of s such that h consist with d and some member of g is more general than h if d neg: remov from s ani hyp inconsisten with d for ech hyp g in g inconsist with d: remov g from g add to g all minim special h of g such that h is consist with d and some member of s is more specif than h remov from g ani hyp that is less than general than anoth hyp in g
" learn problem improv over task t, with respect to per anc measur p base don experi e
lms weight updat rule "error(x)= v(x) - y(x) wi - wi + c * fi * error(x)
" miss attribut decis tree "if node n test a - assign most common valu of a among other exampl sort to node n - assign most common valu of a among other exampl with same target valu - assign prop pi to each possibl valu vi of a. assign fraction pi of each examål to each descend in tree
" condit probabl p(a b)=p(a and b)/p(b)= alpha * p(a and b)
hmm setup "hmm(x,z,pio) transit model p(xt xt-1) observ model p(zt xt) initi distribut pi0 0 p(x0) state transit matrix aij = p(xt=j xt-1=i) observ model bk(zt) = p(zt xt= k)
" hmm interference, filtering, smooth "filter p(xt=k z1:t) = at^k/sum(at^j) smooth p(xt=k z1:t) = at^k * bt^k/ sum(at^j * bt^j):" hmm forward "at^k = p(xt = k, z1:t) for each state k a0^k = pi0 * bk(z0) for each time t = 1,, t for each state k at^k = bk(zt) sum(a_t-1^j * ajk)
" hmm backward "bt^k = p(z_t+1:t xt=k) for each state k bt^k = 1 for each time t = t-1...1 for each state k: bt^k = sum( b_t+1^j ajk bj(z_t+1) )
" hmm learn paramet aij, bk(v) "aij = i- j tran / i - * tran bk(v) = observ v and state k / observ * and state k
" pomdp "pompd(x,a,z,s,r,o) x set of state a set of action z set of observ p(x0) prob of initi state s(x,a,x') = p(x x,a) prob dist over transit r(x,a) o(x',a'z') = p(z x',a) prob dist over observ
naiv bay estim "p(vj d) = {....,vj..,} / d p(ai vj,d) = { } / {....,vj..,}
" smooth kernel "integr k(x)dx = 1 integr xk(x)dx = 0 integr x^2k(x)dx 0
crosscorrel convolut crosscor: g(i,j) = sum_u=-k^k sum_v=-k^k h(u,v)f(i+u,j+v) cons: g(i,j) = sum_u=-k^k sum_v=-k^k h(u,v)f(i-u,j-v)
what is filter and smooth hmm filtering: ask about the state of the process in the end smoothing: ask about a state in the middl of the process
logist regress algorithm iter least squar de(w)= phi'(y-t) h = phi'rphi r = diagon with rn,n =yn(1-yn) w = w - h^-1de(w)
what is the expect maxim algorithm and what is it goal given x observ variabl and z hidden variabl with joint distribut p(x,z theta). the goal is to maximis the likelihood function p(x theta) = sum_z ( p(x,z theta) 1. init theta 2. calcul prob och each possibl z given theta 3. use calcul valu of z to better estim theta repeat until converg
classifi new instanc candid elimin check that it is posit for everi h in s check that it is negat for everi h in g
expect maxim mean init h = (u1,...un) random e: e[zi] = exp(-(1/2s^2)(xi-uj)^2) / sum exp(-(1/2s^2)(xi-un)^2) m: uj = sum e[zij]xi / sum e[zij] replac h with h'
general em problem x, unob z, parametris p(i h) where yi = xi u zi and h paramet goal: find h that maximis e[ln p(i h)
general em algo defin likelihood function q(h',h) which calcul y = x u z use x and paramet h to estim z q(h',h) = e[ ln p(i h) h, x] e: calc q(h h) use h,x to estim prob dist over y q(h h) m: replac h by h' that maximis q h - argmax q(h h)
what is regress analysi we are given a number of predictor (explanatory) variabl and a continu respons variabl (outcome), and we tri to find a relationship between those variabl that allow us to predict an outcome.
what is reinforc learn the goal is to develop a system (agent) that improv it per anc base on interact with the environment. sinc the in ation about the current state of the environ typic also includ a so-cal reward signal, we can think of reinforc learn as a field relat to supervis learning. however, in reinforc learn this feedback is not the correct ground truth label or value, but a measur of how well the action was measur by a reward function. through the interact with the environment, an agent can then use reinforc learn to learn a seri of action that maxim this reward via an exploratori trial-and-error approach or delib planning. a popular exampl of reinforc learn is a chess engine.
describ the relationship between all type of machin learn and particular the applic of unsupervised. in supervis learning, we know the right answer beforehand when we train our model, and in reinforc learning, we defin a measur of reward for particular action by the agent. in unsupervis learning, however, we are deal with unlabel data or data of unknown structure. use unsupervis learn techniques, we are abl to explor the structur of our data to extract meaning in ation without the guidanc of a known outcom variabl or reward function.
what is a typic workflow diagram for use machin learn in predict model 
what popular idea did abraham maslow use in 1966 that relat to classifi selections. "i suppos it is tempting, if the onli tool you have is a hammer, to treat everyth as if it were a nail" each classif algorithm has it inher biases, and no singl classif model enjoy superior if we don't make ani assumpt about the task. in practice, it is therefor essenti to compar at least a hand of differ algorithm in order to train and select the best per ing model.
when was the first perceptron propos and by whom 1957 - frank rosenblatt. a binari classif task where we refer to our two class as 1 (posit class) and -1 (negat class) for simplicity. we can then defin an ation function that take a linear combin of certain input valu and a correspond weight vector.
what is the perceptron updat rule 
what is one-vs-al one-vs.-al (ova), or sometim also call one-vs.-rest (ovr), is a technique, us to extend a binari classifi to multi-class problems. use ova, we can train one classifi per class, where the particular class is treat as the posit class and the sampl from all other class are consid as the negat class. if we were to classifi a new data sample, we would use our train a perceptron model on the iri dataset classifiers, where train a perceptron model on the iri dataset is the number of class labels, and assign the class label with the highest confid to the particular sampl
describ the adapt linear neuron (adaline) the key differ between the adalin rule and rosenblatt perceptron is that the weight are updat base on a linear ation function rather than a unit step function like in the perceptron. in the case of adaline, we can defin the cost function minim cost function with gradient descent to learn the weight as the sum of squar error (sse) between the calcul outcom and the true class label
what is one of the key ingredi of supervis machin learn algorithm to defin an object function that is to be optim dure the learn process. this object function is often a cost function that we want to minimize.
describ the featur scale method call standard give our data the properti of a standard normal distribution. the mean of each featur is center at valu 0 and the featur column has a standard deviat of 1. for example, to standard the jth feature, we simpli need to subtract the sampl mean mu_j from everi train sampl and divid it by it standard deviat std_j.
what is stochast gradient descent sometim also call iter or on-lin gradient descent. instead of updat the weight base on the sum of the accumul error over all samples, we updat the weight increment for each train sample: although stochast gradient descent can be consid as an approxim of gradient descent, it typic reach converg much faster becaus of the more frequent weight updates. sinc each gradient is calcul base on a singl train example, the error surfac is noisier than in gradient descent, which can also have the advantag that stochast gradient descent can escap shallow local minima more readily. to obtain accur s via stochast gradient descent, it is import to present it with data in a random order, which is whi we want to shuffl the train set for everi epoch to prevent cycles. note: in stochast gradient descent implementations, the fix learn rate larg scale machin learn and stochast gradient descent is often replac by an adapt learn rate that decreas over time
what a compromis between batch and stochast gradient descent and what are it benefit a compromis between batch gradient descent and stochast gradient descent is the so-cal mini-batch learning. mini-batch learn can be understood as appli batch gradient descent to smaller subset of the train data—for example, 50 sampl at a time. the advantag over batch gradient descent is that converg is reach faster via mini-batch becaus of the more frequent weight updates. furthermore, mini-batch learn allow us to replac the for-loop over the train sampl in stochast gradient descent (sgd) by vector operations, which can further improv the comput effici of our learn algorithm.
what is the odd ratio p / (1 - p)
what is the logit function logit (p) = log ( (p/(1-p)) ) - the log of the odd ratio. it take input valu in the rang 0 to 1 and tran s them to valu over the entir real number rang
what is the output of the sigmoid function in logist regress output of the sigmoid function is interpret as the probabl of particular sampl belong to class 1 given it featur x parameter by the weight w.
what is the sum squar error function 
how do you deriv the log likelihood cost function for logist regress 
how do you deriv the cost function from the log likelihood 
what is the updat rule for logist regress 
describ varianc and bias in what they measure. varianc measur the consist (or variability) of the model predict for a particular sampl instanc if we would retrain the model multipl times, for example, on differ subset of the train dataset. we can say that the model is sensit to the random in the train data. in contrast, bias measur how far off the predict are from the correct valu in general if we rebuild the model multipl time on differ train datasets; bias is the measur of the systemat error that is not due to randomness.
describ the benefit of regularization. one way of find a good bias-vari tradeoff is to tune the complex of the model via regularization. regular is a veri use method to handl collinear (high correl among features), filter out nois from data, and eventu prevent overfitting. the concept behind regular is to introduc addit in ation (bias) to penal extrem paramet weights.
what is the most common of regular the so-cal l2 regular (sometim also call l2 shrinkag or weight decay), which can be written as follows:
what anoth reason featur scale is so import for regular to work properly, we need to ensur that all our featur are on compar scales.
describ a support vector machin (svm). it can be consid as an extens of the perceptron. use the perceptron algorithm, we minim misclassif errors. however, in svms, our optim object is to maxim the margin. the margin is defin as the distanc between the separ hyperplan (decis boundary) and the train sampl that are closest to this hyperplane, which are the so-cal support vectors. the rational behind have decis boundari with larg margin is that they tend to have a lower general error wherea model with small margin are more prone to overfitting.
what is the slack variabl linear constraint need to be relax for nonlinear separ data to allow converg of the optim in the presenc of misclassif under the appropri cost penalization.
describ logist regress vs svm. in practic classif tasks, linear logist regress and linear svms often yield veri similar s. logist regress tri to maxim the condit likelihood of the train data, which make it more prone to outlier than svms. the svms most care about the point that are closest to the decis boundari (support vectors). on the other hand, logist regress has the advantag that it is a simpler model that can be implement more easily. furthermore, logist regress model can be easili updated, which is attr e when work with stream data.
what the basic idea behind kernel method to deal with such linear insepar data is to creat nonlinear combin of the origin featur to project them onto a higher dimension space via a map function where it becom linear separable.
what the motiv for the kernel trick to solv a nonlinear problem use an svm, we tran the train data onto a higher dimension featur space via a map function and train a linear svm model to classifi the data in this new featur space. then we can use the same map function. to tran new, unseen data to classifi it use the linear svm model. however, one problem with this map approach is that the construct of the new featur is comput veri expensive, especi if we are deal with high-dimension data. this is where the so-cal kernel trick come into play
what the kernel trick in order to save the expens step of calcul a dot product between two point explicit use this map to a higher dimens function, we defin a so-cal kernel function:
rough speaking, what can you view the kernel function as a similar function between a pair of samples.
decis tree classifi are attr e model if we care about ... interpret
give an overview of the decis tree process. we start at the tree root and split the data on the featur that s in the largest in ation gain (ig). in an iter process, we can then repeat this split procedur at each child node until the leav are pure. this mean that the sampl at each node all belong to the same class.
describ maxim in ation gain and it function 
what is reinforc learn the goal is to develop a system (agent) that improv it per anc base on interact with the environment. sinc the in ation about the current state of the environ typic also includ a so-cal reward signal, we can think of reinforc learn as a field relat to supervis learning. however, in reinforc learn this feedback is not the correct ground truth label or value, but a measur of how well the action was measur by a reward function. through the interact with the environment, an agent can then use reinforc learn to learn a seri of action that maxim this reward via an exploratori trial-and-error approach or delib planning. a popular exampl of reinforc learn is a chess engine.
describ the relationship between all type of machin learn and particular the applic of unsupervised. in supervis learning, we know the right answer beforehand when we train our model, and in reinforc learning, we defin a measur of reward for particular action by the agent. in unsupervis learning, however, we are deal with unlabel data or data of unknown structure. use unsupervis learn techniques, we are abl to explor the structur of our data to extract meaning in ation without the guidanc of a known outcom variabl or reward function.
what is a typic workflow diagram for use machin learn in predict model 
what popular idea did abraham maslow use in 1966 that relat to classifi selections. "i suppos it is tempting, if the onli tool you have is a hammer, to treat everyth as if it were a nail" each classif algorithm has it inher biases, and no singl classif model enjoy superior if we don't make ani assumpt about the task. in practice, it is therefor essenti to compar at least a hand of differ algorithm in order to train and select the best per ing model.
when was the first perceptron propos and by whom 1957 - frank rosenblatt. a binari classif task where we refer to our two class as 1 (posit class) and -1 (negat class) for simplicity. we can then defin an ation function that take a linear combin of certain input valu and a correspond weight vector.
what is the perceptron updat rule 
what is one-vs-al one-vs.-al (ova), or sometim also call one-vs.-rest (ovr), is a technique, us to extend a binari classifi to multi-class problems. use ova, we can train one classifi per class, where the particular class is treat as the posit class and the sampl from all other class are consid as the negat class. if we were to classifi a new data sample, we would use our train a perceptron model on the iri dataset classifiers, where train a perceptron model on the iri dataset is the number of class labels, and assign the class label with the highest confid to the particular sampl
describ the adapt linear neuron (adaline) the key differ between the adalin rule and rosenblatt perceptron is that the weight are updat base on a linear ation function rather than a unit step function like in the perceptron. in the case of adaline, we can defin the cost function minim cost function with gradient descent to learn the weight as the sum of squar error (sse) between the calcul outcom and the true class label
what is one of the key ingredi of supervis machin learn algorithm to defin an object function that is to be optim dure the learn process. this object function is often a cost function that we want to minimize.
describ the featur scale method call standard give our data the properti of a standard normal distribution. the mean of each featur is center at valu 0 and the featur column has a standard deviat of 1. for example, to standard the jth feature, we simpli need to subtract the sampl mean mu_j from everi train sampl and divid it by it standard deviat std_j.
what is stochast gradient descent sometim also call iter or on-lin gradient descent. instead of updat the weight base on the sum of the accumul error over all samples, we updat the weight increment for each train sample: although stochast gradient descent can be consid as an approxim of gradient descent, it typic reach converg much faster becaus of the more frequent weight updates. sinc each gradient is calcul base on a singl train example, the error surfac is noisier than in gradient descent, which can also have the advantag that stochast gradient descent can escap shallow local minima more readily. to obtain accur s via stochast gradient descent, it is import to present it with data in a random order, which is whi we want to shuffl the train set for everi epoch to prevent cycles. note: in stochast gradient descent implementations, the fix learn rate larg scale machin learn and stochast gradient descent is often replac by an adapt learn rate that decreas over time
what a compromis between batch and stochast gradient descent and what are it benefit a compromis between batch gradient descent and stochast gradient descent is the so-cal mini-batch learning. mini-batch learn can be understood as appli batch gradient descent to smaller subset of the train data—for example, 50 sampl at a time. the advantag over batch gradient descent is that converg is reach faster via mini-batch becaus of the more frequent weight updates. furthermore, mini-batch learn allow us to replac the for-loop over the train sampl in stochast gradient descent (sgd) by vector operations, which can further improv the comput effici of our learn algorithm.
what is the odd ratio p / (1 - p)
what is the logit function logit (p) = log ( (p/(1-p)) ) - the log of the odd ratio. it take input valu in the rang 0 to 1 and tran s them to valu over the entir real number rang
what is the output of the sigmoid function in logist regress output of the sigmoid function is interpret as the probabl of particular sampl belong to class 1 given it featur x parameter by the weight w.
what is the sum squar error function 
how do you deriv the log likelihood cost function for logist regress 
how do you deriv the cost function from the log likelihood 
what is the updat rule for logist regress 
describ varianc and bias in what they measure. varianc measur the consist (or variability) of the model predict for a particular sampl instanc if we would retrain the model multipl times, for example, on differ subset of the train dataset. we can say that the model is sensit to the random in the train data. in contrast, bias measur how far off the predict are from the correct valu in general if we rebuild the model multipl time on differ train datasets; bias is the measur of the systemat error that is not due to randomness.
describ the benefit of regularization. one way of find a good bias-vari tradeoff is to tune the complex of the model via regularization. regular is a veri use method to handl collinear (high correl among features), filter out nois from data, and eventu prevent overfitting. the concept behind regular is to introduc addit in ation (bias) to penal extrem paramet weights.
what is the most common of regular the so-cal l2 regular (sometim also call l2 shrinkag or weight decay), which can be written as follows:
what anoth reason featur scale is so import for regular to work properly, we need to ensur that all our featur are on compar scales.
describ a support vector machin (svm). it can be consid as an extens of the perceptron. use the perceptron algorithm, we minim misclassif errors. however, in svms, our optim object is to maxim the margin. the margin is defin as the distanc between the separ hyperplan (decis boundary) and the train sampl that are closest to this hyperplane, which are the so-cal support vectors. the rational behind have decis boundari with larg margin is that they tend to have a lower general error wherea model with small margin are more prone to overfitting.
what is the slack variabl linear constraint need to be relax for nonlinear separ data to allow converg of the optim in the presenc of misclassif under the appropri cost penalization.
describ logist regress vs svm. in practic classif tasks, linear logist regress and linear svms often yield veri similar s. logist regress tri to maxim the condit likelihood of the train data, which make it more prone to outlier than svms. the svms most care about the point that are closest to the decis boundari (support vectors). on the other hand, logist regress has the advantag that it is a simpler model that can be implement more easily. furthermore, logist regress model can be easili updated, which is attr e when work with stream data.
what the basic idea behind kernel method to deal with such linear insepar data is to creat nonlinear combin of the origin featur to project them onto a higher dimension space via a map function where it becom linear separable.
what the motiv for the kernel trick to solv a nonlinear problem use an svm, we tran the train data onto a higher dimension featur space via a map function and train a linear svm model to classifi the data in this new featur space. then we can use the same map function. to tran new, unseen data to classifi it use the linear svm model. however, one problem with this map approach is that the construct of the new featur is comput veri expensive, especi if we are deal with high-dimension data. this is where the so-cal kernel trick come into play
what the kernel trick in order to save the expens step of calcul a dot product between two point explicit use this map to a higher dimens function, we defin a so-cal kernel function:
rough speaking, what can you view the kernel function as a similar function between a pair of samples.
decis tree classifi are attr e model if we care about ... interpret
give an overview of the decis tree process. we start at the tree root and split the data on the featur that s in the largest in ation gain (ig). in an iter process, we can then repeat this split procedur at each child node until the leav are pure. this mean that the sampl at each node all belong to the same class.
describ maxim in ation gain and it function 
what are the three main impur measur or split criteria common use in binari decis trees. gini impurity, entropy, and classif error
give the definit of entropi for all non-empti classes. 
describ the intuit of the gini impur intuitively, the gini impur can be understood as a criterion to minim the probabl of misclassif
what should you focus on for impur measur in practic in practic both the gini impur and entropi typic yield veri similar s and it is often not worth spend much time on evalu tree use differ impur criteria rather than experi with differ prune cut-offs.
what the classif error impur i = 1 - max( p(i t) ), this is a use criterion for prune but not recommend for grow a decis tree, sinc it is less sensit to chang in the class probabl of the nodes.
what is the random forest algorithm 1) draw a random bootstrap sampl of size n (random choos n sampl from the train set with replacement). 2) grow a decis tree from the bootstrap sample. at each node: - random select d featur without replacement. - split the node use the featur that provid the best split accord to the object function, for instance, by maxim the in ation gain. 3) repeat the step 1 to 2 k times. 4) aggreg the predict by each tree to assign the class label by major vote.
what are some reason default for the random forest algorithm paramet - the sampl size of the bootstrap sampl is chosen to be equal to the number of sampl in the origin train set, which usual provid a good bias-vari tradeoff. - for the number of featur d at each split, we want to choos a valu that is smaller than the total number of featur in the train set. a reason default that is use in scikit-learn and other implement is d = sqrt(m), where m is the number of featur in the train set.
whi is a lazi learner call lazi it is call lazi not becaus of it appar simplicity, but becaus it doesn't learn a discrimin function from the train data but memor the train dataset instead.
describ parametr vs nonparametr model machin learn algorithm can be group into parametr and nonparametr models. use parametr models, we estim paramet from the train dataset to learn a function that can classifi new data point without requir the origin train dataset anymore. typic exampl of parametr model are the perceptron, logist regression, and the linear svm. in contrast, nonparametr model can't be character by a fix set of parameters, and the number of paramet grow with the train data. two exampl of nonparametr model that we have seen so far are the decis tree classifier/random forest and the kernel svm.
describ the knn algorithm. - choos the number of k and a distanc metric. - find the k nearest neighbor of the sampl that we want to classify. - assign the class label by major vote.
what do you do instead of regular in model where it can't be use in model where regular is not applic such as decis tree and knn, we can use featur select and dimension reduct techniqu to help us avoid the curs of dimensionality.
what a common way to deal with miss numer valu use the mean of that featur or drop the data if it isn't veri much.
what the differ between nomin and ordin categor featur ordin featur can be understood as categor valu that can be sort or ordered. in contrast, nomin featur don't impli ani order. ordin - size of t-shirt, nomin - color of t-shirt
what is one-hot encod the idea behind this approach is to creat a new dummi featur for each uniqu valu in the nomin featur column.
the smaller the test set ... the more inaccur the estim of the general error.
most often, normal refer to... the rescal of the featur to a rang of [0, 1], which is a special case of min-max scaling. (x - x_max) / (x_max - x_min)
whi is standard often more practic for mani machin learn algorithm the reason is that mani linear models, such as the logist regress and svm, initi the weight to 0 or small random valu close to 0. use standardization, we center the featur column at mean 0 with standard deviat 1 so that the featur column take the of a normal distribution, which make it easier to learn the weights. furthermore, standard maintain use in ation about outlier and make the algorithm less sensit to them in contrast to min-max scaling, which scale the data to a limit rang of values.
what is a reason for overfit and way to address it: a reason for overfit is that our model is too complex for the given train data and common solut to reduc the general error are list as follows: - collect more train data - introduc a penalti for complex via regular - choos a simpler model with fewer paramet - reduc the dimension of the data
what techniqu of regular can be use for featur select l1 regular yield spars featur vectors; most featur weight will be zero. sparsiti can be use in practic if we have a high-dimension dataset with mani featur that are irrelevant, especi case where we have more irrelev dimens than samples. in this sense, l1 regular can be understood as a techniqu for featur selection.
what an altern way to reduc the complex of the model and avoid overfit than regular dimension reduct via featur selection.
what are the two main categori of dimension reduct featur select and featur extraction. use featur selection, we select a subset of the origin features. in featur extraction, we deriv in ation from the featur set to construct a new featur subspace.
what are sequenti featur select algorithm a famili of greedi search algorithm that are use to reduc an initi d-dimension featur space to a k-dimension featur subspac where k d.
describ the sequenti backward select (sbs) algorithm. sbs sequenti remov featur from the full featur subset until the new featur subspac contain the desir number of features. in order to determin which featur is to be remov at each stage, we need to defin criterion function sequenti featur select algorithm that we want to minimize. the criterion calcul by the criterion function can simpli be the differ in per anc of the classifi after and befor the remov of a particular feature. then the featur to be remov at each stage can simpli be defin as the featur that maxim this criterion; or, in more intuit terms, at each stage we elimin the featur that caus the least per anc loss after remov
how can we use random forest to per featur select use a random forest, we can measur featur import as the averag impur decreas comput from all decis tree in the forest without make ani assumpt whether our data is linear separ or not.
what an import gotcha concern random forest featur select if two or more featur are high correlated, one featur may be rank veri high while the in ation of the other feature(s) may not be fulli captured.
what is featur extract a method to tran or project the data onto a new featur space. in the context of dimension reduction, featur extract can be understood as an approach to data compress with the goal of maintain most of the relev in ation.
describ pca in a nutshell. it aim to find the direct of maximum varianc in high-dimension data and project it onto a new subspac with equal or fewer dimens that the origin one. the orthogon axe (princip components) of the new subspac can be interpret as the direct of maximum varianc given the constraint that the new featur axe are orthogon to each other
describ the pca algorithm. 
what repres the princip compon the eigenvector of the covari matrix (the direct of maximum variance), wherea the correspond eigenvalu will defin their magnitude.
what is the varianc explain of an eigenvalu lambda_j / sum ( lambda_i) over all i in set d
what doe lda stand for linear discrimin analysi
what the general concept behind lda the general concept behind lda is veri similar to pca, wherea pca attempt to find the orthogon compon axe of maximum varianc in a dataset; the goal in lda is to find the featur subspac that optim class separability. it is a supervis algorithm wherea pca is unsupervised.
summar the key step to the lda algorithm. 
what are the assumpt for lda the assumpt that we make when we are use lda are that the featur are normal distribut and independ of each other. also, the lda algorithm assum that the covari matric for the individu class are identical.
what a mean vector each mean vector store the mean featur valu with respect to the sampl of class i.
how do you comput the within-scatt matrix first note: we want to scale the individu scatter matric befor we sum them up as scatter matrix. note the covari matrix is a normal version of the within-scatt matrix. (1/n)*sw
what is the between class scatter matrix 
what is kernel pca we per a nonlinear map that tran s the data onto a higher-dimension space and use standard pca in this higher-dimension space to project the data back onto a lower-dimension space where the sampl can be separ by a linear classifi (under the condit that the sampl can be separ by densiti in the input space).
how can you obtain reliabl estim of the model general error use cross-valid techniqu such as holdout cross-valid and k-fold cross-validation.
what is model select process of tune and compar differ paramet set to further improv the per anc for make predict on unseen data.
what is k-fold cross-valid we random split the train dataset into k fold without replacement, where k-1 fold are use for the model train and one fold is use for testing. this procedur is repeat k time so that we obtain k model and per anc estimates.
what is the standard valu for k in k-fold cross-valid 10, unless work with small amount of data. in that case, increas k.
what is stratifi k-fold cross-valid the class proport are preserv in each fold to ensur that each fold is repres of the class proport in the train dataset
what is a benefit of learn curv to diagnos if a learn algorithm has a problem with overfit (high variance) or underfit (high bias).
what are the learn curv plot for high bias, high variance, and a good bias-vari trade-off 
what are valid curv use for and what are they valid curv are a use tool for improv the per anc of a model by address issu such as overfit or underfitting. valid curv are relat to learn curves, but instead of plot the train and test accuraci as function of the sampl size, we vari the valu of the model paramet
what would a valid curv look like for a paramet c 
what is nest cross-valid we have an outer k-fold cross-valid loop to split the data into train and test folds, and an inner loop is use to select the model use k-fold cross-valid on the train fold. after model selection, the test fold is then use to evalu the model per ance.
what is precis tp / (tp + fp)
what is recal tp / (tp + fn)
what is f1-score 2 * (precis * recall) / (precis + recall)
what are receiv oper characterist (roc) graph use for select model for classif base on their per anc with respect to the fals posit and true posit rates, which are comput by shift the decis threshold of the classifier. the diagon of an roc graph can be interpret as random guessing, and classif model that fall below the diagon are consid as wors than random guessing. a perfect classifi would fall into the top-left corner of the graph with a true posit rate of 1 and a fals posit rate of 0. base on the roc curve, we can then comput the so-cal area under the curv (auc) to character the per anc of a classif model.
describ micro and macro averag methods. 
what the goal behind ensembl method the goal behind ensembl method is to combin differ classifi into a meta-classifi that has a better general per anc than each individu classifi alon
what is major or plural vote just select the class with the most vote (mode) from the learner for the classification. can use a weight approach as well as probabilistic.
whi do ensembl method work better than individu classifi alon 
what is bag in the context of ensembl instead of use the same train set to fit the individu classifi in the ensemble, we draw bootstrap sampl (random sampl with replacement) from the initi train set, which is whi bag is also known as bootstrap aggregating.
how doe bag affect bias and varianc the bag algorithm can be an effect approach to reduc the varianc of a model. however, bag is ineffect in reduc model bias, which is whi we want to choos an ensembl of classifi with low bias, for example, unprun decis trees.
describ boosting. in boosting, the ensembl consist of veri simpl base classifiers, also often refer to as weak learners, that have onli a slight per anc advantag over random guessing. a typic exampl of a weak learner would be a decis tree stump. the key concept behind boost is to focus on train sampl that are hard to classify, that is, to let the weak learner subsequ learn from misclassifi train sampl to improv the per anc of the ensemble. in contrast to bagging, the initi ulat of boosting, the algorithm use random subset of train sampl drawn from the train dataset without replacement.
what are the four step to the origin boost algorithm 
describ the basic differ in boost with adaboost in contrast to the origin boost procedur as describ here, adaboost use the complet train set to train the weak learner where the train sampl are reweight in each iter to build a strong classifi that learn from the mistak of the previous weak learner in the ensemble.
what are the 9 step to adaboost 
what is sentiment analysi sentiment analysis, sometim also call opinion mining, is a popular sub-disciplin of the broader field of nlp; it analyz the polar of documents. a popular task in sentiment analysi is the classif of document base on the express opinion or emot of the author with regard to a particular topic.
what is the bag of word model 1) we creat a vocabulari of uniqu tokens—for example, words—from the entir set of documents. 2) we construct a featur vector from each document that contain the count of how often each word occur in the particular document.
describ tf-idf. 
what is l2-normal return a vector of length 1 by divid the un-norm featur vector v by it l2-norm
what is word stem the process of tran ing a word into it root that allow us to map relat word to the same stem
what is exploratori data analysi (eda) is an import and recommend first step prior to the train of a machin learn model. for example, it may help us to visual detect the presenc of outliers, the distribut of the data, and the relationship between features.
what are some eda method to use 1) creat a scatterplot matrix that allow us to visual the pair-wis correl between the differ featur in this dataset in one place. 2) creat a correl matrix and mayb creat a heat map plot the correl
what is a correl matrix intuitively, we can interpret the correl matrix as a rescal version of the covari matrix. in fact, the correl matrix is ident to a covari matrix comput from standard data. the correl matrix is a squar matrix that contain the pearson product-mo correl coeffici (often abbrevi as pearson r), which measur the linear depend between pair of features. the correl coeffici are bound to the rang -1 and 1.
what is a pearson correl coeffici pearson correl coeffici can simpli be calcul as the covari between two featur visual the import characterist of a dataset and visual the import characterist of a dataset (numerator) divid by the product of their standard deviat (denominator):
what is ol ordinari least squar (ols) method is to estim the paramet of the regress line that minim the sum of the squar vertic distanc (residu or errors) to the sampl points.
describ the random sampl consensus (ransac) algorithm. fit a regress model to a subset of the data, the so-cal inliers.
we can summar the iter ransac algorithm as follows: 1) select a random number of sampl to be inlier and fit the model. 2) test all other data point against the fit model and add those point that fall within a user-given toler to the inliers. 3) refit the model use all inliers. 4) estim the error of the fit model versus the inliers. 5) termin the algorithm if the per anc meet a certain user-defin threshold or if a fix number of iter has been reached; go back to step 1 otherwise.
what are residu plot sinc our model use multipl explanatori variables, we can't visual the linear regress line (or hyperplan to be precise) in a two-dimension plot, but we can plot the residu (the differ or vertic distanc between the actual and predict values) versus the predict valu to diagnos our regress model. those residu plot are a common use graphic analysi for diagnos regress model to detect nonlinear and outliers, and to check if the error are random distributed.
for a good regress model, what would you expect in the residu plot we would expect that the error are random distribut and the residu should be random scatter around the centerline. if we see pattern in a residu plot, it mean that our model is unabl to captur some explanatori in ation, which is leak into the residu
what is the mean squar error simpli the averag valu of the sse cost function.
what is the coeffici of determination, r^2 a standard version of the mse. in other words, r^2 is the fraction of respons varianc that is captur by the model. the r^2 valu is defin as follows: r^2 = 1 - (sse / sst), where sst = sum(i - mu_y)^2, or in other words, it is simpli the varianc of the response.
put r^2 in term of the mse. r^2 = 1 - mse / var(y)
what are the three most popular approach to linear regress regular ridg regression, least absolut shrinkag and select oper (lasso) and elast net method. 1) ridg regress is an l2 penal model where we simpli add the squar sum of the weight to our least-squar cost function 2) lasso - can lead to spars matrix, is l1 penal model but... a limit of the lasso is that it select at most n variabl if m n 3) elast net - compromis between the two. a l1 penalti to generat sparsiti and a l2 penalti to overcom some of the limit of the lasso, such as the number of select variables.
what is the elast net regular cost function 
give an exampl of when polynomi featur are not alway the best choic for model nonlinear relationships. could propos that a log tran ation of a featur variabl or a squar root may project the data onto a linear featur space suitabl for a linear regress fit.
what other techniqu could you use to deal with nonlinear relationship other than polynomi or featur tran ation a random forest, which is an ensembl of multipl decis trees, can be understood as the sum of piecewis linear function in contrast to the global linear and polynomi regress models. via the decis tree algorithm, we are subdivid the input space into smaller region that becom more manageable.
what the onli differ between random forest classif and regress the onli differ is that we use the mse criterion to grow the individu decis trees, and the predict target variabl is calcul as the averag predict over all decis trees.
what is prototype-bas cluster it mean that each cluster is repres by a prototype, which can either be the centroid (average) of similar point with continu features, or the medoid (the most repres or most frequent occur point) in the case of categor features.
what are the 4 step to the k-mean algorithm 1) random pick k centroid from the sampl point as initi cluster centers. 2) assign each sampl to the nearest centroid 3) move the centroid to the center of the sampl that were assign to it. 4) repeat the step 2 and 3 until the cluster assign do not chang or a user-defin toler or a maximum number of iter is reached. note: when we are appli k-mean to real-world data use a euclidean distanc metric, we want to make sure that the featur are measur on the same scale and appli z-score standard or min-max scale if necessary.
how do we measur similar between object we can defin similar as the opposit of distance, and a common use distanc for cluster sampl with continu featur is the squar euclidean distanc between two point x and y in m-dimension space:
describ the k-mean algorithm as a simpl optim problem. an iter approach for minim the within-clust sum of squar error (sse), which is sometim also call cluster inertia.
what is the k-means++ algorithm in concept, k-mean but place the initi centroid far away from each other.
describ hard vs soft (fuzzy) clusterning. hard cluster describ a famili of algorithm where each sampl in a dataset is assign to exact one cluster, as in the k-mean algorithm that we discuss in the previous subsection. in contrast, algorithm for soft cluster (sometim also call fuzzi clustering) assign a sampl to one or more clusters.
what the differ between k-mean and fuzzi c-mean the fcm procedur is veri similar to k-means, however, we replac the hard cluster assign by probabl for each point belong to each cluster.
what is the elbow method a graphic techniqu to estim the optim number of cluster k for a given task. intuitively, we can say that, if k increases, the distort (within-clust sse) will decrease. this is becaus the sampl will be closer to the centroid they are assign to. the idea behind the elbow method is to identifi the valu of k where the distort begin to increas most rapidly,
what can silhouett analysi be use for silhouett analysi can be use as a graphic tool to plot a measur of how tight group the sampl in the cluster are.
how do you calcul the silhouett coeffici and how do you interpret it 
the two main approach to hierarch cluster are... agglom and divis hierarch cluster
what is divis hierarch cluster from a bird eye view in divis hierarch clustering, we start with one cluster that encompass all our samples, and we iter split the cluster into smaller cluster until each cluster onli contain one sample.
what is agglom hierarch cluster from a bird eye view it take the opposit approach. we start with each sampl as an individu cluster and merg the closest pair of cluster until onli one cluster remains.
what are the two standard algorithm for agglom hierarch cluster singl linkag and complet linkage. use singl linkage, we comput the distanc between the most similar member for each pair of cluster and merg the two cluster for which the distanc between the most similar member is the smallest. the complet linkag approach is similar to singl linkag but, instead of compar the most similar member in each pair of clusters, we compar the most dissimilar member to per the merge.
what is density-bas spatial cluster of applic with nois (dbscan) 
what is deep learn it can be understood as a set of algorithm that were develop to train artifici neural network with mani layer most efficiently.
summar the mlp (multi-lay perceptron) learn procedur in three simpl step and the evalu of the model. 1) start at the input layer, we forward propag the pattern of the train data through the network to generat an output. 2) base on the network output, we calcul the error that we want to minim use a cost function that we will describ later. 3) we backpropag the error, find it deriv with respect to each weight in the network, and updat the model. finally, after repeat the step for multipl epoch and learn the weight of the mlp, we use forward propag to calcul the network output and appli a threshold function to obtain the predict class label in the one-hot representation,
what doe the feedforward in feedforward artifici neural network mean feedforward refer to the fact that each layer serv as the input to the next layer without loops, in contrast to recurr neural network for example.
what is some intuit for the backpropag algorithm in essence, backpropag is just a veri comput effici approach to comput the deriv of a complex cost function. our goal is to use those deriv to learn the weight coeffici for parameter a multi-lay artifici neural network.
what is gradient check it is essenti a comparison between our analyt gradient in the network and numer gradients, where a numer approxim gradient =( j(w + epsilon) - j(w) ) / epsilon, for example.
what the key idea behind convolut neural network (cnns or convnets) the key idea behind convolut neural network is to build mani layer of featur detector to take the spatial arrang of pixel in an input imag into account. they have extraordinari good per anc on imag classif tasks. in cnns, we use recept field to connect the input layer to a featur map. these recept field can be understood as overlap window that we slide over the pixel of an input imag to creat a featur map. the stride length of the window slide as well as the window size are addit hyperparamet of the model that we need to defin a priori. the process of creat the featur map is also call convolution.
what is a pool layer and how doe it tie into cnns in cnns, a convolut layer is follow by a pool layer (sometim also call sub-sampling). in pooling, we summar neighbor featur detector to reduc the number of featur for the next layer. pool can be understood as a simpl method of featur extract where we take the averag or maximum valu of a patch of neighbor featur and pass it on to the next layer. to creat a deep convolut neural network, we stack multipl layers—altern between convolut and pool layers—befor we connect it to a multi-lay perceptron for classification.
what are recurr neural network recurr neural network (rnns) can be thought of as feedforward neural network with feedback loop or backpropag through time. in rnns, the neuron onli fire for a limit amount of time befor they are (temporarily) de ated. in turn, these neuron ate other neuron that fire at a later point in time. basically, we can think of recurr neural network as mlps with an addit time variable. the time compon and dynam structur allow the network to use not onli the current input but also the input that it encount earlier.
what is the softmax function it is a general of the logist function that allow us to comput meaning class-prob in multi-class set (multinomi logist regression). it may help to think of the softmax function as a normal logist function that is use to obtain meaning class-membership predict in multi-class settings.
what is the hyperbol tangent (tanh) ation function it can be interpret as a rescal version of the logist function. (e^z - e^-z) / (e^z + e^-z). the advantag of the hyperbol tangent over the logist function is that it has a broader output spectrum and rang the open interv (-1, 1), which can improv the converg of the back propag algorithm
what are the three main impur measur or split criteria common use in binari decis trees. gini impurity, entropy, and classif error
give the definit of entropi for all non-empti classes. 
describ the intuit of the gini impur intuitively, the gini impur can be understood as a criterion to minim the probabl of misclassif
what should you focus on for impur measur in practic in practic both the gini impur and entropi typic yield veri similar s and it is often not worth spend much time on evalu tree use differ impur criteria rather than experi with differ prune cut-offs.
what the classif error impur i = 1 - max( p(i t) ), this is a use criterion for prune but not recommend for grow a decis tree, sinc it is less sensit to chang in the class probabl of the nodes.
what is the random forest algorithm 1) draw a random bootstrap sampl of size n (random choos n sampl from the train set with replacement). 2) grow a decis tree from the bootstrap sample. at each node: - random select d featur without replacement. - split the node use the featur that provid the best split accord to the object function, for instance, by maxim the in ation gain. 3) repeat the step 1 to 2 k times. 4) aggreg the predict by each tree to assign the class label by major vote.
what are some reason default for the random forest algorithm paramet - the sampl size of the bootstrap sampl is chosen to be equal to the number of sampl in the origin train set, which usual provid a good bias-vari tradeoff. - for the number of featur d at each split, we want to choos a valu that is smaller than the total number of featur in the train set. a reason default that is use in scikit-learn and other implement is d = sqrt(m), where m is the number of featur in the train set.
whi is a lazi learner call lazi it is call lazi not becaus of it appar simplicity, but becaus it doesn't learn a discrimin function from the train data but memor the train dataset instead.
describ parametr vs nonparametr model machin learn algorithm can be group into parametr and nonparametr models. use parametr models, we estim paramet from the train dataset to learn a function that can classifi new data point without requir the origin train dataset anymore. typic exampl of parametr model are the perceptron, logist regression, and the linear svm. in contrast, nonparametr model can't be character by a fix set of parameters, and the number of paramet grow with the train data. two exampl of nonparametr model that we have seen so far are the decis tree classifier/random forest and the kernel svm.
describ the knn algorithm. - choos the number of k and a distanc metric. - find the k nearest neighbor of the sampl that we want to classify. - assign the class label by major vote.
what do you do instead of regular in model where it can't be use in model where regular is not applic such as decis tree and knn, we can use featur select and dimension reduct techniqu to help us avoid the curs of dimensionality.
what a common way to deal with miss numer valu use the mean of that featur or drop the data if it isn't veri much.
what the differ between nomin and ordin categor featur ordin featur can be understood as categor valu that can be sort or ordered. in contrast, nomin featur don't impli ani order. ordin - size of t-shirt, nomin - color of t-shirt
what is one-hot encod the idea behind this approach is to creat a new dummi featur for each uniqu valu in the nomin featur column.
the smaller the test set ... the more inaccur the estim of the general error.
most often, normal refer to... the rescal of the featur to a rang of [0, 1], which is a special case of min-max scaling. (x - x_max) / (x_max - x_min)
whi is standard often more practic for mani machin learn algorithm the reason is that mani linear models, such as the logist regress and svm, initi the weight to 0 or small random valu close to 0. use standardization, we center the featur column at mean 0 with standard deviat 1 so that the featur column take the of a normal distribution, which make it easier to learn the weights. furthermore, standard maintain use in ation about outlier and make the algorithm less sensit to them in contrast to min-max scaling, which scale the data to a limit rang of values.
what is a reason for overfit and way to address it: a reason for overfit is that our model is too complex for the given train data and common solut to reduc the general error are list as follows: - collect more train data - introduc a penalti for complex via regular - choos a simpler model with fewer paramet - reduc the dimension of the data
what techniqu of regular can be use for featur select l1 regular yield spars featur vectors; most featur weight will be zero. sparsiti can be use in practic if we have a high-dimension dataset with mani featur that are irrelevant, especi case where we have more irrelev dimens than samples. in this sense, l1 regular can be understood as a techniqu for featur selection.
what an altern way to reduc the complex of the model and avoid overfit than regular dimension reduct via featur selection.
what are the two main categori of dimension reduct featur select and featur extraction. use featur selection, we select a subset of the origin features. in featur extraction, we deriv in ation from the featur set to construct a new featur subspace.
what are sequenti featur select algorithm a famili of greedi search algorithm that are use to reduc an initi d-dimension featur space to a k-dimension featur subspac where k d.
describ the sequenti backward select (sbs) algorithm. sbs sequenti remov featur from the full featur subset until the new featur subspac contain the desir number of features. in order to determin which featur is to be remov at each stage, we need to defin criterion function sequenti featur select algorithm that we want to minimize. the criterion calcul by the criterion function can simpli be the differ in per anc of the classifi after and befor the remov of a particular feature. then the featur to be remov at each stage can simpli be defin as the featur that maxim this criterion; or, in more intuit terms, at each stage we elimin the featur that caus the least per anc loss after remov
how can we use random forest to per featur select use a random forest, we can measur featur import as the averag impur decreas comput from all decis tree in the forest without make ani assumpt whether our data is linear separ or not.
what an import gotcha concern random forest featur select if two or more featur are high correlated, one featur may be rank veri high while the in ation of the other feature(s) may not be fulli captured.
what is featur extract a method to tran or project the data onto a new featur space. in the context of dimension reduction, featur extract can be understood as an approach to data compress with the goal of maintain most of the relev in ation.
describ pca in a nutshell. it aim to find the direct of maximum varianc in high-dimension data and project it onto a new subspac with equal or fewer dimens that the origin one. the orthogon axe (princip components) of the new subspac can be interpret as the direct of maximum varianc given the constraint that the new featur axe are orthogon to each other
describ the pca algorithm. 
what repres the princip compon the eigenvector of the covari matrix (the direct of maximum variance), wherea the correspond eigenvalu will defin their magnitude.
what is the varianc explain of an eigenvalu lambda_j / sum ( lambda_i) over all i in set d
what doe lda stand for linear discrimin analysi
what the general concept behind lda the general concept behind lda is veri similar to pca, wherea pca attempt to find the orthogon compon axe of maximum varianc in a dataset; the goal in lda is to find the featur subspac that optim class separability. it is a supervis algorithm wherea pca is unsupervised.
summar the key step to the lda algorithm. 
what are the assumpt for lda the assumpt that we make when we are use lda are that the featur are normal distribut and independ of each other. also, the lda algorithm assum that the covari matric for the individu class are identical.
what a mean vector each mean vector store the mean featur valu with respect to the sampl of class i.
how do you comput the within-scatt matrix first note: we want to scale the individu scatter matric befor we sum them up as scatter matrix. note the covari matrix is a normal version of the within-scatt matrix. (1/n)*sw
what is the between class scatter matrix 
what is kernel pca we per a nonlinear map that tran s the data onto a higher-dimension space and use standard pca in this higher-dimension space to project the data back onto a lower-dimension space where the sampl can be separ by a linear classifi (under the condit that the sampl can be separ by densiti in the input space).
how can you obtain reliabl estim of the model general error use cross-valid techniqu such as holdout cross-valid and k-fold cross-validation.
what is model select process of tune and compar differ paramet set to further improv the per anc for make predict on unseen data.
what is k-fold cross-valid we random split the train dataset into k fold without replacement, where k-1 fold are use for the model train and one fold is use for testing. this procedur is repeat k time so that we obtain k model and per anc estimates.
what is the standard valu for k in k-fold cross-valid 10, unless work with small amount of data. in that case, increas k.
what is stratifi k-fold cross-valid the class proport are preserv in each fold to ensur that each fold is repres of the class proport in the train dataset
what is a benefit of learn curv to diagnos if a learn algorithm has a problem with overfit (high variance) or underfit (high bias).
what are the learn curv plot for high bias, high variance, and a good bias-vari trade-off 
what are valid curv use for and what are they valid curv are a use tool for improv the per anc of a model by address issu such as overfit or underfitting. valid curv are relat to learn curves, but instead of plot the train and test accuraci as function of the sampl size, we vari the valu of the model paramet
what would a valid curv look like for a paramet c 
what is nest cross-valid we have an outer k-fold cross-valid loop to split the data into train and test folds, and an inner loop is use to select the model use k-fold cross-valid on the train fold. after model selection, the test fold is then use to evalu the model per ance.
what is precis tp / (tp + fp)
what is recal tp / (tp + fn)
what is f1-score 2 * (precis * recall) / (precis + recall)
what are receiv oper characterist (roc) graph use for select model for classif base on their per anc with respect to the fals posit and true posit rates, which are comput by shift the decis threshold of the classifier. the diagon of an roc graph can be interpret as random guessing, and classif model that fall below the diagon are consid as wors than random guessing. a perfect classifi would fall into the top-left corner of the graph with a true posit rate of 1 and a fals posit rate of 0. base on the roc curve, we can then comput the so-cal area under the curv (auc) to character the per anc of a classif model.
describ micro and macro averag methods. 
what the goal behind ensembl method the goal behind ensembl method is to combin differ classifi into a meta-classifi that has a better general per anc than each individu classifi alon
what is major or plural vote just select the class with the most vote (mode) from the learner for the classification. can use a weight approach as well as probabilistic.
whi do ensembl method work better than individu classifi alon 
what is bag in the context of ensembl instead of use the same train set to fit the individu classifi in the ensemble, we draw bootstrap sampl (random sampl with replacement) from the initi train set, which is whi bag is also known as bootstrap aggregating.
how doe bag affect bias and varianc the bag algorithm can be an effect approach to reduc the varianc of a model. however, bag is ineffect in reduc model bias, which is whi we want to choos an ensembl of classifi with low bias, for example, unprun decis trees.
describ boosting. in boosting, the ensembl consist of veri simpl base classifiers, also often refer to as weak learners, that have onli a slight per anc advantag over random guessing. a typic exampl of a weak learner would be a decis tree stump. the key concept behind boost is to focus on train sampl that are hard to classify, that is, to let the weak learner subsequ learn from misclassifi train sampl to improv the per anc of the ensemble. in contrast to bagging, the initi ulat of boosting, the algorithm use random subset of train sampl drawn from the train dataset without replacement.
what are the four step to the origin boost algorithm 
describ the basic differ in boost with adaboost in contrast to the origin boost procedur as describ here, adaboost use the complet train set to train the weak learner where the train sampl are reweight in each iter to build a strong classifi that learn from the mistak of the previous weak learner in the ensemble.
what are the 9 step to adaboost 
what is sentiment analysi sentiment analysis, sometim also call opinion mining, is a popular sub-disciplin of the broader field of nlp; it analyz the polar of documents. a popular task in sentiment analysi is the classif of document base on the express opinion or emot of the author with regard to a particular topic.
what is the bag of word model 1) we creat a vocabulari of uniqu tokens—for example, words—from the entir set of documents. 2) we construct a featur vector from each document that contain the count of how often each word occur in the particular document.
describ tf-idf. 
what is l2-normal return a vector of length 1 by divid the un-norm featur vector v by it l2-norm
what is word stem the process of tran ing a word into it root that allow us to map relat word to the same stem
what is exploratori data analysi (eda) is an import and recommend first step prior to the train of a machin learn model. for example, it may help us to visual detect the presenc of outliers, the distribut of the data, and the relationship between features.
what are some eda method to use 1) creat a scatterplot matrix that allow us to visual the pair-wis correl between the differ featur in this dataset in one place. 2) creat a correl matrix and mayb creat a heat map plot the correl
what is a correl matrix intuitively, we can interpret the correl matrix as a rescal version of the covari matrix. in fact, the correl matrix is ident to a covari matrix comput from standard data. the correl matrix is a squar matrix that contain the pearson product-mo correl coeffici (often abbrevi as pearson r), which measur the linear depend between pair of features. the correl coeffici are bound to the rang -1 and 1.
what is a pearson correl coeffici pearson correl coeffici can simpli be calcul as the covari between two featur visual the import characterist of a dataset and visual the import characterist of a dataset (numerator) divid by the product of their standard deviat (denominator):
what is ol ordinari least squar (ols) method is to estim the paramet of the regress line that minim the sum of the squar vertic distanc (residu or errors) to the sampl points.
describ the random sampl consensus (ransac) algorithm. fit a regress model to a subset of the data, the so-cal inliers.
we can summar the iter ransac algorithm as follows: 1) select a random number of sampl to be inlier and fit the model. 2) test all other data point against the fit model and add those point that fall within a user-given toler to the inliers. 3) refit the model use all inliers. 4) estim the error of the fit model versus the inliers. 5) termin the algorithm if the per anc meet a certain user-defin threshold or if a fix number of iter has been reached; go back to step 1 otherwise.
what are residu plot sinc our model use multipl explanatori variables, we can't visual the linear regress line (or hyperplan to be precise) in a two-dimension plot, but we can plot the residu (the differ or vertic distanc between the actual and predict values) versus the predict valu to diagnos our regress model. those residu plot are a common use graphic analysi for diagnos regress model to detect nonlinear and outliers, and to check if the error are random distributed.
for a good regress model, what would you expect in the residu plot we would expect that the error are random distribut and the residu should be random scatter around the centerline. if we see pattern in a residu plot, it mean that our model is unabl to captur some explanatori in ation, which is leak into the residu
what is the mean squar error simpli the averag valu of the sse cost function.
what is the coeffici of determination, r^2 a standard version of the mse. in other words, r^2 is the fraction of respons varianc that is captur by the model. the r^2 valu is defin as follows: r^2 = 1 - (sse / sst), where sst = sum(i - mu_y)^2, or in other words, it is simpli the varianc of the response.
put r^2 in term of the mse. r^2 = 1 - mse / var(y)
what are the three most popular approach to linear regress regular ridg regression, least absolut shrinkag and select oper (lasso) and elast net method. 1) ridg regress is an l2 penal model where we simpli add the squar sum of the weight to our least-squar cost function 2) lasso - can lead to spars matrix, is l1 penal model but... a limit of the lasso is that it select at most n variabl if m n 3) elast net - compromis between the two. a l1 penalti to generat sparsiti and a l2 penalti to overcom some of the limit of the lasso, such as the number of select variables.
what is the elast net regular cost function 
give an exampl of when polynomi featur are not alway the best choic for model nonlinear relationships. could propos that a log tran ation of a featur variabl or a squar root may project the data onto a linear featur space suitabl for a linear regress fit.
what other techniqu could you use to deal with nonlinear relationship other than polynomi or featur tran ation a random forest, which is an ensembl of multipl decis trees, can be understood as the sum of piecewis linear function in contrast to the global linear and polynomi regress models. via the decis tree algorithm, we are subdivid the input space into smaller region that becom more manageable.
what the onli differ between random forest classif and regress the onli differ is that we use the mse criterion to grow the individu decis trees, and the predict target variabl is calcul as the averag predict over all decis trees.
what is prototype-bas cluster it mean that each cluster is repres by a prototype, which can either be the centroid (average) of similar point with continu features, or the medoid (the most repres or most frequent occur point) in the case of categor features.
what are the 4 step to the k-mean algorithm 1) random pick k centroid from the sampl point as initi cluster centers. 2) assign each sampl to the nearest centroid 3) move the centroid to the center of the sampl that were assign to it. 4) repeat the step 2 and 3 until the cluster assign do not chang or a user-defin toler or a maximum number of iter is reached. note: when we are appli k-mean to real-world data use a euclidean distanc metric, we want to make sure that the featur are measur on the same scale and appli z-score standard or min-max scale if necessary.
how do we measur similar between object we can defin similar as the opposit of distance, and a common use distanc for cluster sampl with continu featur is the squar euclidean distanc between two point x and y in m-dimension space:
describ the k-mean algorithm as a simpl optim problem. an iter approach for minim the within-clust sum of squar error (sse), which is sometim also call cluster inertia.
what is the k-means++ algorithm in concept, k-mean but place the initi centroid far away from each other.
describ hard vs soft (fuzzy) clusterning. hard cluster describ a famili of algorithm where each sampl in a dataset is assign to exact one cluster, as in the k-mean algorithm that we discuss in the previous subsection. in contrast, algorithm for soft cluster (sometim also call fuzzi clustering) assign a sampl to one or more clusters.
what the differ between k-mean and fuzzi c-mean the fcm procedur is veri similar to k-means, however, we replac the hard cluster assign by probabl for each point belong to each cluster.
what is the elbow method a graphic techniqu to estim the optim number of cluster k for a given task. intuitively, we can say that, if k increases, the distort (within-clust sse) will decrease. this is becaus the sampl will be closer to the centroid they are assign to. the idea behind the elbow method is to identifi the valu of k where the distort begin to increas most rapidly,
what can silhouett analysi be use for silhouett analysi can be use as a graphic tool to plot a measur of how tight group the sampl in the cluster are.
how do you calcul the silhouett coeffici and how do you interpret it 
the two main approach to hierarch cluster are... agglom and divis hierarch cluster
what is divis hierarch cluster from a bird eye view in divis hierarch clustering, we start with one cluster that encompass all our samples, and we iter split the cluster into smaller cluster until each cluster onli contain one sample.
what is agglom hierarch cluster from a bird eye view it take the opposit approach. we start with each sampl as an individu cluster and merg the closest pair of cluster until onli one cluster remains.
what are the two standard algorithm for agglom hierarch cluster singl linkag and complet linkage. use singl linkage, we comput the distanc between the most similar member for each pair of cluster and merg the two cluster for which the distanc between the most similar member is the smallest. the complet linkag approach is similar to singl linkag but, instead of compar the most similar member in each pair of clusters, we compar the most dissimilar member to per the merge.
what is density-bas spatial cluster of applic with nois (dbscan) 
what is deep learn it can be understood as a set of algorithm that were develop to train artifici neural network with mani layer most efficiently.
summar the mlp (multi-lay perceptron) learn procedur in three simpl step and the evalu of the model. 1) start at the input layer, we forward propag the pattern of the train data through the network to generat an output. 2) base on the network output, we calcul the error that we want to minim use a cost function that we will describ later. 3) we backpropag the error, find it deriv with respect to each weight in the network, and updat the model. finally, after repeat the step for multipl epoch and learn the weight of the mlp, we use forward propag to calcul the network output and appli a threshold function to obtain the predict class label in the one-hot representation,
what doe the feedforward in feedforward artifici neural network mean feedforward refer to the fact that each layer serv as the input to the next layer without loops, in contrast to recurr neural network for example.
what is some intuit for the backpropag algorithm in essence, backpropag is just a veri comput effici approach to comput the deriv of a complex cost function. our goal is to use those deriv to learn the weight coeffici for parameter a multi-lay artifici neural network.
what is gradient check it is essenti a comparison between our analyt gradient in the network and numer gradients, where a numer approxim gradient =( j(w + epsilon) - j(w) ) / epsilon, for example.
what the key idea behind convolut neural network (cnns or convnets) the key idea behind convolut neural network is to build mani layer of featur detector to take the spatial arrang of pixel in an input imag into account. they have extraordinari good per anc on imag classif tasks. in cnns, we use recept field to connect the input layer to a featur map. these recept field can be understood as overlap window that we slide over the pixel of an input imag to creat a featur map. the stride length of the window slide as well as the window size are addit hyperparamet of the model that we need to defin a priori. the process of creat the featur map is also call convolution.
what is a pool layer and how doe it tie into cnns in cnns, a convolut layer is follow by a pool layer (sometim also call sub-sampling). in pooling, we summar neighbor featur detector to reduc the number of featur for the next layer. pool can be understood as a simpl method of featur extract where we take the averag or maximum valu of a patch of neighbor featur and pass it on to the next layer. to creat a deep convolut neural network, we stack multipl layers—altern between convolut and pool layers—befor we connect it to a multi-lay perceptron for classification.
what are recurr neural network recurr neural network (rnns) can be thought of as feedforward neural network with feedback loop or backpropag through time. in rnns, the neuron onli fire for a limit amount of time befor they are (temporarily) de ated. in turn, these neuron ate other neuron that fire at a later point in time. basically, we can think of recurr neural network as mlps with an addit time variable. the time compon and dynam structur allow the network to use not onli the current input but also the input that it encount earlier.
what is the softmax function it is a general of the logist function that allow us to comput meaning class-prob in multi-class set (multinomi logist regression). it may help to think of the softmax function as a normal logist function that is use to obtain meaning class-membership predict in multi-class settings.
what is the hyperbol tangent (tanh) ation function it can be interpret as a rescal version of the logist function. (e^z - e^-z) / (e^z + e^-z). the advantag of the hyperbol tangent over the logist function is that it has a broader output spectrum and rang the open interv (-1, 1), which can improv the converg of the back propag algorithm
chi-squar - chi-squar for good of fit - purpose: compar a one-sampl proport to a hypothes valu - variables: one categor with two or more categori - chi-squar for independ - purpose: compar frequenc or proport between two group - variables: two categor with two or more categori each
mcnemar test - purpose: compar one sampl over time (matched, repeat measures, pre-post test) - variables: two categor variabl measur the presenc or absenc (presence=1, absence=0) of the same characterist at time 1 and time 2
cochran q - purpose: compar one sampl over time (three or more measur periods) - variables: three categor variabl measur the presenc or absenc (presence=1, absence=0,) of a characterist at time 1, time 2, time 3, etc.
kappa agreement - purpose: measur the proport of agreement between two raters/test - variables: two categor variabl with an equal number of categori (rater 1 conclud diagnosi [+=1, - =0] and rater 2 conclud diagnosi [+=1, -=0]).
mann whitney u - purpose: test for differ between two independ group on a continu dv. use when dv is not normal distributed. use the median and convert score to ranks. - variables: one categor with two groups; one continu
wilcoxin sign rank test - purpose: test for differ between match pair (repeat measures). use when dv is not normal distributed. use the median and convert score to ranks. - variables: one time-1 continu and one time-2 continuous.
kruskal-walla test - purpose: test for differ between three or more independ group on a continu dv. use when dv is not normal distributed. use the median and convert score to ranks. - variables: one categor with three or more groups; one continuous.
friedman test - purpose: test for differ between three or more time period (or conditions) in one sample. use the median and convert score to ranks. - variables: one group measur at three or more time (or under three or more conditions).
descript statist simpli numer or graphic summari of data, and may includ charts, graphs, and simpl summari statist such as mean and standard deviat to describ characterist of a popul sample.
inferenti statist statist techniqu (e.g., chi-squar test, the t test, the one-way anova) that allow conclus to be drawn about the relationship found among differ variabl in a popul sample.
explanatori studi depend on inferenti question such as: "are women who are sedentari dure the third trimest of pregnanc more or less like to have a cesarean oper than women who exercis regular dure the third trimest "
predict and control studi seek to determin which variabl are predict of other variabl and to determin causal (e.g., one event caus anoth to happen). data for predict and control studi are typic collect use quasi-experiment or experiment studi design in which research introduc an intervent (e.g., chang one of the variabl be examined) as these type of studi are thought to have better validity, make causal infer more solid than with pure observ studi design
random control trial (rcts) are consid experiment design becaus studi particip are random assign to an intervent group or a control group and follow forward in time to determin if the intervent impact on a specif health outcome.
descript question *what is the level of intent to engag in physic iti among a group of adult who recent join a fit facil *what is the actual level of physic iti among a group of adult that recent join a fit facil
inferenti question *doe attitud toward exercis affect particip in physic iti *doe the extent to which particip perceiv themselv as abl to exercis (perceiv behavior control) affect particip in physic iti *do subject norm affect particip in physic iti *doe intent to exercis predict physic iti
research studi design is the art and scienc of conduct studi that maxim reliabl and valid of the find and minim bias or error.
observ studi design observ studi are those in which a phenomenon is simpli observ and no intervent is instituted.
quasi-experiment and experiment studi design quasi-experiment and experiment studi design differ from observ studi design in that the research is an e agent in the experiment work.
descript of the statist analysi first, the data must be enter into a databas on the computer. second, the data must be "cleaned." third, descript statist are use to describ the sampl in term of demograph characteristics. fourth, each hypothesi is list with the inferenti test that will be use to test it.
clean the data clean the data involv make certain that all of the variabl have valid and usabl values.
inferenti statist use to test each hypothesi the third step in data analysi is to list the inferenti statist that will be use to test the hypotheses. the hypotheses, includ the independ and depend variabl in each hypothesis, should be clear state
data analyt life cycle. the overal manag of the availability, usability, integrity, and secur of the data employ in an organ or enterpris
data governance. the overal manag of the availability, usability, integrity, and secur of the data employ in an organ or enterpris
data normalization. in a relat database, it is the process of organ data to minim redund
enterpris in ation manag (eim) (1) ensur the valu of in ation assets, requir an organization-wid perspect of in ation manag functions; it call for explicit structures, policies, processes, technology, and controls. (2) the infrastructur and process to ensur the in ation is trustworthi and action
in ation governance. the account framework and decis right to achiev enterpris in ation manag (eim). ig is the respons of execut leadership for develop and drive the ig strategi throughout the organization. ig encompass both data govern (dg) and in ation technolog govern (itg)
in ation manag (im) life cycle. illustr how in ation move from origin to archiv and/or deletion. the step are compris of design, acquire, process, use, and dispos
descript the averag age of the student in a statist class is 21 years.
inferenti the chanc of win the california lotteri are one chanc in twenty-two million.
inferenti there is a relationship between smoke cigarett and get emphysema
inferenti from past figures, it is predict that 39% of the regist voter in california will vote in the primary.
when is homogen of varianc is import it assur accur estim of the paramet that defin the model and signific tests.
a normal distribut take the shape of a bell. what doe this shape tell the research the mean, median, and mode are in the middl of the curve. 68% of the data in a normal distribut lie within +1 sd from the mean. it is a probabl distribut and demonstr the likelihood of get particular outcom when sampl from a population.
the central limit theorem surmis that a big enough sampl will produc a normal distribution. true or fals true
a sampl of size (n) is taken from a normal distribut population. what sampl size is need in order to have a normal distribut of sampl mean n = 5 n = 10 n = 500 all of the abov
a research want to know the averag weight of femal who are 22 year old. the research obtain the averag weight of 54 kg, from a random sampl of 40 females. identifi if each of the given statement is true or false. the averag weight of 54 kg in a group of femal is a parameter. fals the group of 40 femal is a sample. true the averag weight of all femal who are 22 yrs old is a parameter. true the averag weight of all femal who are 22 yrs old is a statistic. fals
what character a normal distribut a bell shape a mean, median, and mode that are equal a total area under the curv abov the x-axi that is 1
what can a z-score give in ation about the standard deviat (sd) of a distribut
what doe a z-score of 0 correspond to mean
what is alway the 50th percentil median
when is a sampl popul curv is more like to look like the popul curv when the sampl size is greater than 30
which properti do the valu of a variabl have when a nomin level of measur is use they simpli repres categories.
which statement best describ symmetr distribut they have an equal number of data point that appear to the left and to the right of the center.
what doe skew refer to the extent to which data are not symmetr about the center
statement that best describ a stem-and-leaf it show the rang of valu of the variable. it show the shape of the distribut of the variable. it preserv the individu valu of the variable.
which statement best describ standard deviat it is the averag distanc of each point from the mean.
what is true of descript statist they are numer or graphic summari of data.
what is true of inferenti statist they are use to examin relationship between variabl in a data set. they are use to see how well sampl data can be general to the population.
measur scale gender - nomin temperatur in celsius - interv wt in pound - ratio wt in kilogram - ratio age in year - ratio age in categori (0 to 6 months, 7 to 12 months, 13+ months) - ordin
in what order are the step for the knowledg discoveri and data mine process complet selection, pre-processing, tran ation, data mining, and interpret and evalu
in order to de-identifi particip in the data set, what action must be taken to secur the general in ation collect in the survey name must be remov from the general in ation. street number must be remov from the general in ation. phone number and email address must be remov from the general in ation.
you want to examin the relationship between the household incom and each risk behavior (behavior risk factor) for adult younger than 65 year old for the year 2010. which data manag process could be use to examin this relationship retriev all the variabl of behavior risk factor, age, and educ from the year of 2010, select case for 18≤age retriev all the variabl of behavior risk factor, age, and education, select case for 18≤age 65 and year = 2010, and then conduct correl analysis.
what is an essenti tool for effect manag and use the data in the databas and a structur way of document data store in a databas data dictionari
what is not a characterist of central tendenc use number as code repres categori or characteristics, with no order to the categori
what are characterist of central tendenc repres the middl of a distribution, provid in ation about the most typic values, use the mean, median, and mode as it measures.
heat map plot all data point as a cell for two given variabl of interest and, depend on frequenc of observ in each cell, provid color visual high or low frequency.
boxplot chart the median, 25%, 75%, mean, minimum and maximum valu
scatterplot plot a point for two differ numer variabl for each observ of interest
dotplot similar to a histogram, plot the frequenc of observ for a given variabl in a data set.
histogram chart the frequenc of data point (y axis) base upon a particular measur of interest (x axis)
correl mix creat a tabl of numer variabl and, depend on measur of correl between the variables, the color chang to repres strength
what should you look at when interpret a correl coeffici the signific of the correl coeffici the magnitud of the correl coeffici the +/ - sign of the correl coeffici
the plus or minus sign in front of the coeffici identifi the direct of the relationship between variables. a plus (+) indic a posit correl where the variabl either increas or decreas togeth (parallel). a negat (-) correl indic a relationship where one variabl increas as the other decreas (inverse). if r is close to zero (0), it mean there is no relationship between the variables. 
what do correl studi allow the research to do identifi the relationship between two variabl
which statement would be consid a two-tail hypothesi there is a clear relationship between a healthi balanc diet and feel of well-being.
there is a perfect posit correl between two interval/ratio variables. what correl coeffici would the pearson r test give +1.
how much varianc has been explain by a correl of .9 81%
what is multicollinear when predictor variabl correl veri high with each other
confid interv in a, b, and c overlap. 
a research studi the use of a specif type of inhal for asthma sufferers. the s summari is as follows: "predictor signific favor inhal use (adjust r2 = .38; ρ = .015). inhal a was the strongest predictor of improv breath (β = .39; ρ = .001) follow by inhal b (β = .28; ρ = .0015)". what would be the best interpret of these variabl 38% of the variat in inhal
a research studi the use of a specif type of inhal for asthma sufferers. the s summari is as follows: "predictor signific favor inhal use (adjust r2 = .38; ρ = .015). inhal a was the strongest predictor of improv breath (β = .39; ρ = .001) follow by inhal b (β = .28; ρ = .0015)". which statement is true of these find 62% of the variat in s of inhal use (a or b) is due to other factors.
in a studi on child educ investment, research use a simpl linear regress model to studi the relationship between child educ expenditur and the number of children in the family. the depend variabl is averag child educ expenditur in dollars, and the independ variabl is the number of children in the family. the regress coeffici is -2,300. what doe the coeffici mean the averag child educ expenditur decreas by $2,300 with each addit child in the family. *
research use a simpl linear regress model to examin the associ between blood lead level and household soil lead levels. the depend variabl is blood lead level (μg/dl), and the independ variabl is soil lead level (mg/kg). the regress coeffici is 0.15. what doe the coeffici mean for a child blood lead level the level increas by 15μg/dl with everi 100 mg/kg increas in soil lead level.
the sensit is 80/100 = 80%, the specif = 90/100 = 90% 
t-test (student "t") independ t-test - purpose: test for differ in the mean between two groups. - variables: one categor iv with two level (males, females); one continu dv. pair sampl t-test - purpose: test for differ in the mean within one group at differ times. - variables: one categor iv with two level (time 1, time 2); one continu dv.
one-way anova purpose: test for differ in the mean score on a dv across three or more groups. variables: one categor iv with three or more categories; one continu dv. note: one-way mean one independ variabl (iv)
one-way anova with plan comparison purpose: test for differ in the mean score in specif group within a project (subsets). use when power is an issue. variables: one categor iv with three or more categories; one continu dv.
two-way anova purpose: test for differ in the mean score on a dv across three or more groups. variables: one categor iv with three or more categories; one continu dv. note: two-way mean two independ variabl (iv)
mix between-within anova purpose: measur the outcom between group (males/females) and within group (repeat measures). variables: one categor between group iv and one categor within group iv; one continu variable. note: between = two or more differ groups; within = one group measur two or more times.
research use an analysi of varianc to evalu the differ in blood pressur among five group with differ alcohol drink habits. each group has a sampl of n = 10. the analysi produc ss within group = 210, ss between group = 340. what is the ss total 550
research use an analysi of varianc to evalu the differ in blood pressur among five group with differ alcohol drink habits. each group has a sampl of n =10. the analysi produc ss within group = 210, ss between group = 340. what is the ms within group 210 / 45
research use an analysi of varianc to evalu the differ in blood pressur among five group with differ alcohol drink habits. each group has a sampl of n = 10. the analysi produc ss within group = 210, ss between group = 340. what is the f-valu f(4, 45) = 18.21
research use an analysi of varianc to evalu the differ in blood pressur among five group with differ alcohol drink habits. each group has a sampl of n = 10. the analysi produc ss within group = 210, ss between group = 340. what statist conclus can we reach if the correspond p-valu is 0.05 reject h0 and conclud that not all group mean are equal
the one-way anova is best use when the measur scale of the characterist of interest is which measur scale of the characterist of interest make the one-way anova the best choic interv or ratio
what doe the one-way anova compar to determin whether the mean of the group are differ within-group varianc to the between-group varianc
when doe the one-way anova test tell you whether the mean of one or more group is differ from the other
when an anova test is significant, what should the research do next conduct a post-hoc test to determin which mean(s) are signific differ from the other
when would an independ sampl t-test be use instead of an anova when there are onli two categori in the group variabl
when should nonparametr test such as the kruskal-w and the mann-whitney u-test should be use when the data do not meet the distribut requir for a parametr test
what doe a calcul valu of chi-squar compar the frequenc of categori of item in a sampl to the frequenc that are expect in the popul
the correl between the variabl a and b is .12 with a signific of p .01. what is true about the relationship between the variabl there is a small relationship between a and b.
a research identifi a signific posit correl (r = .42) between the number of children a person has and the person life satisfaction. which statement is inappropri to conclud from this research that someon who has children is like to be happier than someon who doesnot
whi doe practice-bas evid (pbe) requir close partner with in atic specialist (choos all that apply.) to design screen and term to captur intervent to creat and maintain the databas requir for pbe studi
how doe practice-bas evid (pbe) differ from evidence-bas practic (ebp) ebp determin best practic use best evid such as s from rcts. pbe is prospect and, ebp is retrospective. pbe attempt to captur the complex and variabl of actual clinic care..
which is not a common step in a practice-bas evid studi obtain patient consent
explain what regular is and how it works. regular is the process of ad a tune paramet to a model to induc smooth in order to prevent overfitting. this is most often done by ad a constant multipl to an exist weight vector. this constant is often either the l1 (lasso) or l2 (ridge), but can in actual can be ani norm. the model predict should then minim the mean of the loss function calcul on the regular train set.
you creat a predict model for a quantit outcom variabl use multipl regression. how would you valid this model propos method for model validation: 1) if the valu predict by the model are far outsid of the respons variabl range, this would immedi indic poor estim or model inaccuracy. 2) if the valu seem to be reasonable, examin the parameters; ani of the follow would indic poor estim or multi-collinearity: opposit sign of expectations, unusu larg or small values, or observ inconsist when the model is fed new data. 3) use the model for predict by feed it new data, and use the coeffici of determin (r squared) as a model valid measure. 4) use data split to a separ dataset for estim model parameters, and anoth for valid predictions. 5) use jackknif resampl if the dataset contain a small number of instances, and measur valid with r squar and mean squar error (mse).
what is multipl regress analysi multipl regress is an extens of simpl linear regression. it is use when we want to predict the valu of a variabl base on the valu of two or more other variables. the variabl we want to predict is call the depend variabl (or sometimes, the outcome, target or criterion variable). the variabl we are use to predict the valu of the depend variabl are call the independ variabl (or sometimes, the predictor, explanatori or regressor variables). for example, you could use multipl regress to understand whether exam per anc can be predict base on revis time, test anxiety, lectur attend and gender. alternately, you could use multipl regress to understand whether daili cigarett consumpt can be predict base on smoke duration, age when start smoking, smoker type, incom and gender. multipl regress also allow you to determin the overal fit (varianc explained) of the model and the relat contribut of each of the predictor to the total varianc explained. for example, you might want to know how much of the variat in exam per anc can be explain by revis time, test anxiety, lectur attend and gender "as a whole", but also the "relat contribution" of each independ variabl in explain the variance.
explain what precis and recal are. how do they relat to the roc curv recal (aka sensitivity; aka true posit rate) = true posit / (true posit + fals negatives) precis = true posit / (true posit + fals positives) fals posit rate (fpr) = fals posit / (fals posit + true negatives) specif = true negatives/(tru negat + fals positives) = 1 - fpr roc curv repres a relat between sensit (recall) and specif (not precision) and is common use to measur the per anc of binari classifiers. however, when deal with high skew datasets, precision-recal (pr) curv give a more repres pictur of per ance.
what is the differ between receiv oper characterist (roc) and precision-recal (pr) curv in the context of machin learn receiv oper characterist (roc) curv are common use to present s for binari decis problem in machin learning. however, when deal with high skew datasets, precision-recal (pr) curv give a more in ativ pictur of an algorithm per ance.
explain type i and type ii errors. in statist there are type i error and type ii errors. relat to true posit and fals posit terminology, a type i error occur when you reject the null hypothesi (as false) when it is actual true, which by convent correspond to a fals positive. a type ii error occur when you accept the null hypothesi (as true) when it is actual false, which by convent correspond to a fals negative.
explain the differ between test sensit and test specif in the context of a medic diagnosis. in medic diagnosis, test sensit is the abil of a test to correct identifi those with the diseas (true posit rate), wherea test specif is the abil of the test to correct identifi those without the diseas (true negat rate).
what is root caus analysi root caus analysi (rca) is a method of problem solv use for identifi the root caus of fault or problems. a factor is consid a root caus if remov thereof from the problem-fault-sequ prevent the final undesir event from recurring; wherea a causal factor is one that affect an event outcome, but is not a root cause.
what is statist power wikipedia defin statist power or sensit of a binari hypothesi test is the probabl that the test correct reject the null hypothesi (h0) when the altern hypothesi (h1) is true. to put in anoth way, statist power is the likelihood that a studi will detect an effect when the effect is present. the higher the statist power, the less like you are to make a type ii error (conclud there is no effect when, in fact, there is).
what do you need to calcul statist power test valu (valu to compar the sampl averag to), sampl averag (valu measur from sampl or expect from sample), sampl size (size of sampl or desir number of respondents), standard deviat for sample, and confid level (aka p-valu or alpha error level; probabl of incorrect reject the null hypothesi that there is no differ in the averag values). a p-valu of 5% correspond to a 95% confid interval.
explain what resampl method are and whi they are useful. also explain their limitations. classic statist parametr test compar observ statist to theoret sampl distributions. resampl a data-driven, not theory-driven methodolog which is base upon repeat sampl within the same sample. resampl refer to method for do one of these estim the precis of sampl statist (medians, variances, percentiles) by use subset of avail data (jackknifing) or draw random with replac from a set of data point (bootstrapping) exchang label on data point when per ing signific test (permut tests, also call exact tests, random tests, or re-random tests) valid model by use random subset (bootstrapping, cross validation)
is it better to have too mani fals positives, or too mani fals negat explain. it depend on the question as well as on the domain for which we are tri to solv the question. in medic testing, fals negat may provid a fals reassur messag to patient and physician that diseas is absent, when it is actual present. this sometim lead to inappropri or inadequ treatment of both the patient and their disease. so, it is desir to have too mani fals positive. for spam filtering, a fals posit occur when spam filter or spam ing techniqu wrong classifi a legitim email messag as spam and, as a , interfer with it delivery. while most anti-spam tactic can or filter a high percentag of unwant emails, do so without creat signific false-posit s is a much more demand task. so, we prefer too mani fals negat over mani fals positives.
what is select bias, whi is it import and how can you avoid it select bias, in general, is a problemat situat in which error is introduc due to a non-random popul sample. for example, if a given sampl of 100 test case was made up of a 60/20/15/5 split of 4 class which actual occur in relat equal number in the population, then a given model may make the fals assumpt that probabl could be the determin predict factor. avoid non-random sampl is the best way to deal with bias; however, when this is impractical, techniqu such as resampling, boosting, and weight are strategi which can be introduc to help deal with the situation.
explain what is overfit (aka high varianc and low bias) and how would you control for it. your model is overfit your train data when you see that the model per s well on the train data but doe not per well on the evalu data. this is becaus the model is memor the data it has seen and is unabl to general to unseen examples. if your model is overfit the train data, it make sens to take action that reduc model flexibility. to reduc model flexibility, tri the following: featur selection: consid use fewer featur combinations, decreas n-gram size, and decreas the number of numer attribut bins. increas the amount of regular used.
explain what is underfit (aka high bias) and how would you control for it. your model is underfit the train data when the model per s poor on the train data. this is becaus the model is unabl to captur the relationship between the input exampl (often call x) and the target valu (often call y). poor per anc on the train data could be becaus the model is too simpl (the input featur are not express enough) to describ the target well. per anc can be improv by increas model flexibility. to increas model flexibility, tri the following: add new domain-specif featur and more featur cartesian products, and chang the type of featur process use (e.g., increas n-gram size) decreas the amount of regular used.
give an exampl of how you would use experiment design to answer a question about user behavior. step 1: formul the research question: what are the effect of page load time on user satisfact rate step 2: identifi variables: we identifi the caus & effect. independ variabl -page load time, depend variable- user satisfact rate step 3: generat hypothesis: lower page download time will have more effect on the user satisfact rate for a web page. here the factor we analyz is page load time. step 4: determin experiment design. we consid experiment complex i.e vari one factor at a time or multipl factor at one time in which case we use factori design (2^k design). a design is also select base on the type of object (comparative, screening, respons surface) & number of factors. here we also identifi within-participants, between-participants, and mix model.for e.g.: there are two version of a page, one with buy button (call to action) on left and the other version has this button on the right. within-particip design - both user group see both versions. between-particip design - one group of user see version a & the other user group version b. step 5: develop experiment task & procedure: detail descript of step involv in the experiment, tool use to measur user behavior, goal and success metric should be defined. collect qualit data about user engag to allow statist analysis. step 6: determin manipul & measur manipulation: one level of factor will be control and the other will be manipulated. we also identifi the behavior measures: latency- time between a prompt and occurr of behavior (how long it take for a user to click buy after be present with products). frequency- number of time a behavior occur (number of time the user click on a given page within a time) duration-length of time a specif behavior lasts(tim taken to add all products) intensity-forc with which a behavior occur ( how quick the user purchas a product) step 7: analyz s identifi user behavior data and support the hypothesi or contradict accord to the observ made for e.g. how major of user satisfact rate compar with page load times.
what is the differ between "long" ("tall") and "wide" at data in most data mine / data scienc applic there are mani more record (rows) than featur (columns) - such data is sometim call "tall" (or "long") data. in some applic like genom or bioin atic you may have onli a small number of record (patients), eg 100, but perhap 20,000 observ for each patient. the standard method that work for "tall" data will lead to overfit the data, so special approach are needed. the problem is not just reshap the data, but avoid fals posit by reduc the number of featur to find most relev ones.
how would you screen for outlier and what should you do if you find one inter quartil rang an outlier is a point of data that lie over 1.5 iqr below the first quartil (q1) or abov third quartil (q3) in a given data set. high = (q3) + 1.5 iqr low = (q1) - 1.5 iqr when you find outliers, you should not remov it without a qualit assess becaus that way you are alter the data and make it no longer pure. it is import to understand the context of analysi or import "the whi question - whi an outlier is differ from other data point " this reason is critical. if outlier are attribut to error, you may throw it out but if they signifi a new trend, pattern or reveal a valuabl insight into the data you should retain it.
what is a recommend engin how doe it work they typic produc recommend in one of two ways: use collabor or content-bas filtering. collabor filter method build a model base on user past behavior (item previous purchased, movi view and rated, etc) and use decis made by current and other users. this model is then use to predict item (or rate for items) that the user may be interest in. content-bas filter method use featur of an item to recommend addit item with similar properties. these approach are often combin in hybrid recommend systems.
the data scientist at "bigmart inc" have collect 2013 sale data for 1559 product across 10 store in differ cities. also, certain attribut of each product base on these attribut and store have been defined. the aim is to build a predict model and find out the sale of each product at a particular store dure a defin period. which learn problem doe this belong to a) supervis learn b) unsupervis learn c) reinforc learn d) none solution: a supervis learn is the machin learn task of infer a function from label train data. here histor sale data is our train data and it contain the label / outcomes.
what is rmse the root-mean-squar error (rmse) is a measur of the differ between valu predict by a model or an estim and the valu actual observed. rmse is use to evalu regress models. the lower the rmse, the better the model. the ula is : rmse = (sqrt(sum(square(predicted_valu - actual_values)) / number of observations))
which methodolog doe decis tree take to decid on first split a) greedi approach b) look-ahead approach c) brute forc approach d) none of these solution: a the process of top-down induct of decis tree (tdidt) is an exampl of a greedi algorithm, and it is by far the most common strategi for learn decis tree from data.
there are 24 predictor in a dataset. you build 2 model on the dataset: 1. bag decis tree and 2. random forest let the number of predictor use at a singl split in bag decis tree is a and random forest is b. which of the follow statement is correct a) a = b b) a b d) cannot be said sinc differ iter use differ number of predictor solution: a random forest use a subset of predictor for model building, wherea bag tree use all the featur at once.
whi do we prefer in ation gain over accuraci when split a) decis tree is prone to overfit and accuraci doesn't help to general b) in ation gain is more stabl as compar to accuraci c) in ation gain choos more impact featur closer to root d) all of these solution: d all the abov option are correct
random forest (while solv a regress problem) have the higher varianc of predict in comparison to boost tree (assumption: both random forest and boost tree are fulli optimized). a) true b) fals c) cannot be determin solution: c it complet depend on the data, the assumpt cannot be made without data.
assum everyth els remain same, which of the follow is the right statement about the predict from decis tree in comparison with predict from random forest a) lower variance, lower bias b) lower variance, higher bias c) higher variance, higher bias d) lower bias, higher varianc solution: d the predict valu in decis tree have low bias but high varianc when compar to random forests. this is becaus random forest attempt to reduc varianc by bootstrap aggregation.
which of the follow tree base algorithm use some parallel (full or partial) implement a) random forest b) gradient boost tree c) xgboost d) both a and c e) a, b and c solution: d onli random forest and xgboost have parallel implementations. random forest is veri easi to parallelize, where as xgboost can have partial parallel implementation. in random forest, all tree grow parallel and final ensembl the output of each tree . xgboost doesn't run multipl tree in parallel like random forest, you need predict after each tree to updat gradients. rather it doe the parallel within a singl tree to creat branch independently.
which of the follow is not possibl in a boost algorithm a) increas in train error. b) decreas in train error c) increas in test error d) decreas in test error e) ani of the abov solution: a boost algorithm minim error in previous predict valu by last estimator. so it alway decreas train error.
let say we have m number of estim (trees) in a boost tree. now, how mani intermedi tree will work on modifi version (or weighted) of data set a) 1 b) m-1 c) m d) can't say e) none of the abov solution: b the first tree in boost tree work on the origin data, wherea all the rest work on modifi version of the data.
what is bias and varianc tradeoff conceptu definit error due to bias: the error due to bias is taken as the differ between the expect (or average) predict of our model and the correct valu which we are tri to predict. of cours you onli have one model so talk about expect or averag predict valu might seem a littl strange. however, imagin you could repeat the whole model build process more than once: each time you gather new data and run a new analysi creat a new model. due to random in the under data sets, the ing model will have a rang of predictions. bias measur how far off in general these model predict are from the correct value. error due to variance: the error due to varianc is taken as the variabl of a model predict for a given data point. again, imagin you can repeat the entir model build process multipl times. the varianc is how much the predict for a given point vari between differ realize of the model.
boost is a general approach that can be appli to mani statist learn method for regress or classification. a) true b) fals solution: a boost is an ensembl techniqu and can be appli to various base algorithms.
generally, in term of predict per anc from highest to lowest, which of the follow arrang are correct: a) bag boost random forest singl tree b) boost random forest singl tree bag c) boost random forest bag singl tree d) boost bag random forest singl tree solution: c general speaking, boost algorithm will per better than bag algorithms. in term of bag vs random forest, random forest work better in practic becaus random forest has less correl tree compar to bagging. and it alway true that ensembl of algorithm are better than singl model
in which of the follow application(s), a tree base algorithm can be appli success a) recogn move hand gestur in real time b) predict next move in a chess game c) predict sale valu of a compani base on their past sale d) a and b e) a, b, and c solution: e option e is correct as we can appli tree base algorithm in all the 3 scenarios.
boost is said to be a good classifi because: a) it creat all ensembl member in parallel, so their divers can be boosted. b) it attempt to minim the margin distribut c) it attempt to maxim the margin on the train data d) none of these solution: b a. tree are sequenti in boosting. they are not parallel b. boost attempt to minim residu error which reduc margin distribut c. as we saw in b, margin are minim and not maximized. therefor b is true
what is cardin in databas in the context of databases, cardin refer to the uniqu of data valu contain in a column. high cardin mean that the column contain a larg percentag of total uniqu values. for example, high-cardin column valu are typic identif numbers, email addresses, or user names. low cardin mean that the column contain a lot of "repeats" in it data range.
which split algorithm is better with categor variabl have high cardin a) in ation gain b) gain ratio c) chang in varianc d) none of these solution: b when high cardin problems, gain ratio is prefer over ani other split technique.
suppos we have miss valu in our data. which of the follow method(s) can help us to deal with miss valu while build a decis tree a) let it be. decis tree are not affect by miss valu b) fill dummi valu in place of missing, such as -1 c) imput miss valu with mean/median d) all of these solution: d all the option are correct.
to reduc under fit of a random forest model, which of the follow method can be use a) increas minimum sampl leaf valu b) increas depth of tree c) increas the valu of minimum sampl to split d) none of these solution: b onli option b is correct, becaus a: increas the number of sampl for a leaf will reduc the depth of a tree, indirect increas underfit b: increas depth will definit decreas help reduc underfit c: increas the number of sampl consid to split will have no effect, as the same in ation will be given to the model.
while creat a decis tree, can we reus a featur to split a node a) yes b) no solution: a yes, decis tree recurs use all the featur at each node.
which of the follow is a mandatori data pre-process step(s) for xgboost a) imput miss valu b) remov outlier c) convert data to numer array / spars matrix d) input variabl must have normal distribut e) select the sampl of record for each tree/ estim solution: c xgboost is doesn't requir most of the pre-process steps, so onli convert data to numer is requir among of the abov list step
decis tree are not affect by multicollinear in features: a) true b) fals solution: a the statement is true. for example, if there are two 90% correl features, decis tree would consid onli one of them for splitting.
for paramet tune in a boost algorithm, which of the follow search strategi may give best tune model: a) random search. b) grid search. c) a or b d) can't say solution: c for a a given search space, random search random pick out hyperparameters. in term of time required, random search requir much less time to converge. grid search determinist tri to find optimum hyperparameters. this is a brute forc approach for solv a problem, and requir much time to give output. both random search or grid search may give best tune model. it depend on how much time and resourc can be alloc for search.
imagin a two variabl predictor space have 10 data points. a decis tree is built over it with 5 leaf nodes. the number of distinct region that will be ed in predictor space a) 25 b) 10 c) 2 d) 5 solution: d the predictor space will be divid into 5 regions. therefore, option d is correct.
in random forest, which of the follow is random select a) number of decis tree b) featur to be taken into account when build a tree c) sampl to be given to train individu tree in a forest d) b and c e) a, b and c solution: d option a is fals becaus the number of tree has to be decid when build a tree. it is not random. option b and c are true
which of the follow are the disadvantag of decis tree algorithm a) decis tree is not easi to interpret b) decis tree is not a veri stabl algorithm c) decis tree will over fit the data easili if it perfect memor it d) both b and c solution: d option a is false, as decis tree are veri easi to interpret option b is true, as decis tree are high unstabl model option c is true, as decis tree also tri to memor noise.
while tune the paramet "number of estimators" and "shrinkag parameter"/"learn rate" for boost algorithm.which of the follow relationship should be kept in mind a) number of estim is direct proport to shrinkag paramet b) number of estim is invers proport to shrinkag paramet c) both have polynomi relationship solution: b it is general seen that smaller learn rate requir more tree to be ad to the model and vice versa. so when tune paramet of boost algorithm, there is a trade-off between learn rate and number of estimators.
let say we have m number of estim (trees) in a xgboost model. now, how mani tree will work on bootstrap data set a) 1 b) m-1 c) m d) can't say e) none of the abov solution: c all the tree in xgboost will work on bootstrap data. therefore, option c is true.
which of the follow statement is correct about xgboost paramet (may be more than one): a) learn rate can go upto 10 b) sub sampl / row sampl percentag should lie between 0 to 1 c) number of tree / estim can be 1 d) max depth can not be greater than 10 solution: b and c a and d are wrong statements, wherea b and c are correct.
predict of individu tree of bag decis tree have higher correl in comparison to individu tree of random forest. a) true b) fals solution: b this is fals becaus random forest has more random generat uncorrel tree than bag decis trees. random forest consid onli a subset of total features. so individu tree that are generat by random forest may have differ featur subsets. this is not true for bag trees.
below is a list of paramet of decis tree. in which of the follow case higher is better a) number of sampl use for split b) depth of tree c) sampl for leaf d) can't say solution: d for all three option a, b and c, it is not necessari that if you increas the valu of paramet the per anc may increase. for example, if we have a veri high valu of depth of tree, the ing tree may overfit the data, and would not general well. on the other hand, if we have a veri low value, the tree may underfit the data. so, we can't say for sure that "higher is better".
when do we use the one sampl t-test to examin the averag differ between a sampl and the known valu of the popul mean.
what are the assumpt of a one sampl t-test - the popul from which the sampl is drawn is normal distributed. - sampl observ are random drawn and independent.
what type of test do you use for "you are told that the averag height of a person in your build (ᶞ) is 68 inches; however, you think the averag person is actual much taller." one sampl t-test. for this scenario: ➢ null hypothesi (h0 ): ᶞ = 68 inch ➢ altern hypothesi (ha ): ᶞ 68 inch
when do we use the the two sampl t test to examin the averag differ between two sampl drawn from two differ populations.
what are the assumpt of the two sampl t-test ➢ the popul from which the sampl are drawn are normal dist. ➢ the standard deviat of the two popul are equal. ➢ sampl observ are random drawn and independent.
what test should you use for the follow problem, "although the difficulti of the sat should not vari across it differ administrations, you believ that time is everything; you suppos that there is a differ in the averag score from test taken in spring and fall." two sampl t-test. for this scenario: ➢ null hypothesi (h0 ): ᶞspring = ᶞfall ➢ altern hypothesi (ha ): ᶞspring ≠ fall
when do we use the f-test to assess whether the varianc of two differ popul are equal
what are the assumpt of the f-test assumptions: ➢ the popul from which the sampl are drawn are normal dist. ➢ sampl observ are random drawn and independent.
what test should you appli to the follow problem: "when we test the difficulti of sat exam in the previous exampl use the two sampl t-test, we should have had equal variances; however, the varianc were slight different. were they signific differ " for this scenario: null hypothesi spring sd = fall sd altern hypothesi (ha) spring sd ≠ fall sd
when do we use one-way anova to assess the equal of mean of two or more groups. nb: when there are exact two groups, this is equival to a two sampl t-test
what are the assumpt of a one-way anova ➢the popul from which the sampl are drawn are normal dist. ➢ the standard deviat of the popul are equal. ➢ sampl observ are random drawn and independ
what test should be appli to the follow problem, "you desir to test the efficaci of differ type of diet on weight loss: low calorie, low carbohydrate, and low fat. you also have a control group for comparison purposes." one-way anova. null hypothesi (h0 ): ᶞlow calori = ᶞlow carbohydr = ᶞlow fat = ᶞcontrol ➢ altern hypothesi (ha ): at least one of the averag amount of weight loss differ from the others.
when do we use the chi-squar test of independ to test whether two categor variabl are independent.
what are the assumpt of a chi-squar test sampl observ are random drawn and independ
what test should you appli to follow problem "a review session was held befor a quiz was administ to a class of students. you would like to determin whether a student grade on the quiz is depend on whether or not they attend the review session." for this scenario: ➢ null hypothesi (h0 ): the two variabl are independ of one another. ➢ altern hypothesi (ha ): the two variabl are depend on each other
what is machin learn machin learn develop from the combin of statist and comput science; it aim to implement algorithm that allow comput to "learn" about the data it analyzes. tradit algorithm requir comput to follow a strict set of program instructions; machin learn algorithm instead assess the data at hand to make in ed decisions. machin learn algorithm have the abil to learn and adapt; tradit algorithm do not.
what is supervis learn your data includ the "truth" that you wish to predict. ➢ use what you know about your observ to construct a model for futur decis making. ■ regress ■ classif
what is unsupervis learn your data doe not includ the "truth" that you wish to predict. ➢ use your data to find under structur to in intrins behavior that is not alreadi explicit available. ■ cluster ■ dimens reduct
what is miss data missing occur when at least some of an observ valu are not present within the dataset. we say that the absent valu are "missing," and that the observ itself is "incomplete."
whi doe miss data matter mani statist method and machin learn techniqu have difficulti incorpor incomplet observ in their algorithms; they simpli don't know what to do when there isn't ani data to crunch!
what wrong with delet miss valu while the solut of complete-cas analysi seem to be the quickest and easiest on the surface, it can sever limit the amount of in ation avail for our tests. ➢ with fewer observ in our dataset we have a smaller sampl size, increas the standard error of ani estim we make.
what are the three type of miss data there are three main type of missingness: ➢ miss complet at random (mcar) ➢ miss at random (mar) ➢ miss not at random (mnar)
what is miss complet at random each piec of data in the overal dataset has an equal like chanc of be absent. ➢ the reason for the missing is neither relat to the observ variabl nor relat to the unobserv variabl of interest; they are independent. mcar data is the best case scenario for miss data in general, becaus it manifest is truli "complet at random"; unfortunately, data that is mcar is also often the rarest of miss data. ❖ when data are mcar, it is ok to ignor these observations; their delet will not end up bias your s becaus there is no under pattern that they reveal.
what type of miss data are the follow exampl 1) you write a survey with 99 question and distribut it to everyon in your office. you random select a few peopl to answer an addit 100th question. 2) a piec of lab equip is suppos to take a measur on each specimen that it encounters. for one arbitrari specimen, the equip malfunct and the measur is not recorded. miss complet at random
what is miss at random data when data are miss at random, the chanc that a piec of data is miss is depend on variabl for which we have complet in ation within our overal dataset. ➢ the probabl a piec of data is miss depend on avail in ation that we have alreadi collected; they are not independent. ❖ mar is the next-best scenario for miss data after mcar because, although each observ has a differ likelihood of missing, we theoret can estim this likelihood. ❖ when data are mar, it is accept to drop these observ from our analysi if we control for the factor that are relat to the missing and adjust for their effects, we can avoid bias in our model.
what is the type of miss data in the follow examples: 1) you write a survey and ask both men and women to submit responses. in the survey, there is a section that ask question about various sport teams. the women you survey are more like to not respond to the sports-rel questions. 2) a photocopi take a log of the amount of copi it is suppos to make, and the amount of ink it use for each job. for particular larg amount of copies, the machin has a higher chanc of malfunct and thus not record the amount of ink it use for the job. miss at random data
what is miss not at random data when data are miss not at random, the chanc that a piec of data is miss is depend on the actual valu of the observ itself. ➢ the valu of the miss piec of data is direct relat to the reason whi it is miss in the first place. ❖ mnar is the worst-cas scenario for miss data becaus it is non-ignorable. we cannot theoret accur estim the miss valu becaus the reason they are miss is not captur within our dataset. ❖ when data are mnar, it is not appropri to drop these observ from our analysis; do so would leav us with a bias dataset, and thus our analys would return bias models.
what do the follow exampl of data match 1) you write a survey and ask individu to report their weight. individu who are particular overweight tend to not answer this question 2) a scale is suppos to measur the weight of various items, but is not sensit enough to detect weight that are less than 5 pounds, and thus doe not record weight for such items. miss not at random data
what is mean valu imput mean valu imput procedure: ➢ comput the averag of the observ valu for a variabl that has missingness. ➢ imput the averag for each of the miss values. ❖ advantages: ➢ one of the simplest way of deal with miss data becaus of it relat straightforward approach. ❖ disadvantages: ➢ can distort the distribut of the variabl and underestim the standard deviation. ➢ can distort relationship between variabl by drag correl estim toward 0
what is simpl random imput simpl random imput procedure: ➢ for each miss valu in a variable, random select a complet valu of the same variable; imput this random select value. ➢ repeat the process until all valu are complete. ❖ advantages: ➢ use true, observ valu to fill in missingness. ❖ disadvantages: ➢ can amplifi outlier observ valu by have them repeat in the dataset. ➢ can induc bias into the dataset
what is regress imput ❖ regress predict procedure: ➢ assum an underlying, linear structur exist in the data. ➢ give weight to a subset of the complet variables. ➢ use a relationship between the complet variabl and the complet observ to imput miss observations. ❖ advantages: ➢ use true, observ valu to fill in missingness. ➢ use the relationship among multipl variabl to fill in missingness. ❖ disadvantages: ➢ must make assumpt about the structur of the data. ➢ can inappropri extrapol beyond the scope of avail in ation in our dataset.
common error type 1. typeerror 2. nameerror 3. valueerror
key error error for when someth can't be found in a dictionari
name error error for problem relat to variabl name
type error error for problem relat to data type
level of log (5) 1. debug 2. info 3. warn 4. error 5. critic
debug (logging) note for you, while work on code
info (logging) note that everyth is go ok
warn (logging) someth seem wrong but not urgent
error (logging) someth did not work
critic (logging) entir program stop run
benefit of make error (2) 1. make debug easier 2. can be dealt with programat
complex - provid in ation on the how quick your code will run - defin by the number of nest for loop
cross-valid model valid techniqu for assess how the s of a statist analysi will general to an independ data set
common type of cross-valid - leav one out - train test split - k-fold
import this (15) 1. beauti is better than ugly. 2. explicit is better than implicit. 3. simpl is better than complex. 4. complex is better than complicated. 5. flat is better than nested. 6. spars is better than dense. 7. readabl counts. 8. special case aren't special enough to break the rules. 9. practic beat purity. 10. error should never pass silently. unless explicit silenced. 11. in the face of ambiguity, refus the temptat to guess. 12. there should be one-- and prefer onli one --obvious way to do it. 13. if the implement is hard to explain, it a bad idea. 14. if the implement is easi to explain, it may be a good idea. 15. namespac are one honk great idea -- let do more of those!
panda data structur (3) 1. seri 2. datafram 3. panel
seri 1d array
datafram 2d label data structur
panel 3d label data structur
type i error - fals posit - reject null even though it true
power probabl of not make a type ii error
type ii error - fail to reject the null even though it fals
machin learn branch of ai concern with the construct and studi of system that can learn from data
facet of machin learn (2) 1. represent 2. general
represent (ml) extract structur from data
general (ml) make predict from data
type of learn (2) 1. supervis 2. unsupervis
supervis learn learn from label data; aim at generalizing/predict
type of supervis learn techniqu  
unsupervis learn learn from unlabel data; aim at extract structur
type of unsupervis learn techniqu (2) 1. cluster 2. natur languag process
supervised/continu regress
supervised/categor classif
unsupervised/continu dimens reduct
unsupervised/categor cluster
vector space featur space that data live
record a singl point in a vector space
other name for model input (6) 1. featur 2. attribut 3. predictor 4. input 5. iv 6. dimens
other name for model output (5) 1. target 2. respons 3. output 4. dv 5. label
other name for a row of data (4) 1. observ 2. datapoint 3. record 4. row
label valu on target valu in supervis learn
featur engin the art of use your input in uniqu way
linear model (equation) yi = b0 + b1x + ei
paramet (linear regression)  
cost function (equation)  
cost function minim the distanc between what your line predict for each point and what was actual observ
goal of linear regress minim the cost function to find the best fit model
error v. model complex graph  
converg graph (cross-validation)  
type of probabl (2) 1. discret 2. continu
probabl the studi of theoret possibl and their likelihood of occur
bay theorem p(b) = p(b a)p(a) - p(b ~a)p(~a); relat to condit probabl of two event
random variabl function map between all real number and the number between 0 and 1
expect valu the weight averag of the numer outcom of a probabl experi
discret random variabl when there are onli finit mani valu obtain by your variabl
law of larg number the averag valu of a larg number of independ sampl of a random variabl x get arbitr close to it expect valu
continu random variabl random variabl that attain a continuum of valu
properti of a gaussian distribut 1. normal distribut 2. center around mean
properti of log gaussian distribut  
properti of exponenti distribut  
properti of poisson distribut  
central limit theorem if we have a lot of small independ thing happen with no bias, then their cumul effect will be given by someth veri close to a normal distribut
aic (equation) 2k - 2ln(l)
regular use to prevent under/overfitting; penal model complex while award for good of fit
ridg regular (equation) lambda sum of b^2
ridg regular (l2) decreas model complex by penal the cost function by forc down the import of each of the variabl
lasso regular (l1) decreas model complex by penal the cost function by essenti elimin (forc to near-zero) the unimport variabl
elast net regular combin of l1 & l2
lasso regular (equation) lambda sum of b
lambda (underfitting) high
lambda (overfitting) low
r squar equat  
sse random left in the model
sst variat in the data
r squar portion of variat explain by the model
null hypothesi model by set all bs to zero; linear relationship found is pure due to chanc
p-valu probabl of find the observ valu s when the null hypothesi is true
log likelihood cost function
bias-vari tradeoff - a way of think about overfit - both are measur of what would happen if you retrain your model mani time on differ set of train data - hold model complex constant, the more data you have the harder it is to overfit
high bias - make a lot of mistak for pretti much ani train set - add more featur
improv high varianc - remov featur - obtain more data
high bias/low varianc underfit
low bias/high varianc overfit
way to improv confid (3) 1. decreas confid 2. be bias 3. increas sampl size
student t-distribut  
linear regress assumpt (5) 1. regress is linear in paramet and correct specifi 2. error are normal distribut 3. error have constant varianc (no heteroskedasticity) 4. error are uncorrel 5. no multi-collinear
auto-regress model regress where one or more previous observ target valu is use as a featur
ar-1 one previous point is among the featur
ar-2 two previous point as featur
knn classif techniqu that use k nearest neighbor to predict what the new point will be
decis boundari  
knn assumpt (2) 1. notion of distanc 2. assumpt that point that are close to one anoth are similar
curs of dimension high dimension space are vast, thus point in these space tend not to be close to one anoth make classif difficult
knn benefit 1. fit fast 2. low complex
knn pitfal 1. lazi 2. predict slow 3. requir a lot of memori
model that requir scale 1. knn 2. svm
logist regress (equation)  
logist regress (graph)  
odd (equation)  
log odd (equation)  
logist function (equation)  
classif method use 1 v all 1. logist regress
classif metric (5) 1. accuraci 2. precis 3. recal (sensitivity) 4. f1 score 5. specif
recal (sensitivity) (equation) tp / (tp + fn)
precis (equation) tp / (tp + fp)
specif (equation) tn / (tn + fp)
accuraci (equation) (tp + tn) / (tp + tn + fp + fn)
precis out of all the case predict positive, how mani time was i right
recal out of all the posit cases, how mani did i find
f1 score harmon mean of precis and recal
f1 score (equation) (2 x precis x recall) / (precis + recall)
receiv oper characterist (lower threshold) - better at catch posit - higher recal / less precis - higher true posit rate - higher fals posit rate
receiv oper characterist (higher threshold) - more sure about posit - lower recal / more precis - lower true posit rate - lower fals posit rate
auc evalu of classif algorithm
type of classifi () 1. knn 2. logist regress 3. svm 4. decis tree 5. random forest
support vector machin (3) 1. linear classifi 2. geometr motiv 3. origin propos for binari class but has been extend to multi-class
svm decis boundari (equation) wtx + b = 0
how do svms work find optim separ hyperplan that has the maximum margin
margin (svm) distanc between the closest point to the separator; no point insid margin
support vector subset of your data closest to decis boundary; they defin the hyperplane; rest of point are essenti irrelev
svm mathemat *** maxim d= 2/ w ; minim wtw
slack variabl can be ad to svm if train set is not linear separable; allow for misclassif of difficult or noisi sampl with a ing soft margin
soft margin (equation) wtw + slack variabl
non-linear svms use for classifi when the data is in higher-dimension space or is not linear
kernel trick function that, given two vectors, implicit comput the dot product between them in a higher dimension space without explicit tran ing them
kernel function function that is equival to an inner product in some featur space; can effici learn nonlinear decis boundari by replac all dot product in svm comput with kerbnel
how to choos a kernel (3) 1. choos correct kernel is non-trivi and may depend on specif task/dataset at hand 2. kernel still need to be tune to get good per anc from classifi 3. popular tune technique: k-fold cross valid
how did you choos paramet for rbf kernel - c - gamma
what model for: a lot of featur and small data - simpl model - linear regress - linearsvc
what model for: few features, decent data - gaussian kernel svc
what model for: few features, lot of data - linear regress - linear svc - gaussian kernel svc too slow
decis tree - use a tree structur to repres a number of possibl decis path and an outcom for each path - each inner node is a decis base on a featur - each leaf node is a predict valu
decis tree advantag (3) 1. easi to understand and interpret 2. both numer and categor featur can be use natur 2. natur multiclass classifi
decis tree disadvantag (4) 1. can overfit to train data with complex tree 2. small chang in input data can in complet differ tree 3. can make mistak with unbalanc class 4. no confid interv
decis tree step (2) 1. build tree split by split 2. find best split you can at each step (entropy)
entropi uncertainti associ with data
when is entropi small - when everi p is close to 0 or 1 - most of data in singl class
when is entropi larg when mani of the p are not close to 0 - data spread across multipl class
entropi (equation) h(s) = -p1 log2 p1 - ... pn log2 pn
entropi (drawing)  
prune (decis trees)  
ensembl method (decis trees)  
bag - bootstrap aggreg - fit a model to each random set of data - each model has one vote, choos max vote
random forest - introduc random when build each tree - don't take best featur split - random choos sqrt(n_feat) featur - choos best split among those
prior initi belief
naiv bay assumpt assum each featur is independ
type of naiv bay () 1. multinomi 2. binomi 3. gaussian 4. bernoulli
gradient descent start with cost function and take step in opposit direct of gradient vector toward minimum
gradient descent - problem with local minima may run into the issu that you reach a local minimum and get stuck
gradient descent - problem with step size too high may get stuck and can't actual find global minimum
type of gradient descent (3) 1. tradit 2. stochast 3. batch
benefit of sgd (3) 1. faster - onli take deriv of a singl point at each step 2. onlin train - onli need to keep singl point in memori 3. cover mani algorithm
svm hing loss
what is batch gradient descent typic use for neural net
perceptron simplest neural net approxim a singl neuron with n binari input
feed forward neural network - entail an input layer which receiv input and feed them forward unchang
hidden layer - each consist of neuron that take the output of the previous layer, per s a calculation, and pass the to the next layer, and an output layer
neuron part of neural network with weight attach to each input and a bias
bias (nn) - ad to end of weight - input alway equal 1
feed forward neural net  
backpropag step (5) 1. run feed forward net on an input vector to produc the output of all the neuron in the network 2. this s in an error for each output neuron -- the differ between it output and target 3. comput the gradient of this error as a function of the neuron weights, and adjust it weight in the direct that most decreas the error 4. propag these output error backward to infer error for the hidden layer 5. comput the gradient of these error and adjust the hidden layer weight in the same manner
likelihood function of a paramet for a fix outcom after data is avail
likelihood (equation) l (b y) p (y b)
likelihood cost function (equation) l (b y) = pip(yi b)
maximum likelihood (equation)  
mle step (3) 1. write down likelihood 2. take natur log and simplifi 3. maxim
in ation gain ***  
maximum likelihood***  
model: a simplifi represent of realiti creat to serv a purpos
predict model: a ula for estim the unknown valu of interest: the target - the ula can be mathematical, logic statement (e.g., rule), etc
prediction: estim an unknown valu (i.e. the target)
instanc / example: repres a fact or a data point describ by a set of attribut (fields, columns, variables, or features)
model induction: the creation of model from data
train data: the input data for the induct algorithm aka: in-sampl data
test data: use onc to test the s of the train and assess suitabl for use. (also use for robust check - aka: out-of-sampl data)
model featur type - numeric: anyth that has some order
model featur type - categor stuff that doe not have an order
dimension of a dataset sum of the dimens of the featur
common data mine task - classif and class probabl estim how like is this consum to respond to our campaign
common data mine task - regress how much will she use the servic
common data mine task - similar match can we find consum similar to my best custom
common data mine task - cluster do my custom natur group
common data mine task - co-occurr group also known as frequent itemset mining, associ rule discovery, and market-basket analysi what item are common purchas togeth
common data mine task - profil (behaviour description) what doe "normal behavior" look like (for example, as baselin to detect fraud)
common data mine task - data reduct which latent dimens describ the consum tast prefer
common data mine task - link predict sinc john and jane share 2 friends, should john becom jane friend
common data mine task - causal model whi are my custom leav :(
supervis method classification, regression, causal modeling, similar matching, link prediction, data reductuin
unsupervis method profiling, co-occur grouping, clustering, data reduction, link prediction, similar match
data scientist: = part hacker + part technologist + part detect + part scientist + part busi analyst + part visual artist
statist infer method for draw conclus about a popul from sampl data
in ation gain the measur of chang in entropi due to ani amount of new infrom be ad
entropi the measur of impur in the data, lower to 0 the better
data format: csv comma delimit file
data format: json javascript object notat
data format: xml extens markup languag
data format: html hyper text markup languag
api: applic program interfac
sourc of error in data data entri error (e.g., telephon call centers) measur error (e.g., improp sampling) distil error (e.g., smooth due to noise) data integr error (e.g., multipl databases)
data semantics: the real-world mean e.g., compani name, day of the month, person height, etc.
data type: interpret in term of scale of measur e.g., quantiti or category, sensibl mathemat operations, data structure, etc.
data types: nomin (catergorical) (n) are equal or not equal to other values, apples, oranges, bananas,...
data types: ordin (o) obey a (greater than) relationship, from small, medium, larg
data types: quantit (q) can do arithmet on them, 10cm, 23cm, etc.
data types: quantit (q) : interv (locat of zero is arbitary) dates. cant be compar directly, onli the differ (interv can be compar
data types: quantit (q) : ratio (zero is fix measurements: lengths, mass, temp. origin is meaningful, can measur ratio and proportions.
a databas map is also call erd - entiti relationship diagram
the onli way to access a databas is via a data base manag system
null can also repres an unknown attribut valu a known, but missing, attribut valu a "not applicable" condit
type of key - foreign key (fk) an attribut whose valu match primari key valu in the relat tabl or must be null
type of key - foreign key (fk) referenti integr fk contain a valu that refer to an exist valid tupl (row) in anoth relat
type of key - secondari key key use strict for data retriev purpos
type of key - superkey an attribut or combin of attribut that uniqu identifi each row in a tabl
type of key - candid key a minim superkey; a superkey that doe not contain a subset of attribut that is itself a superkey
type of key - primari key a candid key select to uniqu identifi all other attribut valu in ani given row; cannot contain null entri
which of the follow object are raw data (as oppos to process data.) answers: a web page download use urllib.request.urlopen() or save from a web browser. a tabl that contain the frequenc of each uniqu word in mobi dick. a tiff imag file obtain by scan a book page. the text of mobi dick extract from a scan imag use optic charact recognit (ocr). a web page download use urllib.request.urlopen() or save from a web browser. a tiff imag file obtain by scan a book page.
which data analysi techniqu is a machin learn techniqu answers: in ation retrieval. linear regressions. decis trees. signal processing. decis trees.
is it a good idea to use inter e gui tool for reproduc data analysi (as oppos to command-lin program tools), and whi answers: no, it is not. inter e gui tool do not record the histori of operations, which make the analysi not reproducible. no, it is not. inter e gui tool are usual oversimplified, strip down version of power programm data process environments. yes, it is. inter e gui tool simplifi data analysi and make it more accessible. yes, it is. inter e gui tools, as a rule, provid more power data analysi techniques. no, it is not. inter e gui tool do not record the histori of operations, which make the analysi not reproducible.
alic notic that the scatter plot of variabl y vs variabl x that she plot as a part of the exploratori data analysis, look almost like a straight diagon line. what statement is alic entitl to make, base on this observ answers: a chang in x caus a chang in y. x and y are correlated. a chang in y caus a chang in x. a chang in x caus a chang in y and a chang in y caus a chang in x. x and y are correlated.
s is a python string. what doe the function s.upper() return answers: a copi of s with each alphabet charact convert to the upper case. a refer to s where each alphabet charact has been convert to the upper case and all other charact removed. a copi of s with each alphabet charact convert to the upper case and all other charact removed. a refer to s where each alphabet charact has been convert to the upper case. a copi of s with each alphabet charact convert to the upper case.
which statement convert a string s into a list of uniqu vowel answer: list(set(c for c in s if c in "aouieaouie")) [c for c in s if c in "aouieaouie"] set(c for c in s if c.isvowel()) list([c for c in s if c in "aouieaouie"]) list(set(c for c in s if c in "aouieaouie"))
infil is an open file of an unknown size. which function can be safe use to read from this file (check all that apply.) answers: infile.read(100) infile.readline() infile.readlines() infile.read() infile.read(100) infile.readline()
bob attempt to download a data file use the follolw statement: data = urllib.request.urlopen ("http://foo.bar/foobar.tgz") however, the url http://foo.bar/foobar.tgz is no longer valid. what will be the valu of the variabl data after the execut of the statement answers: the statement rais an exception. the variabl data doe not exist. the valu of data is none. the valu of data is unknown. the valu of data is an empti string. the statement rais an exception. the variabl data doe not exist.
which regular expression(s) correct describe(s) north american zip code (check all that apply.) answers: r'[0-5]{9} r'[0-9]+ r'[0-9]{5} r'\d{5} r'[0-9]{5} r'\d{5}
which symbol shall be insert instead of the # sign in the follow regular express so that it correct match a human name with an option generat titl (such as jr or sr). r'[a-za-z]+\s[a-za-z]+(\s+(jr\. sr\. ii iii))# answers: + $ * 
what is the best extern storag at for each of the follow python object rich text with emphas words, sections, paragraphs, etc., intend for human users. a two-dimension list of unord float point numbers, intend for human users. a set of dictionaries, intend for anoth python program. all answer choic a. html b. pickl c. csv a. html c. csv b. pickl
alic use beautifulsoup to process an html document. she expect that the tag , that defin a certain divis of the document ( ... ), has an attribut class, and would like to extract the class name. the divis tag is in the variabl fragment. which express correct obtain the class name and store it in the variabl c answers: c=fragment.get("class") c=div["class"] if fragment.has_attr("class"): c=fragment["class"] fragment.find(div="class") if fragment.has_attr("class"): c=fragment["class"]
bob use beautifulsoup to process a well- ed html document. the soup is store in the variabl doc. which python express calcul the first top-level header of the document, repres as a human-read string note: incidentally, this question has two correct answers. select ani correct answer to get full credit. answers: doc.body.h1() doc.body.h1 doc.find_all("h1") doc.h1 doc.body.h1
what is the way to read a csv file without the header row use the standard csv reader answers: read the first line from the file use readline() and then construct a csv reader that read from the file. pass the option nrows=1 to the reader. pass the option header=fals to the reader. read the whole file as a list of strings, remov the first string, and construct a csv reader that read from the list of the remain strings. read the first line from the file use readline() and then construct a csv reader that read from the file.
what symbol can be use as delimit in a csv file answers: ani ascii symbol, as long as it is not use in ani dataitem store in the file. onli commas. ani ascii character. commas, tabs, colons, and vertic bar ani ascii character.
what is data serial answers: the process of assign a serial number to an object. the process of convert an object into a stream of byte in order to store the object to a file. the process of read a stream of byte from a file in order to convert them into an object. the process of store a stream of byte to a file. the process of convert an object into a stream of byte in order to store the object to a file.
the follow html fragment has been convert into a beautifulsoup and store in the variabl soup: take me to the moon! match the python express relat to the soup and their values. all answer choic a. "http://moon.ss" b. " take me to the moon! " c. "take me to the moon!" d. "a" d. "a" b. " take me to the moon! " a. "http://moon.ss" c. "take me to the moon!"
at least what level of data analysi is need to make the follow statement: "base on the observ of 20 student take cmpsc-310, we conclud that 20% of the world popul are female." answers: descriptive. inferential. predictive. exploratory. inferential.
which python oper is use to find the word which are in one set but not in anoth answers: - & ^ ^
whi is it necessari to elimin stopword from a text befor analyz it answers: stopword consider slow down text analysi tasks. stopword make certain text process function crash. stopword make text look more similar that they actual are. stopword are differ in differ languages. stopword make text look more similar that they actual are.
what is the motiv for precompil regular express answers: precompil regular express match faster. there are no advantages, it a pure aesthet choice. precompil regular express are more accurate. precompil regular express match faster.
how mani english name list in the name corpus are also common english word list in the word corpus hint: treat the corpora as set of words. answers: ~6000 ~400 ~800 ~1800 ~1800
what is the differ between the porter and lancast stemmer answers: the lancast stemmer is more aggress than the porter stemmer. (in produc shorter stems.) both stemmer are veri similar, there are no major differences. the porter stemmer, unlik the lancast stemmer, remov prefix as well as endings. the porter stemmer, unlik the lancast stemmer, look up stem on wordnet. the lancast stemmer is more aggress than the porter stemmer. (in produc shorter stems.)
how mani charact doe the follow string have r" \n" answers: answers: 5 4 3 2 4
which regular express match ani posit decim integ number answers: \d*[1-9]\d* [0-9]+ \d+ \d* \d*[1-9]\d*
let corpus be a custom-mad word corpus. what doe the method corpus.raw() return answers: a string consist of all the word in the corpus separ by white spaces. a non-human-read (binary) represent of all the word in the corpus. a list of all the word in the corpus. a string consist of all the word in the corpus separ by line breaks. a string consist of all the word in the corpus separ by line breaks.
what is the second most frequent part of speech, as report by the nltk pos tagger, among the first 10,000 common english word from the word corpus hint: use a counter! answers: proper noun. adverb. common noun. adjective. adject
which of the follow word is probabl an entiti answers: vandal actual strong philadelphia philadelphia
volume, velocity, varieti 3 v for datasci
data driven, expert driven, data + expert driven, descript 4 knowledg level
expert driven comput encod expert opinion and assumptions.
determinist summar data descripit
data driven induc new rule or ula from data.
data + expert driven combin deduct and induct reason to determin caus from measur effect
data mine integr theori and heurist
data mine focus on the entir process of knowledg discovery, includ data cleaning, learning, and integr and visual of s
machin learn focus on improv per anc of a learn agent
machin learn also look at real-tim learn and robot - area not part of data mine
statist more theory-bas and more focus on test hypothes
phase, generic task, special task , process instanc life cycl industri standard process for data mine phase
phase each phase consist of sever second-level generic tasks.
generic task task are intend to be as complet and stabl as possible.
special task task describ how action in the generic task should be carri out in certain specif situations.
process instanc a record of the actions, decisions, and s of an actual data mine engagement.
associations, classification, clustering, deviat detection, estimation, link analysis, summarization, visual major data mine task
associ a & b & c occur frequent (i.e. market basket analysis)
classif predict an item class
cluster find cluster in data
deviat detect find chang
estim predict a continu valu
link analysi find relationship
summar describ a group
visual to facilit human discoveri
big data more real time traditon data not suit well for newer app big data come from web traditon come from thing on paper or phone call tradit data vs big data
data island spreadsheet and low volum db for recordkeep
data warehous support bi and reporting, but restrict robust analysi
analyt sandbox enabl high per anc analyt use in-db processing.
unstructured, quasi-structured, semi-structured, structur big data characteristics: data structur
structur tabular data can be store in sql databas in row and column
semi-structur self describ due to exist of marker and schema is not explicit, must be explor or known beforehand
unstructur come from in ation not organ or easili interpret by tradit databas
big data is data whose scale, diversity, and complex requir new architecture, new tools, techniques, algorithms, and analyt to manag it and extract valu and hidden knowledg from it
real time, valu of big data
architecture, algorithms, techniques, and expert are need challeng of big data
simpl random sampling, stratifi sampling, cluster sampling, sampl method we learn
cluster sampling, divid the popul into group amd sampl those group
stratifi sampl popul get partit into group base on a factor that may influenc the variabl that is be measured.
random sampl a sampl select in such a way that everi element in the popul has a equal probabl of be chosen.
non-prob sampl conveni (accidental) - select on the basi of avail
volum size of big data and it increas exponenti
varieti various ats, types, and chang data structur text, numerical, images, audio, video, sequences, time series, social media data, multi-dim arrays, etc
veloc data is begin generat fast and need to be process fast data flow is continu and massive, batch process doe not work onlin data analyt
verac uncertainti due to data inconsit
variabl 5th v
variabl refer to data whose mean is constant changing.
oltp onlin transact process (dbmss)
olap onlin analyt process (data warehousing)
rtap real-tim analyt process (big data architectur & technology)
kernel trick use the kernel trick to find separ hyperplan in higher dimension space to solv a nonlinear problem use an svm, we tran the train data onto a higher dimension featur space via a map function use the kernel trick to find separ hyperplan in higher dimension space and train a linear svm model to classifi the data in this new featur space. then we can use the same map function use the kernel trick to find separ hyperplan in higher dimension space to tran new, unseen data to classifi it use the linear svm model. however, one problem with this map approach is that the construct of the new featur is comput veri expensive, especi if we are deal with high-dimension data. this is where the so-cal kernel trick come into play. although we didn't go into much detail about how to solv the quadrat program task to train an svm, in practic all we need is to replac the dot product use the kernel trick to find separ hyperplan in higher dimension space by use the kernel trick to find separ hyperplan in higher dimension space. in order to save the expens step of calcul this dot product between two point explicitly, we defin a so-cal kernel function: use the kernel trick to find separ hyperplan in higher dimension space. one of the most wide use kernel is the radial basi function kernel (rbf kernel) or gaussian kernel: the trick is to choos a tran ation so that the kernel can be comput without actual comput the tran ation. replac the dot-product function with a new function that return what the dot product would have been if the data had first been tran ed to a higher dimension space. usual done use the radial-basi function
radial basi function kernel (rbf kernel) use for kernel trick in svms
gaussian kernel use for kernel trick in svms
type of kernel for kernel trick fisher kernel graph kernel kernel smoother polynomi kernel rbf kernel string kernel
what are kernel a kernel is a similar function. it is a function that you, as the domain expert, provid to a machin learn algorithm. it take two input and spit out how similar they are. kernel offer an alternative. instead of defin a slew of features, you defin a singl kernel function to comput similar between images. you provid this kernel, togeth with the imag and label to the learn algorithm, and out come a classifier.
svm - strength and week ...
decis tree classifi - strength and week strength: 1) featur scale is not a requir for decis tree algorithm 2) can visual the dt (use graphviz) weakness: 1) we have to be care sinc the deeper the decis tree, the more complex the decis boundari becomes, which can easili in overfit note: use random forest allow combin weak learner with strong learner
in ation gain (ig) in ation gain is simpli the differ between the impur of the parent node and the sum of the child node impurities—th lower the impur of the child nodes, the larger the in ation gain
gini index ...
entropi ...
classif error this is a use criterion for prune but not recommend for grow a decis tree, sinc it is less sensit to chang in the class probabl of the nodes.
z ,,,
parametr versus nonparametr model machin learn algorithm can be group into parametr and nonparametr models. use parametr models, we estim paramet from the train dataset to learn a function that can classifi new data point without requir the origin train dataset anymore. typic exampl of parametr model are the perceptron, logist regression, and the linear svm. in contrast, nonparametr model can't be character by a fix set of parameters, and the number of paramet grow with the train data. two exampl of nonparametr model that we have seen so far are the decis tree classifier/random forest and the kernel svm. knn belong to a subcategori of nonparametr model that is describ as instance-bas learning. model base on instance-bas learn are character by memor the train dataset, and lazi learn is a special case of instance-bas learn that is associ with no (zero) cost dure the learn process.
nonparametr nonparametr model can't be character by a fix set of parameters, and the number of paramet grow with the train data. two exampl of nonparametr model that we have seen so far are the decis tree classifier/random forest and the kernel svm.
parametr use parametr models, we estim paramet from the train dataset to learn a function that can classifi new data point without requir the origin train dataset anymore. typic exampl of parametr model are the perceptron, logist regression, and the linear svm.
free paramet ...
slack variabl ...
soft-margin classif ...
ovr techniqu ...
l2 regular l2 regular (sometim also call l2 shrinkag or weight decay)
collinear collinear (high correl among features), filter out nois from data, and eventu prevent overfit
partial deriv of the log-likelihood function ...
optim algorithm optim algorithm such as gradient ascent
log-likelihood ...
likelihood ...
sigmoid ...
tbd: cost function becom differenti ...
z ...
quantize ...
the mean of each featur is center at valu 0 and the featur column has a standard deviat of 1 ...
standard ...
featur scale featur scale such as standard
imput miss dat ...
group and grade ...
geometr probabilistic, and logic , ...
z ...
subgroup discoveri ...
scatterplot matrix ...
exploratori data analysi (eda) ...
if we stop at this point and feed the array to our classifi we will make one of the most common mistak in deal with categor data. can you spot the problem although the color valu don't come in ani particular order, a learn algorithm will now assum that green is larger than blue, and red is larger than green , ...
idf invers document frequenc
term frequenc ...
term frequency-invers document frequenc ...
term frequency-invers document frequenc ...
raw term frequenc ...
tbd: convert categor data such as text or words, into a numer , ...
build featur vector from text ...
test set is not to be use for model selection; it onli purpos is to report an unbias estim of the general per anc of a classifi system ...
majorityvotingclassifi ...
machin learn project can be divid into five distinct iti shown as follows:defin the object and specificationprepar and explor the datamodel buildingimplementationtestingdeploy , ...
address overfit and underfit with valid curv ...
valid curv ...
diagnos bias and varianc problem with learn curv when a model has both low train and cross-valid accuracy, which indic that it underfit the train data. common way to address this issu are to 1) increas the number of paramet of the model, for example, by collect or construct addit features, or 2) by decreas the degre of regularization, for example, in svm or logist regress classifiers. when a model suffer from high variance, which is indic by the larg gap between the train and cross-valid accuracy. to address this problem of overfitting, 1) we can collect more train data or reduc the complex of the model, for example, by increas the regular parameter; 2) for unregular models, it can also help to decreas the number of featur via featur select or featur extract (compress data via dimension reduction). 3) collect more train data decreas the chanc of overfitting. however, it may not alway help, for example, when the train data is extrem noisi or the model is alreadi veri close to optimal.
increas the regular parameter; for unregular model ...
high varianc which is indic by the larg gap between the train and cross-valid accuraci , ...
decreas the degre of regular ...
increas the number of paramet ...
valid curv ...
learn curv ...
stratifi k-fold cross-valid ...
leave-one-out (loo) cross-valid ...
evalu predict model ...
fine-tun machin learn model ...
diagnos the common problem ...
unbias estim of a model per anc ...
ztbd: disadvantag of the holdout method is that the per anc estim is sensit to how we partit the train set into the train and valid subset ...
k-fold cross-valid ...
holdout cross-valid ...
tbd: pipelin combin tran er and estim in a pipelin
preprocess techniqu ...
varianc measur ...
node impuriti ...
pca pca attempt to find the orthogon compon axe of maximum varianc in a dataset. kernel princip compon analysi
linear discrimin analysi ...
recurs backward elimin ...
sequenti backward select (sbs) ...
featur select ...
featur extract ...
sequenti featur select ...
one-vs-rest (ovr) ...
regular collect more train dataintroduc a penalti for complex via regularizationchoos a simpler model with fewer parametersreduc the dimension of the data l1 regular can be understood as a techniqu for featur selection.
normal ...
featur scale ...
dummi featur ...
categor featur ...
one-hot encod ...
labelencod ...
encod class label ...
numer featur ...
nomin featur ...
ordin featur ...
categor data ...
estim api ...
imput categor featur valu ...
median or most_frequ ...
mean imput ...
accuraci execut time, memori usage, throughput, tuning, and adapt
tbd: crossov or breed ...
mutat ...
generat ...
elit ...
local minimum ...
global minimum ...
random-restart hill climb ...
cost function cost function is the key to solv ani problem use optim
jaccard coeffici or manhattan  
.  
valu mean more similar." ...
collabor filter the term collabor filter was first use by david goldberg at xerox parc in 1992 in a paper call use collabor filter to weav an in ation tapestry. he design a system call tapestri that allow peopl to annot document as either interest or uninterest and use this in ation to filter document for other people.ther are now hundr of web site that employ some sort of collabor filter algorithm for movies, music, books, dating, shopping, other web sites, podcasts, articles, and even jokes.
use by david goldberg at xerox parc in 1992 in a paper call ""use  
this in ation to filter document for other people.ther are now hundr of web site that employ some sort of  
three class of metrics: central volatility, bumpiness. , ...
correlogram trends, chang point, normal and period , ...
six sigma approxim solutions, the 80/20 rule, cross-validation, design of experiments, modern pattern recognition, lift metrics, third-parti data, mont carlo simulations, or the life cycl of data scienc project , ...
r python (or perl), excel, sql, graphic (visualization), ftp, basic unix command (sort, grep, head, tail, the pipe and redirect operators, cat, cron jobs, and so on) , ...
random variabl probability, mean, variance, percentiles, experiment design, cross-validation, good of fit, and robust statist , ...
the rm4es (research method four elements) is a good framework to summar machin learn compon and processes. the rm4es include:equation: equat are use to repres the model for our researchestimation: estim is the link between equat (models) and the data use for our researchevaluation: evalu need to be per ed to assess the fit between model and the dataexplanation: explan is the link between equat (models) and our research purposes. how we explain our research s often depend on our research purpos and also on the subject we are studi ...
load data with packag like rodbc or rmysqlmanipul data, with packag like stringr or lubridatevisu data, with packag like ggplot2 or leafletmodel data, with packag like random forest or survivalreport s, with packag like shini or markdown , ...
adapt system ...
inter e system ...
distribut systems. ...
from mud to structure. ...
layer pipe and filters, blackboard, broker, model-view-controller, presentation-abstraction-control, microkernel, and reflect , ...
convolut net ...
structur svms ...
structur perceptron ...
crf (condit random fields) ...
structur predict ...
iclr which stand for the intern confer on learn represent , ...
ai or machin learn the main confer are nip and icml, and also confer like ai stats, uai, and kdd, which is more data scienceâ€"ori , ...
stochast gradient descent ...
algorithm a seri of repeat step for carri out a certain type of task with data
angular js an open-sourc javascript librari maintain by googl and the community. let you creat singl web page applic to s
artifici intellig the abil to have machin act with appar intelligence. can be through symbol logic or statist analysi
backpropag an algorithm for iter adjust the weight use in a neural network system. often use to implement gradient descent.
bay theorem an equat for calcul the probabl that someth is true if someth potenti relat is true. p(a b) = p(b a) * p(a) / p(b)
good for situat where you need to know the amount of fals posit (diseases)  
bayesian network graph that compact repres the relationship between random variabl for a given problem
bias in machin learn when a learner consist learn the same thing wrong
big data work with larg dataset that usual requir distribut storag
binomi distribut a distribut of independ event with two mutual exclus possibl outcom a fix number of trial and a constant probabl of success. discret probabl distribution. graph use histograms.
centroid center of a cluster
chi-squar test statist test of whether two categor variabl are independ
classif the identif of two or more discret categori for item classic machin learn task. spam or ham. movi genres. supervis learning.
cluster unsupervis learn techniqu for divid data into group base on an algorithm
coeffici a number or algebra symbol prefix as a multipli to a variabl or unknown quantiti (slope in line equation)
comput linguist also call natur languag process (nlp) convert text of spoken languag into structur data to extract valuabl in ation
confid interv a rang specifi around an estim to indic margin of error combin with a probabl that a valu will fall in that rang
continu variabl a variabl whose valu can be ani of infinit valu
correl coeffici measur of how close two variabl correlate. rang from -1 to 1
correl the degre of relat correspond between two variabl
covari a measur of the relationship between two variabl whose valu are observ at the same time
cross-valid set of techniqu that divid up data into train set and test set usual 80-20. train set are given the correct categor and an algorithm is creat
csv comma separ valu common data file type
d3 data driven document a javascript librari that eas the creation of inter e visual embed in web page
data engin a specialist in data wrangl they build infrastructur for real tangibl analysis. run etl
data mine the use of comput to analyz larg data set to look for pattern that let peopl make busi decis
data scienc the abil to extract knowledg and insight from larg and complex data set
data structur a particular arrang of unit of data such as an array or a tree
data wrangl aka data mung the convers of data use script languag to make it easi to work with
dataframe.boxplot() make a boxplot use matplotlib
dataframe.groupby() split data into differ group depend on the variabl you choos
dataframe.head(n = 5) return first n row of a datafram
dataframe.hist() make a histogram use matplotlib
dataframe['a'].count() count the number of valu in column a get the number of row
dataframe['a'].max() return largest valu in column a
dataframe['a'].mean() return averag of valu in column a
dataframe['a'].sum() add up all valu in column a
decis tree use a tree structur to repres a number of possibl decis path and an outcom for each path
deep learn a multi-level algorithm that gradual identifi thing at higher level of abstract
depend variabl the valu depend on the valu of the independ variabl
dimens reduct use pca or a similar method to find the smallest subset of dimens that captur the most variat
discret variabl a variabl whose potenti valu must be one of a specif number of valu
econometr the use of mathemat and statist method in the field of econom to verifi and develop econom theori
etl extract tran
featur engin use featur to come up with a good model through iter
featur a machin learn express for a piec of measur in ation
gate general architectur for text engineering; open sourc java-bas framework for natur languag process task
gaussian distribut a probabl distribut that when graph is a symmetr bell curv with the mean at the center
gradient boost machin learn techniqu for regress and classification. produc a predict model in the of an ensembl of weak predict model typic decis trees; stage-wis
gradient descent optim algorithm for find the input to a function that produc the optim value; iter
histogram a graphic represent of the distribut of a set of numer data usual a vertic bar graph
hyperplan sub space of one dimens less than it ambient space for 3-d space
import matplotlib.pyplot as plt python modul use for graph data
k mean cluster data mine algorithm to cluster classifi
k-nearest neighbor machin learn algorithm that classifi thing base on their similar to nearbi neighbors. pick the number of neighbor k
latent variabl variabl that are not direct observ but infer from other variabl that are observ
least squar smallest sum of the squar distanc to the data from the line
lift compar the frequenc of an observ pattern with how often you'd expect to see that pattern by chanc near 1 is chanc
linear algebra math that deal with vector space and oper on them such as addit and subtract
linear regress techniqu that look for a linear relationship between two variabl use the line with the least squar
logarithm a quantiti repres the power to which a fix number base
logist regress model where the depend variabl is categorical. estim the probabl of a relationship between a categor variabl and one or more independ variabl
machin learn the use of data-driven algorithm that per better as they have more data to work with; general use cross-valid
matrix two dimension array of valu arrang in row and column
mean absolut error the averag error of all predict valu when compar with observ valu
mean squar error the averag of the squar of all the error when compar predict valu with observ valu
mont carlo method the use of random generat number as part of an algorithm
move averag the mean of time seri data from sever consecut periods; continu updat
n-gram the analysi of sequenc of n items; usual word in natur languag
naiv bay classifi a famili of algorithm that consid everi featur as independ of ani other featur
neural network a robust function that take an arbitrari set of input and fit it to an arbitrari set of output that are binary; uniqu becaus of hidden layer of weight function
object function use to find the optim of an objective; use to solv an optim problem
overfit a model that is too tie to a train set and will not per well on test data
p-valu the probabl under the assumpt of no differ (bill hypothesis)
pagerank an algorithm that determin the import of someth typic to rank it in a list of search s
panda a python librari for data manipul
perceptron the simplest neural network approxim a singl neuron with n binari inputs. it comput a weight sum of the input and "fires" if that weight sum is zero or greater
perl an older script languag with root in pre-unix systems. popular for text process like data cleanup and enhanc
pivot tabl quick summar long list of data without requir the write of ula or copi cells. can be arrang dynam or pivot
poisson distribut a distribut of independ event use to predict the probabl of an event occur in a set time or place
predict analyt the analysi of data to predict futur event usual to aid in busi plan
predict model the develop of drastic model to predict futur event
pca princip compon analysi statist procedur that use an orthogon tran ation to convert a set of observ of possibl correl variabl into a set of valu linear uncorrel variabl call princip compon
prior distribut model the mani plausibl valu of the unknown quantiti to be estim in bay interfer
probabl distribut list of all possibl distinct outcom and their probabl of occur sum is equal to 1
python program languag that is use in data science. easi to use and power for advanc user by use special librari
quartil data set divid into 4 group 25% of data in each
r an open-sourc program languag and environ for statist comput and graph generat
random forest an algorithm use for regress or classif that use a collect of tree data structur tree "vote" on the best model
regress fit a model to data
reinforc learn a class of machin learn algorithm which do not have specif goal but is continu monitor if it do well or not
root mean squar error (rmse) squar root of mean squar error. more popular becaus it give a number that is easier to understand in the unit of the origin observ
rubi a script languag that can be use for data scienc not as popular as python
s curv a pattern in which someth is adopt slowli gain popular quick
sas a commerci drastic softwar suit that includ a program languag
scalar quantiti that has magnitud but no direct in space such as volum or temperatur
serial correl a pattern where valu in a seri are correl can shift time seri by an interv call a lag and then comput the correl of the shift and origin seri
spatiotempor data time seri data that also includ geograph identifi such as latitude-longitud pair
z the iso standard queri languag for relat databas
standard deviat the squar root of the varianc common way to indic how differ a particular measur is from the mean
standard normal distribut a normal distribut with a mean of 0 and a standard deviat of 1
standard score tran ed raw score into unit of standard deviat abov or below the mean
stratifi sampl popul is divid into homogen group call strata
supervis learn a type of machin learn algorithm in which a system is taught to classifi input into specif known class
support vector machin supervis learn classif tool that seek a divid hyperplan for ani number of dimens can be use for regress or classif
t-distribut variat of normal distribut that account for the fact that you'r onli use a sampl of valu not all of them
three v volum veloc
what make big dataset impract ...
time seri data a sequenc of measur of some quantiti taken at differ time often at equal space interv
unstructur in ation manag architectur (uima) framework develop at ibm to analyz unstructur in ation especi natur languag
unsupervis learn class of machin learn algorithm design to identifi group of data without know in advanc what the group will be
varianc how much a list of number vari from the averag the averag of the squar differ of each number from the mean
vector space ...
vector ...
instance-bas learn knn belong to a subcategori of nonparametr model that is describ as instance-bas learning. model base on instance-bas learn are character by memor the train dataset, and lazi learn is a special case of instance-bas learn that is associ with no (zero) cost dure the learn process.
specif the propens of a test/experi to reduc fals posit (reject type 1 errors) the p in "specificity" cue reject fals posit
sensit reduc fals negat the n in "sensitivity" cue reject fals negat
precis the propens of test outcom to lie near/clos to each other: to have low varianc precis mean consist
accuraci the propens of test outcom to lie near/clos to the target: to have low varianc from the target accuraci mean be on-target
type 1 error synonym for fals positive. reduc by test specif
type 2 error synonym for fals negat reduc by test sensit
alpha level synonym for signific level the accept chanc of fals posit
p valu the probabl of the null-hypothesi - usual the probabl that a statist valu s by chance.
confid level  
decreas bias accuraci
is independ of bias precis
low varianc high precis
alpha signific synonym for signific level the accept chanc of fals posit
histogram plot the relat frequenc of an interv
box plot plot the center, quartiles, and outlier of a variable.
bar chart plot a scalar numer valu
interv variabl  
ordin variabl take valu that can be order
ratio variabl take valu have a fixed-zero point that allow meaning divis
categor variabl  
nomin variabl  
numer variabl  
spearman rho  
cramer v  
chi squar  
associ test between ordin variabl  
pearson r test a statist that measur correl between two properti of a set of samples: r - 1 impli correl r - 0 impli no correl r- -1 impli anti-correl
conting tabl  
bivari analysi  
outlier a point which fall more than 1.5 time the interquartil rang abov the third quartil or below the first quartile.
multinomi distribut  
beta distribut  
bernoulli distribut  
dirichlet distribut  
histogram distribut  
surviv  
odd p(y) / p(-y) p(i x) / p(-i x)
likelihood p(i x) / p(i -x)
probabl p(y) repres the percent of popul y in which y occur
mean a statist that measur ..
median a statist that measur ..
mode a statist that measur ..
standard deviat a statist that measur dispers
varianc a statist that measur ..
moment a statist that measur ..
pdf versus probabl  
parametr distribut a famili of distribut the exact member of which is determin by
uni distribut versus discret uni distribut unbias sampl from an interval, e.g. [2..7] versus unbias sampl from a categorical, e.g. {2,3,4,5,6,7}
maximum likelihood estim (see estimateddistribution)
good of fit (see distributionfittest)
regress  
regress model versus distribut model  
quantil  
covari  
correl ... see pearson rho
entropi a statist that measur ..
modern versus classic machin learn (see paclet: guide/scientificdataanalysis)
data set terminolog that refer to a sampl along with some statist values.
tableua  
classif versus regress  
signific level the cut-off valu for a p-valu below which the null-hypothesi is reject and the p-valu statist is accepted.
contrast p-valu and alpha-valu both are probabilities. the p-valu is the probabl the chosen statist occur by chance. the alpha valu is the decis point below which we accept the statist and toler chance. ( http://bit.ly/1qnc7tw )
sampl distribut the distribut of a statist that approxim a popul parameter. e.g. the distribut of averag of sampl
statist either a) a statist function that comput some measur of a sampl of a popul (mean, entropy), or b) the valu of such a function on a particular sampl (e.g. mean age of 1000 us citizen is 32)
paramet a synonym for statist paramet and popul parameter. the (usual unknown) of appli a statist function (see statistic) to the entir population. (e.g. mean age of (all) us citizen is 41) ( http://bit.ly/1qne7vb )
statist infer distribut over observ count of each possibl categori in a set of categor distribut observ
two branch of statist paramet estim hypothesi test
interquartil rang q3 - q1 and contain 50% of the probabl mass
quartil a set of valu repres 25% of the probabl mass.
q1 contain 25% of the probabl mass
q3 contain 75% of the probabl mass
statist model a statist model is a class of mathemat model, which embodi a set of assumpt concern the generat of some sampl data, and similar data from a larger population. a statist model represents, often in consider ideal , the data-gener process. the assumpt embodi by a statist model describ a set of probabl distributions, some of which are assum to adequ approxim the distribut from which a particular data set is sampled. the probabl distribut inher in statist model are what distinguish statist model from other, non-statistical, mathemat models. a statist model is usual specifi by mathemat equat that relat one or more random variabl and possibl other non-random variables. as such, "a model is a al represent of a theory". all statist hypothesi test and all statist estim are deriv from statist models. more generally, statist model are part of the foundat of statist inference.
data scienc data scienc is an interdisciplinari field about process and system to extract knowledg or insight from data in various s, either structur or unstructured,[1][2] which is a continu of some of the data analysi field such as statistics, machin learning, data mining, and predict analytics,[3] similar to knowledg discoveri in databas (kdd). data scienc employ techniqu and theori drawn from mani field within the broad area of mathematics, statistics, oper research,[4] in ation science, and comput science, includ signal processing, probabl models, machin learning, statist learning, data mining, database, data engineering, pattern recognit and learning, visualization, predict analytics, uncertainti modeling, data warehousing, data compression, comput programming, artifici intelligence, and high per anc computing. method that scale to big data are of particular interest in data science, although the disciplin is not general consid to be restrict to such big data, and big data technolog are often focus on organ and preprocess the data instead of analysis. the develop of machin learn has enhanc the growth and import of data science. data scienc affect academ and appli research in mani domains, includ machin translation, speech recognition, robotics, search engines, digit economy, but also the biolog sciences, medic in atics, health care, social scienc and the humanities. it heavili influenc economics, busi and finance. from the busi perspective, data scienc is an integr part of competit intelligence, a newli emerg field that encompass a number of ities, such as data mine and data analysis.[5]
data scientist data scientist use their data and analyt abil to find and interpret rich data sources; manag larg amount of data despit hardware, software, and bandwidth constraints; merg data sources; ensur consist of datasets; creat visual to aid in understand data; build mathemat model use the data; and present and communic the data insights/findings. they are often expect to produc answer in day rather than months, work by exploratori analysi and rapid iteration, and to produc and present s with dashboard ( s of current values) rather than papers/reports, as statistician normal do.[6]
data vizual data visual or data visualis is view by mani disciplin as a modern equival of visual communication. it involv the creation and studi of the visual represent of data, mean "in ation that has been abstract in some schemat , includ attribut or variabl for the unit of in ation".[1] a primari goal of data visual is to communic in ation clear and effici via statist graphics, plot and in ation graphics. numer data may be encod use dots, lines, or bars, to visual communic a quantit message.[2] effect visual help user analyz and reason about data and evidence. it make complex data more accessible, understand and usable. user may have particular analyt tasks, such as make comparison or understand causality, and the design principl of the graphic (i.e., show comparison or show causality) follow the task. tabl are general use where user will look up a specif measurement, while chart of various type are use to show pattern or relationship in the data for one or more variables. data visual is both an art and a science. it is view as a branch of descript statist by some, but also as a ground theori develop tool by others. the rate at which data is generat has increased. data creat by internet iti and an expand number of sensor in the environment, such as satellites, are refer to as "big data". processing, analyz and communic this data present a varieti of ethic and analyt challeng for data visualization. the field of data scienc and practition call data scientist have emerg to help address this challenge.[3]
exploratori data analysi in statistics, exploratori data analysi (eda) is an approach to analyz data set to summar their main characteristics, often with visual methods. a statist model can be use or not, but primarili eda is for see what the data can tell us beyond the al model or hypothesi test task. exploratori data analysi was promot by john tukey to encourag statistician to explor the data, and possibl ulat hypothes that could lead to new data collect and experiments. eda is differ from initi data analysi (ida),[1] which focus more narrowli on check assumpt requir for model fit and hypothesi testing, and handl miss valu and make tran ation of variabl as needed. eda encompass ida.
big data big data is a term for data set that are so larg or complex that tradit data process applic are inadequate. challeng includ analysis, capture, data curation, search, sharing, storage, transfer, visualization, querying, updat and in ation privacy. the term often refer simpli to the use of predict analytics, user behavior analytics, or certain other advanc data analyt method that extract valu from data, and seldom to a particular size of data set.[2] accuraci in big data may lead to more confid decis making, and better decis can in greater oper efficiency, cost reduct and reduc risk. analysi of data set can find new correl to "spot busi trends, prevent diseases, combat crime and so on."[3] scientists, busi executives, practition of medicine, advertis and govern alik regular meet difficulti with larg data set in area includ internet search, finance, urban in atics, and busi in atics. scientist encount limit in e-scienc work, includ meteorology, genomics,[4] connectomics, complex physic simulations, biolog and environment research.[5]
data mine data mine is an interdisciplinari subfield of comput science.[1][2][3] it is the comput process of discov pattern in larg data set involv method at the intersect of artifici intelligence, machin learning, statistics, and databas systems.[1] the overal goal of the data mine process is to extract in ation from a data set and tran it into an understand structur for further use.[1] asid from the raw analysi step, it involv databas and data manag aspects, data pre-processing, model and infer considerations, interesting metrics, complex considerations, post-process of discov structures, visualization, and onlin updating.[1] data mine is the analysi step of the "knowledg discoveri in databases" process, or kdd.[4] the term is a misnomer, becaus the goal is the extract of pattern and knowledg from larg amount of data, not the extract (mining) of data itself.[5] it also is a buzzword[6] and is frequent appli to ani of large-scal data or in ation process (collection, extraction, warehousing, analysis, and statistics) as well as ani applic of comput decis support system, includ artifici intelligence, machin learning, and busi intelligence. the book data mining: practic machin learn tool and techniqu with java[7] (which cover most machin learn material) was origin to be name just practic machin learning, and the term data mine was onli ad for market reasons.[8] often the more general term (larg scale) data analysi and analyt - or, when refer to actual methods, artifici intellig and machin learn - are more appropriate. the actual data mine task is the automat or semi-automat analysi of larg quantiti of data to extract previous unknown, interest pattern such as group of data record (cluster analysis), unusu record (anomali detection), and depend (associ rule mining). this usual involv use databas techniqu such as spatial indices. these pattern can then be seen as a kind of summari of the input data, and may be use in further analysi or, for example, in machin learn and predict analytics. for example, the data mine step might identifi multipl group in the data, which can then be use to obtain more accur predict s by a decis support system. neither the data collection, data preparation, nor interpret and report is part of the data mine step, but do belong to the overal kdd process as addit steps. the relat term data dredging, data fishing, and data snoop refer to the use of data mine method to sampl part of a larger popul data set that are (or may be) too small for reliabl statist infer to be made about the valid of ani pattern discovered. these method can, however, be use in creat new hypothes to test against the larger data populations.
analyt analyt is the discovery, interpretation, and communic of meaning pattern in data. especi valuabl in area rich with record in ation, analyt reli on the simultan applic of statistics, comput program and oper research to quantifi per ance. analyt often favor data visual to communic insight. organ may appli analyt to busi data to describe, predict, and improv busi per ance. specifically, area within analyt includ predict analytics, prescript analytics, enterpris decis management, retail analytics, store assort and stock-keep unit optimization, market optim and market mix modeling, web analytics, sale forc size and optimization, price and promot modeling, predict science, credit risk analysis, and fraud analytics. sinc analyt can requir extens comput (see big data), the algorithm and softwar use for analyt har the most current method in comput science, statistics, and mathematics.[1] analyt is multidisciplinary. there is extens use of mathemat and statistics, the use of descript techniqu and predict model to gain valuabl knowledg from data—data analysis. the insight from data are use to recommend action or to guid decis make root in busi context. thus, analyt is not so much concern with individu analys or analysi steps, but with the entir methodology. there is a pronounc tendenc to use the term analyt in busi set e.g. text analyt vs. the more generic text mine to emphas this broader perspective.[cit needed]. there is an increas use of the term advanc analytics,[cit needed] typic use to describ the technic aspect of analytics, especi in the emerg field such as the use of machin learn techniqu like neural network to do predict modeling.
in ation extract in ation extract (ie) is the task of automat extract structur in ation from unstructur and/or semi-structur machine-read documents. in most of the case this iti concern process human languag text by mean of natur languag process (nlp). recent iti in multimedia document process like automat annot and content extract out of images/audio/video could be seen as in ation extraction. due to the difficulti of the problem, current approach to ie focus on narrowli restrict domains. an exampl is the extract from news wire report of corpor mergers, such as denot by the al relation: a broad goal of ie is to allow comput to be done on the previous unstructur data. a more specif goal is to allow logic reason to draw infer base on the logic content of the input data. structur data is semant well-defin data from a chosen target domain, interpret with respect to categori and context. in ation extract is the part of a greater puzzl which deal with the problem of devis automat method for text management, beyond it transmission, storag and . the disciplin of in ation retriev (ir)[1] has develop automat methods, typic of a statist flavor, for index larg document collect and classifi documents. anoth complementari approach is that of natur languag process (nlp) which has solv the problem of model human languag process with consider success when take into account the magnitud of the task. in term of both difficulti and emphasis, ie deal with task in between both ir and nlp. in term of input, ie assum the exist of a set of document in which each document follow a template, i.e. describ one or more entiti or event in a manner that is similar to those in other document but differ in the details. an example, consid a group of newswir articl on latin american terror with each articl is presum to be base upon one or more terrorist acts. we also defin for ani given ie task a template, which is a(or a set of) case frame(s) to hold the in ation contain in a singl document. for the terror example, a templat would have slot correspond to the perpetrator, victim, and weapon of the terrorist act, and the date on which the event happened. an ie system for this problem is requir to "understand" an attack articl onli enough to find data correspond to the slot in this template.
mece princip the mece principle, pronounc me see', is a group principl for separ a set of item into subset that are mutual exclus and collect exhaustive.[1] the mece principl is use in the busi map process where the optimum arrang of in ation is exhaust and doe not doubl count at ani level of the hierarchy. exampl of mece arrang includ categor peopl by year of birth (assum all year are known). a non-mec exampl would be categor by nationality, becaus nation are neither mutual exclus (some peopl have dual nationality) nor collect exhaust (some peopl have none).
techniqu for analyz quantit data author jonathan koomey has recommend a seri of best practic for understand quantit data. these include: check raw data for anomali prior to per ing your analysis; re-per import calculations, such as verifi column of data that are ula driven; confirm main total are the sum of subtotals; check relationship between number that should be relat in a predict way, such as ratio over time; normal number to make comparison easier, such as analyz amount per person or relat to gdp or as an index valu relat to a base year; break problem into compon part by analyz factor that led to the s, such as dupont analysi of return on equity.[6] for the variabl under examination, analyst typic obtain descript statist for them, such as the mean (average), median, and standard deviation. they may also analyz the distribut of the key variabl to see how the individu valu cluster around the mean. an illustr of the mece principl use for data analysis. the consult at mckinsey and compani name a techniqu for break a quantit problem down into it compon part call the mece principle. each layer can be broken down into it components; each of the sub-compon must be mutual exclus of each other and collect add up to the layer abov them. the relationship is refer to as "mutual exclus and collect exhaustive" or mece. for example, profit by definit can be broken down into total revenu and total cost. in turn, total revenu can be analyz by it components, such as revenu of divis a, b, and c (which are mutual exclus of each other) and should add to the total revenu (collect exhaustive). analyst may use robust statist measur to solv certain analyt problems. hypothesi test is use when a particular hypothesi about the true state of affair is made by the analyst and data is gather to determin whether that state of affair is true or false. for example, the hypothesi might be that "unemploy has no effect on inflation", which relat to an econom concept call the phillip curve. hypothesi test involv consid the likelihood of type i and type ii errors, which relat to whether the data support accept or reject the hypothesis. regress analysi may be use when the analyst is tri to determin the extent to which independ variabl x affect depend variabl y (e.g., "to what extent do chang in the unemploy rate (x) affect the inflat rate (y) "). this is an attempt to model or fit an equat line or curv to the data, such that y is a function of x. necessari condit analysi (nca) may be use when the analyst is tri to determin the extent to which independ variabl x allow variabl y (e.g., "to what extent is a certain unemploy rate (x) necessari for a certain inflat rate (y) "). wherea (multiple) regress analysi use addit logic where each x-variabl can produc the outcom and the x can compens for each other (they are suffici but not necessary), necessari condit analysi (nca) use necess logic, where one or more x-variabl allow the outcom to exist, but may not produc it (they are necessari but not sufficient). each singl necessari condit must be present and compens is not possible.
knowledg represent and reason knowledg represent and reason (kr) is the field of artifici intellig (ai) dedic to repres in ation about the world in a that a comput system can util to solv complex task such as diagnos a medic condit or have a dialog in a natur language. knowledg represent incorpor find from psychology[cit needed] about how human solv problem and repres knowledg in order to design alism that will make complex system easier to design and build. knowledg represent and reason also incorpor find from logic to autom various kind of reasoning, such as the applic of rule or the relat of set and subsets. exampl of knowledg represent alism includ semant nets, system architecture, frames, rules, and ontologies. exampl of autom reason engin includ infer engines, theorem provers, and classifiers.
code in communic and in ation processing, code is a system of rule to convert in ation—such as a letter, word, sound, image, or gesture—into anoth or representation, sometim shorten or secret, for communic through a channel or storag in a medium. an earli exampl is the invent of language, which enabl a person, through speech, to communic what he or she saw, heard, felt, or thought to others. but speech limit the rang of communic to the distanc a voic can carry, and limit the audienc to those present when the speech is uttered. the invent of writing, which convert spoken languag into visual symbols, extend the rang of communic across space and time the process of encod convert in ation from a sourc into symbol for communic or storage. decod is the revers process, convert code symbol back into a that the recipi of that understand time.
data process data process is, generally, "the collect and manipul of item of data to produc meaning in ation."[1] in this sens it can be consid a subset of in ation processing, "the chang (processing) of in ation in ani manner detect by an observer." [note 1] the term data process (dp) has also been use previous to refer to a depart within an organ respons for the oper of data process applications.[2] data process may involv various processes, including: valid - ensur that suppli data is correct and relevant. sort - "arrang item in some sequenc and/or in differ sets." summar - reduc detail data to it main points. aggreg - combin multipl piec of data. analysi - the "collection, organization, analysis, interpret and present of data.". report - list detail or summari data or comput in ation. classif - separ data into various categories.
data set a data set (or dataset) is a collect of data. most common a data set correspond to the content of a singl databas table, or a singl statist data matrix, where everi column of the tabl repres a particular variable, and each row correspond to a given member of the data set in question. the data set list valu for each of the variables, such as height and weight of an object, for each member of the data set. each valu is known as a datum. the data set may compris data for one or more members, correspond to the number of rows. the term data set may also be use more loosely, to refer to the data in a collect of close relat tables, correspond to a particular experi or event. an exampl of this type is the data set collect by space agenc per ing experi with instrument aboard space probes.
raw data raw data, also known as primari data, is data (e.g., numbers, instrument readings, figures, etc.) collect from a source. if a scientist set up a computer thermomet which record the temperatur of a chemic mixtur in a test tube everi minute, the list of temperatur read for everi minute, as print out on a spreadsheet or view on a comput screen is "raw data". raw data has not been subject to processing, "cleaning" by research to remov outliers, obvious instrument read error or data entri errors, or ani analysi (e.g., determin central tendenc aspect such as the averag or median ). as well, raw data has not been subject to ani other manipul by a softwar program or a human researcher, analyst or technician. it is also refer to as primari data. raw data is a relat term (see data), becaus even onc raw data has been "cleaned" and process by one team of researchers, anoth team may consid this process data to be "raw data" for anoth stage of research. raw data can be input to a comput program or use in manual procedur such as analyz statist from a survey. the term "raw data" can refer to the binari data on electron storag devices, such as hard disk drive (also refer to as "low-level data").
field research field research or fieldwork is the collect of in ation outsid a laboratory, librari or workplac setting. the approach and method use in field research vari across disciplines. for example, biologist who conduct field research may simpli observ anim interact with their environments, wherea social scientist conduct field research may interview or observ peopl in their natur environ to learn their languages, folklore, and social structures. field research involv a rang of well-defined, although variable, methods: in al interviews, direct observation, particip in the life of the group, collect discussions, analys of person document produc within the group, self-analysis, s from iti undertaken off- or on-line, and life-histories. although the method general is character as qualit research, it may (and often does) includ quantit dimensions.
experiment data experiment data in scienc are data produc by a measurement, test method, experiment design or quasi-experiment design. in clinic research ani data produc are the of a clinic trial. experiment data may be qualit or quantitative, each be appropri for differ investigations. general speaking, qualit data are consid more descript and can be subject in comparison to have a continu measur scale that produc numbers. wherea quantit data are gather in a manner that is normal experiment repeatable, qualit in ation is usual more close relat to phenomen mean and is, therefore, subject to interpret by individu observers. experiment data can be reproduc by a varieti of differ investig and mathemat analysi may be per ed on these data.
knowledg knowledg is a familiarity, awar or understand of someon or something, such as facts, in ation, descriptions, or skills, which is acquir through experi or educ by perceiving, discovering, or learning. knowledg can refer to a theoret or practic understand of a subject. it can be implicit (as with practic skill or expertise) or explicit (as with the theoret understand of a subject); it can be more or less al or systematic.[1] in philosophy, the studi of knowledg is call epistemology; the philosoph plato famous defin knowledg as "justifi true belief", though this definit is now agre by most analyt philosoph to be problemat becaus of the gettier problems. however, sever definit of knowledg and theori to explain it exist. knowledg acquisit involv complex cognit processes: perception, communication, and reasoning;[2] while knowledg is also said to be relat to the capac of acknowledg in human beings.[3]
philosophi of in ation the philosophi of in ation (pi) is the area of research that studi conceptu issu aris at the intersect of comput science, in ation science, in ation technology, and philosophy. it includes: the critic investig of the conceptu natur and basic principl of in ation, includ it dynamics, utilis and scienc the elabor and applic of in ation-theoret and comput methodolog to philosoph problems.[1]
accuraci vs. precis precis is a descript of random errors, a measur of statist variability. accuraci has two definitions: more commonly, it is a descript of systemat errors, a measur of statist bias; alternatively, iso defin accuraci as describ both type of observ error abov (prefer the term trueness for the common definit of accuracy). in the field of science, engin and statistics, the accuraci of a measur system is the degre of close of measur of a quantiti to that quantiti true value.[1] the precis of a measur system, relat to reproduc and repeatability, is the degre to which repeat measur under unchang condit show the same s.[1][2] although the two word precis and accuraci can be synonym in colloqui use, they are deliber contrast in the context of the scientif method. a measur system can be accur but not precise, precis but not accurate, neither, or both. for example, if an experi contain a systemat error, then increas the sampl size general increas precis but doe not improv accuracy. the would be a consist yet inaccur string of s from the flaw experiment. elimin the systemat error improv accuraci but doe not chang precision. a measur system is consid valid if it is both accur and precise. relat term includ bias (non-random or direct effect caus by a factor or factor unrel to the independ variable) and error (random variability). the terminolog is also appli to indirect measurements—that is, valu obtain by a comput procedur from observ data. in addit to accuraci and precision, measur may also have a measur resolution, which is the smallest chang in the under physic quantiti that produc a respons in the measurement. in numer analysis, accuraci is also the near of a calcul to the true value; while precis is the resolut of the representation, typic defin by the number of decim or binari digits. statist literatur prefer to use the term bias and variabl instead of accuraci and precision: bias is the amount of inaccuraci and variabl is the amount of imprecision. in militari terms, accuraci refer primarili to the accuraci of fire (or "justess de tir"), the precis of fire express by the close of a group of shot at and around the centr of the target.[3]
accuraci vs. measur in industri instrumentation, accuraci is the measur tolerance, or transmiss of the instrument and defin the limit of the error made when the instrument is use in normal oper conditions.[4] ideal a measur devic is both accur and precise, with measur all close to and tight cluster around the true value. the accuraci and precis of a measur process is usual establish by repeat measur some traceabl refer standard. such standard are defin in the intern system of unit (abbrevi si from french: systèm intern d'unités) and maintain by nation standard organ such as the nation institut of standard and technolog in the unit states. this also appli when measur are repeat and averaged. in that case, the term standard error is proper applied: the precis of the averag is equal to the known standard deviat of the process divid by the squar root of the number of measur averaged. further, the central limit theorem show that the probabl distribut of the averag measur will be closer to a normal distribut than that of individu measurements. with regard to accuraci we can distinguish: the differ between the mean of the measur and the refer value, the bias. establish and correct for bias is necessari for calibration. the combin effect of that and precision. a common convent in scienc and engin is to express accuraci and/or precis implicit by mean of signific figures. here, when not explicit stated, the margin of error is understood to be one-half the valu of the last signific place.
quantif in mathemat and empir science, quantif (or quantitation) is the act of count and measur that map human sens observ and experi into member of some set of numbers. quantif in this sens is fundament to the scientif method.
scientif method the scientif method is a bodi of techniqu for investig phenomena, acquir new knowledge, or correct and integr previous knowledge.[2] to be term scientific, a method of inquiri is common base on empir or measur evid subject to specif principl of reasoning.[3] the oxford dictionari onlin defin the scientif method as "a method or procedur that has character natur scienc sinc the 17th century, consist in systemat observation, measurement, and experiment, and the ulation, testing, and modif of hypotheses".[4] the scientif method is an ongo process, which usual begin with observ about the natur world. human be are natur inquisitive, so they often come up with question about thing they see or hear and often develop idea (hypotheses) about whi thing are the way they are. the best hypothes lead to predict that can be test in various ways, includ make further observ about nature. in general, the strongest test of hypothes come from care control and replic experi that gather empir data. depend on how well the test match the predictions, the origin hypothesi may requir refinement, alteration, expans or even rejection. if a particular hypothesi becom veri well support a general theori may be developed.[1] although procedur vari from one field of inquiri to another, identifi featur are frequent share in common between them. the overal process of the scientif method involv make conjectur (hypotheses), deriv predict from them as logic consequences, and then carri out experi base on those predictions.[5][6] a hypothesi is a conjecture, base on knowledg obtain while ulat the question. the hypothesi might be veri specif or it might be broad. scientist then test hypothes by conduct experiments. under modern interpretations, a scientif hypothesi must be falsifiable, impli that it is possibl to identifi a possibl outcom of an experi that conflict with predict deduc from the hypothesis; otherwise, the hypothesi cannot be meaning tested.[7] the purpos of an experi is to determin whether observ agre with or conflict with the predict deriv from a hypothesis.[8] experi can take place in a colleg lab, on a kitchen table, at cern larg hadron collider, at the bottom of an ocean, on mars, and so on. there are difficulti in a ulaic statement of method, however. though the scientif method is often present as a fix sequenc of steps, it repres rather a set of general principles.[9] not all step take place in everi scientif inquiri (or to the same degree), and are not alway in the same order.[10] some philosoph and scientist have argu that there is no scientif method. for example, lee smolin[11] and paul feyerabend (in his against method). nola and sankey remark that "for some, the whole idea of a theori of scientif method is yester-year debate".[12]
fals precis fals precis (also call overprecision, fake precision, misplac precis and spurious accuracy) occur when numer data are present in a manner that impli better precis than is actual the case; sinc precis is a limit to accuracy, this often lead to overconfid in the accuraci as well.[1] madsen piri defin the term "fals precision" in a more general way: when exact number are use for notion that cannot be express in exact terms. for example, "i am 90% sure he is wrong". often fals precis is abus to produc an unwarr confid in the claim: "our mouthwash is twice as good as our competitor's". [2] in scienc and engineering, convent dictat that unless a margin of error is explicit stated, the number of signific figur use in the present of data should be limit to what is warrant by the precis of those data. for example, if an instrument can be read to tenth of a unit of measurement, s of calcul use data obtain from that instrument can onli be confid state to the tenth place, regardless of what the raw calcul return or whether other data use in the calcul are more accurate. even outsid these disciplines, there is a tendenc to assum that all the non-zero digit of a number are meaningful; thus, provid excess figur may lead the viewer to expect better precis than actual exists. however, in contrast, it is good practic to retain more signific figur than this in the intermedi stage of a calculation, in order to avoid accumul round errors. fals precis common aris when high-precis and low-precis data are combined, and in convers of units.
problem solv problem solv consist of use generic or ad hoc methods, in an order manner, for find solut to problems. some of the problem-solv techniqu develop and use in artifici intelligence, comput science, engineering, mathematics, or medicin are relat to mental problem-solv techniqu studi in psychology. the term problem-solv is use in mani disciplines, sometim with differ perspectives, and often with differ terminologies. for instance, it is a mental process in psycholog and a computer process in comput science. problem can also be classifi into two differ type (ill-defin and well-defined) from which appropri solut are to be made. ill-defin problem are those that do not have clear goals, solut paths, or expect solution. well-defin problem have specif goals, clear defin solut paths, and clear expect solutions. these problem also allow for more initi plan than ill-defin problems.[1] be abl to solv problem sometim involv deal with pragmat (logic) and semant (interpret of the problem). the abil to understand what the goal of the problem is and what rule could be appli repres the key to solv the problem. sometim the problem requir some abstract think and come up with a creativ solution.
empiric empiric is a theori that state that knowledg come onli or primarili from sensori experience.[1] one of sever view of epistemology, the studi of human knowledge, along with ration and skepticism, empiric emphas the role of empir evid in the ation of ideas, over the notion of innat idea or traditions;[2] empiricist may argu howev that tradit (or customs) aris due to relat of previous sens experiences.[3] empiric in the philosophi of scienc emphas evidence, especi as discov in experiments. it is a fundament part of the scientif method that all hypothes and theori must be test against observ of the natur world rather than rest sole on a priori reasoning, intuition, or revelation. empiricism, often use by natur scientists, say that "knowledg is base on experience" and that "knowledg is tentat and probabilistic, subject to continu revis and falsification."[4] one of the epistemolog tenet is that sensori experi creat knowledge. the scientif method, includ experi and valid measur tools, guid empir research.
predict d
forecast d
data (computing) data (/ˈdeɪtə/ day-tə, or /ˈdɑːtə/ dah-tə;[1] treat as singular, plural, or as a mass noun) is ani sequenc of one (1) or more symbol given mean by specif act(s) of interpretation. data (or datum - a singl unit of data) is not in ation. data requir interpret to becom in ation. to translat data to in ation, there must be sever known factor considered. the factor involv are determin by the creator of the data and the desir in ation. the term metadata is use to refer the data about the data. metadata may be implied, specifi or given. data relat to physic event or process will also have a tempor component. in almost all case this tempor compon is implied. this is the case when a devic such as a temperatur logger receiv data from a temperatur sensor. when the temperatur is receiv it is assum that the data has a tempor refer of "now". so the devic record the date, time and temperatur together. when the data logger communic temperatures, it must also report the date and time (metadata) for each temperature. digit data is data that is repres use the binari number system of one (1) and zero (0). as oppos to analog representation. in modern (post 1960) comput systems, all data is digital. data within a computer, in most cases, move as parallel data. data move to or from a computer, in most cases, move as serial data. see parallel communic and serial communication. data sourc from an analog device, such as a temperatur sensor, must pass through an "analog to digit converter" or "adc" (see analog-to-digit converter) to convert the analog data to digit data. data repres quantities, characters, or symbol on which oper are per ed by a computer, store and record on magnetic, optical, or mechan record media, and transmit in the of digit electr signals.[2] a program is a set of data that consist of a seri of code softwar instruct to control the oper of a comput or other machine.[3] physic comput memori element consist of an address and a byte/word of data storage. digit data are often store in relat databases, like tabl or sql databases, and can general be repres as abstract key/valu pairs. data can be organ in mani differ type of data structures, includ arrays, graphs, and objects. data structur can store data of mani differ types, includ numbers, string and even other data structures. data pass in and out of comput via peripher devices.
data data (/ˈdeɪtə/ day-tə, /ˈdætə/ da-tə, or /ˈdɑːtə/ dah-tə)[1] is a set of valu of qualit or quantit variables. an exampl of qualit data would be an anthropologist handwritten note about her interview with peopl of an indigen tribe. piec of data are individu piec of in ation. while the concept of data is common associ with scientif research, data is collect by a huge rang of organ and institutions, rang from busi (e.g., sale data, revenue, profits, stock price), govern (e.g., crime rates, unemploy rates, literaci rates) and non-government organ (e.g., census of the number of homeless peopl by non-profit organizations). data is measured, collect and reported, and analyzed, whereupon it can be visual use graphs, imag or other analysi tools. data as a general concept refer to the fact that some exist in ation or knowledg is repres or code in some suitabl for better usag or processing. raw data ("unprocess data") is a collect of number or charact befor it has been "cleaned" and correct by researchers. raw data need to be correct to remov outlier or obvious instrument or data entri error (e.g., a thermomet read from an outdoor arctic locat record a tropic temperature). data process common occur by stages, and the "process data" from one stage may be consid the "raw data" of the next stage. field data is raw data that is collect in an uncontrol "in situ" environment. experiment data is data that is generat within the context of a scientif investig by observ and recording.
in ation in ation (shorten as info) is that which in s. in other words, it is the answer to a question of some kind. it is also that from which data and knowledg can be derived, as data repres valu attribut to parameters, and knowledg signifi understand of real thing or abstract concepts.[1] as it regard data, the in ation exist is not necessarili coupl to an observ (it exist beyond an event horizon, for example), while in the case of knowledge, the in ation requir a cognit observer. at it most fundamental, in ation is ani propag of caus and effect within a system. in ation is convey either as the content of a messag or through direct or indirect observ of some thing. that which is perceiv can be constru as a messag in it own right, and in that sense, in ation is alway convey as the content of a message. in ation can be encod into various s for transmiss and interpret (for example, in ation may be encod into a sequenc of signs, or transmit via a sequenc of signals). it can also be encrypt for safe storag and communication. in ation resolv uncertainty. the uncertainti of an event is measur by it probabl of occurr and is invers proport to that. the more uncertain an event, the more in ation is requir to resolv uncertainti of that event. the bit is a typic unit of in ation, but other unit such as the nat may be used. example: in ation in one "fair" coin ﬂip: log2(2/1) = 1 bit, and in two fair coin flip is log2(4/1) = 2 bits. the concept that in ation is the messag has differ mean in differ contexts.[2] thus the concept of in ation becom close relat to notion of constraint, communication, control, data, , education, knowledge, meaning, understanding, mental stimuli, pattern, perception, representation, and entropy.
measur (data) measur is the assign of a number to a characterist of an object or event, which can be compar with other object or events.[1][2] the scope and applic of a measur is depend on the context and discipline. in the natur scienc and engineering, measur do not appli to nomin properti of object or events, which is consist with the guidelin of the intern vocabulari of metrolog publish by the intern bureau of weight and measures.[2] however, in other field such as statist as well as the social and behavior sciences, measur can have multipl levels, which would includ nominal, ordinal, interval, and ratio scales.[1][3] measur is a cornerston of trade, science, technology, and quantit research in mani disciplines. historically, mani measur system exist for the vari field of human exist to facilit comparison in these fields. often these were achiev by local agreement between trade partner or collaborators. sinc the 18th century, develop progress toward unifying, wide accept standard that ed in the modern intern system of unit (si). this system reduc all physic measur to a mathemat combin of seven base units. the scienc of measur is pursu in the field of metrology.
data report data report is the process of collect and submit data to author entrust with compil statistics. accur data report give rise to accur analys of the fact on the ground; inaccur data report can lead to vast unin ed decis base on erron evidence. when data is not reported, the problem is known as underreporting; the opposit problem lead to fals positives. data report can be an incred difficult endeavor. census bureaus may hire even hundr of thousand of worker to achiev the task of count all of the resid of a country.[1][2] teacher use data from student assess to determin grades; cellphon manufactur reli on sale data from retail to point the way to which model to increas product of. the effect manag of near ani compani reli on accur data.
data analysi analysi of data is a process of inspecting, cleaning, tran ing, and model data with the goal of discov use in ation, suggest conclusions, and support decision-making. data analysi has multipl facet and approaches, encompass divers techniqu under a varieti of names, in differ business, science, and social scienc domains. data mine is a particular data analysi techniqu that focus on model and knowledg discoveri for predict rather than pure descript purposes. busi intellig cover data analysi that reli heavili on aggregation, focus on busi in ation. in statist applications, some peopl divid data analysi into descript statistics, exploratori data analysi (eda), and confirmatori data analysi (cda). eda focus on discov new featur in the data and cda on confirm or falsifi exist hypotheses. predict analyt focus on applic of statist model for predict forecast or classification, while text analyt appli statistical, linguistic, and structur techniqu to extract and classifi in ation from textual sources, a speci of unstructur data. all are varieti of data analysis. data integr is a precursor to data analysis, and data analysi is close link to data visual and data dissemination. the term data analysi is sometim use as a synonym for data modeling.
data process  
data organ  
concept learn process of learn which help new concept
continuous-valu attribut numer ex car hous
discrete-valu attribut nomin ex jobtype, address
induct learn guess knowledg that is statist supported; train data includ desir output
version space boundari group group s: most specif match concept(s) group g: most general match concept(s)
bias learner predict output given input that it has not encountered. high bias can caus an algorithm to miss the relav relat between features; underfitting;
varianc error from sensit to small fluctuat in train set. high varianc can caus over fit model the random nois rather then intend valu
bay optim error the proport of error that cannot be elimin
n-fold cross valid some data remov befor training. then data remov is use to test the training. divid into n subset
leav one out cross valid proport data into differ groups. train group 2-10;test group 1; ect;
confus matrix use to analyz sourc of error
roc curv fro compar algorithm illustr per anc of binari classifi system by threshold
precis tp / (fp + tp)
recal tp / #posit
fals posit incorrect indic that attribut is present
fals negat incorrect indic that attribut is absent
decis tree each intern node test attribute, each branch correspond to attribute, each leaf node assign classifi
entropi e = (-p+log(p)+-plog(-p)) measur of random the higher entropi the harder it is to draw conclus
in ation gain expect reduct in entropi due to sort on a
gain ratio gain(s,a)/split info(s,a)
overfit pay to close attent to each data point
nois random varianc error
artifici neural network learn system set up like the biolog of the brain where each node is an ation node in the neural network
hidden unit tran input into someth the output can use
linearunit a(netinput) = netinput
linearthreshold unit a(netinput) = {1 netinput 0 ; 0 otherwise}
sigmoid unit a(netinput) = 1/1+e^(-netinput)
perceptron an algorithm for learn binari classifi that map it input to a singl binari out valu -model a neuron
multilay perceptron artifici network model that map set of input data onto a set of appropri ouput
gradient descent search determin a weight vector that mnimiz e by start with an arbitrari initi weight vector; then modifi in small steps; continu until the global minimum is reach
batch-mod gradient descent calcul error for all exampl then take a step ; linear unit train rule use gradient descent;
incremental(stochastic) gradient descent calcul error for a static chosen point can approxim batch gradient if n is small enough
convolut neural network feed forward neural network in which the connect pattern between neuron is inspir by the organ of anim cortex; edg detection; convolutionmatrix - run kernel over array of pixel ;
encoder-decord network feed forward neural network rep origin data. - multilay pca with non-linear
long short term memori use no ation function
recurr neural network can use intern unit to maintain in ation
bag for # classifi to generat - select a bag sampl of data - generat classifi
ada-boost adapt boost - the output of other "weak" learner is combin into weight sum that repres output
stack train a learn algorithm to combin the predict of others.
apriori use for mine and associ learning. - highlight trend in databas find associ rule by effici find set of item (itemsets) that meet a minim support criterion
random forest ensembl learn method for classif that oper by construct a multitud of decis tree at train time and output the class that is the mode of the class or mean predict
market basket list of product that are purchas by custom
apriori properti for some support threshold s - if ani itemset i meet the threshold s, then ani nonempti subset of i also meet the threshold - if ani itemset i doe not meet the threshold s, ani superset (includ 0
nearest neighbor in a cluster classif model data is plot within the classif and the new point are then judg and vote by their nearest neighbor
kd-tree this is a type of classif that graph the train data into group and then put the group into a decis tree -construct lookup tree
curs of dimension -nearest neighbor is misl when high dimension space -aris when analyz data in high dimension space -hard to find distribut and measur distanc
first order logic use induct rule to generat concept
cover algorithm each classif method use an algorhithm to generat rule from sampl data
singl point crossov singl point is select all data beyond that point is swap between parent
two point crossov two point are selected; everyth between point is swap
uni crossov use fix mix ratio between parent ; gene level contribut
unsupervis learn use to draw infer from data set without label output - most common method is cluster
cluster algorithm evalu base on the data that was cluster - high score that produc high similar within cluster and low between cluster
dendrogram tree diagram use to illustr the arag of cluster
delay reward rl the agent action determin not onli it immedi reward but also the next state of the enviro - must consid futur action
discount futur reward a reward in the futur is less the reward right now increas exponenti
markov decis process set of states; actions;transition;rewards;transit function;
linear program form: f(x1,x2,..,xn) to be maxim subject to set of constraint of g(x1,x2,..,xn) solv math problem where constraint function maxim use linear combo
slack variabl deal with nonseper data -0 if ansnwer is correct -y "distance" we need to move exampl to make correct
margin maxim this distanc between both point on both side for better accuraci
support vector  
what the trade-off between bias and varianc bias is error due to erron or over simplist assumpt in the learn algorithm you'r using. this can lead to the model underfitting. varianc is error due to too much complex in the learn algorithm you'r using. this lead to the algorithm be high sensit to high degre of variat in your train data, which can lead your model to overfit the data.
how is knn differ from k-mean cluster k-nearest neighbor is a supervis classif algorithm, while k-mean cluster is an unsupervis cluster algorithm. while the mechan may seem similar at first, what this realli mean is that in order for k-nearest neighbor to work, you need label data you want to classifi an unlabel point into (thus the nearest neighbor part). k-mean cluster requir onli a set of unlabel point and a threshold: the algorithm will take unlabel point and gradual learn how to cluster them into group by comput the mean of the distanc between differ points. the critic differ here is that knn need label point and is thus supervis learning, while k-mean doesn't — and is thus unsupervis learning.
explain how a roc curv work the roc curv is a graphic represent of the contrast between true posit rate and the fals posit rate at various thresholds. it often use as a proxi for the trade-off between the sensit of the model (true positives) vs the fall-out or the probabl it will trigger a fals alarm (fals positives).
defin precis and recal recal is also known as the true posit rate: the amount of posit your model claim compar to the actual number of posit there are throughout the data. precis is also known as the posit predict value, and it is a measur of the amount of accur posit your model claim compar to the number of posit it actual claims. it can be easier to think of recal and precis in the context of a case where you'v predict that there were 10 appl and 5 orang in a case of 10 apples. you'd have perfect recall.
whi is "naive" bay naiv despit it practic applications, especi in text mining, naiv bay is consid "naive" becaus it make an assumpt that is virtual imposs to see in real-lif data: the condit probabl is calcul as the pure product of the individu probabl of components. this impli the absolut independ of featur — a condit probabl never met in real life. as a quora comment put it whimsically, a naiv bay classifi that figur out that you like pickl and ice cream would probabl naiv recommend you a pickl ice cream.
explain the differ between l1 and l2 regularization. l2 regular tend to spread error among all the terms, while l1 is more binary/sparse, with mani variabl either be assign a 1 or 0 in weighting. l1 correspond to set a laplacean prior on the terms, while l2 correspond to a gaussian prior. l2 regular tend to spread error among all the terms, while l1 is more binary/sparse, with mani variabl either be assign a 1 or 0 in weighting. l1 correspond to set a laplacean prior on the terms, while l2 correspond to a gaussian prior.
what the differ between type i and type ii error type i error is a fals positive, while type ii error is a fals negative. briefli stated, type i error mean claim someth has happen when it hasn't, while type ii error mean that you claim noth is happen when in fact someth is. a clever way to think about this is to think of type i error as tell a man he is pregnant, while type ii error mean you tell a pregnant woman she isn't carri a baby.
what a fourier tran a fourier tran is a generic method to decompos generic function into a superposit of symmetr functions. a fourier tran convert a signal from time to frequenc domain — it a veri common way to extract featur from audio signal or other time seri such as sensor data.
what is deep learning, and how doe it contrast with other machin learn algorithm deep learn is a subset of machin learn that is concern with neural networks: how to use backpropag and certain principl from neurosci to more accur model larg set of unlabel or semi-structur data. in that sense, deep learn repres an unsupervis learn algorithm that learn represent of data through the use of neural nets.
what cross-valid techniqu would you use on a time seri dataset instead of use standard k-fold cross-validation, you have to pay attent to the fact that a time seri is not random distribut data — it is inher order by chronolog order. if a pattern emerg in later time period for example, your model may still pick up on it even if that effect doesn't hold in earlier years! you'll want to do someth like forward chain where you'll be abl to model on past data then look at forward-fac data. • fold 1 : train [1], test [2] • fold 2 : train [1 2], test [3] • fold 3 : train [1 2 3], test [4] • fold 4 : train [1 2 3 4], test [5] • fold 5 : train [1 2 3 4 5], test [6]
which is more import to you- model accuracy, or model per anc this question test your grasp of the nuanc of machin learn model per ance! machin learn interview question often look toward the details. there are model with higher accuraci that can per wors in predict power — how doe that make sens well, it has everyth to do with how model accuraci is onli a subset of model per ance, and at that, a sometim mislead one. for example, if you want to detect fraud in a massiv dataset with a sampl of millions, a more accur model would most like predict no fraud at all if onli a vast minor of case were fraud. however, this would be useless for a predict model — a model design to find fraud that assert there was no fraud at all! question like this help you demonstr that you understand model accuraci isn't the be-al and end-al of model per ance.
what the f1 score how would you use it the f1 score is a measur of a model per ance. it is a weight averag of the precis and recal of a model, with s tend to 1 be the best, and those tend to 0 be the worst. you would use it in classif test where true negat don't matter much
how would you handl an imbalanc dataset an imbalanc dataset is when you have, for example, a classif test and 90% of the data is in one class. that lead to problems: an accuraci of 90% can be skew if you have no predict power on the other categori of data! here are a few tactic to get over the hump: 1- collect more data to even the imbal in the dataset. 2- resampl the dataset to correct for imbalances. 3- tri a differ algorithm altogeth on your dataset. what import here is that you have a keen sens for what damag an unbalanc dataset can cause, and how to balanc that.
when should you use classif over regress classif produc discret valu and dataset to strict categories, while regress give you continu s that allow you to better distinguish differ between individu points. you would use classif over regress if you want your s to reflect the belonging of data point in your dataset to certain explicit categori (ex: if you want to know whether a name was male or femal rather than just how correl they were with male and femal names.)
name an exampl where ensembl techniqu might be useful. ensembl techniqu use a combin of learn algorithm to optim better predict per ance. they typic reduc overfit in model and make the model more robust (unlik to be influenc by small chang in the train data). you could list some exampl of ensembl methods, from bag to boost to a "bucket of models" method and demonstr how they could increas predict power.
how do you ensur you'r not overfit with a model this is a simpl restat of a fundament problem in machin learning: the possibl of overfit train data and carri the nois of that data through to the test set, therebi provid inaccur generalizations. there are three main method to avoid overfitting: 1- keep the model simpler: reduc varianc by take into account fewer variabl and parameters, therebi remov some of the nois in the train data. 2- use cross-valid techniqu such as k-fold cross-validation. 3- use regular techniqu such as lasso that penal certain model paramet if they'r like to caus overfitting.
what the "kernel trick" and how is it use the kernel trick involv kernel function that can enabl in higher-dimens space without explicit calcul the coordin of point within that dimension: instead, kernel function comput the inner product between the imag of all pair of data in a featur space. this allow them the veri use attribut of calcul the coordin of higher dimens while be comput cheaper than the explicit calcul of said coordinates. mani algorithm can be express in term of inner products. use the kernel trick enabl us effect run algorithm in a high-dimension space with lower-dimension data
pick an algorithm. write the psuedo-cod for a parallel implementation. this kind of question demonstr your abil to think in parallel and how you could handl concurr in program implement deal with big data. take a look at pseudocod framework such as peril-l and visual tool such as web sequenc diagram to help you demonstr your abil to write code that reflect parallelism.
which data visual librari do you use what are your thought on the best data visual tool what import here is to defin your view on how to proper visual data and your person prefer when it come to tools. popular tool includ r ggplot, python seaborn and matplotlib, and tool such as plot.li and tableau.
how would you implement a recommend system for our compani user a lot of machin learn interview question of this type will involv implement of machin learn model to a compani problems. you'll have to research the compani and it industri in-depth, especi the revenu driver the compani has, and the type of user the compani take on in the context of the industri it in. (store a set of keyword alongsid each product, which should essenti be everyth in the titl besid a set of stop words. when a titl is ed, you find ani other product which share keyword in common (with those with one or more in common given priority). you could further enhanc this by assign a score to each keyword base on it scarciti (with more scarc word be given a higher score, as a match on php', for instance, is go to be more relev than a match on programming'), or by track the number of time a user navig manual between a set of products. regardless you'd best start off by make it simple, and then enhanc it as you go on. depend on the size of your databas more advanc techniqu may not be all that fruit
when doe knn struggl the knn doe particular bad where the dataset have more featur as 0. in other words, if the data is sparse.
what is the differ between cluster and systemat sampl cluster sampl is a techniqu use when it becom difficult to studi the target popul spread across a wide area and simpl random sampl cannot be applied. cluster sampl is a probabl sampl where each sampl unit is a collection, or cluster of elements. systemat sampl is a statist techniqu where element are select from an order sampl frame. in systemat sampling, the list is progress in a circular manner so onc you reach the end of the list,it is progress from the top again. the best exampl for systemat sampl is equal probabl method.
what doe p-valu signifi about the statist data p-valu is use to determin the signific of s after a hypothesi test in statistics. p-valu help the reader to draw conclus and is alway between 0 and 1. • p- valu 0.05 denot weak evid against the null hypothesi which mean the null hypothesi cannot be rejected. • p-valu • p-value=0.05i the margin valu indic it is possibl to go either way
what is the goal of a/b test it is a statist hypothesi test for random experi with two variabl a and b. the goal of a/b test is to identifi ani chang to the web page to maxim or increas the outcom of an interest. an exampl for this could be identifi the click through rate for a banner ad.
explain about the box cox tran ation in regress models. for some reason or the other, the respons variabl for a regress analysi might not satisfi one or more assumpt of an ordinari least squar regression. the residu could either curv as the predict increas or follow skew distribution. in such scenarios, it is necessari to tran the respons variabl so that the data meet the requir assumptions. a box cox tran ation is a statist techniqu to tran non-mornla depend variabl into a normal shape. if the given data is not normal then most of the statist techniqu assum normality. appli a box cox tran ation mean that you can run a broader number of tests.
what is the differ between bayesian estim and maximum likelihood estim (mle) in bayesian estim we have some knowledg about the data/problem (prior) .there may be sever valu of the paramet which explain data and henc we can look for multipl paramet like 5 gamma and 5 lambda that do this. as a of bayesian estimate, we get multipl model for make multipl predcit i.e. one for each pair of paramet but with the same prior. so, if a new exampl need to be predict than comput the weight sum of these predict serv the purpose. maximum likelihood doe not take prior into consider (ignor the prior) so it is like be a bayesian while use some kind of a flat prior.
can you cite some exampl where a fals negat import than a fals posit assum there is an airport a which has receiv high secur threat and base on certain characterist they identifi whether a particular passeng can be a threat or not. due to shortag of staff they decid to scan passeng be predict as risk posit by their predict model. what will happen if a true threat custom is be flag as non-threat by airport model anoth exampl can be judici system. what if juri or judg decid to make a crimin go free what if you reject to marri a veri good person base on your predict model and you happen to meet him/her after few year and realiz that you had a fals negat
can you cite some exampl where both fals posit and fals negat are equal import in the bank industri give loan is the primari sourc of make money but at the same time if your repay rate is not good you will not make ani profit, rather you will risk huge losses. bank don't want to lose good custom and at the same point of time they don't want to acquir bad customers. in this scenario both the fals posit and fals negat becom veri import to measure. these day we hear mani case of player use steroid dure sport competit everi player has to go through a steroid test befor the game starts. a fals posit can ruin the career of a great sportsman and a fals negat can make the game unfair
a/b test a statist way of compar two (or more) techniques, typic an incumb against a new rival. a/b test aim to determin not onli which techniqu per s better but also to understand whether the differ is statist significant. a/b test usual consid onli two techniqu use one measurement, but it can be appli to ani finit number of techniqu and measures.
accuraci the fraction of predict that a classif model got right. in multi-class classification, accuraci is defin as follows: accuracy=correct predictionstot number of exampl in binari classification, accuraci has the follow definition: accuracy=(tru positives+tru negatives)/tot number of exampl see true posit and true negative.
activ function a function (for example, relu or sigmoid) that take in the weight sum of all of the input from the previous layer and then generat and pass an output valu (typic nonlinear) to the next layer.
adagrad a sophist gradient descent algorithm that rescal the gradient of each parameter, effect give each paramet an independ learn rate. for a full explanation, see this paper.
auc (area under the roc curve) an evalu metric that consid all possibl classif thresholds. the area under the roc curv is the probabl that a classifi will be more confid that a random chosen posit exampl is actual posit than that a random chosen negat exampl is positive.
backpropag the primari algorithm for per ing gradient descent on neural networks. first, the output valu of each node are calcul (and cached) in a forward pass. then, the partial deriv of the error with respect to each paramet is calcul in a backward pass through the graph.
baselin a simpl model or heurist use as refer point for compar how well a model is per ing. a baselin help model develop quantifi the minimal, expect per anc on a particular problem.
batch the set of exampl use in one iter (that is, one gradient update) of model training. see also batch size.
batch size the number of exampl in a batch. for example, the batch size of sgd is 1, while the batch size of a mini-batch is usual between 10 and 1000. batch size is usual fix dure train and inference; however, tensorflow doe permit dynam batch sizes.
bias an intercept or offset from an origin. bias (also known as the bias term) is refer to as b or w0 in machin learn models. for example, bias is the b in the follow ula: y′=b+w1x1+w2x2+...wnxn not to be confus with predict bias.
binari classif a type of classif task that output one of two mutual exclus classes. for example, a machin learn model that evalu email messag and output either "spam" or "not spam" is a binari classifier.
bin see bucketing.
bucket convert a (usual continuous) featur into multipl binari featur call bucket or bins, typic base on valu range. for example, instead of repres temperatur as a singl continu floating-point feature, you could chop rang of temperatur into discret bins. given temperatur data sensit to a tenth of a degree, all temperatur between 0.0 and 15.0 degre could be put into one bin, 15.1 to 30.0 degre could be a second bin, and 30.1 to 50.0 degre could be a third bin.
calibr layer a post-predict adjustment, typic to account for predict bias. the adjust predict and probabl should match the distribut of an observ set of labels.
candid sampl a training-tim optim in which a probabl is calcul for all the posit labels, using, for example, softmax, but onli for a random sampl of negat labels. for example, if we have an exampl label beagl and dog candid sampl comput the predict probabl and correspond loss term for the beagl and dog class output in addit to a random subset of the remain class (cat, lollipop, fence). the idea is that the negat class can learn from less frequent negat reinforc as long as posit class alway get proper posit reinforcement, and this is inde observ empirically. the motiv for candid sampl is a comput effici win from not comput predict for all negatives.
categor data featur have a discret set of possibl values. for example, consid a categor featur name hous , which has a discret set of three possibl values: tudor, ranch, colonial. by repres hous as categor data, the model can learn the separ impact of tudor, ranch, and coloni on hous price. sometimes, valu in the discret set are mutual exclusive, and onli one valu can be appli to a given example. for example, a car maker categor featur would probabl permit onli a singl valu (toyota) per example. other times, more than one valu may be applicable. a singl car could be paint more than one differ color, so a car color categor featur would like permit a singl exampl to have multipl valu (for example, red and white). categor featur are sometim call discret features. contrast with numer data.
checkpoint data that captur the state of the variabl of a model at a particular time. checkpoint enabl export model weights, as well as per ing train across multipl sessions. checkpoint also enabl train to continu past error (for example, job preemption). note that the graph itself is not includ in a checkpoint.
class one of a set of enumer target valu for a label. for example, in a binari classif model that detect spam, the two class are spam and not spam. in a multi-class classif model that identifi dog breeds, the class would be poodle, beagle, pug, and so on.
class-imbalanc data set a binari classif problem in which the label for the two class have signific differ frequencies. for example, a diseas data set in which 0.0001 of exampl have posit label and 0.9999 have negat label is a class-imbalanc problem, but a footbal game predictor in which 0.51 of exampl label one team win and 0.49 label the other team win is not a class-imbalanc problem.
classif model a type of machin learn model for distinguish among two or more discret classes. for example, a natur languag process classif model could determin whether an input sentenc was in french, spanish, or italian. compar with regress model.
classif threshold a scalar-valu criterion that is appli to a model predict score in order to separ the posit class from the negat class. use when map logist regress s to binari classification. for example, consid a logist regress model that determin the probabl of a given email messag be spam. if the classif threshold is 0.9, then logist regress valu abov 0.9 are classifi as spam and those below 0.9 are classifi as not spam.
collabor filter make predict about the interest of one user base on the interest of mani other users. collabor filter is often use in recommend systems.
confus matrix an nxn tabl that summar how success a classif model predict were; that is, the correl between the label and the model classification. one axi of a confus matrix is the label that the model predicted, and the other axi is the actual label. n repres the number of classes. in a binari classif problem, n=2. for example, here is a sampl confus matrix for a binari classif problem: tumor (predicted) non-tumor (predicted) tumor (actual) 18 1 non-tumor (actual) 6 452 the preced confus matrix show that of the 19 sampl that actual had tumors, the model correct classifi 18 as have tumor (18 true positives), and incorrect classifi 1 as not have a tumor (1 fals negative). similarly, of 458 sampl that actual did not have tumors, 452 were correct classifi (452 true negatives) and 6 were incorrect classifi (6 fals positives). the confus matrix for a multi-class classif problem can help you determin mistak patterns. for example, a confus matrix could reveal that a model train to recogn handwritten digit tend to mistaken predict 9 instead of 4, or 1 instead of 7. confus matric contain suffici in ation to calcul a varieti of per anc metrics, includ precis and recall.
continu featur a floating-point featur with an infinit rang of possibl values. contrast with discret feature.
converg in ally, often refer to a state reach dure train in which train loss and valid loss chang veri littl or not at all with each iter after a certain number of iterations. in other words, a model reach converg when addit train on the current data will not improv the model. in deep learning, loss valu sometim stay constant or near so for mani iter befor final descending, temporarili produc a fals sens of convergence. see also earli stopping. see also boyd and vandenberghe, convex optimization.
convex function a function in which the region abov the graph of the function is a convex set. the prototyp convex function is shape someth like the letter u. for example, the follow are all convex functions: a typic convex function is shape like the letter u'. by contrast, the follow function is not convex. notic how the region abov the graph is not a convex set: local minimum local minimum global minimum a strict convex function has exact one local minimum point, which is also the global minimum point. the classic u-shap function are strict convex functions. however, some convex function (for example, straight lines) are not. a lot of the common loss functions, includ the following, are convex functions: l2 loss log loss l1 regular l2 regular mani variat of gradient descent are guarante to find a point close to the minimum of a strict convex function. similarly, mani variat of stochast gradient descent have a high probabl (though, not a guarantee) of find a point close to the minimum of a strict convex function. the sum of two convex function (for example, l2 loss + l1 regularization) is a convex function. deep model are never convex functions. remarkably, algorithm design for convex optim tend to find reason good solut on deep network anyway, even though those solut are not guarante to be a global minimum.
convex optim the process of use mathemat techniqu such as gradient descent to find the minimum of a convex function. a great deal of research in machin learn has focus on ulat various problem as convex optim problem and in solv those problem more efficiently. for complet details, see boyd and vandenberghe, convex optimization.
convex set a subset of euclidean space such that a line drawn between ani two point in the subset remain complet within the subset. for instance, the follow two shape are convex sets: a rectangl and a semi-ellips are both convex sets. by contrast, the follow two shape are not convex sets: a pie-chart with a miss slice and a firework are both nonconvex sets.
cost synonym for loss.
cross-entropi a general of log loss to multi-class classif problems. cross-entropi quantifi the differ between two probabl distributions. see also perplexity.
custom estim an estim that you write yourself by follow these directions. contrast with pre-mad estimators.
data set a collect of examples.
data set api a high-level tensorflow api for read data and tran ing it into a that a machin learn algorithm requires. a tf.data.dataset object repres a sequenc of elements, in which each element contain one or more tensors. a tf.data.iter object provid access to the element of a dataset. for detail about the dataset api, see import data in the tensorflow programm guide.
decis boundari the separ between class learn by a model in a binari class or multi-class classif problems. for example, in the follow imag repres a binari classif problem, the decis boundari is the frontier between the orang class and the blue class:
dens layer synonym for fulli connect layer.
deep model a type of neural network contain multipl hidden layers. deep model reli on trainabl nonlinearities. contrast with wide model.
dens featur a featur in which most valu are non-zero, typic a tensor of floating-point values. contrast with spars feature.
deriv featur synonym for synthet feature.
discret featur a featur with a finit set of possibl values. for example, a featur whose valu may onli be animal, vegetable, or miner is a discret (or categorical) feature. contrast with continu feature.
dropout regular a of regular use in train neural networks. dropout regular work by remov a random select of a fix number of the unit in a network layer for a singl gradient step. the more unit drop out, the stronger the regularization. this is analog to train the network to emul an exponenti larg ensembl of smaller networks. for full details, see dropout: a simpl way to prevent neural network from overfitting.
dynam model a model that is train onlin in a continu updat fashion. that is, data is continu enter the model.
earli stop a method for regular that involv end model train befor train loss finish decreasing. in earli stopping, you end model train when the loss on a valid data set start to increase, that is, when general per anc worsens.
embed a categor featur repres as a continuous-valu feature. typically, an embed is a translat of a high-dimension vector into a low-dimension space. for example, you can repres the word in an english sentenc in either of the follow two ways: as a million-el (high-dimensional) spars vector in which all element are integers. each cell in the vector repres a separ english word; the valu in a cell repres the number of time that word appear in a sentence. sinc a singl english sentenc is unlik to contain more than 50 words, near everi cell in the vector will contain a 0. the few cell that aren't 0 will contain a low integ (usual 1) repres the number of time that word appear in the sentence. as a several-hundred-el (low-dimensional) dens vector in which each element hold a floating-point valu between 0 and 1. this is an embedding. in tensorflow, embed are train by backpropag loss just like ani other paramet in a neural network.
empir risk minim (erm) choos the model function that minim loss on the train set. contrast with structur risk minimization.
ensembl a merger of the predict of multipl models. you can creat an ensembl via one or more of the following: differ initi differ hyperparamet differ overal structur deep and wide model are a kind of ensemble.
epoch a full train pass over the entir data set such that each exampl has been seen once. thus, an epoch repres n/batch size train iterations, where n is the total number of examples.
estim an instanc of the tf.estim class, which encapsul logic that build a tensorflow graph and run a tensorflow session. you may creat your own custom estim (as describ here) or instanti pre-mad estim creat by others.
exampl one row of a data set. an exampl contain one or more featur and possibl a label. see also label exampl and unlabel example.
fals negat (fn) an exampl in which the model mistaken predict the negat class. for example, the model infer that a particular email messag was not spam (the negat class), but that email messag actual was spam.
fals posit (fp) an exampl in which the model mistaken predict the posit class. for example, the model infer that a particular email messag was spam (the posit class), but that email messag was actual not spam.
fals posit rate (fp rate) the x-axi in an roc curve. the fp rate is defin as follows: fals posit rate=fals positives/(fals positives+tru negatives)
featur an input variabl use in make predictions.
featur column a set of relat features, such as the set of all possibl countri in which user might live. an exampl may have one or more featur present in a featur column. featur column in tensorflow also encapsul metadata such as: the featur data type whether a featur is fix length or should be convert to an embed a featur column can contain a singl feature. "featur column" is google-specif terminology. a featur column is refer to as a "namespace" in the vw system (at yahoo/microsoft), or a field.
featur cross a synthet featur ed by cross (multipli or take a cartesian product of) individu features. featur cross help repres nonlinear relationships.
featur engin the process of determin which featur might be use in train a model, and then convert raw data from log file and other sourc into said features. in tensorflow, featur engin often mean convert raw log file entri to tf.exampl protocol buffers. see also tf.tran . featur engin is sometim call featur extraction.
featur set the group of featur your machin learn model train on. for example, postal code, properti size, and properti condit might compris a simpl featur set for a model that predict hous prices.
featur spec describ the in ation requir to extract featur data from the tf.exampl protocol buffer. becaus the tf.exampl protocol buffer is just a contain for data, you must specifi the following: the data to extract (that is, the key for the features) the data type (for example, float or int) the length (fix or variable) the estim api provid facil for produc a featur spec from a list of featurecolumns.
full softmax see softmax. contrast with candid sampling.
fulli connect layer a hidden layer in which each node is connect to everi node in the subsequ hidden layer. a fulli connect layer is also known as a dens layer.
general refer to your model abil to make correct predict on new, previous unseen data as oppos to the data use to train the model.
general linear model a general of least squar regress models, which are base on gaussian noise, to other type of model base on other type of noise, such as poisson nois or categor noise. exampl of general linear model include: logist regress multi-class regress least squar regress the paramet of a general linear model can be found through convex optimization. general linear model exhibit the follow properties: the averag predict of the optim least squar regress model is equal to the averag label on the train data. the averag probabl predict by the optim logist regress model is equal to the averag label on the train data. the power of a general linear model is limit by it features. unlik a deep model, a general linear model cannot "learn new features."
gradient the vector of partial deriv with respect to all of the independ variables. in machin learning, the gradient is the the vector of partial deriv of the model function. the gradient point in the direct of steepest ascent.
gradient clip cap gradient valu befor appli them. gradient clip help ensur numer stabil and prevent explod gradients.
gradient descent a techniqu to minim loss by comput the gradient of loss with respect to the model parameters, condit on train data. in ally, gradient descent iter adjust parameters, gradual find the best combin of weight and bias to minim loss.
graph in tensorflow, a comput specification. node in the graph repres operations. edg are direct and repres pass the of an oper (a tensor) as an operand to anoth operation. use tensorboard to visual a graph.
heurist a practic and nonoptim solut to a problem, which is suffici for make progress or for learn from.
hidden layer a synthet layer in a neural network between the input layer (that is, the features) and the output layer (the prediction). a neural network contain one or more hidden layers.
hing loss a famili of loss function for classif design to find the decis boundari as distant as possibl from each train example, thus maxim the margin between exampl and the boundary. ksvms use hing loss (or a relat function, such as squar hing loss). for binari classification, the hing loss function is defin as follows: loss=max(0,1−(y′∗y)) where y' is the raw output of the classifi model: y′=b+w1x1+w2x2+...wnxn and y is the true label, either -1 or +1. consequently, a plot of hing loss vs. (y * y') look as follows:
holdout data exampl intent not use ("held out") dure training. the valid data set and test data set are exampl of holdout data. holdout data help evalu your model abil to general to data other than the data it was train on. the loss on the holdout set provid a better estim of the loss on an unseen data set than doe the loss on the train set.
hyperparamet the "knobs" that you tweak dure success run of train a model. for example, learn rate is a hyperparameter. contrast with parameter.
hyperplan a boundari that separ a space into two subspaces. for example, a line is a hyperplan in two dimens and a plane is a hyperplan in three dimensions. more typic in machin learning, a hyperplan is the boundari separ a high-dimension space. kernel support vector machin use hyperplan to separ posit class from negat classes, often in a veri high-dimension space.
independ and ident distribut (iid) data drawn from a distribut that doesn't change, and where each valu drawn doesn't depend on valu that have been drawn previously. an i.i.d. is the ideal gas of machin learning—a use mathemat construct but almost never exact found in the real world. for example, the distribut of visitor to a web page may be i.i.d. over a brief window of time; that is, the distribut doesn't chang dure that brief window and one person visit is general independ of anoth visit. however, if you expand that window of time, season differ in the web page visitor may appear.
infer in machin learning, often refer to the process of make predict by appli the train model to unlabel examples. in statistics, infer refer to the process of fit the paramet of a distribut condit on some observ data. (see the wikipedia articl on statist inference.)
input function in tensorflow, a function that return input data to the training, evaluation, or predict method of an estimator. for example, the train input function return a batch of featur and label from the train set.
input layer the first layer (the one that receiv the input data) in a neural network.
instanc synonym for example.
interpret the degre to which a model predict can be readili explained. deep model are often non-interpretable; that is, a deep model differ layer can be hard to decipher. by contrast, linear regress model and wide model are typic far more interpretable.
inter-rat agreement a measur of how often human rater agre when do a task. if rater disagree, the task instruct may need to be improved. also sometim call inter-annot agreement or inter-rat reliability. see also cohen kappa, which is one of the most popular inter-rat agreement measurements.
iter a singl updat of a model weight dure training. an iter consist of comput the gradient of the paramet with respect to the loss on a singl batch of data.
kera a popular python machin learn api. kera run on sever deep learn frameworks, includ tensorflow, where it is made avail as tf.keras.
kernel support vector machin (ksvms) a classif algorithm that seek to maxim the margin between posit and negat class by map input data vector to a higher dimension space. for example, consid a classif problem in which the input data set consist of a hundr features. in order to maxim the margin between posit and negat classes, ksvms could intern map those featur into a million-dimens space. ksvms use a loss function call hing loss.
l1 loss loss function base on the absolut valu of the differ between the valu that a model is predict and the actual valu of the labels. l1 loss is less sensit to outlier than l2 loss.
l1 regular a type of regular that penal weight in proport to the sum of the absolut valu of the weights. in model reli on spars features, l1 regular help drive the weight of irrelev or bare relev featur to exact 0, which remov those featur from the model. contrast with l2 regularization.
l2 loss see squar loss.
l2 regular a type of regular that penal weight in proport to the sum of the squar of the weights. l2 regular help drive outlier weight (those with high posit or low negat values) closer to 0 but not quit to 0. (contrast with l1 regularization.) l2 regular alway improv general in linear models.
label in supervis learning, the "answer" or " " portion of an example. each exampl in a label data set consist of one or more featur and a label. for instance, in a hous data set, the featur might includ the number of bedrooms, the number of bathrooms, and the age of the house, while the label might be the hous price. in a spam detect dataset, the featur might includ the subject line, the sender, and the email messag itself, while the label would probabl be either "spam" or "not spam."
lambda synonym for regular rate. (this is an overload term. here we'r focus on the term definit within regularization.)
layer a set of neuron in a neural network that process a set of input features, or the output of those neurons. also, an abstract in tensorflow. layer are python function that take tensor and configur option as input and produc other tensor as output. onc the necessari tensor have been composed, the user can convert the into an estim via a model function.
layer api a tensorflow api for construct a deep neural network as a composit of layers. the layer api enabl you to build differ type of layers, such as: tf.layers.dens for a fully-connect layer. tf.layers.conv2d for a convolut layer. when write a custom estimator, you compos layer object to defin the characterist of all the hidden layers. the layer api follow the [keras](#keras] layer api conventions. that is, asid from a differ prefix, all function in the layer api have the same name and signatur as their counterpart in the kera layer api.
learn rate a scalar use to train a model via gradient descent. dure each iteration, the gradient descent algorithm multipli the learn rate by the gradient. the ing product is call the gradient step. learn rate is a key hyperparameter.
least squar regress a linear regress model train by minim l2 loss.
linear regress a type of regress model that output a continu valu from a linear combin of input features.
logist regress a model that generat a probabl for each possibl discret label valu in classif problem by appli a sigmoid function to a linear prediction. although logist regress is often use in binari classif problems, it can also be use in multi-class classif problem (where it becom call multi-class logist regress or multinomi regression).
log loss the loss function use in binari logist regression.
loss a measur of how far a model predict are from it label. or, to phrase it more pessimistically, a measur of how bad the model is. to determin this value, a model must defin a loss function. for example, linear regress model typic use mean squar error for a loss function, while logist regress model use log loss.
machin learn a program or system that build (trains) a predict model from input data. the system use the learn model to make use predict from new (never-before-seen) data drawn from the same distribut as the one use to train the model. machin learn also refer to the field of studi concern with these program or systems.
mean squar error (mse) the averag squar loss per example. mse is calcul by divid the squar loss by the number of examples. the valu that tensorflow playground s for "train loss" and "test loss" are mse.
metric a number that you care about. may or may not be direct optim in a machine-learn system. a metric that your system tri to optim is call an objective.
metric api a tensorflow api for evalu models. for example, tf.metrics.accuraci determin how often a model predict match labels. when write a custom estimator, you invok metric api function to specifi how your model should be evaluated.
mini-batch a small, random select subset of the entir batch of exampl run togeth in a singl iter of train or inference. the batch size of a mini-batch is usual between 10 and 1,000. it is much more effici to calcul the loss on a mini-batch than on the full train data.
mini-batch stochast gradient descent (sgd) a gradient descent algorithm that use mini-batches. in other words, mini-batch sgd estim the gradient base on a small subset of the train data. vanilla sgd use a mini-batch of size 1.
model the represent of what an ml system has learn from the train data. this is an overload term, which can have either of the follow two relat meanings: the tensorflow graph that express the structur of how a predict will be computed. the particular weight and bias of that tensorflow graph, which are determin by training.
model train the process of determin the best model.
momentum a sophist gradient descent algorithm in which a learn step depend not onli on the deriv in the current step, but also on the deriv of the step(s) that immedi preced it. momentum involv comput an exponenti weight move averag of the gradient over time, analog to momentum in physics. momentum sometim prevent learn from get stuck in local minima.
multi-class classif classif problem that distinguish among more than two classes. for example, there are approxim 128 speci of mapl trees, so a model that categor mapl tree speci would be multi-class. conversely, a model that divid email into onli two categori (spam and not spam) would be a binari classif model.
multinomi classif synonym for multi-class classification.
nan trap when one number in your model becom a nan dure training, which caus mani or all other number in your model to eventu becom a nan. nan is an abbrevi for "not a number."
negat class in binari classification, one class is term posit and the other is term negative. the posit class is the thing we'r look for and the negat class is the other possibility. for example, the negat class in a medic test might be "not tumor." the negat class in an email classifi might be "not spam." see also posit class.
neural network a model that, take inspir from the brain, is compos of layer (at least one of which is hidden) consist of simpl connect unit or neuron follow by nonlinearities.
neuron a node in a neural network, typic take in multipl input valu and generat one output value. the neuron calcul the output valu by appli an ation function (nonlinear tran ation) to a weight sum of input values.
node an overload term that mean either of the following: a neuron in a hidden layer. an oper in a tensorflow graph.
normal the process of convert an actual rang of valu into a standard rang of values, typic -1 to +1 or 0 to 1. for example, suppos the natur rang of a certain featur is 800 to 6,000. through subtract and division, you can normal those valu into the rang -1 to +1. see also scaling.
numer data featur repres as integ or real-valu numbers. for example, in a real estat model, you would probabl repres the size of a hous (in squar feet or squar meters) as numer data. repres a featur as numer data indic that the featur valu have a mathemat relationship to each other and possibl to the label. for example, repres the size of a hous as numer data indic that a 200 square-met hous is twice as larg as a 100 square-met house. furthermore, the number of squar meter in a hous probabl has some mathemat relationship to the price of the house. not all integ data should be repres as numer data. for example, postal code in some part of the world are integers; however, integ postal code should not be repres as numer data in models. that becaus a postal code of 20000 is not twice (or half) as potent as a postal code of 10000. furthermore, although differ postal code do correl to differ real estat values, we can't assum that real estat valu at postal code 20000 are twice as valuabl as real estat valu at postal code 10000. postal code should be repres as categor data instead. numer featur are sometim call continu features.
numpi an open-sourc math librari that provid effici array oper in python. panda is built on numpy.
object a metric that your algorithm is tri to optimize.
offlin infer generat a group of predictions, store those predictions, and then retriev those predict on demand. contrast with onlin inference.
one-hot encod a spars vector in which: one element is set to 1. all other element are set to 0. one-hot encod is common use to repres string or identifi that have a finit set of possibl values. for example, suppos a given botani data set chronicl 15,000 differ species, each denot with a uniqu string identifier. as part of featur engineering, you'll probabl encod those string identifi as one-hot vector in which the vector has a size of 15,000.
one vs. all given a classif problem with n possibl solutions, a one-vs.-al solut consist of n separ binari classifiers—on binari classifi for each possibl outcome. for example, given a model that classifi exampl as animal, vegetable, or mineral, a one-vs.-al solut would provid the follow three separ binari classifiers: anim vs. not anim veget vs. not veget miner vs. not miner
onlin infer generat predict on demand. contrast with offlin inference.
oper (op) a node in the tensorflow graph. in tensorflow, ani procedur that creates, manipulates, or destroy a tensor is an operation. for example, a matrix multipli is an oper that take two tensor as input and generat one tensor as output.
optim a specif implement of the gradient descent algorithm. tensorflow base class for optim is tf.train.optimizer. differ optim may leverag one or more of the follow concept to enhanc the effect of gradient descent on a given train set: momentum (momentum) updat frequenc (adagrad = adapt gradient descent; adam = adapt with momentum; rmsprop) sparsity/regular (ftrl) more complex math (proximal, and others) you might even imagin an nn-driven optimizer.
outlier valu distant from most other values. in machin learning, ani of the follow are outliers: weight with high absolut values. predict valu relat far away from the actual values. input data whose valu are more than rough 3 standard deviat from the mean. outlier often caus problem in model training.
output layer the "final" layer of a neural network. the layer contain the answer(s).
overfit creat a model that match the train data so close that the model fail to make correct predict on new data.
panda a column-ori data analysi api. mani ml frameworks, includ tensorflow, support panda data structur as input. see panda documentation.
paramet a variabl of a model that the ml system train on it own. for example, weight are paramet whose valu the ml system gradual learn through success train iterations. contrast with hyperparameter.
paramet server (ps) a job that keep track of a model paramet in a distribut setting.
paramet updat the oper of adjust a model paramet dure training, typic within a singl iter of gradient descent.
partial deriv a deriv in which all but one of the variabl is consid a constant. for example, the partial deriv of f(x, y) with respect to x is the deriv of f consid as a function of x alon (that is, keep y constant). the partial deriv of f with respect to x focus onli on how x is chang and ignor all other variabl in the equation.
partit strategi the algorithm by which variabl are divid across paramet servers.
per anc overload term with the follow meanings: the tradit mean within softwar engineering. namely: how fast (or efficiently) doe this piec of softwar run the mean within ml. here, per anc answer the follow question: how correct is this model that is, how good are the model predict
perplex one measur of how well a model is accomplish it task. for example, suppos your task is to read the first few letter of a word a user is type on a smartphon keyboard, and to offer a list of possibl complet words. perplexity, p, for this task is approxim the number of guess you need to offer in order for your list to contain the actual word the user is tri to type. perplex is relat to cross-entropi as follows:
pipelin the infrastructur surround a machin learn algorithm. a pipelin includ gather the data, put the data into train data files, train one or more models, and export the model to production.
posit class in binari classification, the two possibl class are label as posit and negative. the posit outcom is the thing we'r test for. (admittedly, we'r simultan test for both outcomes, but play along.) for example, the posit class in a medic test might be "tumor." the posit class in an email classifi might be "spam." contrast with negat class.
precis a metric for classif models. precis identifi the frequenc with which a model was correct when predict the posit class. that is: precision=tru positives/(tru positives+fals positives)
predict a model output when provid with an input example.
predict bias a valu indic how far apart the averag of predict is from the averag of label in the data set.
pre-mad estim an estim that someon has alreadi built. tensorflow provid sever pre-mad estimators, includ dnnclassifier, dnnregressor, and linearclassifier. you may build your own pre-mad estim by follow these instructions.
pre-train model model or model compon (such as embeddings) that have been alreadi been trained. sometimes, you'll feed pre-train embed into a neural network. other times, your model will train the embed itself rather than reli on the pre-train embeddings.
prior belief what you believ about the data befor you begin train on it. for example, l2 regular reli on a prior belief that weight should be small and normal distribut around zero.
queue a tensorflow oper that implement a queue data structure. typic use in i/o.
rank overload term in ml that can mean either of the following: the number of dimens in a tensor. for instance, a scalar has rank 0, a vector has rank 1, and a matrix has rank 2. the ordin posit of a class in an ml problem that categor class from highest to lowest. for example, a behavior rank system could rank a dog reward from highest (a steak) to lowest (wilt kale).
rater a human who provid label in examples. sometim call an "annotator."
recal a metric for classif model that answer the follow question: out of all the possibl posit labels, how mani did the model correct identifi that is: recall=tru positives/(tru positives+fals negatives)
rectifi linear unit (relu) an ation function with the follow rules: if input is negat or zero, output is 0. if input is positive, output is equal to input.
regress model a type of model that output continu (typically, floating-point) values. compar with classif models, which output discret values, such as "day lily" or "tiger lily."
regular the penalti on a model complexity. regular help prevent overfitting. differ kind of regular include: l1 regular l2 regular dropout regular earli stop (this is not a al regular method, but can effect limit overfitting)
regular rate a scalar value, repres as lambda, specifi the relat import of the regular function. the follow simplifi loss equat show the regular rate influence: minimize(loss function + λ(regular function)) rais the regular rate reduc overfit but may make the model less accurate.
represent the process of map data to use features.
roc (receiv oper characteristic) curv a curv of true posit rate vs. fals posit rate at differ classif thresholds. see also auc.
root directori the directori you specifi for host subdirectori of the tensorflow checkpoint and event file of multipl models.
root mean squar error (rmse) the squar root of the mean squar error.
savedmodel the recommend at for save and recov tensorflow models. savedmodel is a language-neutral, recover serial at, which enabl higher-level system and tool to produce, consume, and tran tensorflow models. see save and restor in the tensorflow programm guid for complet details.
saver a tensorflow object respons for save model checkpoints.
scale a common use practic in featur engin to tame a featur rang of valu to match the rang of other featur in the data set. for example, suppos that you want all floating-point featur in the data set to have a rang of 0 to 1. given a particular featur rang of 0 to 500, you could scale that featur by divid each valu by 500. see also normalization.
scikit-learn a popular open-sourc ml plat . see www.scikit-learn.org.
semi-supervis learn train a model on data where some of the train exampl have label but other don't. one techniqu for semi-supervis learn is to infer label for the unlabel examples, and then to train on the infer label to creat a new model. semi-supervis learn can be use if label are expens to obtain but unlabel exampl are plentiful.
sequenc model a model whose input have a sequenti dependence. for example, predict the next video watch from a sequenc of previous watch videos.
session maintain state (for example, variables) within a tensorflow program.
sigmoid function a function that map logist or multinomi regress output (log odds) to probabilities, return a valu between 0 and 1. the sigmoid function has the follow ula: y=11+e−σ where σ in logist regress problem is simply: σ=b+w1x1+w2x2+...wnxn in other words, the sigmoid function convert σ into a probabl between 0 and 1. in some neural networks, the sigmoid function act as the ation function.
softmax a function that provid probabl for each possibl class in a multi-class classif model. the probabl add up to exact 1.0. for example, softmax might determin that the probabl of a particular imag be a dog at 0.9, a cat at 0.08, and a hors at 0.02. (also call full softmax.) contrast with candid sampling.
spars featur featur vector whose valu are predomin zero or empty. for example, a vector contain a singl 1 valu and a million 0 valu is sparse. as anoth example, word in a search queri could also be a spars feature—ther are mani possibl word in a given language, but onli a few of them occur in a given query. contrast with dens feature.
squar hing loss the squar of the hing loss. squar hing loss penal outlier more harsh than regular hing loss.
squar loss the loss function use in linear regression. (also known as l2 loss.) this function calcul the squar of the differ between a model predict valu for a label exampl and the actual valu of the label. due to squaring, this loss function amplifi the influenc of bad predictions. that is, squar loss react more strong to outlier than l1 loss.
static model a model that is train offline.
stationar a properti of data in a data set, in which the data distribut stay constant across one or more dimensions. most commonly, that dimens is time, mean that data exhibit stationar doesn't chang over time. for example, data that exhibit stationar doesn't chang from septemb to december.
step a forward and backward evalu of one batch.
step size synonym for learn rate.
stochast gradient descent (sgd) a gradient descent algorithm in which the batch size is one. in other words, sgd reli on a singl exampl chosen uni ly at random from a data set to calcul an estim of the gradient at each step.
structur risk minim (srm) an algorithm that balanc two goals: the desir to build the most predict model (for example, lowest loss). the desir to keep the model as simpl as possibl (for example, strong regularization). for example, a model function that minim loss+regular on the train set is a structur risk minim algorithm. for more in ation, see http://www.svms.org/srm/. contrast with empir risk minimization.
summari in tensorflow, a valu or set of valu calcul at a particular step, usual use for track model metric dure training.
supervis machin learn train a model from input data and it correspond labels. supervis machin learn is analog to a student learn a subject by studi a set of question and their correspond answers. after master the map between question and answers, the student can then provid answer to new (never-before-seen) question on the same topic. compar with unsupervis machin learning.
synthet featur a featur that is not present among the input features, but is deriv from one or more of them. kind of synthet featur includ the following: multipli one featur by itself or by other feature(s). (these are term featur crosses.) divid one featur by a second feature. bucket a continu featur into rang bins. featur creat by normal or scale alon are not consid synthet features.
target synonym for label.
tempor data data record at differ point in time. for example, winter coat sale record for each day of the year would be tempor data.
tensor the primari data structur in tensorflow programs. tensor are n-dimension (where n could be veri large) data structures, most common scalars, vectors, or matrices. the element of a tensor can hold integer, floating-point, or string values.
tensor process unit (tpu) an asic (application-specif integr circuit) that optim the per anc of tensorflow programs.
tensor rank see rank.
tensor shape the number of element a tensor contain in various dimensions. for example, a [5, 10] tensor has a shape of 5 in one dimens and 10 in another.
tensor size the total number of scalar a tensor contains. for example, a [5, 10] tensor has a size of 50.
tensorboard the dashboard that s the summari save dure the execut of one or more tensorflow programs.
tensorflow a large-scale, distributed, machin learn plat . the term also refer to the base api layer in the tensorflow stack, which support general comput on dataflow graphs. although tensorflow is primarili use for machin learning, you may also use tensorflow for non-ml task that requir numer comput use dataflow graphs.
tensorflow playground a program that visual how differ hyperparamet influenc model (primarili neural network) training. go to http://playground.tensorflow.org to experi with tensorflow playground.
tensorflow serv a plat to deploy train model in production.
test set the subset of the data set that you use to test your model after the model has gone through initi vet by the valid set. contrast with train set and valid set.
tf. exampl a standard protocol buffer for describ input data for machin learn model train or inference.
time seri analysi a subfield of machin learn and statist that analyz tempor data. mani type of machin learn problem requir time seri analysis, includ classification, clustering, forecasting, and anomali detection. for example, you could use time seri analysi to forecast the futur sale of winter coat by month base on histor sale data.
train the process of determin the ideal paramet compris a model.
train set the subset of the data set use to train a model. contrast with valid set and test set.
transfer learn transfer in ation from one machin learn task to another. for example, in multi-task learning, a singl model solv multipl tasks, such as a deep model that has differ output node for differ tasks. transfer learn might involv transfer knowledg from the solut of a simpler task to a more complex one, or involv transfer knowledg from a task where there is more data to one where there is less data. most machin learn system solv a singl task. transfer learn is a babi step toward artifici intellig in which a singl program can solv multipl tasks.
true negat (tn) an exampl in which the model correct predict the negat class. for example, the model infer that a particular email messag was not spam, and that email messag realli was not spam.
true posit (tp) an exampl in which the model correct predict the posit class. for example, the model infer that a particular email messag was spam, and that email messag realli was spam.
true posit rate (tp rate) synonym for recall. that is: true posit rate=tru positives/(tru positives+fals negatives) true posit rate is the y-axi in an roc curve.
unlabel exampl an exampl that contain featur but no label. unlabel exampl are the input to inference. in semi-supervis and unsupervis learning, unlabel exampl are use dure training.
unsupervis machin learn train a model to find pattern in a data set, typic an unlabel data set. the most common use of unsupervis machin learn is to cluster data into group of similar examples. for example, an unsupervis machin learn algorithm can cluster song togeth base on various properti of the music. the ing cluster can becom an input to other machin learn algorithm (for example, to a music recommend service). cluster can be help in domain where true label are hard to obtain. for example, in domain such as anti-abus and fraud, cluster can help human better understand the data. anoth exampl of unsupervis machin learn is princip compon analysi (pca). for example, appli pca on a data set contain the content of million of shop cart might reveal that shop cart contain lemon frequent also contain antacids. compar with supervis machin learning.
valid set a subset of the data set—disjunct from the train set—that you use to adjust hyperparameters. contrast with train set and test set.
weight a coeffici for a featur in a linear model, or an edg in a deep network. the goal of train a linear model is to determin the ideal weight for each feature. if a weight is 0, then it correspond featur doe not contribut to the model.
wide model a linear model that typic has mani spars input features. we refer to it as "wide" sinc such a model is a special type of neural network with a larg number of input that connect direct to the output node. wide model are often easier to debug and inspect than deep models. although wide model cannot express nonlinear through hidden layers, they can use tran ation such as featur cross and bucket to model nonlinear in differ ways. contrast with deep model.
supervis learn ml techniqu where you have the answer for a sub-set of data which is use to learn from larg set of data
unsupervis learn ml techniqu where you do not have an answer and are reli on her ml algorithm to identifi structure(pattern or clusters)
regress analysi predict the valu y for a give n set of x variabl
interpol predict of valu within your data set
extrapol predict of valu outsid your data set
categor problem a problem where there are a finit number of answer(yes, no), (dog, cat, snake)
regress problem a problem where there are an infinit amount of answers: "how much is my hous worth "
how do you handl infinit attribut in a learn algorithm support vector machin
cost function - function that measur the distance(cost) between the hypothesi fx and train set - a minim problem
hypothesi - a function we hope repres the actual data set
hypothesi function 
gradient descent - iter optim algorithm - find local minimum by repeat take step in the direct of the steepest descent(slope) - the size of the step is call the "learn weight"
gradient descent equat 
gradient descent step size/slop - when alpha (step size, slope) is too small more calcul will be made on the way to the local minimum - when alpha (step size, slope)i too larg it may fail to converg or even diverg - as we approach the local minimum the slope get steeper, eventu reach 0
slop observ - rise/run - larger the slope the steeper - smaller the slope the more gradual - slope of 0 is horizont line
tangent line a straight line is said to be a tangent of a curv y=f(x) at a point x=c on the curv if the line pass through the point (c, f(c)) on the curv and has slope f (c) where f ' is the deriv of f.
m x n matrix - m row - n column
matrix addit - must be same dimens
matrix multipl  
matrix transpos  
matrix ident  
matrix solv first polynomi  
gradient descent & matrix multipl  
perceptron a devic that make decis by weigh up evidence. take binari inputs, x1, x2...and produc a singl binari output. to comput the output, you weight the import of input to the output. the output, 0 or 1, is determin by whether the sum is less than or greater than a threshold value. output={0 if ∑jwjxj≤ threshold 1 if ∑jwjxj threshold }
bias a measur of how easi it is to get the perceptron to output a 1. or how easi it is to get the perceptron to fire. if the bias is veri negative, it will be diffiult to output a 1.
input layer input like x1 and x2 drawn as a layer of perceptrons, think of them as unit simpli defin to output desir valu x1, x2....
learn algorithm can automat tune the weight and bias of a network of artifici neuron
sigmoid neuron sigmoid neuron are similar to perceptrons, but modifi so that small chang in their weight and bias caus onli a small chang in their output. while sigmoid neuron have much of the same qualit behaviour as perceptrons, they make it much easier to figur out how chang the weight and bias will chang the output.
sigmoid function by use the actual σσ function we get, as alreadi impli above, a smooth out perceptron. indeed, it the smooth of the σσ function that is the crucial fact
cost function we can measur the accuraci of our hypothesi function by use a cost function. this take an averag differ (actual a fancier version of an average) of all the s of the hypothesi with input from x and the actual output y's. j(θ0,θ1)=12m∑i=1m(y^i−yi)2=12m∑i=1m(hθ(xi)−yi)2 to break it apart, it is 12 x¯ where x¯ is the mean of the squar of hθ(xi)−yi , or the differ between the predict valu and the actual value. this function is otherwis call the "squar error function", or "mean squar error". the mean is halv (12) as a conveni for the comput of the gradient descent, as the deriv term of the squar function will cancel out the 12 term. the follow imag summar what the cost function does:
classif to attempt classification, one method is to use linear regress and map all predict greater than 0.5 as a 1 and all less than 0.5 as a 0. however, this method doesn't work well becaus classif is not actual a linear function. the classif problem is just like the regress problem, except that the valu we now want to predict take on onli a small number of discret values. for now, we will focus on the binari classif problem in which y can take on onli two values, 0 and 1. (most of what we say here will also general to the multiple-class case.) for instance, if we are tri to build a spam classifi for email, then x(i) may be some featur of a piec of email, and y may be 1 if it is a piec of spam mail, and 0 otherwise. hence, y∈{0,1}. 0 is also call the negat class, and 1 the posit class, and they are sometim also denot by the symbol "-" and "+." given x(i), the correspond y(i) is also call the label for the train example.
sigmoid function also call the "logist function"
hθ(x)=g(θtx)z=θtxg(z)=11+e−z the function g(z), shown here, map ani real number to the (0, 1) interval, make it use for tran ing an arbitrary-valu function into a function better suit for classification. hθ(x) will give us the probabl that our output is 1. for example, hθ(x)=0.7 give us a probabl of 70% that our output is 1. our probabl that our predict is 0 is just the complement of our probabl that it is 1 (e.g. if probabl that it is 1 is 70%, then the probabl that it is 0 is 30%). hθ(x)=p(y=1 x;θ)=1−p(y=0 x;θ)p(y=0 x;θ)+p(y=1 x;θ)=1
decis boundari the line that separ the area where y = 0 and where y = 1. it is creat by our hypothesi function.
logist regress cost function we cannot use the same function that we use for linear regress becaus the logist function will caus the output to be wavy, caus mani local optima. in other words, it will not be a convex function.
convex optimizationfunct there can be onli one optim solution, which is global optim or you might prove that there is no feasibl solut to the problem, while in b) ------- may have multipl local optim point and it can take a lot of time to identifi whether the problem has no solut or if the solut is global. hence, the effici in time of the ---- --- problem is much better. from my experi a ----- problem usual is much more easier to deal with in comparison to a non ---- problem which take a lot of time and it might lead you to a dead end.
optim algorithm gradient descent; conjug gradient; bfgs; l-bfgs
the advantag of other optim algorithm over gradient descent - no need to manual pick the alpha - faster
the disadvantag of other optim algorithm over gradient descent more complex
sigmoid function 
precis (true positives) / (# predict positive)
recal (true positives) / (# actual positive)
f score 1/2 * (precis * recall) / (precis + recall)
large-margin classifi  
support-vector machin  
decis bounderi  
kernel a similar function
gaussian kernel defin by a mean and variance. or in multi-diment case, a mean vector and covari matrix.
"linear kernel"  
cluster  
iid ident distribut and independ drawn
densiti estim estim the probabl densiti function from which iid data has been drawn.
blind signal separ the problem of separ mix data from two sourc into the two seper signals. for example, seper audio of two peopl speak over eachoth into the seper voic signals.
factor analysi  
expect maximis general of k-mean clustering, where the correspond is base on a kernel similar function, rather than be a hard 0 or 1 base on the closest cluster center.
dimension reduct linear and non-linear.
linear dimension reduct 1) fit a gaussian distribution. 2) calcul the gaussian covari matrix eigenvectors. 3) pick eigenvector which maxim eigenvalues. 4) this defin the princip dimens of the data. project data onto this vector.
spectral cluster cluster by affin or close of points. use princip compon analysis.
princip compon analysi find the eigenvector of the data. the one with larg eigenvalu correspond to the princip compon or dimens of the data.
what is featur scale method of appli weight to featur
what rang is use for featur scale 0-1
what is the ula for featur scale (x-xmin)/(xmax-xmin)
what two algorithm doe featur scale help with k-mean and svm rbf kernal
what two algorithm doe featur scale not help with linear regress and decis tree
what are two way to select featur selectpercentil and selectkbest
what is selectpercentil select the x% of featur that are most power (where x is a parameter)
what is selectkbest select the k featur that are most power (where k is a parameter)
what are two issu with overfit and underfit high bias and high varianc
what is high bias 1. pay littl attent to the data 2. oversimplifi 3. high error on train set
what is high varianc 1. pay too much attent to the data 2. much higher error rate on test data 3. consid overfit
what is the ideal featur select 1. fit data but with minimum number of featur 2. larg r2 (proxim of data to to fit regress line) 3. low sse (sum of squar errors), or the data is close to regress line
what is regular find the ideal number of featur to maxim model qualiti
what doe lasso regress do penal overfit
what is principl compon analysi (pca) reposit the 0 point to the middl of the dataset and redraw the x & y axi lose in ation by collaps data into one line
what are measur vs latent featur latent featur are group of measur featur e.g. given featur of a house, what is it price measur - squar footage, no of room latent - size, neighborhood
what are composit featur some instanc a larger number of smaller featur might denot an under trend
what problem are great for unsupervis learn data that isn't clean or flagged, e.g. cluster
what is a good cluster algorithm k-mean
what are the two step to k-mean cluster 1. assign cluster 2. optim cluster
how doe k-mean optim itself find the best centroid by assign mani centroid
what are some challeng with k-mean cluster 1. hill climb algorithm 2. veri depend on where you initi put your centroid 3. cluster chang with a fix dataset 4. veri import to assign cluster initi
what are the three set in classifi train 1. train 2. valid 3. test
what two set are use to train the classifi 1. train 2. valid
whi is the test set kept separ to avoid overtrain the classifi to the valid and train set
what is a decis tree tier set of decis to group and classifi a dataset and predict outcomes. e.g.: get out of bed (- make breakfast or exercise) or go back to bed (- sleep or look at phone) so, if i get out of bed, i'm not go to look on my phone or sleep
what is naiv bay naiv bay classifi assum that the presenc of a particular featur in a class is unrel to the presenc of ani other feature. e.g.: if 90% of spam email contain "cheap" (a feature) then we can estim that most inbox email that contain "cheap" are spam
what is gradient descent method of reduc the error rate in an algorithm by reduc the error on linear regress lines. e.g.: step down to the bottom (solution) a mountain (problem) to avoid ani cliffs. make one step, decid the next best step, and so on.
what is least squar in linear regress squar the line to data error and add them up
what is logist regress classifi data on a scatter plot and separ that data in the best way possibl e.g. boy and girl at a high school danc (girl on one side and boy on the other). draw a line between them and you have logist regress
what is the log loss function penal error in gradient descent e.g. all kid in high school are hormon (minor penalty) but the kid that should be in class but skip are penal more. add up all of the error and you have the qualiti of students. if you put all the kid skip class in their own school and all the kid attend in a differ school then your error rate would be best optim to get the best school.
what is a support vector machin maxim the minimum distanc of all errors. e.g. the dish is good by itself but to enhanc the dish, you put the most amount of salt in a dish without it tast too salti
what is a neural network take an input layer - hidden layer of logist regress - output of the hidden layer are binari that go to the output layer e.g. is that a hous cat input layer is whiskers, fur, paws, large. hidden layer find that cat are small (so the output of the hidden layer is 1, 1, 1 ,0). becaus not all featur (outputs) from the hidden layer are true, it not a hous cat.
what is the kernal method when you can't use logist regress becaus there isn't a clear delin between the two groups, you need to draw a curv line. multipli x & y to separ group on a 3d plane. e.g. monkey in the middle. if there are two peopl on either side of the person in the middle, how do you draw a straight line to separ the two group (e.g. logist regression) you can't. you have to draw a curv line.
what is k-mean cluster find the centroid of separ group e.g. how do you find out where to draw your servic line in you'r a pizza parlor find popul densiti for each pizza locat and put your pizza parlor there.
machin learn a comput program is said to learn from from experi e with respect to some task t and some per anc measur p, if it per anc on t, as measur by p, improv with experi e.
supervis learn we are given a data set and alreadi know what our correct output should look like, have the idea that there is a relationship between the input and the output. categor into "regression" and "classification" problems.
regress we are tri to predict s within a continu output, mean that we are tri to map input variabl to some continu function.
classif we are tri to predict s in a discret output. in other words, we are tri to map input variabl into discret categories.
unsupervis learn allow us to approach problem with littl or no idea what our s should look like. we can deriv structur from data where we don't necessarili know the effect of the variables.
cluster deriv a structur by group the data base on relatiship among the variabl
linear regress tri to fit a linear continu function to the data. univari or multivariate.
hypothesi function function to start with - improv with iter - reli on theta
cost function take an averag (actual a fancier version of an average) of all the s of the hypothesi with input from x compar to the actual output y's. we'r use the "squar error function".
gradient descent is an algorithm to improv a hypothesi function in respect to some cost function.
convex curv evenly, resembl a segment of the outer edg of a sphere
featur scale mean normal of data to speed up gradient descend
debug gradient descent make a plot with number of iter on the x-axis. now plot the cost function, j(θ) over the number of iter of gradient descent. if j(θ) ever increases, then you probabl need to decreas α.
automat converg test declar converg if j(θ) decreas by less than e in one iteration, where e is some small valu such as 10^-3
normal equat version of find the optimum without iteration. no featur scaling!
gradient descent vs normal equat need to choos alpha no need to choos alpha need mani iter no need to iter work well when n is larg slow if n is veri larg
negat class negat mean not have the symptom absens of someth - 0
posit class presenc of someth we are look for - 1
multiclass classif problem more class than the classic 2
logist regress a kind of regress analysi often use when the depend variabl is dichotom and score 0 or 1. it is usual use for predict whether someth will happen or not, such as graduation, busi failure, or heart attack-anyth that can be express as event/non-event. independ variabl may be categor or continu in logist regress analysis.
sigmoid function an s-shape mathamat curv is often use to describ the ation function of a neuron over time
decis boundari a hypersurfac that partit the under vector space into two sets, one for each class.
one-vs-al classif multiclass classif reduc a classif problem with multipl featur that have to be predict to a simpl classif problem by look at one featur at a time. then to determin the final predict we take the max of all the predict values.
underfit -coeffici are general bias (as well as inconsistent) "high bias" fit a line to a curv
overfit when the model describ an error or nois instead of the actual model. a problem that occur when data-min is use to creat over complex model that are not suit to make accur predictions. "high variance" if we have too mani featur the learn hypothesi may fit the train set veri well, but fail to generalzi to new examples(predict price on new examples)
model select algorithm algorithm that automat select a good model function for a dataset.
regular penal the higher-ord paramet to avoid overfit and get a simpler hypothesis.
regular paramet lambda - control the trade-off between fit the data well & keep the paramet small.
the "one learn algorithm" hypothesi hypothesi that there aren't n differ expert system in the brain but instead all region of the brain are theoret capabl of do everi path.
bias unit x_0 = 1
weight of the model paramet of the model in neuron network
hidden layer the second layer of a three-lay network where the input layer send it signals, per s intermediari process
neural network interconnect neural cells. with experience, network can learn, as feedback strengthen or inhibit connect that produc certain s. comput simul of neural network show analog learning.
forward propag in neuron network the process of calcul the subsequ layer of the network. each layer depend on the calcul done on the layer befor it.
architectur the connect pattern of the layer of a neuron network is also call it :
random initi symmetri break for neural network is achiev by:
autonom drive a car drive by itself without human interact
misclassif error defin test error as sum error for classif (fals prediction)
machin learn diagnost a test that you can run to gain insight what is/isn't work with a learn algorithm, and gain guidanc as to how best improv it per ance.
true posit hypothes correct predict posit output.
true negat hypothes correct predict negat output.
precis of all predict positives, how mani are posit
recal of all posit how mani were predict as posit
larg data rational assum data avail is suffici complex to theoret be abl to predict outcome. how big should the data-set be use a learn algorithm with mani featur - low bias - and a larg data set - low variance.
support vector machin can extrapol in ation from one dimension data (input space) and some in ation about weight & correl relationship to anoth dimens (featur space)
gaussian kernel similar function with exp(x) - landmark distanc
memor memorizing, given facts, is an obvious task in learning. this can be done by store the input sampl explicitly, or by identifi the concept behind the input data, and memor their general rules.
general the abil to identifi the rules, to generalize, allow the system to make predict on unknown data.
induct bias / learn bias the set of assumpt that the learner use to predict output given input that it has not encountered. e.g. maximum margin bias (svm), nearest neighbor bias (knn), etc.
loss function tell us how bad a system predict is in comparison to the truth. e.g. can be seen as a measur of error.
data generat distribut now that we have defin our loss function, we need to consid where the data (train and test) come from. the model that we will use is the probabilist model of learning. namely, there is a probabl distribut d over input/output pairs. this is often call the data generat distribution. a use way to think about d is that it give high probabl to reason (x, y) pairs, and low probabl to unreason (x, y) pairs.
expect loss the loss l (loss function) we expect given a data generat distribut d.
train error the train error is simpli our averag error over the train data.
formal definit of induct machin learn given (i) a loss function l and (ii) a sampl d from some unknown distribut d, you must comput a function f that has low expect error e over d with respect to l.
regular help avoid overfit by reduc the magnitud of a certain feature.
greedi algorythm greedi algorithm work by make the decis that seem most promis at ani moment; it never reconsid this decision, whatev situat may aris later. they are shortsight in their approach in the sens that they take decis on the basi of in ation at hand without worri about the effect these decis may have in the future.
"divid & conquer" algorithm a divid and conquer algorithm work by recurs break down a problem into two or more sub-problem of the same (or related) type (divide), until these becom simpl enough to be solv direct (conquer).
shallow decis tree we limit the depth of the decis tree
reason for failur in ml *noise* in the train data: 1) at featur level (e.g. incorrect valu such as typos) or 2) at label level (e.g. the wrong label is assign to a set of features). *insuffici features*: there are not enough featur / data avail for a learn algorithm to work. *more than one correct answer*: there might exist more than one correct answer. *induct bias* is too far away from the concept that is be learned.
hyperparamet a paramet that control other paramet in the model. cannot naiv adjust use the train data but need a valid set or develop data becaus if we do it on the train data we risk overfit wherea if we do it on the test data we break the rule that test data has alway have to be unseen.
converg read this: https://www.researchgate.net/post/how_to_proof_the_convergence_properties_of_a_metaheuristic_algorithm
hyperspher a geometr figur in four or more dimens which is analog to a cube in three dimensions.
hypercub a geometr figur in four or more dimens which is analog to a cube in three dimensions.
knn scale you should normal when the scale of a featur is irrelev or misleading, and not normal when the scale is meaningful. k-mean consid euclidean distanc to be meaningful. if a featur has a big scale compar to another, but the first featur truli repres greater diversity, then cluster in that dimens should be penalized.
how do we measur the *accuracy* of a hypothesi function by use a *cost function*, usual denot by j.
what is the definit of a *cost function* of a supervis learn problem take an averag differ of all the s of the hypothesi with input from x and the actual output y's.
what are altern term of a cost function #2 • *squar error function*. • *mean squar error*.
state the algorithm for *gradient descent*. repeat until convergence, where j=0,1 repres the featur index number.
how doe gradient descent converg with a *fixed* step size alpha #2 • as we approach a local minimum, gradient descent will take smaller steps. • thus no need to decreas alpha over time.
what is the algorithm for implement gradient descent for *linear regression* #2 • we can substitut our actual cost function and our actual hypothesi function. • m is the size of the train set, theta 0 a constant that will be chang simultan with theta 1 and x, y are valu of the given train set (data).
give a deriv of for a singl exampl in batch gradient descent! (gradient descent for linear regression) deriv of a singl variabl in gradient descent.
what is *batch gradient descent* #2 (gradient descent for linear regression) • gradient descent on the origin cost function j. •this method look at everi exampl in the entir train set on everi step.
how doe batch gradient descent differ from gradient descent (gradient descent for linear regression) while gradient descent can be suscept to local minima in general, batch gradient descent has onli one global, and no other local, optima.
depict an exampl of gradient descent as it is run to minim a quadrat function. #2 • shown is the trajectori taken by gradient descent, which was initi at 48,30. • the x in the figur (join by straight lines) mark the success valu of theta that gradient descent went through as it converg to it minimum.
what is *multivari linear regression* linear regress with multipl variables.
what is the notat for equat where we can have ani number of input variabl (multivari linear regression) notation.
what is the multivari of a hypothesi function multivari of the hypothesi function.
what is the intuit of the multivari of a hypothesi function in the exampl of estim hous price #2 • we can think about theta 0 as the basic price of a house, theta 1 as the price per squar meter, theta 2 as the price per floor, etc. • x1 will be the number of squar meter in the house, x2 the number of floors, etc.
give the *vectorization* of the multivari of a hypothesi function. use the definit of matrix multiplication, our multivari hypothesi function can be concis repres as:
whi do we assum that x0=1 in multivari linear regress convention.
what is the gradient descent for multipl variabl #2 • the gradient descent equat itself is general the same . • we just have to repeat it for our n features.
how can we *speed up* gradient descent we can speed up gradient descent by have each of our input valu in rough the same range.
whi doe *featur scaling* speed up gradient descent #2 • this is becaus theta will descend quick on small rang and slowli on larg ranges. • thus it will oscil ineffici down to the optimum when the variabl are veri uneven.
what are the ideal rang of our input variabl in gradient descent #2 • for exampl a rang between minus 1 and 1. • these aren't exact requirements; we are onli tri to speed thing up.
what is *featur scaling* #2 • involv divid the input valu by the rang (i.e. the maximum valu minus the minimum value) of the input variable. • result in a new rang of just 1.
what is *mean normalization* #2 • involv subtract the averag valu for an input variabl from the valu for that input variable. • result in a new averag valu for the input variabl of just zero.
how do you implement both featur scale and mean normal #2 featur scale and mean normalization.
how can you *debug* gradient descent #3 • make a plot with number of iter on the x-axis. • now plot the cost function j of theta over the number of iter of gradient descent. • if j of theta ever increases, then you probabl need to decreas alpha.
how can the step paramet alpha in gradient descent caus bug #2 • if alpha is too small: slow convergence. • if alpha is too large: may not decreas on everi iter and thus may not converge.
what is the *automat converg test* in gradient descent #2 • declar converg if j of theta decreas by less than e in one iteration, where e is some small valu such as 0.001. • howev in practic it difficult to choos this threshold value.
how can we improv our featur (multivari linear regression) #2 • we can *combine* multipl featur into one. • for example, we can combin x1 and x2 into a new featur x3 by take x1 time x2.
how can we *improve* the of our hypothesi function (multivari linear regression) by make it a *quadratic*, cubic or squar root function (or ani other ).
what import thing should one keep in mind if one chang the of a hypothesi function (multivari linear regression) #2 • if you creat new featur when do polynomi regress then *featur scaling* becom veri important. • for example, if x has rang 1 - 1000 then rang of x^2 becom 1 - 1000000.
state the *normal equat ula*! normal equat formula.
compar gradient descent and the normal equation! the follow is a comparison of gradient descent and the normal equation:
doe featur scale speed up the implement of the normal equat there is *no need* to do featur scale with the normal equation.
what is the complex of comput the invers with the normal equat with the normal equation, comput the invers has complex of n cubed.
when might it be a good time to go from a normal solut to an iter process when the number of exampl exceed *10,000* due to the complex of the normal equation.
which function do we want to use in octav when implement the normal equat #2 • use the pinv function rather than inv'. • the pinv function will give you a valu of theta even if x transpos x is not invertible.
what are common caus for x transpos x to be *noninvertible* #2 • redund features, where two featur are veri close relat (i.e. they are linear dependent). • too mani featur (e.g. m ≤ n). in this case, delet some featur or use "regularization".
how do we chang the of our binari hypothesi function to be continu in the rang between 0 and 1 by use the *sigmoid function*, also call the *logist function*.
how can we interpret the output of our logist function h of theta of a given input variabl give us the probabl that our output is 1.
how can we get our discret 0 or 1 classif from a logist function we can translat the output of the hypothesi function as follows:
what is the *decis boundary* given a logist function #2 • the decis boundari is the line that separ the area where y = 0 and where y = 1. • it is creat by our hypothesi function.
how doe the *cost function* for a logist regress look like #2 • we cannot use the same cost function that we use for linear regress becaus the logist function will caus the output to be wavy, caus mani local optima. • in other words, it will not be a convex function.
plot the cost function j, if the correct answer for y is 1. • if our correct answer y is 1, then the cost function will be 0 if our hypothesi function output 1. • if our hypothesi approach 0, then the cost function will approach infinity.
plot the cost function, if the correct answer for y is 0. #2 • if our correct answer y is 0, then the cost function will be 0 if our hypothesi function also output 0. • if our hypothesi approach 1, then the cost function will approach infinity.
how can we *simplify* our cost function (logist regress model) we can compress our cost function two condit case into one case.
give the vector implement of our simplifi cost function! (logist regress model) a vector implement is:
give the vector implement for gradient descent! (logist regress model) a vector implement is:
what is *gradient descent* for our simplifi cost function (logist regress model) #2 • notic that this algorithm is ident to the one we use in linear regression. • we still have to simultan updat all valu in theta.
depict an exampl of *one-versus-all* to classifi 3 classes! (multiclass classification) the follow imag show how one could classifi 3 classes:
what is the implement of *one-versus-all* in multiclass classif #2 • train a logist regress classifi h of theta for each class to predict the probabl that y = i . • to make a predict on a new x, pick the class that maxim h of theta.
what is *underfitting* underfitting, or high bias, is when the of our hypothesi function h map poor to the trend of the data.
what usual *causes* underfit it is usual caus by a function that is too simpl or use too few features.
what is *overfitting* overfitting, or *high variance*, is caus by a hypothesi function that fit the avail data but doe not general well to predict new data.
what usual *causes* overfit it is usual caus by a complic function that creat a lot of unnecessari curv and angl unrel to the data.
what are the two *main options* to address the issu of overfit #4 *reduc the number of features*: • manual select which featur to keep. • use a model select algorithm. *regularization*: • keep all the features, but reduc the magnitud of paramet theta j. • regular work well when we have a lot of slight use features.
in a basic sense, what are *neurons* neuron are basic comput unit that take inputs, call *dendrites*, as electr inputs, call "spikes", that are channel to output , call *axons*.
what are the *dendrites* in the model of neural network in our model, our dendrit are like the input features.
what are the *axons* in the model of neural network in our model, the axon are the s of our hypothesi function.
what is the *bia unit* of a neural network #2 • the input node x0 is sometim call the "bia unit." • it is alway equal to 1.
what are the *weights* of a neural network use the logist function, our "theta" paramet are sometim call "weights".
what is the * ation* function of a neural network the logist function (as in classification) is also call a *sigmoid (logistic) ation function*.
how do we label the hidden layer of a neural network #2 • we label these intermedi or hidden layer nodes. • the node are also call * ation units*.
how do we determin the dimens of the matric of weight (neural network) #2 • the +1 come from the addit of the "bia nodes. • in other word the output node will not includ the bias node while the input will.
how do we obtain the valu for each of the ation nodes, given a single-lay neural network with 3 ation node and a 4-dimension input #2 • we appli each row of the paramet to our input to obtain the valu for one ation node. • our hypothesi output is the logist function appli to the sum of the valu of our ation nodes, which have been multipli by the paramet matrix theta 2.
give an exampl of the implement of the *or-function* as a neural network! the follow is an exampl of the logic oper or', mean either x1 is true or x2 is true, or both:
give an exampl of the implement of the *and-function* as a neural network! the follow is an exampl of the logic oper and, mean it s onli true if both x1 and x2 are 1.
what are the theta-matric for implement the logic function and', nor', and or as a neural network theta matric for neural network implementation.
how can we implement the xnor oper with a neural network we can implement the xnor oper by use two hidden layers.
give an exampl of a neural network which classifi data into one of four categories! the inner layers, each provid us with some new in ation which lead to our final hypothesi function.
state the *cost function* for neural networks. #3 • the doubl sum simpli add up the logist regress cost calcul for each cell in the output layer. • the tripl sum simpli add up the squar of all the individu theta s in the entir network. • the i in the tripl sum doe not refer to train exampl i.
how are the variabl l, s of l and k in the cost function of a neural network defin #3 • l = total number of layer in the network. • s of l = number of unit (not count bias unit) in layer l. • k = number of output units.
defin *backpropagation* for neural networks. *backpropagation* is neural-network terminolog for *minimizing* our cost function.
what doe the matrix delta in the back propag algorithm do the capital-delta matrix d is use as an "accumulator" to add up our valu as we go along and eventu comput our partial derivative.
state the *backpropag algorithm*. backpropag algorithm.
which method random initi our weight for our theta matric of a neural network we initi each theta l,i,j to a random valu between minus epsilon and epsilon.
give the setup of use a neural network. #4 • pick a network *architecture*. • choos the *layout* of your neural network. • number of input units; dimens of featur x i. • number of output units; number of classes. • number of hidden unit per layer; usual more the better.
how doe one *train* a neural network #6 1. random initi the weights. 2. implement forward propagation. 3. implement the cost function. 4. implement backpropagation. 5. use gradient check to confirm that your backpropag works. 6. use gradient descent to minim the cost function with the weight in theta.
what code is implement if we per forward *and* back propag when we per forward and back propagation, we loop on everi train example.
how can we break down our decis process *decid what to do next* #6 • *get more train examples*: fix high variance. • *tri smaller set of features*: fix high variance. • *ad features*: fix high bias. • *ad polynomi features*: fix high bias. • *decreas lambda*: fix high bias. • *increas lambda*: fix high variance.
what issu pose a neural network with fewer paramet a neural network with fewer paramet is *prone to underfitting*.
what issu pose a neural network with more paramet a larg neural network with more paramet is *prone to overfitting*.
how can you address the overfit of a larg neural network in this case you can use regular (increas λ) to address the overfitting.
what bias-vari tradeoff do lower-ord polynomi (low model complexity) have lower-ord polynomi (low model complexity) have *high bias* and *low variance*.
what is the issu with higher-od polynomi in regard to fit the train data and test data higher-ord polynomi (high model complexity) fit the *train data* extrem well and the *test data* extrem poorly.
what bias-vari tradeoff do higher-ord polynomi (high model complexity) have higher-ord polynomi (high model complexity) have low bias on the train data, but veri high variance.
whi doe train an algorithm on a veri few number of data point easili have 0 error becaus we can alway find a quadrat curv that touch exact those number of points.
how doe one experi *high bias* with a low train set size *low train set size*: caus j train of theta to be low and j cv of theta to be high.
how doe one experi *high bias* with a larg train set size *larg train set size*: caus both j train of theta and j cv of theta to be high with j train approxim equal to j cv.
what approach will not general help much by itself, when a learn algorithm is suffer from high bias if a learn algorithm is suffer from high bias, *get more train data* will not (bi itself) help much.
how doe one experi *high variance* with a low train set size *low train set size*: j train of theta will be low and j cv of theta will be high.
how doe one experi *high variance* with a larg train set size *larg train set size*: j train of theta increas with train set size and j cv of theta continu to decreas without level off.
under which circumst will *get more train data* help a learn algorithm to per better if a learn algorithm is suffer from high variance, *get more train data* is like to help.
what is the relationship between the degre of the polynomi d and the underfit or overfit of our hypothesi #2 • *high bias (underfitting)*: both j train(θ) and j cv(θ) will be high. also, j cv(θ) is approxim equal to j train(θ). • *high varianc (overfitting)*: j train(θ) will be low and j cv(θ) will be much greater than j train(θ).
which 3 separ error valu can we calculate, if we break down our dataset as such: • train set: 60%. • cross valid set: 20%. • test set: 20%. 1. optim the paramet in theta (θ) use the train set for each polynomi degree. 2. find the polynomi degre d with the least error use the cross valid set. 3. estim the general error use the test set with j test, use d = theta from polynomi with lower error.
which function return the valu for jval and gradient in a singl turn function [jval, gradient] = costfunction(theta) jval = [...code to comput j(theta)...]; gradient = [...code to comput deriv of j(theta)...]; end
given costfunction(), what do we have to do to implement fminunc() we can use octav "fminunc()" optim algorithm along with the "optimset()" function that creat an object contain the option we want to send to "fminunc()"
how can we approach regular use the altern method of the non-it normal equat to add in regularization, the equat is the same as our original, except that we add anoth term insid the parentheses:
given a train set and a test set, what is the new procedur for evalu a hypothesi the new procedur use these two set is then: 1. learn θ and minim jtrain(θ) use the train set 2. comput the test set error jtest(θ)
what is the test set error for linear regress for linear regress
what is the test set error for linear classif for classif ~ misclassif error (aka 0/1 misclassif error):
what is the averag test error for the test set the averag test error for the test set is. this give us the proport of the test data that was misclassified.
just becaus a learn algorithm fit a train set well, that doe not mean [...] just becaus a learn algorithm fit a train set well, that doe not mean it is a good hypothesis.
the error of your hypothesi as measur on the data set with which you train the paramet will be [...] the error of your hypothesi as measur on the data set with which you train the paramet will be lower than ani other data set.
model select without the valid set 1. optim the paramet in θ use the train set for each polynomi degree. 2. find the polynomi degre d with the least error use the test set. 3. estim the general error also use the test set with jtest(θ(d)), (d = theta from polynomi with lower error);
what is the consequ of select a model without the valid set in this case, we have train one variable, d, or the degre of the polynomial, use the test set. this will caus our error valu to be greater for ani other set of data.
how can we solv the problem of select a model with onli the train set to solv this, we can introduc a third set, the cross valid set, to serv as an intermedi set that we can train d with. then our test set will give us an accurate, non-optimist error.
model select with the valid set 1. optim the paramet in θ use the train set for each polynomi degree. 2. find the polynomi degre d with the least error use the cross valid set. 3. estim the general error use the test set with jtest(θ(d)), (d = theta from polynomi with lower error);
what are the benefit of select a model with the valid set this way, the degre of the polynomi d has not been train use the test set. note: be awar that use the cv set to select d mean that we cannot also use it for the valid curv process of set the lambda valu
high bias (underfitting): both jtrain(θ) and jcv(θ) will be high. also, jcv(θ)≈jtrain(θ)
high varianc (overfitting): jtrain(θ) will be low and jcv(θ) will be much greater thanjtrain(θ).
a larg lambda [...], which [...]. a larg lambda heavili penal all the θ parameters, which great simplifi the line of our ing function, so caus underfitting.
the relationship of λ to the train set and the varianc set is as follows: low λ: jtrain(θ) is low and jcv(θ) is high (high variance/overfitting).
the relationship of λ to the train set and the varianc set is as follows: intermedi λ: jtrain(θ) and jcv(θ) are somewhat low and jtrain(θ)≈jcv(θ).
the relationship of λ to the train set and the varianc set is as follows: larg λ both jtrain(θ) and jcv(θ) will be high (underfit /high bias)
what do we need in order to choos the model and the regular λ 1. creat a list of lambda (i.e. λ∈{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24}); 2. creat a set of model with differ degre or ani other variants. 3. iter through the λs and for each λ go through all the model to learn some θ. 4. comput the cross valid error use the learn θ (comput with λ) on the jcv(θ) without regular or λ = 0. 5. select the best combo that produc the lowest error on the cross valid set. 6. use the best combo θ and λ, appli it on jtest(θ) to see if it has a good general of the problem.
as the train set get larger, the error [...] increases. as the train set get larger, the error for a quadrat function increases.
the error valu will [...] after a certain m, or train set size. the error valu will plateau out after a certain m, or train set size.
with high bias low train set size: [...] low train set size: caus jtrain(θ) to be low and jcv(θ) to be high.
with high bias larg train set size: [...] larg train set size: caus both jtrain(θ) and jcv(θ) to be high with jtrain(θ)≈jcv(θ).
with high varianc low train set size: [...] low train set size: jtrain(θ) will be low and jcv(θ) will be high.
with high varianc larg train set size: larg train set size: jtrain(θ) increas with train set size and jcv(θ) continu to decreas without level off. lso, jtrain(θ) jcv(θ) but the differ between them remain significant.
how can we tell which paramet θ to leav in the model (known as "model selection") there are sever way to solv this problem: - get more data (veri difficult). - choos the model which best fit the data without overfit (veri difficult). - reduc the opportun for overfit through regularization.
bias: approxim error (differ between expect valu and optim value) - high bias = underfit (bu) - jtrain(θ) and jcv(θ) both will be high and jtrain(θ) ≈ jcv(θ)
variance: estim error due to finit data - high varianc = overfit (vo) - jtrain(θ) is low and jcv(θ) ≫jtrain(θ)
intuit for the bias-vari trade-off: complex model = [...] complex model = sensit to data = much affect by chang in x = high variance, low bias.
intuit for the bias-vari trade-off: simpl model = [...] simpl model = more rigid = doe not chang as much with chang in x = low variance, high bias.
regular effect - small valu of λ allow model to becom fine tune to nois lead to larg varianc = overfitting. - larg valu of λ pull weight paramet to zero lead to larg bias = underfit
model complex effect lower-ord polynomi (low model complexity) have high bias and low variance. in this case, the model fit poor consistently. higher-ord polynomi (high model complexity) fit the train data extrem well and the test data extrem poorly. these have low bias on the train data, but veri high variance. in reality, we would want to choos a model somewher in between, that can general well but also fit the data reason well.
a typic rule of thumb when run diagnost is more train exampl fix high varianc but not high bias. fewer featur fix high varianc but not high bias. addit featur fix high bias but not high variance. the addit of polynomi and interact featur fix high bias but not high variance. when use gradient descent, decreas lambda can fix high bias and increas lambda can fix high varianc (lambda is the regular parameter). when use neural networks, small neural network are more prone to under-fit and big neural network are prone to over-fitting. cross-valid of network size is a way to choos alternatives.
differ way we can approach a machin learn problem collect lot of data (for exampl "honeypot" project but doesn't alway work) develop sophist featur (for example: use email header data in spam emails) develop algorithm to process your input in differ way (recogn misspel in spam).
the recommend approach to solv machin learn problem is start with a simpl algorithm, implement it quickly, and test it early. plot learn curv to decid if more data, more features, etc. will help error analysis: manual examin the error on exampl in the cross valid set and tri to spot a trend.
it import to get error s as [...]. otherwis it is difficult to [...] it import to get error s as a single, numer value. otherwis it is difficult to assess your algorithm per anc
it is sometim difficult to tell whether a reduct in error is actual an improv of the algorithm. when doe this usual happen this usual happen with skew classes; that is, when our class is veri rare in the entir data set. or to say it anoth way, when we have lot more exampl from one class than from the other clas
predicted: 1, actual: 1 predicted: 1, actual: 1 --- true posit
predicted: 0, actual: 0 predicted: 0, actual: 0 --- true negat
predicted: 0, actual, 1 predicted: 0, actual, 1 --- fals negat
predicted: 1, actual: 0 predicted: 1, actual: 0 --- fals posit
precision: of all patient we predict where y=1, [...] precision: of all patient we predict where y=1, what fraction actual has cancer
recall: of all the patient that actual have cancer, what [...] recall: of all the patient that actual have cancer, what fraction did we correct detect as have cancer
when is the f1 score not defin if an algorithm predict onli negat like it doe in one of exercises, the precis is not defined, it is imposs to divid by 0.
we might want a confid predict of two class use logist regression. one way is to [...] we might want a confid predict of two class use logist regression. one way is to increas our threshold. this way, we onli predict cancer if the patient has a 70% chance.
what tradeoff will we have if we want a more confid predict of two class use logist regress do this, we will have higher precis but lower recal (refer to the definit in the previous section).
what is the if we want to get a veri safe predict this will caus higher recal but lower precision.
trade off precis and recal the greater the threshold, the [...] the greater the threshold, the greater the precis and the lower the recall.
trade off precis and recal the lower the threshold, the [...] the lower the threshold, the greater the recal and the lower the precisio
trade off precis and recal in order to turn these two metric into one singl number, we can [...] in order to turn these two metric into one singl number, we can take the f value.
in order for the f score to be large, [...] in order for the f score to be large, both precis and recal must be large.
we want to train precis and recal on the [...] we want to train precis and recal on the cross valid set so as not to bias our test set
in certain cases, an "inferior algorithm," if given [...], can [...] in certain cases, an "inferior algorithm," if given enough data, can outper a superior algorithm with less data.
we must choos our featur to have [...]. a use test is: [...] we must choos our featur to have enough in ation. a use test is: given input x, would a human expert be abl to confid predict y
what is the "rational for larg data" rational for larg data: if we have a low bias algorithm (mani featur or hidden unit make a veri complex function), then the larger the train set we use, the less we will have overfit (and the more accur the algorithm will be on the test set).
the support vector machin (svm) is yet anoth type of supervis machin learn algorithm. it is sometim [...] the support vector machin (svm) is yet anoth type of supervis machin learn algorithm. it is sometim cleaner and more powerful.
to make a support vector machine, we will modifi the first term of the cost function so that [...] to make a support vector machine, we will modifi the first term of the cost function so that when θtx (from now on, we shall refer to this as z) is greater than 1, it output 0.
similar (to make a support vector machine), we modifi the second term of the cost function so that [...] similar (to make a support vector machine), we modifi the second term of the cost function so that when z is less than -1, it output 0.
how are the term we shall cost1(z) and cost0(z) in a svm defin cost1(z) is the cost for classifi when y=1, and cost0(z) is the cost for classifi when y=0
how do we tran the cost function from (regularized) logist regress for svms we may tran this into the cost function for support vector machin by substitut cost0(z) and cost1(z):
what is the convent when use regular in svms onvent dictat that we regular use a factor c, instead of λ, like so: this is equival to multipli the equat by c=1/λ.
finally, note that the hypothesi of the support vector machin is not interpret as [...] (as it is for the hypothesi of logist regression). instead, it [...] finally, note that the hypothesi of the support vector machin is not interpret as the probabl of y be 1 or 0 (as it is for the hypothesi of logist regression). instead, it output either 1 or 0.
a use way to think about support vector machin is to think of them as [...] a use way to think about support vector machin is to think of them as larg margin classifi
larg margin intuit if c is veri large, we must choos θ paramet such that [...] if c is veri large, we must choos θ paramet such that:
in svms, the decis boundari has the special properti that it is [...] in svms, the decis boundari has the special properti that it is as far away as possibl from both the posit and the negat examples.
the [...] is call the margin. the distanc of the decis boundari to the nearest exampl is call the margin.
sinc svms maxim this margin, it is often call a [...] sinc svms maxim this margin, it is often call a larg margin classifier.
the svm will separ the negat and posit exampl by a larg margin. this larg margin is onli achiev when [...] this larg margin is onli achiev when c is veri large.
data is linear separ when [...] data is linear separ when a straight line can separ the posit and negat examples.
if we have [...], then we can reduc c. if we have outlier exampl that we don't want to affect the decis boundary, then we can reduc c.
increas and decreas c is similar to [...], and can [...] increas and decreas c is similar to respect decreas and increas λ, and can simplifi our decis boundary.
kernel allow us to [...] kernel allow us to make complex, non-linear classifi use support vector machines.
ho do we find the "similarity" of x and some landmark l(i) the "similarity" of x and some landmark l(i):
what are the properti of the similar function there are a coupl properti of the similar function:
in other words, if x and the landmark are close, then [...], and if x and the landmark are far away from each other, the [...] in other words, if x and the landmark are close, then the similar will be close to 1, and if x and the landmark are far away from each other, the similar will be close to 0.
each landmark give us [...] each landmark give us the featur in our hypothesis:
one way to get the landmark is to [...] one way to get the landmark is to put them in the exact same locat as all the train examples.
if c is large, then we get [...] if c is large, then we get higher variance/low bias
if c is small, then we get [...] if c is small, then we get lower variance/high bias
with a larg σ2, the featur fi [...] with a larg σ2, the featur fi vari more smoothly, caus higher bias and lower variance.
with a small σ2, the featur fi [...] with a small σ2, the featur fi vari less smoothly, caus lower bias and higher varianc
there are lot of good svm librari alreadi written. a. ng often use [...] there are lot of good svm librari alreadi written. a. ng often use liblinear and libsvm'.
use an svm in practic application, the choic you do need to make are: [...] choic of paramet c choic of kernel (similar function) no kernel ("linear" kernel) -- give standard linear classifi choos when n is larg and when m is small gaussian kernel (above) -- need to choos σ2 choos when n is small and m is larg
note: do per [...] befor use the gaussian kernel. note: do per featur scale befor use the gaussian kernel.
note: not all similar function are valid kernels. they must satisfi [...] which guarante that the svm packag optim [...] note: not all similar function are valid kernels. they must satisfi "mercer theorem" which guarante that the svm packag optim run correct and do not diverge.
logist regress vs. svms if n is larg (relat to m), then [...] if n is larg (relat to m), then use logist regression, or svm without a kernel (the "linear kernel") in the second example, we have enough exampl that we may need a complex non-linear hypothesi
logist regress vs. svms if n is small and m is intermediate, then [...] if n is small and m is intermediate, then use svm with a gaussian kernel in the second example, we have enough exampl that we may need a complex non-linear hypothesi
logist regress vs. svms if n is small and m is large, then [...] if n is small and m is large, then manual create/add more features, then use logist regress or svm without a kernel. in the last case, we want to increas our featur so that logist regress becom applicable.
unsupervis learn is contrast from supervis learn becaus it [...] unsupervis learn is contrast from supervis learn becaus it use an unlabel train set rather than a label one.
cluster is good for [...] market segment social network analysi organ comput cluster astronom data analysi
the k-mean algorithm is the most popular and wide use algorithm for [...] the k-mean algorithm is the most popular and wide use algorithm for automat group data into coher subsets.
k-mean algorithm random initi two point in the dataset call the cluster centroids. cluster assignment: assign all exampl into one of two group base on which cluster centroid the exampl is closest to. move centroid: comput the averag for all the point insid each of the two cluster centroid groups, then move the cluster centroid point to those averages. re-run (2) and (3) until we have found our clusters.
k-mean algorithm our main variabl are [...] k (number of clusters) train set x(1),x(2),...,x(m) where x(i)∈rn note that we will not use the x0=1 convention.
what is the implement for k-mean algorithm the first for-loop is the cluster assign step the second for-loop is the move centroid step where we move each centroid to the averag of it group.
recal some of the paramet we use in our k-mean algorithm! c(i) = index of cluster (1,2,...,k) to which exampl x(i) is current assign μk= cluster centroid k (μk∈ℝn) μc(i) = cluster centroid of cluster to which exampl x(i) has been assign
what is the cost function for k-mean use these variabl we can defin our cost function:
in the cluster assign step, our goal is to: [...] minim j(...) with c(1),...,c(m) (hold μ1,...,μk fixed)
in the move centroid step, our goal is to: [...] minim j(...) with μ1,...,μk
with k-means, it is not possibl [...] with k-means, it is not possibl for the cost function to sometim increase. it should alway descend.
what is our optim object in k-mean our optim object is to minim all our paramet use the abov cost function:
in case where [...] it is strong recommend to run a loop of random initializations. in case where k 10 it is strong recommend to run a loop of random initializations.
k-mean can get stuck [...]. to decreas the chanc of this happening, you can [...] k-mean can get stuck in local optima. to decreas the chanc of this happening, you can run the algorithm on mani differ random initializations.
choos the number of cluster the elbow method [...] the elbow method: plot the cost j and the number of cluster k. the cost function should reduc as we increas the number of clusters, and then flatten out. choos k at the point where the cost function start to flatten out. however, fair often, the curv is veri gradual, so there no clear elbow.
note: j will alway decreas as k is increased. the one except is if k-mean [...] note: j will alway decreas as k is increased. the one except is if k-mean get stuck at a bad local optimum.
anoth way to choos k is to observ how well k-mean per s on a downstream purpose. in other word [...] in other words, you choos k that prove to be most use for some goal you'r tri to achiev from use these clusters.
we may want to reduc the dimens of our featur if [...] we may want to reduc the dimens of our featur if we have a lot of redund data.
do dimension reduct will [...] do dimension reduct will reduc the total data we have to store in comput memori and will speed up our learn algorithm.
note: in dimension reduction, we are reduc our featur rather than [...]. note: in dimension reduction, we are reduc our featur rather than our number of examples.
motiv for dimension reduct motiv i: data compress motiv ii: visual
the most popular dimension reduct algorithm is [...] the most popular dimension reduct algorithm is princip compon analysi (pca)
pca problem ulat given two features, x1 and x2, we want to find a singl line that effect describ both featur at once. we then map our old featur onto this new line to get a new singl feature.
the goal of pca is to [...] the goal of pca is to reduc the averag of all the distanc of everi featur to the project line. this is the project error.
pca is not linear regression! in linear regression, we are minim the squar error from everi point to our predictor line. these are vertic distances. in pca, we are minim the shortest distance, or shortest orthogon distances, to our data points.
in pca, we are take a number of featur x1,x2,...,xn, and find a [...]. we aren't tri to [...] and we aren't [...] in pca, we are take a number of featur x1,x2,...,xn, and find a closest common dataset among them. we aren't tri to predict ani and we aren't appli ani theta weight to the features.
befor we can appli pca, there is a [...] befor we can appli pca, there is a data pre-process step we must per
data preprocess in pca given train set: x(1),x(2),...,x(m) preprocess (featur scaling/mean normalization): replac each x(i)j with x(i)j−μj if differ featur on differ scale (e.g., x1 = size of house, x2 = number of bedrooms), scale featur to have compar rang of valu
so, pca has two tasks: [...] so, pca has two tasks: figur out u(1),...,u(k) and also to find z1,z2,...,zm.
1. step in pca 1. comput "covari matrix" this can be vector in octav as: sigma = (1/m) * x' * x;
2. step in pca 2. comput "eigenvectors" of covari matrix σ this can be vector in octav as: [u,s,v] = svd(sigma);
3. step in pca 3. take the first k column of the u matrix and comput z
summar the whole pca algorithm in octav is roughly: 
if we use pca to compress our data, how can we uncompress our data, or go back to our origin number of featur we can do this with the equation: x(1)approx=ureduce⋅z(1)
choos the number of princip compon in other words, the squar project error divid by the total variat should be less than one percent, so that 99% of the varianc is retained.
algorithm for choos k in pca tri pca with k=1,2,... comput ureduce,z,x check the ula given abov that 99% of the varianc is retained. if not, go to step one and increas k.
the most common use of pca is to [...] the most common use of pca is to speed up supervis learning.
bad use of pca tri to prevent overfitting. we might think that reduc the featur with pca would be an effect way to address overfitting. it might work, but is not recommend becaus it doe not consid the valu of our s y. use just regular will be at least as effective.
stochast gradient cost function stochast gradient descent is written out in a differ but similar way:
stochast gradient descnet algorithm 1. random shuffl the dataset 2. for i=1...m θj:=θj−α(hθ(x(i))−y(i))⋅x(i)j
stochast gradient descent will be unlik to [...] and will instead [...], but usual yield a that is close enough stochast gradient descent will be unlik to converg at the global minimum and will instead wander around it randomly, but usual yield a that is close enough
stochast gradient descent will usual take [...] pass through your data set to get near the global minimum. stochast gradient descent will usual take 1-10 pass through your data set to get near the global minimum.
mini-batch gradient descent instead of use all m exampl as in batch gradient descent, and instead of use onli 1 exampl as in stochast gradient descent, we will use some in-between number of exampl b
typic valu for b in mini-batch gradient descent typic valu for b rang from 2-100 or so.
whi is it possibl that you may get a slight better solut with stochast gradient descent with a smaller learn rate that is becaus stochast gradient descent will oscil and jump around the global minimum, and it will make smaller random jump with a smaller learn rate.
how do we choos the learn rate α for stochast gradient descent one strategi is to plot the averag cost of the hypothesi appli to everi 1000 or so train examples. if you increas the number of exampl you averag over to plot the per anc of your algorithm, the plot line will becom smoother. with a veri small number of exampl for the average, the line will be too noisi and it will be difficult to find the trend.
with a continu stream of user to a website, we can [...], where we collect [...] for the featur in x to predict some behavior y. with a continu stream of user to a website, we can run an endless loop that get (x,y), where we collect some user action for the featur in x to predict some behavior y.
map reduc and data parallel we can divid up batch gradient descent and dispatch the cost function for a subset of the data to mani differ machin so that we can train our algorithm in parallel.
your learn algorithm is mapreduc if [...] your learn algorithm is mapreduc if it can be express as comput sum of function over the train set
which algorithm are easili paralleliz linear regress and logist regress are easili parallelizable.
map reduc and data parallel for neural network for neural networks, you can comput forward propag and back propag on subset of your data on mani machines. those machin can report their deriv back to a master server that will combin them.
what will mapreduc to with the "dispatch jobs" mapreduc will take all these dispatch (or mapped') job and reduc them by calculating:
a veri common applic of anomali detect is [...] a veri common applic of anomali detect is detect fraud:
if our anomali detector is flag too mani anomal examples, then we need to [...] if our anomali detector is flag too mani anomal examples, then we need to decreas our threshold ϵ
what is the function of the gaussian distribut the full function is as follows:
algorithm for anomali detect with gaussian distribut choos featur xi that you think might be indic of anomal examples. fit paramet μ1,...,μn,σ21,...,σ2n calcul μj calcul σ2j given a new exampl x, comput p(x) anomali if p(x)
what are possibl evalu metric for anomali detect true positive, fals positive, fals negative, true negative. precision/recal f1 score
use anomali detect when... we have a veri small number of posit exampl (y=1 ... 0-20 exampl is common) and a larg number of negat (y=0) examples. we have mani differ "types" of anomali and it is hard for ani algorithm to learn from posit exampl what the anomali look like; futur anomali may look noth like ani of the anomal exampl we'v seen so far.
use supervis learn when... we have a larg number of both posit and negat examples. in other words, the train set is more even divid into classes. we have enough posit exampl for the algorithm to get a sens of what new posit exampl look like. the futur posit exampl are like similar to the one in the train set.
how can we check if our featur are gaussian we can check that our featur are gaussian by plot a histogram of our data and check for the bell-shap curve.
say we are tri to recommend movi to customers. we can use the follow definit [...] we can use the follow definit
%% chang octav prompt ps1( );
% display them: a = pi
disp(a)  
disp(sprintf('2 decimals: %0.2f', a))  
disp(sprintf('6 decimals: %0.6f', a))  
at long  
a  
at short  
a  
% from 1 to 2, with stepsiz of 0.1. use for plot axe v = 1:0.1:2
% from 1 to 6, assum stepsiz of 1 (row vector) v = 1:6
% same as c = [2 2 2; 2 2 2] c = 2*ones(2,3)
% 1x3 vector of one w = ones(1,3)
% drawn from a uni distribut w = rand(1,3)
% drawn from a normal distribut (mean=0, var=1) w = randn(1,3)
% (mean = -6, var = 10) - note: add the semicolon w = -6 + sqrt(10)*(randn(1,10000));
% plot histogram use 10 bin (default) hist(w)
% plot histogram use 50 bin hist(w,50)
% note: if hist() crashes, tri [...] % note: if hist() crashes, tri "graphics_toolkit('gnu_plot')"
% 4x4 ident matrix i = eye(4)
% 1x2 matrix: [(number of rows) (number of columns)] sz = size(a)
% number of row size(a,1)
% number of col size(a,2)
% size of longest dimens length(v)
%% load data load q1y.dat % alternatively, load('q1y.dat')
% list variabl in workspac who
% list variabl in workspac (detail view) whos
% clear command without ani arg clear all var clear q1i
% first 10 element of q1x (count down the columns) v = q1x(1:10);
% save variabl v into file hello.mat save hello.mat v;
% save as ascii save hello.txt v -ascii;
% index is (row,col) a(3,2)
% get the 2nd row. a(2,:)
% get the 2nd col a(:,2)
% print all the element of row 1 and 3 a([1 3],:)
% chang second column a(:,2) = [10; 11; 12]
% append column vec a = [a, [100; 101; 102]];
% select all element as a column vector. a(:)
% concaten a and b matric side by side c = [a b]
c = [a, b]  
% concaten a and b top and bottom c = [a; b]
% element-wis multipl a .* b
% element-wis squar of each element in a a .^ 2
% element-wis reciproc 1./v
% function like this oper element-wis on vec or matric log(v)
exp(v)  
abs(v)  
% v + 1 % same v + ones(length(v), 1)
% matrix transpos a'
% val - maximum element of the vector a and index - index valu where maximum occur [val,ind] = max(a)
% if a is matrix, return max from each column val = max(a)
% check which valu in a are less than 3 a 3
% give locat of element less than 3 find(a 3)
% generat a magic matrix - not much use in ml algorithm a = magic(3)
% row, column indic for valu match comparison [r,c] = find(a =7)
% maximum along columns(default to column - max(a,[])) max(a,[],1)
% maximum along row max(a,[],2)
% matrix invers (pseudo-inverse) pinv(a)
inv(a'*a)*a  
% "hold off" to turn off hold on;
% divid plot into 1x2 grid, access 1st element subplot(1,2,1);
% divid plot into 1x2 grid, access 2nd element subplot(1,2,2);
% chang axi scale axis([0.5 1 -1 1]);
% comma-chain function calls. a=1,b=2,c=3
a=1;b=2;c=3;  
%% plot t = [0:0.01:0.98];
y1 = sin(2*pi*4*t);  
plot(t,y1);  
y2 = cos(2*pi*4*t);  
hold on;  
xlabel('time');  
ylabel('value');  
legend('sin','cos');  
title('mi plot');  
print -dpng myplot.png  
% to add the path for the current session of octave: addpath('/path/to/function/')
% to rememb the path for futur session of octave, after execut addpath above, also do: savepath
octav function can return more than one valu function [y1, y2] = squareandcubethisno(x)
y1 = x^2  
y2 = x^3  
differ star/snowflak scheme *star scheme:* consist of singl fact tabl and a singl tabl for each dimension. each tupl in fact tabl consist of a pointer to each of the dimens provid it multidimension coordin and store numer measur for those coordinates. *snow flake schemas*: refin of star scheme where dimension hierarchi is explicit repres by normal the dimension tables. star is pure denormalized, wherea snowflak can be normalized.
data warehous (definit and goal) - *s*ubject-oriented, *i*ntegrated, *n*on-volatile, *t*ime-vari collect of data - collect of decis support technolog - aim at enabl the knowledg worker to make better & faster organiz decisions.
olap = *o*n*l*in *a*nalyt *p*rocessing. - relat low volum of transactions. - queri are often veri complex and involv aggregations. - respons time is an effect measure. - wide use by data mine techniques. - aggregated, histor data, store in multi-dimension schema (usual star schema).
oltp = *o*n*l*in *t*ransact *p*rocessing. - larg number of short on-lin transact (insert, update, delete). - veri fast queri processing, maintain data integr in multi-access environ - effect measur by number of transactions/sec. - detail and current data, and schema use to store transact databas is the entiti model
differ olap / oltp olap contain data from *sever oltp databases* over potenti *longer time periods*, combin with data from *extern sources*. therefor an olap databas is typic larger than an oltp database.
data mart build a *data warehous is a long and complex process* which may take mani year to develop. data mart: *department subset* focuss on select subject (e.g. a market data mart may includ onli customer, product and sale in ation)
applic of sql 1. *data definit languag (ddl)*; instruct to build and maintain data (creat table) - use by databas develop 2. *data control languag (dcl)*; instruct for maintain the data base (grant) - use by databas admin 3. *data manipul languag (dml)*; instruct to manipul the data (select) - use by end user
3 vs in big data model 1. *volume*: current increas at a high speed due to devices/interact nowadays, make big data differ from earlier days. 2. *velocity*: time period when data is refreshed. earlier: weekly/month (also reports). nowaday veloc increas rapid becaus of real time contact, contribut to the volum of data. 3. *variety*: earlier: onli certain extern dataset (e.g. from crm systems). nowaday more data sourc avail such as social media and open sourc data.
benefits/problems/challeng for big data *benefits*: -fine-tun of marketing, -client base segmentation, -autom decis making. *problems*: -lack knowledg (inexpert), -lack of staff, -lack of current databas software, -data load slow in current databas software. *challenges*: -data growth, -data infrastructure, -data integration, -data velocity. data variety, data visualization.
how to improv the market per anc step 1: set up *kpi framework* & *collect data* step 2: *merging* data to get a holist market view on per anc step 3/4: *transpos insights* into *challeng and opportunities* step 5: *creat initiatives*
3 challeng for organ (big data) *technical*: scatter and fragment dataset due to differ aggreg level of data sourc but also differ storag (e.g. mrx agency). first one solv by e.g. power pc, the second by physic integration. *analytical*: lack of similar and synchron custom perspect and consist segmentation; explain past instead of futur and data at differ aggreg level *business*: no link with metric of the p&l for assign and accept from financi department. need to defin kpi metric within and across data sources.
main characterist of unstructur data (or semi-structur data) - unstructur data is *not direct suited* for analyses, requir *structur befor they can be store in tabl or spreadsheet* for analyses. - it is not alway clear *what you can do* with unstructur data. - unstructur data are *often not complet without structure* (e.g. email log)
differ between structur and unstructur data - structur data come in *fix at*, base on a *detail record* and *variabl structure*. good *label of values* in the databas and high-qual data. - unstructur data need a lot *more process and handling* befor it becom useful. it is also *not alway obvious what you can do* with it. - unstructur data *grow much faster* - structur data can often be store effici (rich data requir much more data storage!)
4 issu of unstructur data - *filtering:* need comput algorithm for filter relev content. - *structuring* (involv classif or grading), need to group data, relat to categori and those that are not relat to categori - *need to be match to data from other sources:* easi if you can identifi a uniqu key for a uniqu customer, howev in mani case not avail e.g. social media - need to do base on e.g. zipcod and age - *compani lack skill to generat insight from unstructur data:* appropri hard-/ softwar is not alway avail and analyst have not alway requir it-skills.
problem in data qualiti - *accuracy*, e.g. measur error. peopl don't correct typo or to fill in right numbers. or how volatil download new twitter data or download new fresh sampl when analys the data - *consistency*, check on the ranges. - *completeness*, miss observ or variabl
what is data cleans (data scrubbing) -*act of detect and correct (or removing) corrupt or inaccur records* from a record set, tabl or database. - *goal*: not just clean up but also bring *consistency* to differ set of data that have been merg from separ data sourc e.g. correct of typo and spell error or proper label mislabel data.
5 step for cleans technic correct data ('fix step) 1) *p*ars 2) *c*orrect 3) *s*tandard 4) *m*atch 5) *c*onsolid
tidi data accord to wickham (2014) tidi data is a *standard way* of map the mean of a dataset to it *structure*. it facilit analysis. 1. each *variabl s a column* 2. each *observ s a row* 3. each type of *observ unit s a table*
machin learn & predict statist - share common methodolog (e.g. logist regression) - field aros from differ sources: - statist = social sciences, econom - machin learn = comput scienc - statist = low dimension problem emphas al statist infer - machin learn = outcome-ori focus on accur predict make - less restrict and al than classic statist
machin learn use *pre-classifi train data* to teach a machin through an algorithm an *infer function* with which it is abl to *classifi unseen new data*. - veri close to forecast or predict model in mrx. - train in machin learn = paramet estim in mrx - by assign weight to the input factor
supervis learn - aim to determin a rule that map attribut of an observ (input of the rule) to a label or categori (output of predict rule). - algorithm is train with data contain both the input (e.g., word & word combinations), as well as the desir output (i.e., "spam"/"ham"). - e.g. svm, decis tree (crt), naïv bayes, neural networks. - face recognition, sentiment analysis, languag detection, fraud detect
unsupervis learn - no desir output in the train data. - consequently, the algorithm involv cannot "learn" how to classifi or predict new data points. - aim at identifi a structur within the data (usual use in data mine where interest is more in find pattern or phenomena than assign labels) - pattern recognition, cluster identification, opinion mining, referr and collabor filter
nearest neighbor & entropi - neighbor help assign peopl into group or predict peopl behavior given previous observ from similar peopl - neighbor approach are good in assess total similarity. - entropi allow determin which factor or featur help best to make good predictions. - entropi refer to the degre of heterogen within a group. - high entropi = all group member share mani differ featur - low entropi score = high degre of common featur amongst group members.
support vector machin - svms tri to identifi a function that use a number of featur from a train set to split observ into two separ classes. - for ani new observ —out of the train sample— the function can then determin with the help of the featur to which class the new object belongs. - aim: plane that maxim the margin between the two differ classes.
non-linear separ data & svms - *soft margin machines*: relax the rule for the margin and allow few "misclassified" observ to be within the margin. - *kernel base svms*: split up data in a more dimension space to find a plane that then help to separ data into groups. - usual general quit well, while be robust to over-fit issues. however, non-linear kernel usual requir more test effort and longer train times.
decis tree idea: start out with the complet train set, and determin a seri of split to creat more homogen sub-groups, creat a classif that is as good as possible, with a minimum number of splits. - at each split, a variabl is select (base on entropi and in ation gain) that s the basi for a decis rule that drive the split.
ensembl method - combin *differ tree models* togeth aggreg decis rule across differ tree model to obtain *better predict per ance*. - *reduc variance* by take *repeat random samples* with replac and *averag ing predictions* - *bagging, random forest, boosting*
neural network - inspir by how the human brain works. - just as the neuron in a human brain, a neural network consist of a number of interconnect element that tran input to outputs. - their weight sum is process by some ation function, and this s in a binari outcome. - such an artifici neuron is call a perceptron. - organ in input/hidden/output layer
overfit the more specif model is to train data, the less it will be abl to general to new data points. by enforc a near-perfect fit (includ maximum number of variabl into model to maxim degre of explain variance) of our train data set, we sacrific generalizability.
natur languag process (nlp) -goal: determin text sentiments. - with the help of pre-label text (i.e., posit and negative) the machin learn to understand how the occurr of certain word or word combin determin the favor of a text. - document for each label categori get strip into a term document matrix (tdm) - each column = ani word that occur - each row = differ documents. - incom new, unlabel text can then been assign to the two categori
4 fundament verb of data manipul (wickham, 2014) • *filtering:* remov or subset observ base on some condition. • *tran :* ad or modifi variables. • *aggregate:* collaps multipl valu into a singl valu e.g. by sum or take mean • *sort:* chang the order of observ
data clean approach accord to rahm & do (2000) 1) detect and remov all major error and inconsist both on dividu data sourc and when integr multipl data sources. 2) don't per in isol but togeth with schema-rel data tran ation base on comprehens metadata. 3) workflow infrastructur should be support to execut all data tran ation step for multipl sourc & larg data set in reliabl and effici way.
singl sourc problem accord to rahm & do (2000) - can be divid into schema and instanc levels. - schema-rel data qualiti problem occur becaus of lack of appropri model: specif or application-specif integr constraint (e.g. due to data model limit or poor schema design) - or becaus onli few integr constraint were defin to limit overhead for integr control. - instance-problem relat to error and inconsist that can't be prevent at schema level (e.g. misspellings).
multipl sourc problem (rahm & do, 2000) multipl sourc problem can be divid into schema and instanc levels. - at schema level, data model and schema design differ are to be address by the step of schema translat and schema integr respectively.
two approach for data analysi (rahm & do, 2000) data profil & data mining. - data profil focuss on the instanc analysi of individu attributes, deriv in ation such as data type, length, valu range, discret valu and their frequenc etc. - data mine help discov specif data pattern in larg data set e.g. relationship between sever attributes.
consequ of miss data - *inaccur predictions* & *biased/ineffici estimates* - predict will be not accur and the descript coeffici that you are get are not as good as they could be. - either valu is not precis (biased) or procedur becom less effici (larger uncertainti henc inefficient) when estim those problems.
how to detect outlier - descript and graphic summaries. - if x is less then q1 - 1,5 x iqr (inter quartil rang = q3 - q1) or more than q3 + 1,5 x iqr. - potenti solut for this is trimming; throw away the most extrem x% on both side - remov outlier by default: may throw away interest part. alway investig outlier and what caus them.
machin learn - concern with comput program that automat improv their per anc through experi - basic concept is use train data to teach a machin through an algorithm a function with which the machin is abl to classifi new unseen data.
advantag of machin learn • develop system that can *automat adapt and customize* themselv to individu user • discov *new knowledge* from larg databas (data mining) • abl to *mimic human and replac certain tasks* which requir some intellig • develop *system that are too difficult/ expensive* to construct manual (specif detail skill or knowledg tune to a specif task)
disadvantag of machin learn - take *long to train* - *mani subjectives* - *stop criteria* matter - effect are *not interpretable*
aim of kdd process knowledg discoveri in database: identifi implicit, valid, novel, potenti use and understand pattern in data
step in the kdd process 1. *data clean and integration*, miss values/merg from multipl sourc 2. *data select and tran ation*, select relev data/choos aggreg level 3. *data mining*, appli method to find pattern 4. *evalu and presentation*, confirm hypothesis/ find new patterns. use visual techniqu to present, sould conclud in new knowledge.
assess qualiti of a classif - *% correct classified:* percentag that is correct classifi - *top decil lift:* predict the top 10%. - *lift curve:* cumul percentag of custom at x axi and cumul percentag of e.g. churn at y axi - *gini (area under the curve):* focus on the overal per anc of the model.
advantag of neural network • *predict accuracy* is general high, • it is *robust*, also when train set contain error of noisi data • *output may be discrete, real-valu or a vector* of sever discret or real-valu attribut • *fast evaluation* of the learn target function
disadvantag of neural network • take *long train time* • *difficult to understand* the learn function (weights) • *not easi to incorpor domain knowledge*
4 dimens accord to berinato (2016) *idea illustration* (declar / conceptual) *idea generation* (exploratori / conceptual) *everyday dataviz* (declar / data-driven) *visual discovery* (exploratori / data-driven)
eelementari task order from most to least accur 1. posit along common scale 2. posit along nonalign scale 3. length, direct and angl 4. area 5. volume, curvatur 6. shading, color satur
4 categori of preattent attribut color, , position, animation.
type of dashboard - *strategic;* high level manag and execut see in ation and trend - *analytical:* technic for busi users, in ation actual search themselves, dive in data. - *operational:* what is happen now, abl to make my decis real time
data qualiti distinguish characterist describ fit of a dataset/record for a particular use that one may have in mind for the data.
statist analysi valu chain (de jong & van der loo) 1) raw data 2) technic correct data 3) consist data 4) statist s 5) format output
data mine - 3rd step in kdd process - extract of implicit, previous unknown and potenti use in ation from data - explor & analysis, by automat or semiautomat means, of larg quantiti of data in order to discov meaning pattern (decript and prediction)
differ boost & bagging/rf - boost is *sequential*, bag and rf base on independ *resampling* (bootstrapping) - in boost *each follow tree is weighted* accord to the wrong classifi observ of previous tree. - new (test) observ have to go through the *entir sequenc of trees* until the final outcom becom clear. - for bag and rf the tree are built on independ sampl of the data, *then aggregated* - either *major vote* (discret outcomes) or *simpl averaging* (continu outcomes)
applic of tiday data - *manipulation* (filter, tran , aggregate, sort) - *visualization*: tool need to be input-tidi - *modeling*: most model tool work best with tidi dataset
5 most common problem with messi data 1) *column header are values* not variabl name 2) multipl variabl are *store in one column* 3) variabl are *store in both row and columns* 4) *multipl type of observ units* in same tbl 5) a singl *observ unit is store in multipl tables*
imput of miss valu miss data for a subject is imput by a valu that is predict use the subject other known characteristics. - mcar = you can use simpl techniqu without bias - mnar = depend on valu itself - mar = depend on other, observ variabl singl imput multipl imput (use various estimates)
supervis learn supervis learn is a type of machin learn algorithm that use a known dataset (call the train dataset) to make predictions. the train dataset includ input data and respons values. from it, the supervis learn algorithm seek to build a model that can make predict of the respons valu for a new dataset. a test dataset is often use to valid the model. use larger train dataset often yield model with higher predict power that can general well for new datasets. call supervis learn becaus the data is label with the "correct" responses.
regress vs classif regression: the output variabl take continu values. - price of hous given a size. classification: the output variabl take class labels, or discret valu output - breast cancer, malign or benign almost like quantit vs categor
cluster a method of unsupervis learn - a good way of discov unknown relationship in datasets. cluster analysi or cluster is the task of group a set of object in such a way that object in the same group (call a cluster) are more similar (in some sens or another) to each other than to those in other group (clusters). it is a main task of exploratori data mining, and a common techniqu for statist data analysis, use in mani fields, includ machin learning, pattern recognition, imag analysis, in ation retrieval, bioin atics, data compression, and comput graphics. cluster analysi itself is not one specif algorithm, but the general task to be solved. it can be achiev by various algorithm that differ signific in their notion of what constitut a cluster and how to effici find them. popular notion of cluster includ group with small distanc among the cluster members, dens area of the data space, interv or particular statist distributions. cluster can therefor be ulat as a multi-object optim problem. the appropri cluster algorithm and paramet set (includ valu such as the distanc function to use, a densiti threshold or the number of expect clusters) depend on the individu data set and intend use of the s. cluster analysi as such is not an automat task, but an iter process of knowledg discoveri or inter e multi-object optim that involv trial and failure. it is often necessari to modifi data preprocess and model paramet until the achiev the desir properties.
unsupervis learn unsupervis learn is the machin learn task of infer a function to describ hidden structur from unlabel data. sinc the exampl given to the learner are unlabeled, there is no error or reward signal to evalu a potenti solution. this distinguish unsupervis learn from supervis learn and reinforc learning. unsupervis learn is close relat to the problem of densiti estim in statistics.[1] howev unsupervis learn also encompass mani other techniqu that seek to summar and explain key featur of the data.
cocktail parti effect/problem the cocktail parti effect is the phenomenon of be abl to focus one auditori attent on a particular stimulus while filter out a rang of other stimuli, much the same way that a partygo can focus on a singl convers in a noisi room. exampl of sourc separation.
synonym for input variabl featur
synonym for output variabl target
what is "hypothesis" in machin learn hypothesis: a hypothesi is a certain function that we believ (or hope) is similar to the true function, the target function that we want to model. in context of email spam classification, it would be the rule we came up with that allow us to separ spam from non-spam emails.
train sampl definit train sample: a train sampl is a data point x in an avail train set that we use for tackl a predict model task. for example, if we are interest in classifi emails, one email in our dataset would be one train sample. sometimes, peopl also use the synonym term train instanc or train example.
target function definit target function: in predict modeling, we are typic interest in model a particular process; we want to learn or approxim a particular function that, for example, let us distinguish spam from non-spam email. the target function f(x) = y is the true function f that we want to model. the target function is the (unknown) function which the learn problem attempt to approximate.
model definit model: in machin learn field, the term hypothesi and model are often use interchangeably. in other sciences, they can have differ meanings, i.e., the hypothesi would be the "educ guess" by the scientist, and the model would be the manifest of this guess that can be use to test the hypothesis.
learn algorithm learn algorithm: again, our goal is to find or approxim the target function, and the learn algorithm is a set of instruct that tri to model the target function use our train dataset. a learn algorithm come with a hypothesi space, the set of possibl hypothes it can come up with in order to model the unknown target function by ulat the final hypothesi
classifi classifier: a classifi is a special case of a hypothesi (nowadays, often learn by a machin learn algorithm). a classifi is a hypothesi or discrete-valu function that is use to assign (categorical) class label to particular data points. in the email classif example, this classifi could be a hypothesi for label email as spam or non-spam. however, a hypothesi must not necessarili be synonym to a classifier. in a differ application, our hypothesi could be a function for map studi time and educ background of student to their futur sat scores.
what doe this hypothesi repres h_theta(x) = theta_0 + theta_1 x univari linear regress model
cost function vs gradient descent a cost function is someth you want to minimize. for example, your cost function might be the sum of squar error over your train set. gradient descent is a method for find the minimum of a function of multipl variables. so you can use gradient descent to minim your cost function. if your cost is a function of k variables, then the gradient is the length-k vector that defin the direct in which the cost is increas most rapidly. so in gradient descent, you follow the negat of the gradient to the point where the cost is a minimum. if someon is talk about gradient descent in a machin learn context, the cost function is probabl impli (it is the function to which you are appli the gradient descent algorithm).
what is sse the sum of squar error
deriv sse given a linear regress model, the differ at each predict point with the correct point is given by diff = y_i - (mx + b)
whi do we squar instead of use the absolut valu when calcul varianc and standard deviat first i'll answer the mathemat question ask in the question details, which i'm go to restat becaus i think it is state wrong: the short answer is "becaus of jensen inequality." see http://en.wikipedia.org/wiki/jen... and the rest of the articl for context. it say in particular that for a concav function what about the more general question, "whi varianc " i don't believ there is ani compel conceptu reason to use varianc as a measur of spread. if forc to choose, my guess is that most peopl would say more robust measur like interquartil rang or mad better captur the concept of "spread" in most cases. but varianc (and more general "sum of squares") has some attr e properties, mani of which flow from the pythagorean theorem one way or another. here some of them, without much math: we can decompos sum of squar into meaning compon like "between group variance" and "within-group variance." to general the abov point, when a random variabl y y is part explain by anoth random variabl x x there is a use decomposit of the varianc of y y into the part explain by x x and the unexplain part. (see http://en.wikipedia.org/wiki/law...). if we think more broad about mean squar error, this too can be decompos into the sum of varianc and squar bias. it is easi to interpret this total error as the sum of "systemat error" and "noise." often we want to minim our error. when the error is a sum of squares, we are minim someth quadratic. this is easili accomplish by solv linear equations. so yes, varianc and mean squar error are conveni rather than conceptu necessities. but they are conveni conveniences.
3d surfac plot - how can it be use to plot the cost function theta 0 and theta 1 in a univari linear regress can be plot on the x and y axes. the z axi will indic the actual cost
what are contour plot a contour plot is a graphic techniqu for repres a #d surfac by plot constant z slices, call contour onto a 2dimension at. that is, given a valu for z, line are drawn for connect the x,y, coordin twhere that z valu occurs. the circl in a contour plot are call level set - the function j is equal here.th center of the contour plot is the minimum of the cost function typic in ml.
what letter is typic use to depict a cost function function j
gradient descent algorithm imag for gradient descent gradient descent is a first-ord optim algorithm. to find a local minimum of a function use gradient descent, one take step proport to the negat of the gradient (or of the approxim gradient) of the function at the current point. use with simultan updat of the paramet of success suscept to fall into local optimum depend on initialization.
what are first order method in numer analysi in numer analysis, method that have at most linear local error are call first order methods. they are frequent base on finit differences, a local linear approximation.
what doe the deriv of a function tell us the deriv of a function of a real variabl measur the sensit to chang of a quantiti (a function valu or depend variable) which is determin by anoth quantiti (the independ variable). deriv are a fundament tool of calculus. for example, the deriv of the posit of a move object with respect to time is the object velocity: this measur how quick the posit of the object chang when time is advanced. the deriv of a function of a singl variabl at a chosen input value, when it exists, is the slope of the tangent line to the graph of the function at that point. the tangent line is the best linear approxim of the function near that input value. for this reason, the deriv is often describ as the "instantan rate of change", the ratio of the instantan chang in the depend variabl to that of the independ variable.
what is the downsid of use an alpha (learn rate) that is too small gradient descent can be way too slow.
what is the downsid of use an alpha (learn rate) that is too big gradient descent can overshoot the minimum and it may fail to converg or even diverge.
what happen if you initi a paramet at a local minimum and attempt to use gradient descent on it the deriv turn out to be zero becaus the tangent is a flat line mean that regardless of alpha it is multipli by zero, indic no change.
whi is it unnecessari to chang alpha over time to ensur that the gradient descent converg to a local minimum as we approach a local minimum, the gradient descent will take smaller step becaus of the chang of the deriv or the steep of the cost function j. don't need to worri about divergence.
convex function in mathematics, a real-valu function defin on an interv is call convex (or convex downward or concav upward) if the line segment between ani two point on the graph of the function lie abov or on the graph, in a euclidean space (or more general a vector space) of at least two dimensions. equivalently, a function is convex if it epigraph (the set of point on or abov the graph of the function) is a convex set. well-known exampl of convex function includ the quadrat function {\ x^{2}} x^{2} and the exponenti function {\ e^{x}} e^{x} for ani real number x. convex function play an import role in mani area of mathematics. they are especi import in the studi of optim problem where they are distinguish by a number of conveni properties. for instance, a (strictly) convex function on an open set has no more than one minimum.
"batch" gradient descent (bgd or gd) each step of gradient descent use all the train examples. batch gd - this is differ from (sgd - stochast gradient descent or mb-gd - mini batch gradient descent) in gd optimization, we comput the cost gradient base on the complet train set; hence, we sometim also call it batch gd. in case of veri larg datasets, use gd can be quit cost sinc we are onli take a singl step for one pass over the train set -- thus, the larger the train set, the slower our algorithm updat the weight and the longer it may take until it converg to the global cost minimum (note that the sse cost function is convex).
gradient descent for linear regress (review again)
what is overfit in statist and machin learning, one of the most common task is to fit a "model" to a set of train data, so as to be abl to make reliabl predict on general untrain data. in overfitting, a statist model describ random error or nois instead of the under relationship. overfit occur when a model is excess complex, such as have too mani paramet relat to the number of observations. a model that has been overfit has poor predict per ance, as it overreact to minor fluctuat in the train data. how to avoid overfit cross-validation, regularization, earli stopping, pruning, bayesian prior on paramet or model comparison and more!
what is cross-valid cross-validation, sometim call rotat estimation,[1][2][3] is a model valid techniqu for assess how the s of a statist analysi will general to an independ data set. it is main use in set where the goal is prediction, and one want to estim how accur a predict model will per in practice. in a predict problem, a model is usual given a dataset of known data on which train is run (train dataset), and a dataset of unknown data (or first seen data) against which the model is test (test dataset).[4] the goal of cross valid is to defin a dataset to "test" the model in the train phase (i.e., the valid dataset), in order to limit problem like overfitting, give an insight on how the model will general to an independ dataset (i.e., an unknown dataset, for instanc from a real problem), etc.
k-fold in k-fold cross-validation, the origin sampl is random partit into k equal size subsamples. of the k subsamples, a singl subsampl is retain as the valid data for test the model, and the remain k − 1 subsampl are use as train data. the cross-valid process is then repeat k time (the folds), with each of the k subsampl use exact onc as the valid data. the k s from the fold can then be averag to produc a singl estimation. the advantag of this method over repeat random sub-sampl (see below) is that all observ are use for both train and validation, and each observ is use for valid exact once. 10-fold cross-valid is common used,[7] but in general k remain an unfix parameter. when k=n (the number of observations), the k-fold cross-valid is exact the leave-one-out cross-validation. in stratifi k-fold cross-validation, the fold are select so that the mean respons valu is approxim equal in all the folds. in the case of a dichotom classification, this mean that each fold contain rough the same proport of the two type of class labels. ultim this help fix the problem that we want to maxim both the train and test set in cross-validation.
what is a simpl linear regress in statistics, simpl linear regress is the least squar estim of a linear regress model with a singl explanatori variable. in other words, simpl linear regress fit a straight line through the set of n point in such a way that make the sum of squar residu of the model (that is, vertic distanc between the point of the data set and the fit line) as small as possible. minim squar error
what doe theta typic repres in stat/ml quit often θ stand for the set of paramet of a distribution.
linear regress ula  
sse ula observ - mean for each observ squared.
x^i_j notat in ml mean an index into a train set for the ith train exampl and jth featur (input variable)
hypothesi model rememb that the hypothesi model or set is what is depict with h(theta)
multivari linear regress ask: whi is the notat shorter and what doe that conveni notat indic
whi use featur that are on a similar scale contour plot with differ scale featur will be extrem thin or extrem fat ing in a veri slow gradient descent (converg is slower) get them to a -1
what doe it mean for an algorithm to converg an iter algorithm is said to converg when as the iter proceed the output get closer and closer to a specif value. in some circumstances, an algorithm will diverge; it output will undergo larger and larger oscillations, never approach a use . the "converg to a global optimum" phrase in your first sentenc is a refer to algorithm which may converge, but not to the "optimal" valu (e.g. a hill-climb algorithm which, depend on initi conditions, may converg to a local maximum, never reach the global maximum).
what is normal in statist and applic of statistics, normal can have a rang of meanings.[1] in the simplest cases, normal of rate mean adjust valu measur on differ scale to a notion common scale, often prior to averaging. in more complic cases, normal may refer to more sophist adjust where the intent is to bring the entir probabl distribut of adjust valu into alignment. in the case of normal of score in educ assessment, there may be an intent to align distribut to a normal distribution. a differ approach to normal of probabl distribut is quantil normalization, where the quantil of the differ measur are brought into alignment.
what is featur scale featur scale is a method use to standard the rang of independ variabl or featur of data. in data processing, it is also known as data normal and is general per ed dure the data preprocess step. the rang of valu of raw data vari widely, in some machin learn algorithms, object function will not work proper without normalization[cit needed]. for example, the major of classifi calcul the distanc between two point by the euclidean distance[cit needed]. if one of the featur has a broad rang of values, the distanc will be govern by this particular feature[cit needed]. therefore, the rang of all featur should be normal so that each featur contribut approxim proportion to the final distance[cit needed]. anoth reason whi featur scale is appli is that gradient descent converg much faster with featur scale than without it[cit needed]. some method are rescaling, standardization, scale to unit length.
higher deriv let f be a differenti function, and let f ′(x) be it derivative. the deriv of f ′(x) (if it has one) is written f ′′(x) and is call the second deriv of f. similarly, the deriv of a second derivative, if it exists, is written f ′′′(x) and is call the third deriv of f. continu this process, one can define, if it exists, the nth deriv as the deriv of the (n-1)th derivative. these repeat deriv are call higher-ord derivatives. the nth deriv is also call the deriv of order n.
standard vs normal normal rescal the valu from to a rang of [0,1]. this might use in some case where all paramet need to have the same posit scale, but outlier from data set are lost. xchang = (x - xmin)/(xmax-xmin) standard rescal data to have a mean of 0 and standard deviat of 1 (unit variance). xchang = (x-mean)/sd for most applic standard is recommended. in the busi world, "normalization" typic mean that the rang of valu are "normal to be from 0.0 to 1.0". "standardization" typic mean that the rang of valu are "standardized" to measur how mani standard deviat the valu is from it mean. however, not everyon would agre with that. it best to explain your definit befor you use them. in ani case, your tran ation need to provid someth useful.
definit of stochast random determined; have a random probabl distribut or pattern that may be analyz statist but may not be predict precisely.
how to make sure gradient descent is work proper creat an automat converg test - declar converg base on amount of decreas of j(theta) plot on graph, y axi be j and axi be number of iterations.
what doe j(theta) increas tell you about ur gradient descent it not work lol. use a bigger alpha. on the other end if you use too big of an alpha you'll end up with a bowl shape curv and you might be move farther away from convergence.
what to do when j(theta) is move up and down in wave use a smaller alpha!!
for a suffici small alpha... j(theta) should decreas everi iter
cubic functions... 
inflect point inflect point are where the function chang concavity. sinc concav up correspond to a posit second deriv and concav down correspond to a negat second derivative, then when the function chang from concav up to concav down (or vise versa) the second deriv must equal zero at that point. so the second deriv must equal zero to be an inflect point. but don't get excit yet. you have to make sure that the concav actual chang at that point.
what is the ula for theta in a normal equat 
machin learn field of studi that give comput the abil to learn without be explicit programmed.
well-pos learn problem a comput program is said to learn from experi e with respect to some task t and some per anc measur p, if it per anc on t, as measur by p, improv with experi e.
abstract essenc of ml represent + evalu + optimis
machin learn learn from experience. it also call supervis learning, were e is the supervision.
pattern recognit find pattern without experience. it also call unsupervis learning.
classif ml task where t has a discret set of outcom • often classif is binari • examples: • face detect • smile detect • spam classif • hot/cold
regress ml task where t has a real-valu outcom on some continu sub-spac examples: • age estim • stock valu predict • temperatur predict • energi consumpt predict
label the valu that h aim to predict example: •facial express of pain •impact of diet on astronaut in space •predict of hous price
features/attribut measur valu of variabl that correl with the label y examples: • sender domain in spam detect • mouth corner locat in smile detect • temperatur in forest fire predict • pixel valu in face detect
train algorithm given a model h with solut space s and a train set {x,y}, a learn algorithm find the solut that minimis the cost function j(s)
well-pos learn problem a comput program is said to learn from experi e with respect to some task t and some per anc measur p, if it per anc on t, as measur by p, improv with experi e.
machin learn broad definit field of studi that give comput the abil to learn without be explicit program
cost function squar error cost function
local minima is the smallest valu of the function. but it might not be the onli one.
local maxima the real smallest valu of the function.
linear basi function  
general classif problem if class are disjoint, i.e. each pattern belong to one and onli one class then input space is divid into decis region separ by decis boundari or surfac
decis surfac are • linear function of x • defin by (d-1)-dimension hyperplan in the ddimension input space
linear separ linear separ data: • dataset whose class can be separ by linear decis surfac • impli no class-overlap • class can be divid by e.g. line for 2d data or plane in 3d data
orthogon two vector and are orthogon if they'r perpendicular in this case, their inner product is 0: a · b = 0
train lda objective: find (i.e. learn) that minimis some error function on the train set. signific approaches: • least squar • fisher • perceptron
least squar lda benefits: exact close solut disatvantages: • not probabilist • sensit to outlier • problem with multipl class and/ or unbalanc data
insight: lda is a of dimension reduction. y = wt x high-dimension input is map by w onto a singl dimens y
fisher lda fisher idea is to adjust/constrain so that class separ in 1d is maximis
artifici neural net feed-forward neural network/multilay perceptron one of mani ann we focus on the multilay perceptron realli multipl layer of logist regress model
the simplest ann consist of •a layer of d input node •a layer of hidden node •a layer of output node •fulli connect between layer
curs of dimension when d becom large, learn problem can becom veri difficult. for example: when divid a space (e r^d) into regular cells, the number of cell grow exponenti with d. in linear regress a polynomi model of order m has d^m coeffici a sphere in high dimens has most of it volum in an infinitesim thin slice near the surfac
extrem dimension case in an extreme, degener case, if d n, each exampl can be uniqu describ by a set of featur values.
hidden layer(s) can have arbitrari number of nodes/unit have arbitrari number of link from input node and to output node (or to next hidden layer) there can be multipl hidden layer (default is a fulli interconnect graph, i.e. everi input node is link to everi hidden node, and everi hidden node to everi output node)
hidden unit activ common function for are the sigmoid or h(·) tanh:
relu rectifi linear unit new trend, respons for great deal of deep learn success: • no vanish gradient problem • can model ani posit real valu • can stimul spars
output layer can be: singl node for binari classif singl node for regress n node for multi-class classif (one network can also cover multipl output variables, thus increas the number of nodes.)
network topolog variat include: arbitrari number of layer fewer hidden unit than input unit (caus in effect dimension reduction, equival to pca) skip-lay connect (see below) fully/spars interconnect network
train a network choic of depend on the output variables: *uniti for regress *logist sigmoid for (multipl independent) binari classif *softmax for exclus (1-of-k) multiclass classif (train a network involv find the that minimis some error function)
nn error function *regression: *binari classif *multipl independ binari classification: *multi-class classif (mutual exclusive):
gradient point in the direct of: steepest ascent
in order to optimis the per anc of ann an error function on the train set must be minimis this is done by adjusting: *weight connect node *paramet of non-linear function h(a)
eigenvalu given an invert matrix , an eigenvalu equat can be foundin term of a set of orthogon vector , and scalar such that: m
jacobian, first-ord partial deriv of function : can be use to determin if a point is a (local) extreme.
hessian, second-ord partial deriv of function : y = f(x) can be use to determin if a stationari point is a (local) minimum or maximum.
gradient descent is a poor algorithm itself. better variant exist: conjug gradient quasi newton onlin gradient descent
onlin gradient descent on-lin gradient descent updat weight vector one data point at a time maximum likelihood-bas error function use a sum of term for independ data points: *handl redund better *can deal with new data better *good chanc of escap local minima
backpropag use to calcul deriv of error function effici error propag backward layer by layer
backprop is for: arbitrari feed-forward topolog differenti nonlinear ation function broad class of error function
error backpropag 1. appli input vector to network and propag forward 2. evalu d(k) for al output unit 3. backpropag d to obtain d(j) for all hidden unit 4. evalu error deriv as:
regular maximum likelihood generalis error (i.e. cross-validation) regularis error (penalis larg weights) earli stop
deep learn basic a neural network mani hidden layer major breakthrough in pre-train treat each layer first as an unsupervis restrict boltzmann machin to initialis weight then do standard supervis backpropag can be use for unsupervis learn and dimension reduct
regular regular is a techniqu use in an attempt to solv the overfit problem in statist models.*
what is deep learn definition: • hierarch organis with more than one (non-linear) hidden layer in-between the input and the output variabl • output of one layer is the input of the next layer
deep learn method (deep) neural network • convolut neural network • restrict boltzmann machines/deep belief network • recurr neural network
convolu+on neural network is a type of feed-forward artifici neural network in which the connect pattern between it neuron is inspir by the organ of the anim visual cortex, whose individu neuron are arrang in such a way that they respond to overlap region tile the visual field.[1]
how is deep neural network optimis optimis through gradient descent! (forward-backward algorithm) penalis complex solut to avoid overfit
cost function - l2 norm  
cost function - euclidean distanc  
cost function - l1 norm  
cost function - manhattan  
cost function - citi distanc  
basic decis tree decis tree appli a seri of linear decisions, that often depend on onli a singl variabl at a time. such tree partit the input space into cuboid regions, gradual refin the level of detail of a decis until a leaf node has been reached, which provid the final predict label.
tree compon root node, branch, node, leaf node.
branch factor branch factor of node at level l is equal to the number of branch it has to node at level l + 1
cart classif and regress tree
six general question to decid on decis tree algorithm: 1. how mani split per node (properti binari or multi valued) 2. which properti to test at each node 3. when to declar a node to be leaf 4. how to prune a tree that has becom too larg (and when is a tree too large) 5. if a leaf node is impure, how to assign a categori label 6. how to deal with miss data
tree variati tree are call monothet if one property/vari is consid at each node, polythet otherwis
what tree are prefer we prefer simple, compact trees, follow occam razor
how to make tree compact to do so, we will seek to minimis impur of data reach descend node
occam razor all thing be equal - the simplest explan is the best
the principl of plural plural should not be posit without necess
the principl of parsimoni it is pointless to do with more what is done with less
misclassif impur is the minimum probabl that train exampl will be misclassifi at node n
bay error the bay error rate is the theoret lowest possibl error rate for a given classifi and a given problem (dataset). for real data, it is not possibl to calcul the bay error rate, although upper bound can be given when certain assumpt on the data are made. the bay error function most as a theoret devic in machin learn and pattern recognit research.
generalis generalis is the desir properti of a classifi to be abl to predict the label of unseen exampl correctly. a hypothesi generalis well if it can predict an exampl come from the same distribut as the train exampl well.
overfit a hypothesi is said to be overfit if it predict per anc on the train data is overoptimist compar to that on unseen data. it present itself in complic decis boundari that depend strong on individu train examples.
stop criteria reach a node with a pure sampl is alway possibl but usual not desir as it usual caus over-fitting.
three common way to decid when to stop split decis tree: valid set cross-valid hypothesi test (chi-squar statistic)
evalu procedur • for larg datasets, a singl split is usual sufficient. • for smaller datasets, reli on cross valid
valid set criterion split train data in a train set and a valid set (e.g. 66% train data and 34% valid data) keep split nodes, use onli the train data to learn decisions, until the error on the valid set stop go down.
cross-valid in n-fold cross-validation, a dataset is split into n rough equal size partitions, such that each exampl is assign to one and onli one fold. at each iter a hypothesi is learn use n-1 fold as the train set and predict are made on the n'th fold. this is repeat until a predict is made for all n examples, and an error rate for the entir dataset is obtained. cross-valid maximis the amount of data avail to train and test on, at cost of increas time to per the evaluation. • train data segment between differ fold should never overlap! • train and test data in the same fold should never overlap! error estim can either be done per fold separ (as shown above), or delay by collat all predict per fold.
cross-valid criterion split train data in a number of fold for each fold, train on all other fold and make predict on the held-out test fold combin all predict and calcul error if error has gone down, continu split nodes, otherwise, stop
prune first fulli train a tree, without stop criterion after training, prune tree by elimin pair of leaf node for which the impur penalti is small
multivari tree i instead of monothet decis at each node, we can learn polythet decisions. this can be done use mani linear classifiers, but keep it simple!
miss attribut it is common to have exampl in your dataset with miss attributes/variables. one way of train a tree in the presenc of miss attribut is remov all data point with ani miss attributes. a better method is to onli remov data point that miss a requir attribut when consid the test for a given node for a given attribute. this is a great benefit of tree (and in general of combin models, see later slide)
id3 inter e dichotomis version 3 use for nominal, unordered, input data only. everi split has branch factor , where is the number of valu a variabl can take (e.g. bin of discretis variable) has as mani level as input variabl
c4.5 successor of id3 multiway split are use statist signific split prune
regress tree train in a veri similar way leaf node are now continu valu - the valu at a leaf node is that assign to a test exampl if it reach it leaf node label assign is e.g. mean valu of it data sampl problem: node make hard decisions, which is particular undesir in a regress problem, where a smooth function is sought.
model combin view • decis tree combin a set of model (the nodes) • in ani given point in space, onli one model (node) is respons for make predict • process of select which model to appli can be describ as a sequenti decis make process correspond to the travers of a binari tree
random forest veri good per anc (speed, accuracy) when abund data is avail use bootstrapping/bag to initialis each tree with differ data use onli a subset of variabl at each node use a random optimis criterion at each node project featur on a random differ manifold at each node
measur of classif accuraci classif error rate cross valid recall, precision, confus matrix receiv oper curves, two-altern forc choic
estim hypothesi accuraci sampl error vs. true error confid interv
sampl theori basic binomi and normal distribut mean and varianc
compar hypothes t-test analysi of varianc (anova) test
classif measur - error rate common per anc measur for classif problem success: instanc class is predict correct (true posit (tp) / negat (tn)) error: instanc class is predict incorrect (fals posit (fp) / negat (fn)) fals posit - type i error. fals negat - type ii error classif error rate: proport of instanc misclassifi over the whole set of instanc
simpl data split fix train, develop and test sets: bootstrapping: cross-valid
fix train, develop and test sets: random split data into training, development, and test sets. doe not make use of all data to train or test good for larg dataset
bootstrapping: random select a subset to be train set random select a subset to be test set can be repeat mani time has theoret problem of statist signific when repeat
cross-valid random split data into n fold and iter use one as test set all data use to test, and almost all to train good for small set
evalu procedur for larg datasets, a singl split is usual sufficient. for smaller datasets, reli on cross valid
overfit can occur when: learn is per ed for too long (e.g. in neural networks) the exampl in the train set are not repres of all possibl situat (is usual the case! ) model paramet are adjust to unin ativ featur in the train set that have no causal relat to the true under target function!
f-measur compar differ approach is difficult when use multipl evalu measur (e.g. recal and precision) f-measur combin recal and precis into a singl measure:
roc curv receiv oper characterist (roc) curv plot tp vs fp rate
confus matrix easi to see if the system is common mislabel one class as anoth
hypothesi qualiti we want to know how well a machin learner, which learn the hypothesi as the approxim of the target function , per s in term of correct classifi novel, unseen exampl we want to assess the confid that we can have in this classif measur
the true error of hypothesi h is the probabl that it will misclassifi a random drawn exampl from distribut : d however, we cannot measur the true error. we can onli estim it by observ the sampl error es
sampl error in statistics, sampl error is incur when the statist characterist of a popul are estim from a subset, or sample, of that population.
a bernoulli trial is a trial with a binari outcome, for which the probabl that the outcom is 1 equal p (think of a coin toss of an old warp coin with the probabl of throw head be p). a bernoulli experi is a number of bernoulli trial per ed after each other. these trial are i.i.d. by definition.
binomi distribut in probabl theori and statistics, the binomi distribut with paramet n and p is the discret probabl distribut of the number of success in a sequenc of n independ yes/no experiments, each of which yield success with probabl p.
normal distribut the normal distribut has mani use properties. it is fulli describ by it mean and varianc and is easi to use in calculations. the good thing: given enough experiments, a binomi distribut converg to a normal distribution.
t-test assess whether the mean of two distribut are statist differ from each other
signific level signific level α%: α time out of 100 you would find a statist signific differ between the distribut even if there was none. it essenti defin our toler level. if the calcul t valu is abov the threshold chosen for statist signific then the null hypothesi that the two group do not differ is reject in favour of the altern hypothesis: the group do differ.
test for compar distribut t-test compar two distribut anova compar multipl distribut if null-hypothesi is refuted, there are at least two distribut with signific differ mean doe not tell you which they are!
latent variabl latent variabl are variabl that are hidden in the data. they are not direct observed, but must be inferred. cluster is one way of find discret latent variabl in data.
discret latent variabl are hidden variabl that can take onli a limit number of discret valu (e.g. gender or basic emotion).
cluster applic market segment social network analysi vector quantis facial point detect
k-mean cluster in ally, goal is to find group of point that are close to each other but far from point in other group • each cluster is defin entir and onli by it centre, or mean valu µk
k-mean algorithm i 1. initialis uk random 2. minimis j with respect to rnk , keep uk fix 3. minimis j with respect uk to , keep rnk fix 4. repeat until converg step 2 is call expect step, step 3 the maximis step
k-mean issu converg is guarante but not necessarili optim - local minima like to occur • depend larg on initi valu of uk • hard to defin optim number k • k-mean algorithm is expensive: requir euclidean distanc comput per iter • each instanc is discret assign to one cluster • euclidian distanc is sensit to outlier
robbins-monro •address the slow updat speed of the m-step in k-mean •use linear regress (see lectur 1)
k-medoid cluster address issu with quadrat error function (l2-norm, euclidean norm) replac l2 norm with ani other dissimilar measur (v...)
random initialis random initialis k-mean cluster use actual instanc as cluster centr run k-mean and store centr and final cost function pick cluster of iter with lowest cost function as optim solut most use if k 10
choos k elbow method • visual inspect • downstream analysi
probabl theori recap p(x) = margin distribut p(x,y) = joint distribut p(x y) = condit distribut
mixtur of gaussian • simpl ulation: densiti model with richer represent than singl gaussian
iid independ and ident distribut random variabl
em algorythm issu • take a long time • often initialis use k-mean
data mine is the quest to extract knowledg and/ or unknown interest pattern from appar unstructur data aka knowledg discoveri from data (kdd) • data mine bit of a misnom - in ation/ knowledg is mined, not data.
knowledg discoveri process 1. data clean - remov nois and inconsist 2. data integr - combin data sourc 3. data select - retriev relev data from db 4. data tran ation - aggreg etc. (cf. featur extraction) 5. data mine - machin learn 6. pattern evalu - identifi truli interest pattern 7. knowledg represent - visualis and transfer new knowledg
arff attribute-rel file format
hdf5 much more complex file at design for scientif data handl it can store heterogen and hierarch organis data • it has been design for effici
dm type of data • relat databas • data warehous • transact databas • object-rel databas • temporal/sequence/time-seri databas • spatial and spatio-tempor databas • text & multimedia databas • heterogen & legaci databas • data stream
dm function concept/class descript • characteris • discrimin • frequent patterns/associations/correl • classif and regress (prediction) • cluster analysi • outlier analysi • evolut analysi
the goal of data mine is to find interest patterns! an interest pattern is: 1. easili understood 2. valid on new data with some degre of certainti 3. potenti use 4. novel
dm object interest support: p(x u y ) percentag of transact that a rule satisfi confidence: p(i x) degre of certainti of a detect association, i.e. the probabl that a transact contain x also contain y
dm subject interest subject measur requir a human with domain knowledg to provid measures: •unexpect s contradict a priori belief •action •expect s confirm hypothesi
dm system can be divid into type base on a number of variables: •kind of databas •kind of knowledg •kind of techniqu •target applic
dm task primit dm task primit s the basi for dm queries. dm primit specify: •set of task-relev data to be mine •kind of knowledg to be mine •background knowledg to be use •interesting measur and threshold for pattern evalu •represent for visualis discov pattern
dm queri languag dm queri languag incorpor primit allow flexibl interact with dm system provid foundat for build user-friend gui example: dmql
dm integr with dbs/ data warehouses•tight coupl •no coupl - dms will not utilis ani db/dw system functionalit •loos coupl - use some db/dw functionality, in particular data fetching/storing, •semi-tight coupl - in addit to loos coupl use sorting, indexing, aggregation, histogram analysis, multiway join, and statist primit avail in db/dw system •tight coupl
dirti data incomplete: noisy: inconsistent:
•caus of incomplet data: •"not applicable" data valu when collect •differ consider between the time when the data was collect and when it is analysed. •human/hardware/softwar problem
•caus of noisi data (incorrect values): •faulti data collect instrument •human or comput error at data entri •error in data transmiss
•caus of inconsist data: •differ data sourc •function depend violat (e.g., modifi link data)
import of clean • if you have good data, the rest will follow
data qualiti measur multi-dimension measur of data qualiti • a well-accept multidimension view: • accuraci • complet • consist • timeli • believ • valu ad • interpret • access • broad categories: • intrinsic, contextual, representational, and access
major dm prep task *data clean • fill in miss values, smooth noisi data, identifi or remov outliers, and resolv inconsist *data integr • integr of multipl databases, data cubes, or file *data tran ation • normalis and aggreg * data reduct • obtain reduc represent in volum but produc the same or similar analyt s *data discretis • part of data reduct but with particular importance, especi for numer data
noisi data nois is a random error or varianc in a measur variabl
techniqu for cancel out noise: 1. bin - first sort data, then distribut over local bin 2. regress - fit a parametr function to the data (e.g. linear or quadrat function) 3. cluster
noisi data - bin cancel nois by binning: 1. sort data 2. creat local group of data 3. replac origin valu by: 3.1. the bin mean 3.2. the closest min/max valu of the bin
noisi data - regress cancel nois by regression: 1. fit a parametr function to the data use minimis of e.g. least squar error 2. replac origin valu by the parametr function valu
noisi data - clustering: cancel nois by clustering: 1. cluster data into n group 2. replac origin valu by mean of cluster 3. or: use to detect outlierst
data integr • entiti identif problem • redund detect • correl analysi • detect and resolut of data valu conflict • e.g. weight units, in/exclus of tax
data transform data tran ation alter origin data to make it more suitabl for data mining. • smooth (nois cancellation) • aggreg • generalis • normalis • attribute/featur construct
itemset simpli a set of item (cf set theory)
mine frequent pattern • one approach to data mine is to find set of item that appear togeth frequently: frequent itemset • to be frequent some minimum threshold of occurr must be exceed • other frequent pattern of interest: • frequent sequenti pattern • frequent structur pattern
associ rule reflect item that are frequent found (purchased) together, i.e. they are frequent itemset • in ation that custom who buy beer also buy crisp is e.g. encod as: beer ) crisps[support = 2%, conf idenc = 75%]
support and confid are measur of pattern interesting
rule support: support(a - b) = p(a u b)
• rule confid confidence(a - b) = p (b a)
frequent itemset • absolut support of an itemset is it frequenc count • relat support is the frequenc count of the itemset divid by the total size of the dataset
min-max normalisation: • enabl cost-funct minimis techniqu to function properly, take all attribut into equal account • tran s all attribut to lie on the rang [0, 1] or [-1, 1] • linear tran ation of all data
z-score normalis better terminolog is zero-mean normalis • min-max normalis cannot cope with outliers, z-score normalis can • tran s all attribut to have zero mean and unit standard deviat • outlier are in the heavy-tail of the gaussian • still a linear tran ation of all data:
data reduct should remov what unnecessary, yet otherwis maintain the distribut and properti of the origin data • data cube aggreg • attribut subset select (featur selection) • dimension reduct (manifold projection) • numeros reduct • discretis
attribut subset select attribut subset select = featur select featur select is a of dimension reduct in ml, henc the dm term dimension reduct for manifold project is problematic. approaches: • exact solut infeas • greedi forward select • backward elimin • forward-backward • decis tree induct
numeros reduct reduc the number of instanc rather than attributes. much more dangerous, as it risk chang the data distribut properties! • parametris • discretis • sampl
data discretis group a possibl infinit space to a discret set of possibl valu for categor data: • super-categori for real numbers: • bin • histogram analysi • cluster
instanc reduct reduc the number of instanc rather than attributes. much more dangerous, as it risk chang the data distribut properties! • duplic remov • random sampl • cluster sampl • stratifi sampl
complex item set the general rule procedur for find frequent item set would be: 1. find all frequent itemset 2. generat strong associ rule however, this is terribl costly, with the total number of item set to be check for 100 item being:
simpler method for complex item set close frequent itemset: x is close if there exist no super-set y such that y has the same support count as x • maxim frequent itemset: x is frequent, and there exist no superset y of x that are also frequent
apriori algorithm apriori algorithm is a fast way of find frequent itemset
rule base learn equival in express power to tradit (mono-thetic) decis trees, but with more flexibl • they produc rule set as solutions, in the of a set of if... then rule
predic a logic statement, general as boolean logic
rule set • singl rule are not the solut of the problem, they are member of rule set • rule in a rule set cooper to solv the problem. togeth they should cover the whole search space
evalu rule • a good rule should not make mistak and should cover as mani exampl as possibl complexity: favour rule with simpl predic (occam razor)
evalu rule set a complet rule set should be good at classifi all the train exampl • complexity: favour rule set with the minim number of rule
learn rule set learn rule sequentially, one at a time • also known as separate-and-conqu learn all rule togeth • direct rule learn • deriv rule from decis tree
there are three reason to reduc the dimension of a featur set: 1. remov featur that have no correl with the target distribut 2. remove/combin featur that have redund correl with target distribut 3. extract new featur with a more direct correl with target distribut
degre of freedom of variability: number of way data can change/numb of separ tran ation possibl
intrins dimension subspac of data space that captur degre of variabl only, and is thus the most compact possibl represent
featur select featur select return a subset of origin featur set. it doe not extract new features. benefits: featur retain origin mean after determin select features, select process is fast disadvantages: cannot extract new featur which have stronger correl with target variabl
search method exhaust greedi forward select greedi backward elimin forward-backward approach
filter score correl mutual in ation entropi classif rate regress score
mutual inform give a measur of how close two compon of a joint distribut are to be independent:
forward select 1.start with empti sf set and candid set be all origin featur 2. find featur with highest filter score 3.remov featur from candid set 4.add featur to sf set 5.repeat step 2-4 until converg
backward elimin 1.start with complet sf set (contain all origin features) 2. find featur that, when removed, reduc the filter score least 3.remov featur from sf set 4.repeat step 2-3 until converg
forward-backward algorithm first appli forward select and then filter redund element use backward elimin
cfs correl base featur select (cfs) select featur in a forward-select manner. look at each step at both correl with target variabl and alreadi select features.
pca manifold project assum gaussian latent variabl and gaussian observ variabl distribut linear-gaussian depend of the observ variabl on the latent variabl also known as karhunen-loèv tran
pca requir calcul of: mean of observ variabl covari of observ variabl eigenvalue/eigenvector comput of covari matrix
ann featur select artifici neural network can implicit per featur select a multi-lay neural network where the first hidden layer has fewer unit (nodes) than the input layer call auto-associ network
parametr methods: mani method learn paramet of predict function (e.g. linear regression, anns) after training, train set is discarded. predict pure base on learn paramet and new data
memory-bas methods: use all train data in everi predict (e.g. knn) becom a kernel method if use a non-linear exampl comparison/metric:
• kernel method map a non-linear separ input space to anoth space which hope is linear separ • this space is usual higher-dimensional, possibl infinit the key element is that they do not actual map featur to this space, instead they return the distanc between element in this space • this implicit map is call the (definition) trick
spars kernel method must be evalu on all train exampl dure test must be evalu on all pair of pattern dure train *train take a long time *test too *memori intens (both disk/ram) solution: spars method
three way of construct new kernels: direct from featur space map propos kernel direct combin of exist (valid) kernel * multipl by a constant * exponenti of a kern * sum of two kernel * product of two kernel *left/right multipl by ani function of x/x
common use kernel linear kernel: polynomi kernel gaussian kernel (gaussian kernel is probabl the most frequent use kernel out there - gaussian kernel map to infinit featur space)
kernel is a shortcut that help us do certain calcul faster which otherwis would involv comput in higher dimension space.
kernel method kernel method map a non-linear separ input space to anoth space which hope is linear separ • this space is usual higher-dimensional, possibl infinit • even the non-linear kernel method essenti solv a linear optimis problem!!!!
kernel trick the key element of kernel method is that they do not actual map featur to this space, instead they return the distanc between element in this space this implicit map is call the (definition)
spars kernel method must be evalu on all train exampl dure test must be evalu on all pair of pattern dure train train take a long time test too memori intens (both disk/ram) solution: spars method
maximum margin classifi is a classifi which is abl to give an associ distanc from the decis boundari for each example.
linearly-separ svm satisfis solut (e.g. perceptron algorithm): find a solution, not necessarili the best best is that solut that promis maximum generalis
slack variabl slack variabl introduc to solv optimis problem by allow some train data to be misclassifi slack variabl en = 0 give a linear penalti to exampl lie on the wrong side of the d.b.: point on correct side of db tn ! y(xn) , otherwis
relev vector machin model the typic point of a data set, rather than atypical( a la densiti estimation) while remain a (very) spars (like heat map) represent return a true posterior natur extend to multi-classif fewer paramet to tune
svms seek a decis boundari that maximis the margin
svm margin is defin as the minimum distanc between decis boundari and ani sampl of a class
maximum margin classifi this turn out to be a solut where decis boundari is determin by nearest point onli minim set of point span decis boundari sought these point are call support vector
small set of svs mean that our solut is now spars
non-linear separ problem usual problem aren't linear separ (not even in featur space) perfect separ of train data class would caus poor generalis due to massiv overfit
soft margin we have effect replac the hard margin with a soft margin new optimis goal is maximis the margin while penalis point on the wrong side of d.b.:
multiclass svm svm is an inher binari classifi two strategi to use svms for multiclass classification: one-vs-al one-vs-on problems: ambigu (both strategies) imbalanc train set (one-vs-all)
one-class svm unsupervis learn problem similar to probabl densiti estim instead of a pdf, goal is to find smooth boundari enclos a region of high densiti of a class
prior probabl is the probabl of encount a class without observ ani evid can be generalis for the state of ani random variabl easili obtain from train data (i.e. counting)
joint probabl joint probabl is the probabl of encount a particular class while simultan observ the valu of one or more other random variabl can be generalis for the state of ani combin of random variabl
bay theorem posterior =(likelihood x prior)/evid
minimum error rate goal is to minimis error rate
normal densiti by far the most (ab)us densiti function is the normal or gaussian densiti
all probabl theori can be express in term of two rule product rule sum rule
direct pgn: edg have direct (bayesian network)
undirect pgn no edg direct (markov random field)
direct acycl graph (dags) are bayesian network mean there are no cyclic path from ani node back to itself
some variabl are observed, other are hidden/lat exampl observed: label of a train set exampl hidden: learn weight of a model
pgns are generat model allow us to sampl from the probabl distribut it defin
ncestral sampl is a simpl sampl method well suit to pgns
condit independ in pgn is the pgn mechan to show in ation in term of interest aspect of probabl distribut
block path when a path is ed, no in ation can flow through it this mean that observ c, if it s a path a-c-b, it mean there is no ad valu in observ a, and b is fulli determin by c
sequenc data sequenc data is data that come in a particular order opposit of independent, ident distribut (i.i.d.) strong correl between subsequ element dna time seri facial express speech recognit weather predict action plan
1st order markov model restrict to encod sequenti correl on previous element onli
a latice/trelli diagram visualis state transit over time also good tool to to visualis optim path through state (viterbi algorithm)i
emiss probabl probabl of observ variabl
acquir emiss wide rang of option to model ****** probabilities: discret tabl gaussian mixtur of gaussian neural networks/rvm etc to mode
what is the idea behind find- algorithm what are problem and what are advantag idea: find maxim specif hypothes problems: - learn noth from negat examples, - cannot tell whether it learn a concept, - cannot tell whether train data is inconsist good: pick maxim specif h (but depend of h there might be sever solutions).
describ possibl disadvantag of the find-s- algorithm. learn noth from negat examples, cannot tell whether it learn a concept, cannot tell whether train data is inconsist
provid the most specif hypothesi which is learn accord to the find- algorithm when use the exampl below. provid the individu learn steps, i.e., provid the most specif hypothesi after each learn step. exampl a1 a2 a3 classif 1 s o p true 2 s r t true 3 t r p fals 4 t o p fals s0={ } s1={ } s2={ } s3={ } → is this right
when is hypothesi h consist with train exampl d of target concept c hypothes h is consist with train exampl d of target concept c i ff h(x) = c(x) (correct classified) for all train exampl x; c(x)) element d consistent(h;d) all (x; c(x)) element d : h(x) = c(x)
what is the version space the version space vs(h;d) with respect to the hypothesi space h and train exampl d is the subset of hypothes from h consist with all train exampl in d (i.e. version space = all consist hypothesis). vs(h;d) = {h element h consistent(h;d)}
what is the idea, problem and advantag of the list-then-elimin algorithm idea: start with all hypothesis, then for each exampl elimin the inconsist hypothes for a larg vs one need mani or few quit in ativ examples, if vs is ∅ there are inconsist good: comput complet vs (ideal onli one hypothesi remains) problems: can onli be appli to finit h, requir enumer all hypothesi → impract for real problem
what are the two boundari of the version space boundari of the version space: - general boundari g of vs(h;d) is the set of maxim general member - specif boundari s of vs(h;d) is the set of it maxim specif member everi member of the vs lie between (including) these boundari
what is the candidate-elimin algorithm candid elimin is a learn algorithm that, in each step, tri to generat a descript which is consist with all previous observ exampl in a train set. that descript could hypothet then be use to classifi exampl outsid the train set. idea: comput whole vs. like list-then-elimin start with complet vs but do not name them explicitly. instead repres vs by it boundaries. start with most general g0 and most specif s0 . those delimit the whole vs. now for each train exampl special g and general s until they overlap.
given the general boundari g={strong, , } and the specif boundari s={strong, sunny, warm}v{strong, cloudy, cool}, which we learn use the candid elimin algorithm. provid the complet version space, includ more general than relations. provid a definit of your choic of ing more general than relat answer right vs = { , , , , , , } the most general boundari in this exampl is , have less ' 's make the boundari more specific.
what is an induct bias the induct bias of a machin learn algorithm is the set of assumpt that must be ad to the observ data to get a logic deduct from them. that mean that it is some prefer of the algorithm for a specif set of hypothes base on a set of train observations.
whi an induct bias the hypothesi space limit what the algorithm can find. e.g. by choos conjunct hypothesi we have bias the learner. however, when repres all concept (disjunct of all posit examples), we would learn nothing. we would onli write the data differently. there would be no general possibl and converg would onli be achiev when all possibl instanc would have been presented. how is general achiev the induct leap came about via the independ of the attribut under the construct of h. - bias-fre learn system make no a-priori assumpt ! it can onli collect exampl without general - induct learn make sens onli with prior assumpt (bias)! - learn is simply: collect examples, interpol among the exampl accord to the induct bias, eli acquir exampl follow the suggest from the version space - for everi appli learn system the induct bias should be clear!
which of the learn algorithm you heard about in the lectur (candid elimin and find-s) has the stronger bias the induct bias of the candid elimin algorithm is the assumpt that the target concept is contain in the given hypothesi space. the induct bias of the find- algorithm is that the ing hypothesi label all new instanc as negat instanc unless the opposit is entail by it prior knowledg from the train set. this has a realli big impact as negat exampl are ignor completely. this mean find- has the stronger bias.
what is the induct bias of a learner provid an exampl use the „weather" problem, which was discuss in the lectur  
what is the induct bias of the nearest neighbor classifi assum that most of the case in a small neighborhood in featur space belong to the same class. given a case for which the class is unknown, guess that it belong to the same class as the major in it immedi neighborhood. this is the bias use in the k-nearest neighbor algorithm. the assumpt is that case that are near each other tend to belong to the same class.
what is the idea of decis tree the idea of a decis tree is to classifi data by a sequenc of interpret steps. each node test a valu and each branch stand for one of the valu of this attributes. each endnod than provid a classification.
decis trees: top-down induct learn a decis tree is find the best order to ask the attribut valu (best mean we want a small tree → in ation gain)
what is the problem domain of decis tree learn a discret target function for instanc describ as attribute-valu pairs. disjunct hypothesi required. possibl for noisi data. decis tree can be written as a set of rule (e.g. as disjunct of conjunct of attribut values: each path is one conjunction, combin all path with disjunction).
what is the idea of id3 learn algorithm idea: find best attribut by the distribut over the examples. put the best one at the root and the possibl valu as branches. search second best valu for each of the branches...
provid pseudocod for id3 learn algotithm while not perfect classif do a ← decis attribut with highest gain node ← a as decis attribut for all valu of a → new descend of node sort train exampl accord to leaf node iter over new node end while
explain hypothesi space search by id3 id3 search the tree that build up possibl decis trees. sinc this space is complete, the target hypothesi is sure in there. output is a singl hypothesi (not the version space) → no queri to resolv among compet hypotheses. no backtrack implement → end up in local maxima. statistically-bas search choic → robust to noisi data. disadvantage: all exampl need for everi choice, no increment learning.
what is the induct bias in id3 h is power set of instanc of x → unbias search no, short tree are prefer (correspond to occam razor), high info gain attribut near root are preferred. bias is prefer for some hypothes rather than a restrict of hypothesi space.
what doe it mean if some features( x1 and x2) do not appear in the decis tree sepal length and sepal width are not relev for the classification. this might be either becaus they are redund or becaus they are independ of the class.
tree 2 onli has a 96% accuraci on the train set. whi might this tree still be prefer over tree 1 tree 1 is probabl overfit to this specif dataset, i.e. it has not onli captur the structur but also the nois in the data. it probabl won't general as well as the second tree. anoth advantag of tree 2 is that it is faster at classifi new data sinc less comput have to be made. this differ is hard notic however.
how can we avoid overfit stop grow when data split is no more statist signific or grow tree & post-prune.
what is reduc error prune remov node to achiev better general on test data. prune node n: remov subtre of n and make n a leaf node, assign most common classif of it affili train examples. check all node for pruning, actual remov the one that s in highest per anc increas (greedy). do while per anc on test data increases. properties: produc smallest version of most accur tree / remov node produc by nois (nois per definit not present in test data). if few data, use post prune instead.
what is rule post prune build decis tree (allow over fitting), convert tree to rule (one rule for each path from root to leaf), prune each rule (remov ani precondit that improv accuraci n test data), sort final data by accuraci on valid set and appli them in this order for new classification. whi prune rule onli path are prunde, not entir substre → more sensit prune / no hierarchi while pruning, even prune of root node possibl / better readabl
how can we obtain binari attribut in continu valu attribut to includ continu valu attributes, defin threshold to obtain binari attribute. threshold should be chosen to gain info: sort of attribut set over examples, find boundari where classif changes, set threshold at boundaries.
what is the problem with attribut with mani valu in classif what is a solut for this problem if an attribut has mani values, gain tend to select it (even if nonsense, becaus it enabl perfect classification). however, this prevent good generalization. one solution: gainratio instead of gain: st is subset of s for which a has valu vi and a has n differ valu in total. splitin ation is the entropi with respect to the attribut values. normal the gain'. gainratio favor attribut with fewer values, if two attribut yield same gain. gainratio not defin for attribut with same valu for all exampl (zero denominator), but they are useless anyway and have to be excluded.
which method can be use if we have miss attribut in decis tree - if node n test a, assign most common valu of a among other exampl sort to n - assign most common valu of a among other exampl with same target valu - assign probabl pi to each possibl valu vi of a (prob estim from valu distribut of exampl assign to n). - assign fraction pi of exampl with miss valu to each descend in tree.
explain what entropi and in ation gain is use for he decis which attribut to choos is base on entropy: in mani machin learn applic it is crucial to determin which criterion are necessari for a good classification. decis tree have those criterion close to the root, impos an order from signific to less signific criterions. one way to select the most import criterion is to compar it in ation gain or it entropi to others.
exampl a1 a2 classif 1 a d + 2 a e - 3 b d + 4 c d + 5 b e + calcul the entropi e(sa) for the given the tabl , where sa is the subset of s for which an atribut x has the valu v. x=(a1,a2). this is, calcul the entropi for each attribut value. it is suffici to provid the s-log2(y). exact calcul of the logarithm is not necessary. what is the in ation gain of a1 and a2 relat to the given train exampl  
what can caus outlier - measur and technic error (cut-o e.g. may lead to high concentr of valu at the boundaries) - unexpect true effect (can be model by two or more overlaid distribut p(x) = (1 -p) * pa(x) + p * pb(x) p - data with inher high variabl (can be model by distribut with broad flanks. there are differ type of outlier which can have differ causes. they could aris through measur or technic error when collect data. this may be connect to have a sharp cut-off in regard to the rang of measurements, which could lead to a high concentr of valu at the artifici boundari of an experiment. howev they may also show us a true under effect in our data that we didn't expect or account for. this might be the case when we are treat the measur as a singl distribution, when in realiti there are actual two under distributions. lastly, our distribut might actual natur have a high variance, which make outlier or extrem valu a natur part of the distribution.
what is the problem with outlier outlier can drastic spoil statistics, especi in small data sets. there are some measures, that are robust against outlier even without explicit detect (e.g. median).
what do we do with them (in general) z-valu and rosner test usual outlier are detect & removed. but to do this, we first need to defin what is regular. most often we use normal distribut here, for multivari data, cluster algorithm with normal distribut assum for each cluster can be used. first, we need to detect probabl outliers. in order to decid which data point we want to declar as an outlier we have to find a model for regular, mean "not outlying", data points. what we do most of the time is to assum a normal distribut under the data (or a multivari distribut where each cluster is normal distributed). one option is to calcul the z-valu for each data point (a measur of the distanc from the mean in term of the standard deviation) -- data point with a high z-valu would be regard outliers, a common threshold would be a z valu bigger than 3. this can be improv by use the median instead of the mean and tweak the threshold. the rosner test take it one step further by iter calcul z-valu and remov found outlier until none can be found anymore. this can be done one outlier at a time or k outlier at a time for more efficiency. a differ approach would be to not remov the outlier completely, but to weight them accord to the z-values. and last an altern for complet remov would be to fill up the emerg gap with valu that fit the distribut better.
how doe the z-test work detect outlier vie z-values: zt = xt -mu / sigma zt is a measur for the distanc of xt from the mean mu normal with the standard deviat sigma. normally, data with z 3 are consid outliers. improvement: outlier use mu, use median instead (common with threshold 3,5 then)
what is the rosner test describ it purpos and provid it al definit the rosner test is an iter procedur to remov outlier of a data set via a z-test while new outlier are found do calcul mean mu or median m and sd sigma find data point xit* with largest z-value: i* = argmaxtzt if xt* is an outlier then remov xt* end if end while
what to do with outlier - removal: simple, but loss of in ation - weight accord to z-valu - remov outlier and fi ll up gap with method follow
what are pro and con of the eukledian distanc pro: simpl and frequent measur con: no individu weight of compon
what are pro and con of the pearson distanc pro: weight dimens accord to their standard deviat con: correl vector compon are overweight
what is the idea and what are pro and con of the mahalanobi distanc idea: scale distanc use the covari matrix c pro: scale & translat invari / if c is unit matrix → euclidian distanc con: scale might destroy structur within data the point of equal mahalanobi distanc to a center an ellipsoid
what ai the idea of the manhattan distanc (aka citi distance) ham distanc (# of posit where two binari string di er) is equal to manhattan distanc for binari
what is the idea of the chebyshev distanc (aka maximum/chessboard distance) idea: minimum number of move a king need between two posit on a chessboard
what is the p-norm general norm e.g. p=2 -- euclidean distanc p=1 -- manhattan distanc p-- inf maximum distanc
what is the idea behind nomin scale what is a possibl problem and solut so far, all dist. measur reli on the topolog of r^n. but there are data with other topolog (angular attributes...). solution: emb differ topolog into r^n nomin scales: map nomin attribut to real values. (stone;wood; metal) → ((1, 0, 0), (0, 1, 0), (0, 0, 1)) problem: for larg n of attribut values, dimension becom too high, a solut would be to choos normal random vector instead.
describ in your own words: how doe the em-algorithm deal with the miss valu problem in the em-algorithm all known valu are consid via their likelihood depend on the distribution. in the same way hidden (i.e. missing) valu are consid as depend on the probabl distribut and addit on the known values. so the complet distribut can be seen as the product of two probabl distribut (known and miss values). the algorithm search for the paramet that maxim the log-likelihood. as they depend on the miss values, those are averag out. in an iter procedur the estim paramet is improv (m-step) follow by averag over the miss valu use the obtain paramet (e-step). this will lead the estim of the paramet to converg to a local maximum which hope is close to the real paramet value. the principl in handl miss valu here is to not tri to regain them somehow, but to invent valu from a model obtain through the probabl distribution. in the best case this doe not lead to in ation loss, although it general does. however, this at least make the exist valu technic usable.
what is the motiv behind cluster cluster are basic structures. additionally, cluster in some featur space may indic close of the data on a semant level. cluster data impli rules. compress can be achiev by transmit onli cluster centers. however, it is not alway trivial to defin clusters, this depend on the scale and the shape one want to achieve.
what are possibl distanc measur of cluster minimum distance: dmin(x, y ) maximum distance: dmax(x, y ) mean of all distances: dmean(x, y ) distanc of centers: dmean(x, y ) d = pdist2(x,y) return a matrix d contain the euclidean distanc between each pair of observ in the mx-by-n data matrix x and my-by-n data matrix y. row of x and y correspond to observations, column correspond to variables. d is an mx-by-mi matrix, with the (i,j) entri equal to distanc between observ i in x and observ j in y. the (i,j) entri will be nan if observ i in x or observ j in y contain nans.
what is the bias in cluster algorithm all cluster algorithm have a bias: - the bias prefer a certain cluster model that compromis scale & shape of the cluster - optim the bias can be chosen explicitly, but usual it is part built in in the algorithm - the adjust paramet are usual process parameters, not model paramet - the connect between the paramet and the cluster model has to be infer from the way the algorithm is work - hierarch cluster solv the problem for the scale paramet by have all differ scale solut present in an order way
what are two complementari method in hierach cluster - agglom clustering: start with each data point as a cluster, then merg recurs bottom up - divis clustering: start with all data point as a singl cluster, split cluster recurs top down the is a dendrogram repres all data in a hierarchi of clusters.
provid a pseudocod algorithm for agglom hierarch complet linkag cluster initialization: assign each of n data element to a cluster ci i = 1...n while n 2 do find the pair of cluster ci , cj , i merge: ci ← ciu if j end if n - - end while optim the linkag criterion requir a distanc measure. this is were the algorithm can be modified.
name differ linkag criterion - singl linkag clustering: use minimum cluster distance. prefer chain - complet linkag clustering: use the maximum cluster distance. prefer compact cluster - averag linkag cluster (upgma): use mean cluster distance. - centroid clustering: use centroid distance. cluster are repr. by their centroid / real valu attribut need for centroid comput / when join clusters,. ing centroid is domin by the cluster with more member
what is the differ between single- and complete-linkag cluster single-linkag tend to chain cluster along the data. that is whi it combin the point in the center area with those in the bottom right corner. complete-linkag prefer compact cluster and thus combin each of the point heavi area individu without merg them.
explain the idea behe ward minimum varianc clustering. what are properti idea: merg the two cluster for which the increas in total varianc is minim this approach is optim based. however, it can be implement by a distanc measur properties: prefer spheric cluster and cluster of similar size. robust against nois but not against outliers.
explain the idea behind minimum varianc cluster idea: for merging, do not onli take inter-clust distanc into account but also consid size of the cluster (prefer small ones)
what are the properti of hierach cluster - ani distanc measur can be use - we onli need the distanc matrix (not the data) - no paramet - efficiency: agglom o(n^3) (optim slink o(n^2)) / divisive: o(2^n) (optim clink o(n^2)) - ing dendrogram offer altern solut (but has to be analyzed) - cut off at differ level of dendrogram may be necessari to get compar cluster - outlier are fulli incorpor
provid a pseudocod for basic maxim algorithm (note:~x = vektor x) initialization: partit data somehow into cluster c1...cn while stop criterion not reach do choos an exampl ~x at random, denot it cluster as c(~x) random select a target cluster ct comput the chang of the good function: delta = e('~x element ct')- e('~x element c(~x)') if delta e 0 then put ~x from c(~x) to ct els put ~x from c(~x) to ct with probabl e^(belta delta e) end if increas beta end while
what are properti of the basic maxim algorithm - mayb caught in local optima - depend on initi partit - to escap local optima, downhil step are accept with probabl ebelta delta e - simul annealing: initi small allow frequent downhil steps, increas it make them more unlik - for the follow optim criteria, onli w need to be computed, due to a = b +w
what is meant by "compress by clustering" dea: given we have a data set consist of d-dimension vector that shall be transmit over some channel where bit per data point depend on dimens d. we can now cluster our data, transmit the cluster center onc and then onli transmit for each data point the cluster that repres it best. a small number of cluster has high compress but bad quality, a high number of cluster vice versa.
what is the idea of k-mean cluster idea: divid dataset d into cluster c1...ck which are repres by their k center of graviti (aka means) ~w1... ~wk and then minim the quadrat error iter k-means: start with random chosen refer vectors, assign data to best match refer vector, updat refer vector by shift them to the mean of their cluster, stop if cluster center haven't move more element
what are properti of k-mean - number of cluster is the onli paramet (implicit defin scale and shape). - greedi optim → local optima, depend on initi conditions. - fast algorithm.
what is the idea behe soft cluster so far: cluster as set of data point belong to their respect centers. → disjoint cluster / hard cluster (each point assign to one cluster) drawback → no way to express uncertainti about an assignment. soft cluster idea: describ data by a probabl distribut p(~x). a data point is assign to a cluster by probabl (express uncertainty). cluster have no boundari and are repres by gaussians.
what are properti of em-algorithm em yield onli local optima comput much more expens than k-mean precaut against collaps of gaussian to a singl point necessari k-mean can provid use initi for ~muk, local pca for ck there are k! equival solut → paramet identif might be difficult
what is the idea behind conceptu cluster idea: employ idea of cluster and decis tree learn for unsupervis classifcation. most known algorithm: cobweb. motiv by the drawback of id3 and inspir by cognit classifcation: categori ation is strong connect to ing prototyp concept (basic level categori and general and special of it). idea of cobweb: - unsupervis learn - increment learn - probabilist representation: gradual assign of object to categori - no a priori fix number of categori - realiz by global util function which determines: # of categori / # of hierarch level / assign of object to categori
what are the curs of dimension and it implic for pattern classif explain how this phenomenon could be use to one advantag curs of dimension describ the phenomenon that in high dimension vector spaces, two random drawn vector will almost alway be close to orthogon to each other. this is a real problem in data mine problems, where for a higher number of features, the number of possibl combin and therefor the volum of the ing featur space exponent increases. in such a high dimension space, data vector from real data set lie far away from each other (which mean dens sampl becom impossible, as there aren't enough sampl close to each other). this also lead to the problem that pair of data vector have a high probabl of have similar distanc and to be close to orthogon to each other. the is that cluster becom realli difficult, as the vector more or less stand on their own and distanc measur cannot be appli easily. this is actual an advantag if you want to discrimin between a high number of individu (see bertillonage, where use onli 11 featur s in a featur space big enough to discrimin humans), but if you want to get usabl in ation out of data, such a singl out of sampl is a great disadvantage.
explain in your own word the concept of descript and intrins dimension intrins dimension exist in contrast to the descript dimension of data, which is defin by the number of paramet use to produc or repres the raw data (i.e. the number of pixel in an unprocess image). addit to this represent dimensionality, there is also a (most of the time smaller) number of independ paramet which is necessari to describ the data, alway in regard to a specif problem we want to use the data on. for example: a data set might consist of a number of portraits, all with size 1920x1080 pixels, which constitut their descript dimensionality. to do some facial recognit on these portrait however, we do not need the complet descript dimens space (which would be way too big anyway), but onli a few independ paramet (which we can get by do pca and look at the eigenfaces). this is possibl becaus the data never fill out the entir high dimension vector space but instead concentr along a manifold of a much lower dimensionality.
what is the curs of dimension combinator explosion: n^d (volum of ing space) combin for d variabl with n valu each, which make n^d point necessari to sampl the whole space !-- sampl for larg d becom impossible. ani real data set will not be abl to will a high dimension space
what are properti of high dimension space random pair of vector are like to have similar distanc and be close to perpendicular. also the volum of the surfac layer dramat increas compar to the volum of the inner part (overal probabl that one dimens has an extrem valu increases).
what are aim of dimens reduct - find local dimension of the data manifold - find new coordin system to repres the data manifold and project the data to it - get rid of empti part in the space - new paramet may be more meaning
what is the idea of pca - find subspac that captur most of the data variance. - unsupervised. - given: data set d = {~x1, ~x2,...}, ~xi element r^d with zero mean: = sum over xi(~xi) = 0 -- pca find m d orthonorm vector ~p1...~pm such that ~pi are the direct of the largest varianc of d.
what are the featur of pca the eigenvector are call princip components, which can be interpret as features, recept field or filter kernels. expans after the m d largest eigenvector is the optim linear method to minim the mean squar reconstruct error
pca: how can we find a suitabl number m of eigenvector look for aprupt jump in the spectrum of eigenvalu indic a suitabl cut off. pca doe not generat structur - it onli make exist structur accessible.
for data structur is normal pca appropri linear data structur problem occur if data structur is non-linear. better solut here is local pca
what is the idea behind local pca first clustering, than pca in each cluster, or better, iter improv posit of cluster center and local pcs. for continous, non-clust data: -- no contin descript of the manifold -- differ cluster are possible, lead to entir differ local project
what are princip curv princip curves: can handl nonlinear distribut via nonlinear basi functions. data is project on the princip curve. however, an appropri dimension and exibl paramet have to be found.
what is the differ between pca and princip curv pca: mean squar error function for a data set given by a probabl densiti p(~x): e=1/2*integral(~x- sum(a1(~x)*~pt)^2*p(~x)d~x)) approxim the manifold by m vector ~pi, find coeffcient ai(~x) for each ~x princip curves: general approxim to a parameter surfac ~x (a1,..., a2 ~w) of m dimensions. the vector ~w element of r^n -- paramet that determin the shape of the surfac minim e =1/2*integral((~x-~x(a1(~x))...am(~(x)j ~w))^2*p(~x)d~x) the m paramet ai(~x) determin the point on the surfac of ~x that best match ~x and have to be comput for each ~x individu (in a 2d exampl dataset, there is onli one parameter, sinc m = 1 and ~x is a curve.) the number n of paramet in ~w are respons for the abil of ~x to t a manifold (small n underfit, larg n overfit). for a good fit, addit smooth constraint should be used. ~w can be fit use e.g. gradient descent (normal stochast approxim is use (downhil step over one singl sample), sinc otherwis each step would requir integr over all data).
kohonen net too complic -- lernen auf lücke
what are common choic on which to decid the number of dimens to project the data - eigenvalu magnitudes: find the cut-off depth. this is use for classif problems, especi for problem to be solv by computers. - visualization: choos the number of dimens which is use to visual the data in a meaning way. this choic depend a lot on your problem definition. for print 2d is usual a good choic - but mayb your data is just veri nice for !d already. or mayb you are use a glyph plot (see sheet 06) which can repres high dimension data. - classif s: in the eigenfac assign below we figur out that the number of princip compon (and thus the number of dimensions) can have a crucial impact on classif rates. it is thus an option to fine tune the number of dimens for a good classif on some train set.
how mani princip compon are there at most when you appli the pca with the 20 train face imag provid how mani princip compon were there for the 16 binari imag if we made a pca on all of them there are at most 20 princip compon possibl for the face images, but onli four princip compon for the binari images.
whi do we need data visual techniqu and what are techniqu to visual high dimension data sometim it is necessari to visual high dimension data and a project via pca or similar method might not help enough: we might lose in ation in a 2d projection. in those case it is use to come up with other represent of data which we could potenti print on a sheet of paper. techniqu are usual glyphs, but differ kind of project might alreadi be enough (take in ation loss into account).
whi did chernoff use face for his represent whi not someth else, like dog or hous human are except good at face recognition. it is veri easi to realiz if one eye is bigger than anoth or eye brow are closer togeth in face-lik imag for human than for exampl figur out differ in window size or chang in roof skew between houses.
explain at least one other data visual techniqu from the lectur - scatterplot matrix: scatter data into plot for each combin of two attributes. - glyphs: map data dimens onto paramet for geometr figures, e.g. star glyphs, arrows. properti could be lengths, widths, orientations, colors, ... - parallel coordinates: use featur dimens as one axi and featur valu as another. plot datapoint as lines. - projections: sever differ techniqu to project the data: pca, scale strategies, ...
what is a scatterplot matrix matrix of 2d plot of all possibl combin of avail dimensions. if ing all axe is infeasible, pca may yield suitabl directions.
what are glyph map each dimens onto a paramet of a geometr gure. glyph are normal perceiv as a whole ! extract in ation of glyph requir training! examples: star glyphs, parallel coordinates, chernov faces.
what is the idea behind a project pursuit what is interest what is the procedur name four criteria to measur teh deviat of a distribut from the standard normal distribution. what are problem idea: project onto 2-3 select direct like pca, but choos direct that exhibit interest structure. what is interest varianc / non-gaussian distribut (structure) / cluster procedure: 1. select 1 to 3 direct (bi pca or simpli origin dimensions) 2. project onto these direct & get densiti p(~x) of project data 3. comput index how interest (accord to some criterion) the data is 4. maxim index by search for better direct criteria: - firedman-turkey-index minim if p is a parabol function similar to norm. distribut - hermite/ hall index minim when p is a standar normal distribut - natur hermit index like hermit index onli higher weight of the center - entropi index minim by standard normal distribut problems: maxim requir estim of densiti p(~x) of the project data. method are kernel densiti estim (parzen windows) / orthonorm function expans
what is the idea behind multidimension scale idea: find a lower dimension manifold such that project preserv the structur of the data as good as possible. we must defin structure. important: distanc between data points. given r^d and lower dimension space r^d find a map : r^d-- r^d such that the distanc between data point in r^d are well approxim by the distanc of the project of those data point in r^d.
what are the properti of al neuron - iti communic via axon is repres as real number x (aka spike freq) - for a neuron with d input -- ~x element of r^d is the input vector - each input xi is weight by a weight wi - ation is the sum of the weight input - output y is then given by feed the ation into an ation function: y = y(s)
name and explain the three type of learn in artifici neural network unsupervis learning: no teacher, unlabel examples. effect of learn is code in the learn rules. supervis learning: teacher/label examples. learn is direct at map the input part of the exampl to the label as an output. reinforc learning: weak teacher: agent tri to reach goal and onli get feedback if goal has been reach or not. agent has to find way (creat mapping) itself. more realist learn becom popular: weak supervis and semi-supervis learning: pre structur of data with unsupervis learning, use of rare label data after unsupervis learning.
explain hebb rule increas weight wi accord to the product of the ation of the neuron and the input via this channel i so far, weight can becom arbitrarili large. to prevent this, there are sever solutions: decay term (includ constant forgetting): delta~w = epsilon *y(~x~w)~x - gamma~w dynam normal to ~w = 1: delta~w = epsilon *y(~x~w)~x - gamma~w*( ~w^2 - 1) explicit normal to ~w = 1: ~w^new = (~w^old+delta~w)/( ~w^old+delta~w ) oja rule, use weight decay y^2: delta~w = epsilon * y(~x~w)(~x - ~y(~x~w) ~w)
what is the effect of hebb rule on a pair of neuron the connect between two neurons, repres by the weight of the postsynapt neuron, will increas dure train until an equilibrium with the decay term is reached. then, the weight is proport to the correl of the iti of the neurons.
what is the effect of hebb rule on the weight vector ~w converg to the eigenvector of c with the largest eigenvalue. hebbain learn find the largest princip compon similar to pca compar to a winner take it all rule, the hebb-train neuron are affect by all stimuli: close input has less effect than far input. this is whi ~w is adjust accord to the data variance.
what is call "the anti hebb rule" explain! habitu ~w has a parallel and an orthogon compon to ~x. for contin train with the same ~x the parallel compon becom 0, while the orthogon compon doe not change. the anti-hebb rule lead to habituation: ~w becom orthogon to the repeat stimulus -- ~w filter out new stimuli. for sever habitu stimuli, onli the ~w compon orthogon to the space span by those stimuli can pass the filter.
name and explain two way to extract more princip compon ~v from d 1. succes applic of singl hebb neurons: extract ~v1, project d to space orthorgon to ~v1 and extract ~v2 and so on -- singl neuron need less step (on get the correct input for them. good for sequenti train 2. method by sanger: chain of later coupl neurons, each train by hebb rule. first neuron get unfilt input, second get input minus direct of the first weight vector and so on. -- each step requir train of the complet chain, while first neuron are not proper trained, later neuron receiv input where pc with larger eigenvalu are not filter out correctly. good for parallel training.
what is a perceptron abl to do what is it not abl to do a perceptron repres a d-1 dimension decis surface, a hyperplan othorgon to ~w. necessari conditon: onli solvabl with a perceptron if data is linear seperable. this is the case for mani basic logic operations, except for xor (xor can be solv by distort of featur space or ad of further input channel like x3 = x1 * x2)
what is the idea behind multilay percepton idea: solv nonlinear seper task with nonlinear separatric & class cover disjoint part by combin sever perceptron and also general to sever output
give the sigmoid function and it first derivative. whi is it use for multi-lay perceptron σ(t)=1/(1+e^(−t)) ∂σ/∂t=σ(t)(1−σ(t)) the sigmoid function is common use becaus of it nice analyt properties: it domain is [0,1], it is non-linear, strict monotonous, continuous, differenti and the deriv can be express in term of the origin function at the given point. this allow us to avoid redund calculations. the sigmoid function is a special case of the more general logist function which can be found in mani differ fields: biology, chemistry, economics, demographi and recent most prominently: artifici neural networks.
describ the train of a perceptron and describ two mode of learn perceptron is iter train use labl data d = (~x, ~y), ~x^n element r^d perceptron train rule: delta ~w = epsilon (t - y)~x converg can be shown if learn rate epsilon is suffici small (and task is solvabl by a perceptron). the learn rule can be deriv from minim the mean squar error. two model of learning: -batch mode: gradient descent with respect to entir train set: delta~w =epsilon sum((ti-y(~xi))~xi) - increment mode: gradient descent with respect to one exampl (~xi, ti) delta~w = (ti - y(~xi))~xi for small : increment mode can be shown to approxim batch mode arbitrarili close.
what is the general srchitectur of an mlp - input layer (layer 0): one neuron for each dimens of input vector (input neuron onli repres input) - at least one hidden layer (l1...lh), hidden layer can have differ number of neuron - output layer(lh+1): one node per dimens of the output - feed forward architecture: onli connect from layer k to i with k - more notation: neuron in layer i: 1(i)...n(i) / output of neuron i in layer k: oi(k) / weight from neuron k in layer n to neuron i in layer m: wik(m, n) sigmoid ation function σ are use (success of linear tran s is itself a singl linear tran , so noth to gain here). σ enforc a decision, soft step requir (backpropag requir di erentiability). squash map incom info to smaller range.
train mlp + backpropag pseudocod use same error function as befor & minim by gradient descent. problem: all deriv on the path from wik(m,n) to output layer required. the backpropag algorithm provid a scheme for ecient comput of all requir deriv and weight adaptations. initi ~w random while stop criterion is met do propag ~x forward trough net to obtain ~y comput error ti - yi(~x) between actual output and desir output for each output dimens i propag error backward trough net to find respons weight updat weight end while weight of neuron j is adapt in proport to ation of neuron in previous layer to which it is connect % weight error it caus at the output. this complex scheme is necessari sinc target valu are onli avail for the outputs, not the hidden layer neuron (so a target valu has to be constructed). bp-algorithm per s a stochast apprxim of gradient descent for error function e.
what are problem with minim in mlps what are way to avoid local minima minim here is still comput expensive, suffer from local optima, is diffcult to termin (good fit but no overfitting). way to avoid local minima: - repeat train with differ initi weight to find differ basin of attract - annealing: add noise, e.g. everi n learn steps: wji(k + 1, k) = wji(k + 1, k) + t gji(k + 1, k) g is a gaussian random variabl with mu= 0, t is the temperatur anneal improv minim but requir longer learning. - step size adaptation: increas in epsilon at region and decreas in steep terrain - momentum: delta wji(k + 1, k)(t))epsilon deltaj(k + 1)oi(k) + wji(k + 1, k)(t - 1) t is a step counter, so direct of step t-1 is kept to some degree. this avoid abrupt chang of direct and therebi counteract stope at minor minima and oscillations. weight decay: over larg weight are problematic, they lead to binari decis of singl neurons. therefor an addit quadrat regular term can be ad to the error function to avoid larg weights. this lead to a decay part in the learn rule.
how doe backpropag in mlps work multilay perceptron (mlps) can be regard as a simpl concaten (and parallelization) of sever perceptrons, each have a specifi ation function σ and a set of weight wij. the idea that this can be done was discov earli after the invent of the perceptron, but peopl didn't realli use it in practic becaus nobodi realli knew how to figur out the appropri wij. the solut to this problem was the discoveri of the backpropag algorithm which consist of two steps: first propag the input forward through the layer of the mlp and store the intermedi s and then propag the error backward and adjust the weight of the unit accordingly. an updat rule for the output layer can be deriv straightforward. the rule for the intermedi layer can be deriv veri similar and onli requir a slight shift in perspect - the mathemat for that are howev not in the standard toolkit so we are go to omit the calcul and refer you to the lectur slides. we take the least-squar approach to deriv the updat rule, i.e. we want to minim the loss function l=1/2*(y−t)^2 where t is the given (true) label from the dataset and y is the (single) output produc by the mlp. to find the weight that minim this express we want to take the deriv of l w.r.t. wi where we are now go to assum that the wi are the one direct befor the output layer ∂l/∂wi=(y−t)oi*y(1−y)
what are the step of the algorithm of implement an mlp 1.initi your mlp. use as mani input neuron as there are dimens in the data. input neuron alway expect 1d input. then creat neuron for each hidden and the output layer. each neuron in the hidden and output layer expect as mani input as there are neuron in the layer befor them. 2.initi the neuron weights. for each neuron in layer 1...lh+1 initi the weight to small random valu 3.implement the ation (feed-forward) step. a.decompos the input into it compon and pass them to the correct input neuron. b.each input neuron pass it unprocess input to the next layer. that mean each neuron in layer 1 receiv all output from each input layer as it own input. oi(0)=xi c.calcul the weight sum of their input and appli their ation function σ for each neuron in the layer 1...lh+1. this is best done iter layer by layer, as each layer input is the output of it preced layer (note: wj0(k,k) denot the bias for neuron j in layer k): d.the ing oi(lh+1) are the output yi for each output neuron i. 4.implement the adapt (backpropagation) step. a.comput the error between the target and output compon to calcul the error signal δi(lh+1) : δi(lh+1)=oi(lh+1) (1−oi(lh+1)) (ti−oi(lh+1)) b. calcul the error signal δi(k) for each hidden layer k, start with k=lh and go down to k=1. c. adapt the weight for each neuron in the hidden and output layers. δwji(k+1,k)=ϵδj(k+1)oi(k)
how do you find the appropri architectur for your mlp what do layer 1, 2, and 3 defin are there restrict regard the number of node funashi: arbitrari bound contin map can be approxim with arbitrari precis with one hidden layer with sigmoid ation function and a layer with linear output function cybenko: 2 hidden layer are suffici to approxim ani function with arbitrari precis (however, layer migth be huge) how so -- 1st layer defin hyperplan -- 2nd layer defin and over the hyperplan (convex areas) -- 3rd layer defin or over convex area no restrict on number of nodes, but: -- less node than input layer may destroy degre of freedom (if present in the data) -- more node may facilit represent but cannot invent addit degre of freedom hidden layer may serv to discov featur in the input data (more dimens than need etc)
mlp encod general : for an arbitrari distribut of vectors, the n neuron of the hidden layer (aka bottleneck) converg to span the subspac of the input space which is span by the n princip compon with the largest eigenvectors.
sortierarfgab neural architectur in the follow we will look at n x n connect matrices, sinc they compris the complet wire in ation - fulli connect -- hig complex dynam (chaotic) -spars connect -- some statement about dynam are possible, interest case - diagon matrix -- boring, each neuron onli connect to itself - tridiagon matrix -- chain of n neurons, each neuron has forward & backward coupl to it neighbour (and itself) - lower triangular matrix -- feed forward network, no recurreny, type of mlp, diagon = 0 so no slef coupl - diagon -- seper local network - diagon + spars connect -- small world network - symmetr matrix -- hop eld networks, dynam converg to point attractor
describ recurr network feed-forward network can be comput sequenti (updating), sinc there are no cycl -- they are timeless not possibl for recurr network (due to cycles) -- recurr network are dynam systems, time is important. they can either be describ contin by differenti equat or discret by the use of synchron (next timestep for all neurons) or asynchron updat (random updat neurons). phase space: space of all possibl state of a network (all possibl ation vectors) evolut of the system is trajectori threw the state space basic scenarios: -- converg to point attractor -- converg to limit cycl -- converg to strang attractor -- chao
what are radial basi function radial basi function are all function that fullfil the follow criteria: the valu of the function for a certain point depend onli on the distanc of that point to the origin or some other fix center point. in mathemat ulat that spell out to: ϕ(x)=ϕ(∥x∥) or ϕ(x,c)=ϕ(∥x−c∥). notic that it is not necessari (but most common) to use the norm as the measur of distance.
what is the structur of a rbfn rbfn are network that contain onli one hidden layer. the input is connect to all the hidden units. each of the hidden unit has a differ radial basi function that is sensit to rang in the input domain. the output is then a linear combin of the outpus ot those functions.
how is a rbfn train note: all input data has to be normalized. train a rbfn is a two-step process. first the function in the hidden layer are initialized. this can be either done by sampl from the input data or by first per ing a k-mean clustering, where k is the number of node that have to be initialzed. the second step fit a linear model with coeffici wi to the hidden layer output with respect to some object function. the object function depend on the task: it can be the least squar function, or the weight can be adapt by gradient descent.
what do both model (rbfn and mlp) have in common where do they differ rbfn: - non-linear layer feedforward network - hidden neuron use radial basi functions, output neuron use linear function - univers approxim - learn usual affect onli one or some rbf mlp - non-linear layer feedforward network - input, hidden and output-lay all use the same ation function - univers approxim - learn affect mani weight throught the network
when would you use a rbfn instead of a multilay perceptron rbfns are more robust to nois and should therefor be use when the data contain false-positives.
what is the idea behind instanc base learn idea: simpli store the train exampl d = (~xn,~tn), ~xn element r^d,n ,
name and explain two simpl approach of instanc base learning. nearest neighbour algorithm training: memor all exampl application: for new ~x find best match and give output for of this best match. k-nearest neighbour algorithm for unknwon ~x find set s of nearest neighbour of store examples. discret valu output: vote contin output: use mean
what are the properti of instanc base learn 1. plain nearest neighbour -- hard boundari (assign new output to voronoi tessel cell) k-nearest -- allow contin transit (new, unseen outputs) 2. suitabl k depend on local intrins data dim 3. training: veri fast / requir memori (no compr) / no info wast / no param or complex procedur 4. application: may be slow (when mani examp stored) / sensit to error & nois
what us the idea behind distanc weight k-nearest neighbour idea: nearer neighbour are more important. weight averag with distanc to the input with invers distanc as weight cool thing: now neighbour can be the entir set of exampl
what is the idea behind local weight regress construct better local approxim of ~y(~x) by comput t function in the region around the sampl two choices: what fit function (linear, quadratic,...) & what error function (what will be minim local!!!)
what are properti of local method in instanc base learn in mlp like ann singl weight adapt might influenc per anc of the whole net (all ouput channels). they are a veri delic and non robust structure. death aka remov of singl neuron might have drastic consequ for the network. local method network are far more robust, sinc comput of output onli (or mainly) reli on local unit and can be nice compens for by other units. also adapt has onli local effects.
how is learn in a self-organ map achiev (as oppos to techniqu use in mlp ) in self-organ map the node take part in competit learning. they compet for each input and a winner is dertermin whos weight (and the weight of his neighbors) are the onli weight that are adapted. mlps use error correct learn where a delta of the output to a target valu is comput and use to adapt all weight in the network.
what would be an altern to initi node random in self-organ map the nodes/weight can be sampl from the subspac span by the larg princip components. with this method learn should becom faster sinc the node alreadi have a good initi fit to the structur of the data. this method onli work if the dataset is not essenti non-linear-
whi are self-organ map possibl interest for cognit scientist in general it could be argu that self-organ map work in a way similar as the brain when it handel differ sensori input in differ part of the brain. the area themselv are topolog structur in a way that similar input ate the same area (just like the som as well)
how doe the lda classifi work what restrict have to be fullfil by the data for this method to work and whi the lda classifi assum two normal distribut class of data with the same covari matric each. it subtract the mean of the two distribut from each other and normal by the invers variance. take the inner product of the weight (the normal differ function) with the new data point yield either a posit or a negat valu which can be compar to the threshold to determin the estim for the class of the new data point.
how doe the nearest neighbor classifi work when would you use it and how is it train for a new sampl the nearest neighbor classifi search for the correspond nearest data point in our train data. you would use it when memori and runtim time are not of great importance, as train is instant: you just store the train data. however, classif becom a comput more expens task.
name some differ between a svm and a mlp. when would you use which svm: - linear separatrix (unless use kernel trick) - binari classifi (extens to cascad possible) - no onlin train - can deal with outliers: slack variabl mlp: - non-linear separatrix - multipl class possibl - onlin train possibl - outlier affect the whole network
well-pos learn problem a comput program is said to learn from (e)xperi e with respect to some (t)ask t and some (p)er anc measur p, if it per anc on t, as measur by p, improv with experi e.
machin learn field of studi that give comput the abil to learn without be explicit programmed.
machin learn broad definit field of studi that give comput the abil to learn without be explicit programmed.
abstract essenc of ml represent + evalu + optimis
machin learn learn from experience. it also call supervis learning, were experi e is the supervision.
pattern recognit find pattern without experience. it also call unsupervis learning.
unsupervis lear - we onli have xi values, but no explicit target labels. - you want to do someth with them.
unsupervis learn task - outlier detection: is this a normal xi - data visualization: what doe the high-dimension x look like - associ rules: which xij occur togeth - latent-factors: what part are the xi made from - ranking: which are the most import xi - clustering: what type of xi are there
classif ml task where t has a discret set of outcomes. often classif is binary. examples: • face detect • smile detect • spam classif • hot/cold
regress ml task where t has a real-valu outcom on some continu sub-spac examples: • age estim • stock valu predict • temperatur predict • energi consumpt predict
label valu that h aim to predict example: • facial express of pain • impact of diet on astronaut in space • predict of hous price
train algorithm given a model h with solut space s and a train set {x,y}, a learn algorithm find the solut that minim the cost function j(s)
features/attribut measur valu of variabl that correl with the label y examples: • sender domain in spam detect • mouth corner locat in smile detect • temperatur in forest fire predict • pixel valu in face detect
cost function squar error cost function. j(s)
local minima the smallest valu of the function. but it might not be the onli one.
general classif problem if class are disjoint, i.e. each pattern belong to one and onli one class then input space is divid into decis region separ by decis boundari or surfac
decis surfac are • linear function of x • defin by (d-1) dimension hyperplan in the d dimension input space.
linear separ linear separ data: • dataset whose class can be separ by linear decis surfac • impli no class-overlap • class can be divid by e.g. line for 2d data or plane in 3d data
orthogon - two vector and are orthogon if they'r perpendicular - if their inner product is 0: a · b = 0
lda - linear discrimin analysi - most common use as dimension reduct techniqu in the pre-process step for pattern-classif and machin learn applications. - the goal is to project a dataset onto a lower-dimension space with good class-separ in order avoid overfit
train lda objective: find (i.e. learn) that minim some error function on the train set. signific approaches: • least squar • fisher • perceptron
artifici neural net • feed-forward neural network/multilay perceptron one of mani ann • we focus on the multilay perceptron • realli multipl layer of logist regress model
the simplest ann consist of • a layer of d input node • a layer of hidden node • a layer of output node • fulli connect between layer
curs of dimension the curs of dimension refer to how certain learn algorithm may per poor in high-dimension data. first, it veri easi to overfit the the train data, sinc we can have a lot of assumpt that describ the target label (in case of supervis learning). in other word we can easili express the target use the dimens that we have. second,w may need to increas the number of train data exponentially, to overcom the curs of dimension and that may not be feasible. third, in ml learn algorithm that depend on the distance, like k-mean for cluster or k nearest neighbors, everyth can becom far from each other and it difficult to interpret the distanc between the data points.
extrem dimension case in an extreme, degener case, if d n, each exampl can be uniqu describ by a set of featur values.
hidden layer(s) can - have arbitrari number of nodes/unit - have arbitrari number of link from input node and to output node (or to next hidden layer) - there can be multipl hidden layer
hidden unit activ common function for are unit step, sigmoid or logist and tanh
relu rectifi linear unit new trend, respons for great deal of deep learn success. advantages: • no vanish gradient problem • can model ani posit real valu • can stimul spars
output layer can be • singl node for binari classif • singl node for regress • n node for multi-class classif
network topolog variat include: • arbitrari number of layer • fewer hidden unit than input unit (caus in effect dimension reduction, equival to pca) • skip-lay connect • fully/spars interconnect network
train a network train a nn involv find the paramet that minim some error function choic of ation function depend on the output variables: - uniti for regress - logist sigmoid for (multipl independent) binari classif - softmax for exclus (1-of-k) multiclass classif
nn error function regression: - binari classif - multipl independ binari classification: - multi-class classif (mutual exclusive):
gradient point in the direct of steepest ascent
how to train ann an error function on the train set must be minimized. this is done by adjusting: - weight connect nodes. - paramet of non-linear function h(a).
eigenvalu given an invert matrix , an eigenvalu equat can be found in term of a set of orthogon vector , and scalar such that: m
deriv measur of how fast function valu chang with the chang of the argument. so if you have the function f(x)=x^2 you can comput it deriv and obtain a knowledg how fast f(x+t) chang with small enough t. this give you knowledg about basic dynam of the function
gradient gradient show you in multidimension function the direct of the biggest valu chang (which is base on the direct derivatives) . so given a function i.e. g(x,y) = -x+y^2 you know, that it is better to minim the valu of x, while strong maxim the valu of y. this is a base of gradient base methods, like steepest descent technique.
batch gradient descent - vanilla gradient descent, aka batch gradient descent - make small chang in weight that most rapid improv task per anc gradient descent comput the gradient of the cost function w.r.t. to the paramet θ for the entir train dataset - can be veri slow - intract for dataset that don't fit in memori - doesn't allow us to updat our model online, i.e. with new exampl on-the-fly. - guarante to converg to the global minimum for convex error surfac and to a local minimum for non-convex surfaces.
on-lin gradient descent on-lin (or schotastic) gradient descent also known as increment gradient descent updat paramet one data point at a time. - handl redund better. (batch gd has redundancy) - usual much faster than batch gd. - sgd per s frequent updat with a high varianc that caus the object function to fluctuat heavili - can deal with new data better. - good chanc of escap local minima. however, when we slowli decreas the learn rate, sgd show the same converg behaviour as batch gradient descent
backpropag - use to calcul deriv of error function effici - error propag backward layer by layer
backprop is for: arbitrari feed-forward topolog differenti nonlinear ation function broad class of error function
error backpropag 1. appli input vector to network and propag forward 2. evalu d(k) for all output unit 3. backpropag d to obtain d(j) for all hidden unit 4. evalu error deriv as:
regular regular is a techniqu use in an attempt to solv the overfit problem in statist models.
regular - maximum likelihood general error (i.e. cross-validation) - regular error (penal larg weights) - earli stop
deep learn - basic a neural network with mani hidden layer - can be use for unsupervis learn and dimension reduct
what is deep learn definition: • hierarch organ with more than one (non-linear) hidden layer in-between the input and the output variabl • output of one layer is the input of the next layer
deep learn method (deep) neural network • convolut neural network • restrict boltzmann machines/deep belief network • recurr neural network
convolut neural network type of feed-forward artifici neural network in which the connect pattern between it neuron is inspir by the organ of the anim visual cortex, whose individu neuron are arrang in such a way that they respond to overlap region tile the visual field.
how is deep neural network optim optim through gradient descent! (forward-backward algorithm) - penal complex solut to avoid overfit
cost function - ℓ2 norm in order to avoid over-fitting, one common approach is to add a penalti term to the cost function. common choic are the ℓ2-norm, given as: where c0 is the unregular cost
cost function - ℓ1 norm 
dropout a veri differ approach to avoid over-fit is to use an approach call dropout. here, the output of a random chosen subset of the neuron are temporarili set to zero dure the train of a given mini-batch. this make it so that the neuron cannot over adapt to the output from prior layer as these are not alway present. it has enjoy wide-spread adopt and massiv empir evid as to it usefulness.
cost function - euclidean distanc distanc measur between a pair of sampl p and q in an n-dimension featur space
cost function - manhattan or citi distanc calcul the distanc between real vector use the sum of their absolut difference. also call citi block distanc
basic decis tree decis tree appli a seri of linear decisions, that often depend on onli a singl variabl at a time. such tree partit the input space into cuboid regions, gradual refin the level of detail of a decis until a leaf node has been reached, which provid the final predict label.
tree compon root node, branch, node, leaf node.
branch factor branch factor of node at level l is equal to the number of branch it has to node at level l + 1
cart classif and regress tree
six general question to decid on decis tree algorithm: 1. how mani split per node (properti binari or multi valued) 2. which properti to test at each node 3. when to declar a node to be leaf 4. how to prune a tree that has becom too larg (and when is a tree too large) 5. if a leaf node is impure, how to assign a categori label 6. how to deal with miss data
tree variati tree are call monothet if one property/vari is consid at each node, polythet otherwis
what tree are prefer we prefer simple, compact trees, follow occam razor
how to make tree compact to do so, we will seek to minimis impur of data reach descend node
occam razor all thing be equal - the simplest explan is the best
the principl of plural plural should not be posit without necessity.
the principl of parsimoni it is pointless to do with more what is done with less.
misclassif impur minimum probabl that train exampl will be misclassifi at node n
bay error - the bay error rate is the theoret lowest possibl error rate for a given classifi and a given problem (dataset). - for real data, it is not possibl to calcul the bay error rate, although upper bound can be given when certain assumpt on the data are made. - the bay error function most as a theoret devic in machin learn and pattern recognit research.
generalis general is the desir properti of a classifi to be abl to predict the label of unseen exampl correctly. a hypothesi general well if it can predict an exampl come from the same distribut as the train exampl well.
overfit a hypothesi is said to be overfit if it predict per anc on the train data is overoptimist compar to that on unseen data. it present itself in complic decis boundari that depend strong on individu train examples.
stop criteria reach a node with a pure sampl is alway possibl but usual not desir as it usual caus over-fitting.
three common way to decid when to stop split decis tree - valid set - cross-valid - hypothesi test (chi-squar statistic)
evalu procedur • for larg datasets, a singl split is usual sufficient. • for smaller datasets, reli on cross valid
valid set criterian split train data in a train set and a valid set (e.g. 66% train data and 34% valid data). keep split nodes, use onli the train data to learn decisions, until the error on the valid set stop go down.
cross-valid in k-fold cross-validation, a dataset is split into k rough equal size partitions, such that each exampl is assign to one and onli one fold. at each iter a hypothesi is learn use k-1 fold as the train set and predict are made on the k'th fold. this is repeat until a predict is made for all k folds, and an error rate for the entir dataset is obtained. cross-valid maximis the amount of data avail to train and test on, at cost of increas time to per the evaluation. • train data segment between differ fold should never overlap • train and test data in the same fold should never ovelap error estim can either be done per fold separately, or delay by collat all predict per fold.
cross-valid criterion split train data in a number of folds. for each fold, train on all other fold and make predict on the held-out test fold. combin all predict and calcul error. if error has gone down, continu split nodes, otherwise, stop
prune first fulli train a tree, without stop criterion after training, prune tree by elimin pair of leaf node for which the impur penalti is small
multivari tree instead of monothet decis at each node, we can learn polythet decisions. this can be done use mani linear classifiers, but keep it simple!
miss attribut it is common to have exampl in your dataset with miss attributes/variables. one way of train a tree in the presenc of miss attribut is remov all data point with ani miss attributes. a better method is to onli remov data point that miss a requir attribut when consid the test for a given node for a given attribute. this is a great benefit of tree (and in general of combin models,)
id3 inter e dichotom version 3 use for nominal, unordered, input data only. everi split has branch factor , where is the number of valu a variabl can take (e.g. bin of discret variable) has as mani level as input variabl
c4.5 - successor of id3. - multiway split are used. - statist signific split pruning.
regress tree - train in a veri similar way - leaf node are now continu valu - the valu at a leaf node is that assign to a test exampl if it reach it - leaf node label assign is e.g. mean valu of it data sampl problem: node make hard decisions, which is particular undesir in a regress problem, where a smooth function is sought.
model combin view • decis tree combin a set of model (the nodes) • in ani given point in space, onli one model (node) is respons for make predict • process of select which model to appli can be describ as a sequenti decis make process correspond to the travers of a binari tree
random forest 1. veri good per anc (speed, accuracy) when abund data is available. 2. use bootstrapping/bag to initi each tree with differ data. 3. use onli a subset of variabl at each node. 4. use a random optim criterion at each node. 5. project featur on a random differ manifold at each node.
measur of classif accuraci classif error rate cross valid recall, precision, confus matrix receiv oper curves, two-altern forc choic
estim hypothesi accuraci sampl error vs. true error confid interv
sampl theori basic binomi and normal distribut mean and varianc
compar hypothes t-test analysi of varianc (anova) test
simpl data split - fix train, develop and test set - bootstrap - cross-valid
fix train, develop and test set - random split data into training, development, and test sets. - doe not make use of all data to train or test - good for larg dataset
bootstrap estim the sampl distribut of an estim by resampl with replac from the origin sample.
cross-valid - random split data into n fold and iter use one as test set - all data use to test, and almost all to train - good for small set
evalu procedur for singl split or cross valid - for larg datasets, a singl split is usual sufficient. - for smaller datasets, reli on cross valid
overfit can occur when - learn is per ed for too long (e.g. in neural networks) - the exampl in the train set are not repres of all possibl situat (is usual the case!) - model paramet are adjust to unin ativ featur in the train set that have no causal relat to the true under target function.
confus matrix easi to see if the system is common mislabel one class as anoth
classif measur - error rate common per anc measur for classif problem 1. success: instanc class is predict correct (true posit (tp) / negat (tn)). 2. error: instanc class is predict incorrect (fals posit (fp) / negat (fn)). 3. fals posit - type i error. fals negat - type ii error. 4. classif error rate: proport of instanc misclassifi over the whole set of instances.
f-measur compar differ approach is difficult when use multipl evalu measur (e.g. recal and precision) f-measur combin recal and precis into a singl measur
accuraci (tp + tn)/ (tp + tn + fp + fn)
accuraci may not be use measur in case where 1- there is a larg class skew 2- there are differenti misclassif cost - say, get a posit wrong cost more than get a negat wrong. 3- we are interest in a subset of high confid predict
tpr - true posit rate - recal tp/actual posit = tp/tp + fn
fals posit rate fp/actual negat = fp/tn + fp
specif  
sensit  
roc curv receiv oper characterist (roc) curv plot tp vs fp rate
hypothesi qualiti - we want to know how well a machin learner, which learn the hypothesi as the approxim of the target function , per s in term of correct classifi novel, unseen exampl - we want to assess the confid that we can have in this classif measur
the true error of hypothesi h probabl that it will misclassifi a random drawn exampl from distribut : d however, we cannot measur the true error. we can onli estim it by observ the sampl error es
sampl error in statistics, sampl error is incur when the statist characterist of a popul are estim from a subset, or sample, of that population.
a bernoulli trial - it is a trial with a binari outcome, for which the probabl that the outcom is 1 equal p (think of a coin toss of an old warp coin with the probabl of throw head be p). - a bernoulli experi is a number of bernoulli trial per ed after each other. these trial are i.i.d. by definition.
binomi distribut in probabl theori and statistics, the binomi distribut with paramet n and p is the discret probabl distribut of the number of success in a sequenc of n independ yes/no experiments, each of which yield success with probabl p.
normal distribut - the normal distribut has mani use properties. it is fulli describ by it mean and varianc and is easi to use in calculations. - the good thing: given enough experiments, a binomi distribut converg to a normal distribution.
t-test assess whether the mean of two distribut are statist differ from each other
signific level signific level α%: α time out of 100 you would find a statist signific differ between the distribut even if there was none. it essenti defin our toler level. if the calcul t valu is abov the threshold chosen for statist signific then the null hypothesi that the two group do not differ is reject in favor of the altern hypothesis: the group do differ.
test for compar distribut - t-test compar two distribut - anova compar multipl distribut - if null-hypothesi is refuted, there are at least two distribut with signific differ mean doe not tell you which they are!
latent variabl - latent variabl are variabl that are hidden in the data. they are not direct observed, but must be inferred. - cluster is one way of find discret latent variabl in data.
discret latent variabl hidden variabl that can take onli a limit number of discret valu (e.g. gender or basic emotion).
cluster applic - market segment - social network analysi - vector quantize - facial point detect
k-mean cluster in ally, goal is to find group of point that are close to each other but far from point in other group • each cluster is defin entir and onli by it centre, or mean valu µk
k-mean algorithm 1. assign each xi to it closest mean. 2. updat the mean base on assign 3. repeat until converg
k-mean issu converg is guarante but not necessarili optim - local minima like to occur • depend larg on initi valu of uk. • hard to defin optim number k. • k-mean algorithm is expensive: requir euclidean distanc comput per iteration. • each instanc is discret assign to one cluster. • euclidian distanc is sensit to outliers.
robbins-monro •address the slow updat speed of the m-step in k-mean •use linear regress (see lectur 1)
k-medoid cluster address issu with quadrat error function (l2-norm, euclidean norm) replac l2 norm with ani other dissimilar measur (v...)
random initi - random initi k-mean cluster use actual instanc as cluster center - run k-mean and store center and final cost function - pick cluster of iter with lowest cost function as optim solut - most use if k 10
choos k elbow method • visual inspect • downstream analysi
probabl theori recap p(x) = margin distribut p(x,y) = joint distribut p(x y) = condit distribut
mixtur of gaussian simpl ulation: densiti model with richer represent than singl gaussian
i.i.d. independ and ident distribut random variabl
em algorithm issu • take a long time • often initialis use k-mean
data mine - quest to extract knowledg and/ or unknown interest pattern from appar unstructur data. aka knowledg discoveri from data (kdd) • data mine bit of a misnom - in ation/ knowledg is mined, not data.
knowledg discoveri process 1. data clean - remov nois and inconsist 2. data integr - combin data sourc 3. data select - retriev relev data from db 4. data tran ation - aggreg etc. (cf. featur extraction) 5. data mine - machin learn 6. pattern evalu - identifi truli interest pattern 7. knowledg represent - visual and transfer new knowledg
arff attribute-rel file format
hdf5 • much more complex file at design for scientif data handl • it can store heterogen and hierarch organ data. • it has been design for efficiency.
dm type of data • relat databas • data warehous • transact databas • object-rel databas • temporal/sequence/time-seri databas • spatial and spatio-tempor databas • text & multimedia databas • heterogen & legaci databas • data stream
dm function concept/class descript • character • discrimin • frequent patterns/ associations/ correl • classif and regress (prediction) • cluster analysi • outlier analysi • evolut analysi
the goal of data mine is to find interest patterns!. an interest pattern is: 1. easili understood. 2. valid on new data with some degre of certainty. 3. potenti useful. 4. novel.
dm object interest support: p(x u y ) percentag of transact that a rule satisfi confidence: p(i x) degre of certainti of a detect association, i.e. the probabl that a transact contain x also contain y
dm subject interest subject measur requir a human with domain knowledg to provid measures: • unexpect s contradict apriori belief • action • expect s confirm hypothesi
dm system can be divid into type base on a number of variabl • kind of databas • kind of knowledg • kind of techniqu • target applic
dm task primit dm task primit s the basi for dm queries. dm primit specify: • set of task-relev data to be mine • kind of knowledg to be mine • background knowledg to be use • interesting measur and threshold for pattern evalu • represent for visual discov patterns.
dm queri languag • dm queri languag incorpor primit • allow flexibl interact with dm system • provid foundat for build user-friend gui • example: dmql
dm integr with dbs/ data warehous • no coupl - dms will not util ani db/dw system function • loos coupl - use some db/dw functionality, in particular data fetching/stor • semi-tight coupl - in addit to loos coupl use sorting, indexing, aggregation, histogram analysis, multiway join, and statist primit avail in db/dw system • tight coupl
dirti data incomplet noisi inconsist
caus of incomplet data • "not applicable" data valu when collect • differ consider between the time when the data was collect and when it is analyzed. • human/ hardware/ softwar problem
caus of noisi data (incorrect values) • faulti data collect instrument • human or comput error at data entri • error in data transmiss
caus of inconsist data • differ data sourc • function depend violat (e.g., modifi link data)
import of clean data if you have good data, the rest will follow
data qualiti measur multi-dimension measur of data qualiti • a well-accept multidimension view: • accuraci • complet • consist • timeli • believ • valu ad • interpret • access • broad categories: • intrinsic, contextual, representational, and access
major dm prep task - data clean : fill in miss values, smooth noisi data, identifi or remov outliers, and resolv inconsist - data integr : integr of multipl databases, data cubes, or file - data tran ation : normalis and aggreg - data reduct : obtain reduc represent in volum but produc the same or similar analyt s - data discret : part of data reduct but with particular importance, especi for numer data
noisi data nois is a random error or varianc in a measur variabl
techniqu for cancel out nois 1. bin - first sort data, then distribut over local bin 2. regress - fit a parametr function to the data (e.g. linear or quadrat function) 3. cluster
noisi data - bin cancel nois by binning: - sort data - creat local group of data - replac origin valu by: ______ the bin mean ______ the closest min/max valu of the bin
noisi data - regress cancel nois by regression: 1. fit a parametr function to the data use minim of e.g. least squar error 2. replac origin valu by the parametr function valu
noisi data - cluster cancel nois by cluster - cluster data into n group - replac origin valu by mean of cluster or: - use to detect outlier
data integr • entiti identif problem • redund detect • correl analysi • detect and resolut of data valu conflict • e.g. weight units, in/exclus of tax
data tran ation data tran ation alter origin data to make it more suitabl for data mining. • smooth (nois cancellation) • aggreg • generalis • normalis • attribute/featur construct
itemset simpli a set of item (cf set theory)
mine frequent pattern • one approach to data mine is to find set of item that appear togeth frequently: frequent itemset • to be frequent some minimum threshold of occurr must be exceed • other frequent pattern of interest: ____ frequent sequenti pattern ____ frequent structur pattern
associ rule reflect item that are frequent found (purchased) together, i.e. they are frequent itemset • in ation that custom who buy beer also buy crisp is e.g. encod as: beer ) crisps[support = 2%, confid = 75%]
support and confid are measur of pattern interesting
rule support support(a - b) = p(a u b)
rule confid confidence(a - b) = p (b a)
frequent itemset • absolut support of an itemset is it frequenc count • relat support is the frequenc count of the itemset divid by the total size of the dataset
min-max normal • enabl cost-funct minim techniqu to function properly, take all attribut into equal account • tran s all attribut to lie on the rang [0, 1] or [-1, 1] • linear tran ation of all data
z-score normalis better terminolog is zero-mean normal • min-max normal cannot cope with outliers, z-score normal can. • tran s all attribut to have zero mean and unit standard deviation. • outlier are in the heavy-tail of the gaussian. • still a linear tran ation of all data.
data reduct should remov what unnecessary, yet otherwis maintain the distribut and properti of the origin data • data cube aggreg • attribut subset select (featur selection) • dimension reduct (manifold projection) • numeros reduct • discret
attribut subset select featur select featur select is a of dimension reduct in ml, henc the dm term dimension reduct for manifold project is problematic. approaches: • exact solut infeas • greedi forward select • backward elimin • forward-backward • decis tree induct
numeros reduct reduc the number of instanc rather than attributes. much more dangerous, as it risk chang the data distribut properties. • parametr • discret • sampl
data discret group a possibl infinit space to a discret set of possibl valu for categor data: ________ super-categori for real numbers: ________ bin ________ histogram analysi ________ cluster
instanc reduct reduc the number of instanc rather than attributes. much more dangerous, as it risk chang the data distribut properti • duplic remov • random sampl • cluster sampl • stratifi sampl
complex itemset the general rule procedur for find frequent item set would be: 1. find all frequent itemset 2. generat strong associ rule however, this is terribl costly, with the total number of item set to be check for 100 item be
simpler method for complex itemset close frequent itemset: x is close if there exist no super-set y such that y has the same support count as x maxim frequent itemset: x is frequent, and there exist no superset y of x that are also frequent
apriori algorithm apriori algorithm is a fast way of find frequent itemset
rule base learn equival in express power to tradit (mono-thetic) decis trees, but with more flexibl • they produc rule set as solutions, in the of a set of if... then rule
predic a logic statement, general as boolean logic
ruleset • singl rule are not the solut of the problem, they are member of rule set • rule in a rule set cooper to solv the problem. togeth they should cover the whole search space
evalu rule • a good rule should not make mistak and should cover as mani exampl as possibl complexity: favour rule with simpl predic (occam razor)
evalu ruleset a complet rule set should be good at classifi all the train exampl complexity: favour rule set with the minim number of rules.
learn ruleset learn rule sequentially, one at a time • also known as separate-and-conqu learn all rule togeth • direct rule learn • deriv rule from decis tree
there are three reason to reduc the dimension of a featur set 1. remov featur that have no correl with the target distribut 2. remove/combin featur that have redund correl with target distribut 3. extract new featur with a more direct correl with target distribution.
degre of freedom of variabl number of way data can change/ number of separ tran ation possibl
intrins dimension subspac of data space that captur degre of variabl only, and is thus the most compact possibl represent
featur select featur select return a subset of origin featur set. it doe not extract new features. benefits: • featur retain origin mean • after determin select features, select process is fast disadvantages: • cannot extract new featur which have stronger correl with target variabl
search method • exhaust • greedi forward select • greedi backward elimin • forward-backward approach
filter score • correl • mutual in ation entropi • classif rate • regress score
mutual in ation give a measur of how close two compon of a joint distribut are to be independ
forward select 1. start with empti sf set and candid set be all origin featur 2. find featur with highest filter score 3. remov featur from candid set 4. add featur to sf set 5. repeat step 2-4 until converg
backward elimin 1. start with complet sf set (contain all origin features) 2. find featur that, when removed, reduc the filter score least 3. remov featur from sf set 4. repeat step 2-3 until converg
forward-backward algorithm first appli forward select and then filter redund element use backward elimin
cfs • correl base featur select (cfs) select featur in a forward-select manner. • look at each step at both correl with target variabl and alreadi select features.
pca • manifold project • assum gaussian latent variabl and gaussian observ variabl distribut • linear-gaussian depend of the observ variabl on the latent variabl • also known as karhunen-loèv tran
pca requir calcul of • mean of observ variabl • covari of observ variabl • eigenvalue/eigenvector comput of covari matrix
ann featur select • artifici neural network can implicit per featur select • a multi-lay neural network where the first hidden layer has fewer unit (nodes) than the input layer • call auto-associ network
parametr method • mani method learn paramet of predict function (e.g. linear regression, anns) • after training, train set is discarded. • predict pure base on learn paramet and new data.
memory-bas method • use all train data in everi predict (e.g. knn) • becom a kernel method if use a non-linear exampl comparison/ metric
kernel shortcut that help us do certain calcul faster which otherwis would involv comput in higher dimension space.
kernel method map a non-linear separ input space to anoth space which hope is linear separ • this space is usual higher-dimensional, possibl infinit • the key element is that they do not actual map featur to this space, instead they return the distanc between element in this space • this implicit map is call the (definition) trick
kernel method kernel method map a non-linear separ input space to anoth space which hope is linear separ • this space is usual higher dimensional, possibl infinit • even the non-linear kernel method essenti solv a linear optim problem!!!!
kernel trick the key element of kernel method is that they do not actual map featur to this space, instead they return the distanc between element in this space this implicit map is call the (definition)
spars kernel method • must be evalu on all train exampl dure test • must be evalu on all pair of pattern dure train - train take a long time - test too - memori intens (both disk/ ram) solution: spars method
three way of construct new kernel direct from featur space map propos kernel direct combin of exist (valid) kernel • multipl by a constant • exponenti of a kernel • sum of two kernel • product of two kernel • left/right multipl by ani function of x/x
common use kernel • linear kernel • polynomi kernel • gaussian kernel (gaussian kernel is probabl the most frequent use kernel out there - gaussian kernel map to infinit featur space)
spars kernel method • must be evalu on all train exampl dure test • must be evalu on all pair of pattern dure train • train take a long time • test too • memori intens (both disk/ram) solution: spars method
maximum margin classifi classifi which is abl to give an associ distanc from the decis boundari for each example.
linearly-separ svm satisfi solut (e.g. perceptron algorithm): find a solution, not necessarili the best best is that solut that promis maximum generaliz
slack variabl slack variabl introduc to solv optim problem by allow some train data to be misclassifi slack variabl en = 0 give a linear penalti to exampl lie on the wrong side of the d.b.: point on correct side of db tn ! y(xn) , otherwis
relev vector machin model the typic point of a data set, rather than atypical( a la densiti estimation) while remain a (very) spars (like heat map) represent return a true posterior natur extend to multi-classif fewer paramet to tune
svm margin is defin as the minimum distanc between decis boundari and ani sampl of a class
svms seek a decis boundari that maxim the margin
maximum margin classifi - this turn out to be a solut where decis boundari is determin by nearest point onli - minim set of point span decis boundari sought - these point are call support vector
small set of svs mean that our solut is now spars
non-linear separ problem usual problem aren't linear separ (not even in featur space) perfect separ of train data class would caus poor general due to massiv overfit
soft margin we have effect replac the hard margin with a soft margin new optim goal is maxim the margin while penal point on the wrong side of d.b.
multiclass svm svm is an inher binari classifier. two strategi to use svms for multiclass classification: - one-vs-al - one-vs-on problems: - ambigu (both strategies) - imbalanc train set (one-vs-all)
one-class svm - unsupervis learn problem - similar to probabl densiti estim - instead of a pdf, goal is to find smooth boundari enclos a region of high densiti of a class
prior probabl - probabl of encount a class without observ ani evid - can be general for the state of ani random variabl - easili obtain from train data (i.e. counting)
joint probabl - joint probabl is the probabl of encount a particular class while simultan observ the valu of one or more other random variabl - can be general for the state of ani combin of random variabl
bay theorem posterior = (likelihood x prior)/evid
minimum error rate goal is to minimis error rate
normal densiti by far the most (ab)us densiti function is the normal or gaussian densiti
all probabl theori can be express in term of two rule - product rule - sum rule
direct pgn edg have direct (bayesian network)
undirect pgn no edg direct (markov random field)
direct acycl graph (dags) are bayesian networks. mean there are no cyclic path from ani node back to itself
some variabl are observed, other are hidden/lat exampl observed: label of a train set exampl hidden: learn weight of a model
pgns are generat model allow us to sampl from the probabl distribut it defin
ancestr sampl is a simpl sampl method well suit to pgns
condit independ in pgn is the pgn mechan to show in ation in term of interest aspect of probabl distribut
block path when a path is ed, no in ation can flow through it this mean that observ c, if it s a path a-c-b, it mean there is no ad valu in observ a, and b is fulli determin by c
sequenc data sequenc data is data that come in a particular order opposit of independent, ident distribut (i.i.d.) strong correl between subsequ element - dna - time seri - facial express - speech recognit - weather predict - action plan
1st order markov model restrict to encod sequenti correl on previous element onli
a latice/trelli diagram visual state transit over time also good tool to to visual optim path through state (viterbi algorithm)
emiss probabl probabl of observ variabl
acquir emiss wide rang of option to model ****** probabilities: - discret tabl - gaussian - mixtur of gaussian - neural networks/rvm etc to mode
perceptron algorithm perceptron is model after neuron in the brain. it has m input valu (which correspond with the m featur of the exampl in the train set) and one output value. each input valu x_i is multipli by a weight-factor w_i. if the sum of the product between the featur valu and weight-factor is larger than zero, the perceptron is ate and fire a signal (+1). otherwis it is not ated. the weight sum between the input-valu and the weight-values, can mathemat be determin with the scalar-product . to produc the behaviour of fire a signal (+1) we can use the signum function sgn(); it map the output to +1 if the input is positive, and it map the output to -1 if the input is negative. thus, this perceptron can mathemat be model by the function y = sgn(b+ ). here b is the bias, i.e. the default valu when all featur valu are zero.
