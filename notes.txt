2018_04_16
  MORNING:
    - exported data to csv
    - built initial file structure for problem exploration
    - wrote function to strip html
      - also strips any formulas with <w, x> format
    - wrote function to read csv, apply names
    - wrote function to return dataframe w/o html

  AFTERNOON:
    - collapsed dataframe into single record with both question and answer text
    - Performed topic modeling with LDA, LSI and NMF
      - Discovered that many of my flashcards have error messages due to the export from anki, due to latex not being installed
    - attempting to resolve latex issues with anki to see if i can get a clean import
      - resolved latex issue related to bad data export
    - Currently, there are about 4200 unique words
    - Switched from CountVectorizer to TfidfVectorizer
      - set max document frequency to 80% to drive down commonly occurring words
    - attempting to remove commonly occurring latex words
      - exploring word counts in TfidfVectorizer
      - creating wordcloud function so I can see prominent (and meaningless) words
        - generated wordclouds for raw text and for text after removing html

2018_04_17
  MORNING:
    - consider removing \item, \begin, \end, and other document formatting tags, but not other math-meaningful latex
      - removed item
        -generated wordcloud
      - removed all document latex, chose to leave in latex pertaining to formulas
        - generated wordcloud
    - investigating nan values (110 records)
      - don't want to drop nans as the index to the original cards is meaningful
      - the first nan is a meaningful card that has had its string removed
      - the error is not occurring in the cleaning function
      - it turns out that the answer column has some nan values.
        - correcting this by preprocessing nan values with replacing nan values with ' ' blank space
    - drilling into erroneous words in wordcloud
      - added 'ttt' and 'tt' to latex remove function
        - generated wordcloud
    - shapes are consistent across dataframe, series, and models
    - creating dictionary of latex terms for initially removing all latex, later transforming all latex to meaningful words
      - completed dictionary, updated remove latex function
      - output wordcloud

  AFTERNOON
    - Stemming tf-idf matrix
      - fixed removal of 'tt' string
      - implemented snowball stemmer for tf-idf
    - Reworking LDA to take CountVectorizer instead of TfidfVectorizer, as per reading on the topic

2018_04_18
  MORNING
    - compiling more data
      - biology deck
      - history deck
    - adding function to remove anki closure snippets (c1::, c2::, etc)
    - editing README to reflect discussion with Eliot
    - making graphic to show simple application model
    - EDA on new data
      - getting some word counts for the different documents
        - output image
        - keep in mind this is raw, no stemming, html, etc is still in the data
      - getting Euclidean distances
        - output image
        - this is raw, no stemming, html/latex still in data
      - getting Cosine Similarity
        - output image
        - this is raw, no stemming, html/latex still in data
      - MultiDimensional Scaling
        - output plot for Euclidean distance
        - output plot for cosine similarity
  AFTERNOON
    - Decided to output cleaned data to new text file on every run
      - this mimics eventually having data in postgres
    - EDA on cleaned new data (only on Count vectors, not on tf-idf yet)
      - Euclidean distance output is not interpretable in this space
      - Cosine Similarity increases across the cleaned corpora
        - makes sense since stemming and cleaning data removes many unique words, and, since at this point the 3 corpora are being compared as if they are three documents, cosine similarity would increase
      - Output word maps for each corpora
        - Biology is showing some javascript that will need to be cleaned out
          - indicated this with red circles on wordmap
        - History looks really good, this is probably due to the lack of customizations in the cards themselves.
    - Attempting to use pyLDAvis tool
      - build full corpora
        - output wordmap
          - note there are still some javascript words that need to be cleaned
      - run pyLDAvis within Jupyter notebook on full corpus
        - on the full corpora, the topics separate into 3 distinct categories with a lot of distance
        - generated two html files which I will attempt to embed within the markdown
      - run pyLDAvis on individual 3 separate datasets

THEN: visualize output of models and dimension reduction

Topic Coherence: https://rare-technologies.com/what-is-topic-coherence/
  - https://nbviewer.jupyter.org/github/dsquareindia/gensim/blob/280375fe14adea67ce6384ba7eabf362b05e6029/docs/notebooks/topic_coherence_tutorial.ipynb#topic=0&lambda=1&term=

DECISIONS:
  - although I wanted to implement Word2Vec and do k-means, I decided in the interest of time to topic model using LDA
