2018_04_16
  MORNING:
    - exported data to csv
    - built initial file structure for problem exploration
    - wrote function to strip html
      - also strips any formulas with <w, x> format
    - wrote function to read csv, apply names
    - wrote function to return dataframe w/o html

  AFTERNOON:
    - collapsed dataframe into single record with both question and answer text
    - Performed topic modeling with LDA, LSI and NMF
      - Discovered that many of my flashcards have error messages due to the export from anki, due to latex not being installed
    - attempting to resolve latex issues with anki to see if i can get a clean import
      - resolved latex issue related to bad data export
    - Currently, there are about 4200 unique words
    - Switched from CountVectorizer to TfidfVectorizer
      - set max document frequency to 80% to drive down commonly occurring words
    - attempting to remove commonly occurring latex words
      - exploring word counts in TfidfVectorizer
      - creating wordcloud function so I can see prominent (and meaningless) words
        - generated wordclouds for raw text and for text after removing html

2018_04_17
  MORNING:
    - consider removing \item, \begin, \end, and other document formatting tags, but not other math-meaningful latex
      - removed item
        -generated wordcloud
      - removed all document latex, chose to leave in latex pertaining to formulas
        - generated wordcloud
    - investigating nan values (110 records)
      - don't want to drop nans as the index to the original cards is meaningful
      - the first nan is a meaningful card that has had its string removed
      - the error is not occurring in the cleaning function
      - it turns out that the answer column has some nan values.
        - correcting this by preprocessing nan values with replacing nan values with ' ' blank space
    - drilling into erroneous words in wordcloud
      - added 'ttt' and 'tt' to latex remove function
        - generated wordcloud
    - shapes are consistent across dataframe, series, and models
    - creating dictionary of latex terms for initially removing all latex, later transforming all latex to meaningful words
      - completed dictionary, updated remove latex function
      - output wordcloud

  AFTERNOON
    - Stemming tf-idf matrix
      - fixed removal of 'tt' string
      - implemented snowball stemmer for tf-idf
    - Reworking LDA to take CountVectorizer instead of TfidfVectorizer, as per
      reading


NEXT: most frequent word stems

NEXT: make pie charts for some of the rows for a given model

NEXT: implement Word2Vec and run same models

THEN: visualize output of models and dimension reduction
